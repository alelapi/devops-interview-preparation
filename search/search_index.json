{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"DevOps Interview Preparation","text":"<p>This repository contains a collection of comprehensive answers to commonly asked questions in DevOps interviews, focusing on areas like CI/CD, Docker, Kubernetes, Monitoring, and Infrastructure as Code. Each section is tailored to provide detailed insights and practical examples to help you prepare for your interview effectively.</p>"},{"location":"architectural-patterns/bulkhead/","title":"Bulkhead Pattern","text":""},{"location":"architectural-patterns/bulkhead/#description","title":"Description","text":"<p>The Bulkhead Pattern is a resilience pattern that isolates resources (e.g., threads, connections) to prevent failures in one part of a system from affecting the entire application. It\u2019s inspired by bulkheads in ships that compartmentalize sections to prevent flooding.</p>"},{"location":"architectural-patterns/bulkhead/#how-it-works","title":"How It Works","text":"<ul> <li>Separate workloads into isolated compartments (e.g., thread pools, services).</li> <li>If one compartment fails, the others remain unaffected.</li> </ul>"},{"location":"architectural-patterns/bulkhead/#advantages","title":"Advantages","text":"<ul> <li>Fault Isolation: Limits the impact of failures.</li> <li>Improved Stability: Protects critical components from being overwhelmed.</li> </ul>"},{"location":"architectural-patterns/bulkhead/#challenges","title":"Challenges","text":"<ul> <li>Resource Management: Properly partitioning resources can be complex.</li> <li>Overhead: Requires additional infrastructure for isolation.</li> </ul>"},{"location":"architectural-patterns/bulkhead/#example-use-case","title":"Example Use Case","text":"<p>In a travel booking system:</p> <ul> <li>Payment, inventory, and notification services each have dedicated thread pools.</li> <li>A failure in the notification service does not impact payments or inventory.</li> </ul>"},{"location":"architectural-patterns/circuit_breaker/","title":"Circuit Breaker Pattern","text":""},{"location":"architectural-patterns/circuit_breaker/#description","title":"Description","text":"<p>The Circuit Breaker Pattern is a resilience design pattern that prevents a system from performing operations likely to fail. It works similarly to an electrical circuit breaker: when failures reach a certain threshold, the circuit \u201copens\u201d and subsequent requests are rejected, allowing the system to recover gracefully.</p>"},{"location":"architectural-patterns/circuit_breaker/#how-it-works","title":"How It Works","text":"<ol> <li>Closed State: The system operates normally, and requests are passed through.</li> <li>Open State: After detecting repeated failures, the circuit breaker trips, and further requests are rejected.</li> <li>Half-Open State: The system periodically allows some requests to test if the service has recovered.</li> </ol>"},{"location":"architectural-patterns/circuit_breaker/#advantages","title":"Advantages","text":"<ul> <li>Failure Isolation: Prevents cascading failures.</li> <li>Improved Stability: Allows the failing component to recover without overwhelming it.</li> </ul>"},{"location":"architectural-patterns/circuit_breaker/#challenges","title":"Challenges","text":"<ul> <li>Configuration: Setting thresholds and timeout values requires careful tuning.</li> <li>Fallback Logic: Needs proper implementation for degraded behavior.</li> </ul>"},{"location":"architectural-patterns/circuit_breaker/#example-use-case","title":"Example Use Case","text":"<ul> <li>A microservice calls an external payment gateway.</li> <li>If the payment gateway fails repeatedly, the circuit breaker opens and stops retrying requests, preventing system overload.</li> </ul>"},{"location":"architectural-patterns/event_driven/","title":"Event-Driven Architecture","text":""},{"location":"architectural-patterns/event_driven/#description","title":"Description","text":"<p>Event-Driven Architecture (EDA) is a design paradigm in which systems communicate and interact through the production, detection, and consumption of events. Events are messages that signify a change in state or an occurrence within a system. This architecture promotes loose coupling, scalability, and responsiveness, making it suitable for distributed systems and microservices.</p>"},{"location":"architectural-patterns/event_driven/#how-it-works","title":"How It Works","text":"<ol> <li>Event Producers: Generate events whenever a change or action occurs (e.g., order creation).</li> <li>Event Bus/Message Broker: Events are published to an intermediary, such as Kafka, RabbitMQ, or AWS SNS/SQS.</li> <li>Event Consumers: Services or components subscribe to and consume relevant events, acting upon them.</li> </ol>"},{"location":"architectural-patterns/event_driven/#advantages","title":"Advantages","text":"<ul> <li>Loose Coupling: Producers and consumers are decoupled.</li> <li>Scalability: Event-driven systems scale well to handle increasing event loads.</li> <li>Real-Time Processing: Enables immediate response to events.</li> </ul>"},{"location":"architectural-patterns/event_driven/#challenges","title":"Challenges","text":"<ul> <li>Complexity: Requires careful orchestration and monitoring.</li> <li>Debugging: Troubleshooting distributed events can be difficult.</li> </ul>"},{"location":"architectural-patterns/event_driven/#example-use-case","title":"Example Use Case","text":"<p>An e-commerce system:</p> <ul> <li>Producer: \u201cOrder Placed\u201d event is published.</li> <li>Consumers: Inventory service reserves stock; Payment service processes payment; Notification service sends confirmation.</li> </ul>"},{"location":"architectural-patterns/event_sourcing/","title":"Event Sourcing Pattern","text":""},{"location":"architectural-patterns/event_sourcing/#description","title":"Description","text":"<p>Event Sourcing is a design pattern where state changes in a system are stored as a series of immutable events. The current state is derived by replaying these events in sequence, rather than storing the state directly.</p>"},{"location":"architectural-patterns/event_sourcing/#how-it-works","title":"How It Works","text":"<ol> <li>Events are generated for each state change (e.g., \u201cOrder Placed\u201d).</li> <li>Events are persisted to an event store.</li> <li>The system reconstructs state by replaying events from the store.</li> </ol>"},{"location":"architectural-patterns/event_sourcing/#advantages","title":"Advantages","text":"<ul> <li>Auditability: Provides a complete history of state changes.</li> <li>Scalability: Efficiently handles high volumes of write operations.</li> </ul>"},{"location":"architectural-patterns/event_sourcing/#challenges","title":"Challenges","text":"<ul> <li>Complexity: Rebuilding state and handling event evolution can be tricky.</li> <li>Storage: Large event histories may require significant storage.</li> </ul>"},{"location":"architectural-patterns/event_sourcing/#example-use-case","title":"Example Use Case","text":"<p>A banking application:</p> <ul> <li>Events: \u201cAccount Created\u201d, \u201cDeposit Made\u201d, \u201cWithdrawal Made\u201d.</li> <li>State reconstruction: Replaying events rebuilds the account balance.</li> </ul>"},{"location":"architectural-patterns/saga_pattern/","title":"Saga Pattern","text":""},{"location":"architectural-patterns/saga_pattern/#description","title":"Description","text":"<p>The Saga Pattern is a design pattern used to manage long-running business transactions in a distributed system. It breaks the transaction into a series of smaller steps, with each step executed by a service. If a failure occurs, compensating actions are triggered to roll back previous steps.</p>"},{"location":"architectural-patterns/saga_pattern/#how-it-works","title":"How It Works","text":"<ol> <li>Choreography: Services communicate through events to execute the steps.</li> <li>Orchestration: A central controller coordinates and manages the saga\u2019s steps.</li> </ol>"},{"location":"architectural-patterns/saga_pattern/#advantages","title":"Advantages","text":"<ul> <li>Data Consistency: Ensures eventual consistency in distributed systems.</li> <li>Fault Tolerance: Compensating actions handle failures gracefully.</li> </ul>"},{"location":"architectural-patterns/saga_pattern/#challenges","title":"Challenges","text":"<ul> <li>Complexity: Implementing compensating actions and managing workflows is challenging.</li> <li>Debugging: Harder to trace failures in large-scale sagas.</li> </ul>"},{"location":"architectural-patterns/saga_pattern/#example-use-case","title":"Example Use Case","text":"<p>An e-commerce order process:</p> <ol> <li>Order service places an order.</li> <li>Inventory service reserves stock.</li> <li>Payment service processes payment.</li> <li>If payment fails, the compensating action releases the reserved stock.</li> </ol>"},{"location":"architectural-patterns/strangler/","title":"Strangler Pattern","text":""},{"location":"architectural-patterns/strangler/#description","title":"Description","text":"<p>The Strangler Pattern is a modernization strategy used to incrementally migrate functionality from a legacy system to a new system. The new system is developed alongside the old one, gradually \u201cstrangling\u201d and replacing parts of the legacy system until it can be retired.</p>"},{"location":"architectural-patterns/strangler/#how-it-works","title":"How It Works","text":"<ol> <li>A proxy or routing layer intercepts requests.</li> <li>New functionality is implemented in the new system.</li> <li>Requests for migrated features are routed to the new system.</li> <li>The legacy system is gradually decommissioned.</li> </ol>"},{"location":"architectural-patterns/strangler/#advantages","title":"Advantages","text":"<ul> <li>Low Risk: Incremental migration reduces the risk of failures.</li> <li>Continuous Delivery: New features can be delivered iteratively.</li> </ul>"},{"location":"architectural-patterns/strangler/#challenges","title":"Challenges","text":"<ul> <li>Complexity: Requires careful planning and routing of requests.</li> <li>Dual Systems: Maintaining both systems during migration increases operational overhead.</li> </ul>"},{"location":"architectural-patterns/strangler/#example-use-case","title":"Example Use Case","text":"<ul> <li>An e-commerce platform migrates its monolithic inventory management system to microservices. A routing layer gradually shifts inventory queries to the new microservices while the old system remains operational.</li> </ul>"},{"location":"aws/appsync/","title":"AppSync","text":""},{"location":"aws/appsync/#overview","title":"Overview","text":"<p>AWS AppSync is a fully managed service that enables developers to create scalable GraphQL APIs. At its core, AppSync leverages GraphQL\u2019s powerful query language to simplify how applications retrieve and manipulate data. This approach ensures applications can efficiently obtain exactly the data they need through a single endpoint, reducing unnecessary data transfer and improving performance.</p> <p>The service excels at data integration, seamlessly combining information from multiple sources into a unified API. AppSync natively supports various AWS services including DynamoDB for NoSQL storage, Aurora for relational databases, and OpenSearch for search capabilities. When developers need to integrate with custom data sources or implement complex business logic, AWS Lambda functions can be utilized to extend AppSync\u2019s functionality.</p> <p>Real-time data capabilities are built into AppSync through WebSocket connections or MQTT over WebSocket protocols. This enables applications to receive instant updates when data changes, making it ideal for building responsive, real-time applications.</p> <p>Mobile application developers particularly benefit from AppSync\u2019s features. The service provides robust support for local data access and synchronization, enabling offline functionality and ensuring data consistency across devices. This is crucial for maintaining a smooth user experience in mobile applications where network connectivity may be intermittent.</p> <p>The foundation of any AppSync implementation begins with a GraphQL schema. This schema defines the API\u2019s type system, describing the data structure and relationships between different types. By uploading a schema, developers establish the contract between their API and its clients, making it clear what data can be queried and how it can be manipulated.</p>"},{"location":"aws/appsync/#security","title":"Security","text":"<p>AppSync implements a comprehensive security model with multiple authorization mechanisms to protect your GraphQL APIs:</p> <p>API Key Authentication provides a straightforward way to secure your API during development or for simple use cases. While simple to implement, it offers basic security suitable for testing or public read-only APIs.</p> <p>AWS IAM Authentication enables fine-grained access control using AWS Identity and Access Management. This method is particularly powerful for applications running within the AWS ecosystem, supporting IAM users, roles, and even cross-account access patterns. It integrates seamlessly with other AWS services and provides detailed audit logging through AWS CloudTrail.</p> <p>OpenID Connect Authentication supports integration with third-party identity providers through JSON Web Tokens (JWT). This enables organizations to leverage their existing identity infrastructure while maintaining secure access to their GraphQL APIs.</p> <p>Amazon Cognito User Pools Authentication provides a complete identity management solution. It handles user registration, authentication, and account recovery while seamlessly integrating with AppSync. This option is particularly valuable for mobile and web applications requiring user authentication.</p> <p>For production deployments requiring custom domains and HTTPS support, AWS CloudFront can be placed in front of AppSync. This not only provides additional security through SSL/TLS encryption but also enables content caching and improved global access through CloudFront\u2019s edge locations.</p>"},{"location":"aws/cdk/","title":"Cloud Development Kit (CDK)","text":""},{"location":"aws/cdk/#introduction","title":"Introduction","text":"<p>The AWS Cloud Development Kit (CDK) is an open-source software development framework for defining cloud infrastructure as code using familiar programming languages. Instead of writing JSON or YAML templates (like in CloudFormation), you can use programming languages like TypeScript, Python, Java, C#, or Go to define your AWS infrastructure.</p>"},{"location":"aws/cdk/#key-concepts","title":"Key Concepts","text":""},{"location":"aws/cdk/#constructs","title":"Constructs","text":"<p>Constructs are the basic building blocks of CDK applications. They represent AWS resources or combinations of resources. There are three levels of constructs:</p> <ol> <li>Level 1 (L1) - Low-level constructs that directly represent AWS CloudFormation resources</li> <li>Level 2 (L2) - Higher-level constructs that provide defaults and best practices</li> <li>Level 3 (L3) - Pattern constructs that represent multi-resource patterns for common architectures</li> </ol>"},{"location":"aws/cdk/#stacks","title":"Stacks","text":"<p>Stacks are the unit of deployment in CDK. They contain constructs and map directly to CloudFormation stacks. Stacks handle: - Resource grouping - Deployment boundaries - Permission boundaries - Resource naming</p>"},{"location":"aws/cdk/#apps","title":"Apps","text":"<p>An App is the root construct that contains one or more stacks. It serves as the entry point of your CDK application and handles: - Stack synthesis - Asset bundling - Deployment orchestration</p>"},{"location":"aws/cdk/#getting-started","title":"Getting Started","text":""},{"location":"aws/cdk/#prerequisites","title":"Prerequisites","text":"<ul> <li>Node.js (&gt;= 10.13.0)</li> <li>AWS CLI configured with appropriate credentials</li> <li>IDE or text editor of choice</li> </ul>"},{"location":"aws/cdk/#installation","title":"Installation","text":"<pre><code># Install CDK CLI globally\nnpm install -g aws-cdk\n\n# Verify installation\ncdk --version\n</code></pre>"},{"location":"aws/cdk/#project-initialization","title":"Project Initialization","text":"<pre><code># Create a new CDK project\nmkdir my-cdk-app\ncd my-cdk-app\ncdk init app --language typescript\n</code></pre>"},{"location":"aws/cdk/#basic-example","title":"Basic Example","text":"<p>Here\u2019s a simple example of creating an S3 bucket using CDK in TypeScript:</p> <pre><code>import * as cdk from 'aws-cdk-lib';\nimport * as s3 from 'aws-cdk-lib/aws-s3';\n\nexport class MyS3Stack extends cdk.Stack {\n  constructor(scope: cdk.App, id: string, props?: cdk.StackProps) {\n    super(scope, id, props);\n\n    new s3.Bucket(this, 'MyFirstBucket', {\n      versioned: true,\n      encryption: s3.BucketEncryption.S3_MANAGED,\n      removalPolicy: cdk.RemovalPolicy.DESTROY\n    });\n  }\n}\n</code></pre>"},{"location":"aws/cdk/#common-commands","title":"Common Commands","text":"<ul> <li><code>cdk init</code> - Create a new CDK project</li> <li><code>cdk synth</code> - Synthesize CloudFormation template</li> <li><code>cdk diff</code> - Compare deployed stack with current state</li> <li><code>cdk deploy</code> - Deploy the stack to AWS</li> <li><code>cdk destroy</code> - Destroy the stack</li> </ul>"},{"location":"aws/cdk/#best-practices","title":"Best Practices","text":""},{"location":"aws/cdk/#code-organization","title":"Code Organization","text":"<ul> <li>Keep stacks focused and single-purpose</li> <li>Use constructs to encapsulate reusable components</li> <li>Leverage environment-specific configuration</li> <li>Follow programming language best practices</li> </ul>"},{"location":"aws/cdk/#security","title":"Security","text":"<ul> <li>Use IAM roles with least privilege</li> <li>Enable encryption by default</li> <li>Implement security groups properly</li> <li>Use VPC endpoints where appropriate</li> </ul>"},{"location":"aws/cdk/#cost-management","title":"Cost Management","text":"<ul> <li>Use Tags for cost allocation</li> <li>Implement lifecycle rules for storage</li> <li>Consider reserved instances for stable workloads</li> <li>Monitor usage with AWS Cost Explorer</li> </ul>"},{"location":"aws/cdk/#advanced-features","title":"Advanced Features","text":""},{"location":"aws/cdk/#asset-bundling","title":"Asset Bundling","text":"<p>CDK can bundle assets (like Lambda functions or Docker images) during deployment: <pre><code>new lambda.Function(this, 'MyFunction', {\n  runtime: lambda.Runtime.NODEJS_14_X,\n  code: lambda.Code.fromAsset('lambda'),\n  handler: 'index.handler'\n});\n</code></pre></p>"},{"location":"aws/cdk/#custom-constructs","title":"Custom Constructs","text":"<p>Create reusable infrastructure patterns: <pre><code>export class CustomVpc extends Construct {\n  constructor(scope: Construct, id: string, props?: CustomVpcProps) {\n    super(scope, id);\n    // Implementation\n  }\n}\n</code></pre></p>"},{"location":"aws/cdk/#context-and-environment","title":"Context and Environment","text":"<p>Handle environment-specific configurations: <pre><code>const environmentName = this.node.tryGetContext('environment');\n</code></pre></p>"},{"location":"aws/cdk/#workflow","title":"Workflow","text":"<p>The standard AWS CDK development workflow is similar to the workflow you\u2019re already familiar as a developer. There are a few extra steps:</p> <ol> <li> <p>Create the app from a template provided by AWS CDK - Each AWS CDK app should be in its own directory, with its own local module dependencies. Create a new directory for your app. Now initialize the app using the <code>cdk init</code> command, specifying the desired template (\u201capp\u201d) and programming language. The <code>cdk init</code> command creates a number of files and folders inside the created home directory to help you organize the source code for your AWS CDK app.</p> </li> <li> <p>Add code to the app to create resources within stacks - Add custom code as is needed for your application.</p> </li> <li> <p>Build the app (optional) - In most programming environments, after making changes to your code, you\u2019d build (compile) it. This isn\u2019t strictly necessary with the AWS CDK\u2014the Toolkit does it for you so you can\u2019t forget. But you can still build manually whenever you want to catch syntax and type errors.</p> </li> <li> <p>Synthesize one or more stacks in the app to create an AWS CloudFormation template - Synthesize one or more stacks in the app to create an AWS CloudFormation template. The synthesis step catches logical errors in defining your AWS resources. If your app contains more than one stack, you\u2019d need to specify which stack(s) to synthesize.</p> </li> <li> <p>Deploy one or more stacks to your AWS account - It is optional (though good practice) to synthesize before deploying. The AWS CDK synthesizes your stack before each deployment. If your code has security implications, you\u2019ll see a summary of these and need to confirm them before deployment proceeds. <code>cdk deploy</code> is used to deploy the stack using CloudFormation templates. This command displays progress information as your stack is deployed. When it\u2019s done, the command prompt reappears.</p> </li> </ol>"},{"location":"aws/cdk/#troubleshooting","title":"Troubleshooting","text":"<p>Common issues and solutions:</p> <ol> <li>Deployment Failures</li> <li>Check CloudFormation console for detailed error messages</li> <li>Verify IAM permissions</li> <li> <p>Review resource limits</p> </li> <li> <p>Synthesis Issues</p> </li> <li>Validate TypeScript/programming language syntax</li> <li>Check for circular dependencies</li> <li> <p>Verify construct properties</p> </li> <li> <p>Runtime Errors</p> </li> <li>Review CloudWatch logs</li> <li>Check resource configurations</li> <li>Verify network connectivity</li> </ol>"},{"location":"aws/cdk/#resources","title":"Resources","text":"<ul> <li>Official AWS CDK Documentation</li> <li>AWS CDK API Reference</li> <li>AWS CDK Workshop</li> <li>AWS CDK GitHub Repository</li> </ul>"},{"location":"aws/cdk/#contributing","title":"Contributing","text":"<p>The AWS CDK is open source and welcomes contributions. You can: - Report bugs - Submit feature requests - Create pull requests - Share construct libraries</p>"},{"location":"aws/cdk/#conclusion","title":"Conclusion","text":"<p>The AWS CDK represents a significant evolution in infrastructure as code, enabling developers to use familiar programming languages and concepts to define cloud infrastructure. Its combination of high-level abstractions and fine-grained control makes it a powerful tool for modern cloud development.</p>"},{"location":"aws/cloudhsm/","title":"CloudHSM (Cloud Hardware Security Module)","text":""},{"location":"aws/cloudhsm/#overview","title":"Overview","text":"<p>AWS CloudHSM is a cloud-based hardware security module (HSM) that enables customers to generate and use their own encryption keys on the AWS Cloud with full control and ownership.</p>"},{"location":"aws/cloudhsm/#key-features","title":"Key Features","text":"<ul> <li>Dedicated hardware security modules</li> <li>Fully managed, single-tenant HSM instances</li> <li>Supports industry-standard encryption standards</li> <li>Compliant with various security regulations (FIPS 140-2 Level 3)</li> </ul>"},{"location":"aws/cloudhsm/#primary-use-cases","title":"Primary Use Cases","text":"<ul> <li>Cryptographic key management</li> <li>Secure key storage</li> <li>Encryption and decryption operations</li> <li>Public Key Infrastructure (PKI)</li> <li>Regulatory compliance requirements</li> </ul>"},{"location":"aws/cloudhsm/#security-capabilities","title":"Security Capabilities","text":"<ul> <li>Generate and protect cryptographic keys</li> <li>Perform cryptographic operations</li> <li>Secure key lifecycle management</li> <li>Isolation from other AWS customers</li> </ul>"},{"location":"aws/cloudhsm/#supported-standards","title":"Supported Standards","text":"<ul> <li>PKCS#11</li> <li>OpenSSL</li> <li>Microsoft CryptoNG (CNG)</li> <li>Java Cryptography Extensions (JCE)</li> </ul>"},{"location":"aws/cloudhsm/#benefits","title":"Benefits","text":"<ul> <li>Enhanced security through hardware-based key protection</li> <li>Meets strict compliance and regulatory requirements</li> <li>Scalable encryption infrastructure</li> <li>Reduced operational complexity</li> </ul>"},{"location":"aws/codestar/","title":"CodeStar","text":""},{"location":"aws/codestar/#overview","title":"Overview","text":"<p>AWS CodeStar is a cloud-based development service that simplifies the process of developing, building, and deploying applications on Amazon Web Services (AWS).</p>"},{"location":"aws/codestar/#key-features","title":"Key Features","text":"<ul> <li>Unified Project Management: Provides a single interface to manage software development projects</li> <li>Quick Project Setup: Offers pre-configured project templates for multiple programming languages</li> <li>Integrated Tools: Seamlessly connects with AWS development and deployment services</li> </ul>"},{"location":"aws/codestar/#core-services-integration","title":"Core Services Integration","text":"<ul> <li>AWS CodeCommit (Source Control)</li> <li>AWS CodeBuild (Continuous Integration)</li> <li>AWS CodePipeline (Continuous Delivery)</li> <li>AWS CodeDeploy (Application Deployment)</li> </ul>"},{"location":"aws/codestar/#benefits","title":"Benefits","text":"<ul> <li>Accelerates software development workflow</li> <li>Reduces complexity of managing multiple AWS services</li> <li>Enables faster project initialization</li> <li>Provides consistent development environment</li> </ul>"},{"location":"aws/codestar/#supported-languages","title":"Supported Languages","text":"<ul> <li>Python</li> <li>Java</li> <li>Node.js</li> <li>.NET</li> <li>JavaScript</li> <li>PHP</li> </ul>"},{"location":"aws/codestar/#use-cases","title":"Use Cases","text":"<ul> <li>Web Applications</li> <li>Microservices</li> <li>Backend Services</li> <li>Cloud-native Applications</li> </ul>"},{"location":"aws/sar/","title":"Serverless Application Repository (SAR)","text":""},{"location":"aws/sar/#overview","title":"Overview","text":"<p>AWS Serverless Application Repository is a managed service that enables developers to find, deploy, and publish serverless applications quickly and easily.</p>"},{"location":"aws/sar/#key-features","title":"Key Features","text":""},{"location":"aws/sar/#application-discovery","title":"Application Discovery","text":"<ul> <li>Centralized repository of pre-built serverless applications</li> <li>Discover and use applications from:</li> <li>AWS-published applications</li> <li>Community-contributed applications</li> <li>Private organizational applications</li> </ul>"},{"location":"aws/sar/#deployment-capabilities","title":"Deployment Capabilities","text":"<ul> <li>One-click deployment of serverless applications</li> <li>Instant integration with AWS Lambda, API Gateway, and other serverless services</li> <li>Simplified infrastructure setup through CloudFormation templates</li> </ul>"},{"location":"aws/sar/#publishing-options","title":"Publishing Options","text":"<ul> <li>Public sharing of serverless applications</li> <li>Private sharing within organizations</li> <li>Ability to publish your own reusable serverless components</li> </ul>"},{"location":"aws/sar/#use-cases","title":"Use Cases","text":"<ul> <li>Rapid prototyping</li> <li>Accelerating development</li> <li>Sharing common serverless patterns</li> <li>Learning serverless architecture best practices</li> </ul>"},{"location":"aws/sar/#supported-application-types","title":"Supported Application Types","text":"<ul> <li>AWS Lambda functions</li> <li>API Gateway configurations</li> <li>Serverless workflows</li> <li>Event-driven applications</li> <li>Microservices components</li> </ul>"},{"location":"aws/sar/#benefits","title":"Benefits","text":"<ul> <li>Reduces time-to-market for serverless applications</li> <li>Promotes code reuse</li> <li>Ensures consistent deployment patterns</li> <li>Simplifies serverless application management</li> <li>Facilitates knowledge sharing across development teams</li> </ul>"},{"location":"aws/sar/#security-and-governance","title":"Security and Governance","text":"<ul> <li>Configurable visibility settings</li> <li>Version control for applications</li> <li>Integration with AWS Identity and Access Management (IAM)</li> <li>Compliance with organizational policies</li> </ul>"},{"location":"aws/step_function/","title":"Step Functions","text":"<p>AWS Step Functions is a serverless orchestration service that allows you to coordinate multiple AWS services into scalable workflows. It provides a visual interface for designing and executing workflows, making it easier to build and maintain applications.</p>"},{"location":"aws/step_function/#key-features","title":"Key Features","text":""},{"location":"aws/step_function/#1-visual-workflow-design","title":"1. Visual Workflow Design","text":"<ul> <li>Provides a graphical user interface to design and visualize workflows.</li> <li>Workflows are defined using Amazon States Language (ASL), a JSON-based language.</li> </ul>"},{"location":"aws/step_function/#2-service-orchestration","title":"2. Service Orchestration","text":"<ul> <li>Integrates seamlessly with AWS services like Lambda, DynamoDB, S3, ECS, SNS, and more.</li> <li>Supports both standard and express workflows, allowing flexibility based on use case.</li> </ul>"},{"location":"aws/step_function/#3-error-handling-and-retry","title":"3. Error Handling and Retry","text":"<ul> <li>Built-in error handling and retry logic to manage failures gracefully.</li> <li>Allows branching and fallback strategies to handle errors.</li> </ul>"},{"location":"aws/step_function/#4-step-execution-monitoring","title":"4. Step Execution Monitoring","text":"<ul> <li>Provides detailed logs and metrics through Amazon CloudWatch.</li> <li>Supports step-by-step debugging and monitoring.</li> </ul>"},{"location":"aws/step_function/#5-serverless-and-fully-managed","title":"5. Serverless and Fully Managed","text":"<ul> <li>No need to manage infrastructure or scaling.</li> <li>Automatically scales based on workflow execution demand.</li> </ul>"},{"location":"aws/step_function/#workflow-types","title":"Workflow Types","text":""},{"location":"aws/step_function/#1-standard-workflows","title":"1. Standard Workflows","text":"<ul> <li>Designed for long-running, durable workflows.</li> <li>Features: </li> <li>Execution duration up to 1 year.</li> <li>Exactly-once execution semantics.</li> <li>High durability and resilience.</li> </ul>"},{"location":"aws/step_function/#2-express-workflows","title":"2. Express Workflows","text":"<ul> <li>Optimized for high-volume, short-duration workflows.</li> <li>Features:</li> <li>Execution duration up to 5 minutes.</li> <li>At-least-once execution semantics.</li> <li>Lower cost, designed for high-throughput applications.</li> </ul>"},{"location":"aws/step_function/#task-types","title":"Task Types","text":"<p>AWS Step Functions supports various task types that allow you to perform different operations in a workflow. Below are the key task types:</p>"},{"location":"aws/step_function/#1-task","title":"1. Task","text":"<ul> <li>Executes a unit of work, such as invoking an AWS Lambda function or running a job on AWS Batch.</li> <li>Defined using the <code>Resource</code> field to specify the AWS service or API action.</li> </ul>"},{"location":"aws/step_function/#2-parallel","title":"2. Parallel","text":"<ul> <li>Executes multiple branches of a workflow simultaneously.</li> <li>Useful for scenarios requiring parallel processing.</li> </ul>"},{"location":"aws/step_function/#3-map","title":"3. Map","text":"<ul> <li>Processes a collection of items iteratively.</li> <li>Similar to a \u201cfor loop\u201d and can run iterations in parallel.</li> </ul>"},{"location":"aws/step_function/#4-choice","title":"4. Choice","text":"<ul> <li>Adds conditional logic to workflows.</li> <li>Routes execution based on the evaluation of input data.</li> </ul>"},{"location":"aws/step_function/#5-wait","title":"5. Wait","text":"<ul> <li>Delays execution for a specified time or until a specific timestamp.</li> </ul>"},{"location":"aws/step_function/#6-pass","title":"6. Pass","text":"<ul> <li>Passes input to the next state without performing any work.</li> <li>Useful for testing and placeholder states.</li> </ul>"},{"location":"aws/step_function/#7-succeed","title":"7. Succeed","text":"<ul> <li>Marks the workflow as successfully completed.</li> </ul>"},{"location":"aws/step_function/#8-fail","title":"8. Fail","text":"<ul> <li>Stops the workflow and marks it as failed.</li> <li>Can include error details for debugging.</li> </ul>"},{"location":"aws/step_function/#9-activity","title":"9. Activity","text":"<ul> <li>Represents a task performed by a worker program outside of Step Functions.</li> <li>Requires integration with an external worker.</li> </ul>"},{"location":"aws/step_function/#10-service-integration","title":"10. Service Integration","text":"<ul> <li>Directly integrates with AWS services such as S3, DynamoDB, ECS, and more, without needing AWS Lambda.</li> </ul>"},{"location":"aws/step_function/#common-use-cases","title":"Common Use Cases","text":"<ol> <li>Data Processing Pipelines</li> <li> <p>Automate ETL jobs, orchestrate machine learning model training, or coordinate big data workflows.</p> </li> <li> <p>Microservices Orchestration</p> </li> <li> <p>Coordinate interactions between microservices in distributed systems.</p> </li> <li> <p>IoT Applications</p> </li> <li> <p>Process and analyze data from IoT devices, triggering workflows based on incoming data.</p> </li> <li> <p>Application Backends</p> </li> <li> <p>Automate approval workflows, payment processing, or user registration flows.</p> </li> <li> <p>Disaster Recovery</p> </li> <li>Implement failover mechanisms and ensure system resilience.</li> </ol>"},{"location":"aws/step_function/#amazon-states-language-asl","title":"Amazon States Language (ASL)","text":""},{"location":"aws/step_function/#key-elements-of-asl","title":"Key Elements of ASL","text":"<ol> <li>States</li> <li> <p>Define individual steps in the workflow, such as tasks, choices, or parallel steps.</p> </li> <li> <p>Transitions</p> </li> <li> <p>Specify how the workflow progresses from one state to another.</p> </li> <li> <p>Error Handling</p> </li> <li>Define retry policies, catch blocks, and fallback mechanisms for errors.</li> </ol>"},{"location":"aws/step_function/#example-state-machine-definition","title":"Example State Machine Definition","text":"<pre><code>{\n  \"Comment\": \"An example of a simple Step Function workflow\",\n  \"StartAt\": \"HelloWorld\",\n  \"States\": {\n    \"HelloWorld\": {\n      \"Type\": \"Task\",\n      \"Resource\": \"arn:aws:lambda:us-east-1:123456789012:function:HelloWorldFunction\",\n      \"End\": true\n    }\n  }\n}\n</code></pre>"},{"location":"aws/step_function/#integration-with-aws-services","title":"Integration with AWS Services","text":"<ul> <li>AWS Lambda: Execute serverless functions for tasks within workflows.</li> <li>Amazon DynamoDB: Store and retrieve data as part of the workflow.</li> <li>Amazon S3: Trigger workflows based on events like object creation.</li> <li>Amazon ECS: Manage containerized tasks and services.</li> <li>SNS/SQS: Send notifications or queue messages during workflows.</li> </ul>"},{"location":"aws/step_function/#benefits","title":"Benefits","text":"<ol> <li>Improved Resilience: Built-in error handling and retries ensure reliable execution.</li> <li>Faster Development: Visual interface and easy integration with AWS services reduce development effort.</li> <li>Cost-Effective: Pay-as-you-go pricing with no upfront costs.</li> <li>Scalable and Secure: Automatically scales with demand and integrates with IAM for fine-grained access control.</li> </ol>"},{"location":"aws/step_function/#pricing","title":"Pricing","text":""},{"location":"aws/step_function/#standard-workflows","title":"Standard Workflows:","text":"<ul> <li>Charged per state transition: $0.025 per 1,000 transitions.</li> </ul>"},{"location":"aws/step_function/#express-workflows","title":"Express Workflows:","text":"<ul> <li>Charged based on requests and runtime.</li> <li>Example: $1.00 per 1 million requests and $0.00000456 per GB-second runtime.</li> </ul> <p>Refer to the AWS Step Functions Pricing Page for detailed information.</p>"},{"location":"aws/step_function/#best-practices","title":"Best Practices","text":"<ol> <li>Optimize State Transitions: Minimize the number of state transitions to reduce costs.</li> <li>Use Express Workflows for High-Volume Tasks: Choose express workflows for short-duration, high-volume tasks.</li> <li>Implement Robust Error Handling: Define retries, catch blocks, and fallback states.</li> <li>Monitor and Debug: Use CloudWatch logs and metrics for monitoring and troubleshooting.</li> <li>Test Thoroughly: Use the AWS Step Functions console to simulate and test workflows before production.</li> </ol> <p>For more details, refer to the AWS Step Functions Documentation.</p>"},{"location":"aws/sts/","title":"Security Token Service (STS)","text":""},{"location":"aws/sts/#core-purpose","title":"Core Purpose","text":"<p>AWS Security Token Service enables organizations to grant limited and temporary access to AWS resources, with credential validity up to one hour.</p>"},{"location":"aws/sts/#credential-generation-methods","title":"Credential Generation Methods","text":"<p>STS provides multiple mechanisms for obtaining temporary credentials:</p>"},{"location":"aws/sts/#role-assumption-methods","title":"Role Assumption Methods","text":"<ul> <li>AssumeRole: Enables role assumption within or across AWS accounts</li> <li>AssumeRoleWithSAML: Returns credentials for SAML-authenticated users</li> <li>AssumeRoleWithWebIdentity: Generates credentials for users logged via identity providers (Facebook, Google, OIDC)</li> <li>GetSessionToken: Supports multi-factor authentication for users and root accounts</li> <li>GetFederationToken: Obtains temporary credentials for federated users</li> <li>GetCallerIdentity: Returns details about the IAM user or role used in an API call</li> <li>DecodeAuthorizationMessage: Decodes error messages when an AWS API is denied</li> </ul>"},{"location":"aws/sts/#role-assumption-process","title":"Role Assumption Process","text":"<p>To assume a role using STS: - Define an IAM Role within your account or across accounts - Specify which principals can access the role - Use the AssumeRole API to retrieve credentials - Utilize temporary credentials valid between 15 minutes to 1 hour</p>"},{"location":"aws/sts/#multi-factor-authentication-mfa-support","title":"Multi-Factor Authentication (MFA) Support","text":"<p>STS provides robust MFA capabilities through the <code>GetSessionToken</code> method: - Requires appropriate IAM policy with condition <code>aws:MultiFactorAuthPresent:true</code> - Generates temporary credentials including:     - Access ID     - Secret Key     - Session Token     - Expiration date</p>"},{"location":"aws/sts/#troubleshooting","title":"Troubleshooting","text":"<p>When an autherization error is raised like this one: <pre><code>Encoded authorization failure message: 6h34GtpmGjJJUm946eDVBfzWQJk6z5GePbbGDs9Z2T8xZj9EZtEduSnTbmrR7pMqpJrVYJCew2m8YBZQf4HRWEtrpncANrZMsnzk\n</code></pre></p> <p>It\u2019s possilble to decode the message using: <pre><code>aws sts decode-authorization-message\n</code></pre></p>"},{"location":"aws/sts/#important-note","title":"Important Note","text":"<p>For web identity authentication, AWS recommends using Cognito Identity Pools instead of direct STS web identity credential generation.</p>"},{"location":"aws/swf/","title":"Simple Workflow Service (SWF)","text":""},{"location":"aws/swf/#overview","title":"Overview","text":"<p>Amazon Simple Workflow Service (SWF) is a web service that helps developers coordinate and manage complex distributed application workflows across multiple computing devices and services.</p>"},{"location":"aws/swf/#key-characteristics","title":"Key Characteristics","text":"<ul> <li>Manages workflow coordination and execution</li> <li>Enables complex, long-running business processes</li> <li>Supports both human-driven and automated tasks</li> <li>Provides robust tracking and state management</li> </ul>"},{"location":"aws/swf/#core-capabilities","title":"Core Capabilities","text":"<ul> <li>Distributed task coordination</li> <li>Workflow state tracking</li> <li>Automatic error handling</li> <li>Flexible task scheduling</li> <li>Support for human intervention tasks</li> </ul>"},{"location":"aws/swf/#common-use-cases","title":"Common Use Cases","text":"<ul> <li>Business process management</li> <li>Media processing workflows</li> <li>E-commerce order fulfillment</li> <li>Scientific and research computational workflows</li> <li>Large-scale data processing pipelines</li> </ul>"},{"location":"aws/swf/#key-components","title":"Key Components","text":"<ul> <li>Workflow Starters</li> <li>Deciders</li> <li>Activity Workers</li> <li>Task Lists</li> </ul>"},{"location":"aws/swf/#advantages","title":"Advantages","text":"<ul> <li>Handles complex workflow logic</li> <li>Manages application state and execution</li> <li>Provides fault-tolerant workflow management</li> <li>Scales automatically with AWS infrastructure</li> </ul>"},{"location":"aws/trusted_advisor/","title":"Trusted Advisor","text":"<p>AWS Trusted Advisor is a tool designed to help AWS customers optimize their cloud environments. It provides real-time guidance on improving the performance, security, cost efficiency, fault tolerance, and service limits of AWS resources.</p>"},{"location":"aws/trusted_advisor/#key-features","title":"Key Features","text":""},{"location":"aws/trusted_advisor/#1-best-practices-checks","title":"1. Best Practices Checks","text":"<p>Trusted Advisor evaluates your AWS environment against a set of best practices in five categories: - Cost Optimization: Identify underutilized or idle resources to reduce costs. - Performance: Improve the performance of your applications. - Security: Enhance security by identifying vulnerabilities or misconfigurations. - Fault Tolerance: Increase system availability and reduce downtime. - Service Limits: Monitor usage to prevent service interruptions caused by resource limits.</p>"},{"location":"aws/trusted_advisor/#2-personalized-recommendations","title":"2. Personalized Recommendations","text":"<p>Trusted Advisor provides tailored recommendations based on your AWS environment, enabling proactive management of resources.</p>"},{"location":"aws/trusted_advisor/#3-actionable-insights","title":"3. Actionable Insights","text":"<p>Each recommendation includes detailed steps to resolve the identified issues, making it easy to implement best practices.</p>"},{"location":"aws/trusted_advisor/#4-integration-with-aws-services","title":"4. Integration with AWS Services","text":"<ul> <li>AWS Organizations: Centralized management for multi-account environments.</li> <li>AWS Support Plans: Access to additional checks and features with Business or Enterprise Support plans.</li> <li>Amazon CloudWatch: Set up alarms for specific Trusted Advisor metrics.</li> </ul>"},{"location":"aws/trusted_advisor/#5-automated-notifications","title":"5. Automated Notifications","text":"<p>Receive regular updates and notifications about your environment\u2019s health.</p>"},{"location":"aws/trusted_advisor/#core-checks","title":"Core Checks","text":""},{"location":"aws/trusted_advisor/#cost-optimization","title":"Cost Optimization","text":"<ul> <li>Identify idle or underutilized EC2 instances, EBS volumes, and load balancers.</li> <li>Recommend reserved instance purchases for cost savings.</li> </ul>"},{"location":"aws/trusted_advisor/#performance","title":"Performance","text":"<ul> <li>Monitor high-utilization EC2 instances and recommend scaling or upgrading.</li> <li>Evaluate Auto Scaling configurations for efficiency.</li> </ul>"},{"location":"aws/trusted_advisor/#security","title":"Security","text":"<ul> <li>Highlight open access permissions in security groups.</li> <li>Identify exposed IAM access keys.</li> <li>Ensure MFA (Multi-Factor Authentication) is enabled for root accounts.</li> </ul>"},{"location":"aws/trusted_advisor/#fault-tolerance","title":"Fault Tolerance","text":"<ul> <li>Detect Amazon RDS backups not configured.</li> <li>Identify instances running on single Availability Zones.</li> </ul>"},{"location":"aws/trusted_advisor/#service-limits","title":"Service Limits","text":"<ul> <li>Track usage against AWS service limits to avoid disruptions.</li> <li>Recommend actions to stay within limits or request increases.</li> </ul>"},{"location":"aws/trusted_advisor/#accessing-trusted-advisor","title":"Accessing Trusted Advisor","text":""},{"location":"aws/trusted_advisor/#aws-management-console","title":"AWS Management Console","text":"<ol> <li>Log in to the AWS Management Console.</li> <li>Navigate to Trusted Advisor from the \u201cAWS Management &amp; Governance\u201d section.</li> <li>Select a category to view specific checks and recommendations.</li> </ol>"},{"location":"aws/trusted_advisor/#api-and-sdk-access","title":"API and SDK Access","text":"<ul> <li>Use the AWS Support API to programmatically retrieve Trusted Advisor check results.</li> </ul>"},{"location":"aws/trusted_advisor/#available-checks-based-on-support-plan","title":"Available Checks Based on Support Plan","text":"Category Free Tier Business &amp; Enterprise Cost Optimization Basic Full Performance Limited Full Security Limited Full Fault Tolerance Limited Full Service Limits Basic Full"},{"location":"aws/trusted_advisor/#common-use-cases","title":"Common Use Cases","text":"<ol> <li>Cost Reduction</li> <li> <p>Identify unused resources like idle EC2 instances and unused EBS volumes to reduce costs.</p> </li> <li> <p>Improved Security</p> </li> <li> <p>Address misconfigured security groups or unused IAM credentials to strengthen security.</p> </li> <li> <p>Enhanced Performance</p> </li> <li> <p>Ensure application performance by scaling under-provisioned resources.</p> </li> <li> <p>Avoiding Downtime</p> </li> <li>Proactively monitor service limits to avoid disruptions caused by exceeded quotas.</li> </ol>"},{"location":"aws/trusted_advisor/#best-practices","title":"Best Practices","text":"<ol> <li>Regular Monitoring</li> <li> <p>Set up a schedule to review Trusted Advisor recommendations regularly.</p> </li> <li> <p>Integrate with Automation Tools</p> </li> <li> <p>Use AWS Lambda or other automation tools to act on Trusted Advisor findings.</p> </li> <li> <p>Enable Notifications</p> </li> <li> <p>Set up CloudWatch alarms to notify you about critical checks.</p> </li> <li> <p>Leverage AWS Organizations</p> </li> <li>Use Trusted Advisor for centralized management in multi-account setups.</li> </ol>"},{"location":"aws/trusted_advisor/#limitations","title":"Limitations","text":"<ol> <li>Some advanced checks require Business or Enterprise Support plans.</li> <li>Trusted Advisor does not provide deep analytics for custom applications.</li> </ol> <p>For more details, visit the AWS Trusted Advisor Documentation.</p>"},{"location":"aws/computing/amplify/","title":"Amplify","text":""},{"location":"aws/computing/amplify/#overview","title":"Overview","text":"<p>AWS Amplify is a comprehensive set of tools and services for building and deploying full-stack mobile and web applications, often described as the \u201cElastic Beanstalk for mobile and web applications\u201d.</p>"},{"location":"aws/computing/amplify/#core-components","title":"Core Components","text":""},{"location":"aws/computing/amplify/#amplify-studio","title":"Amplify Studio","text":"<ul> <li>Visually build full-stack applications</li> <li>Integrated front-end UI and backend development</li> <li>Provides graphical interface for app creation</li> </ul>"},{"location":"aws/computing/amplify/#amplify-libraries","title":"Amplify Libraries","text":"<p>Connect applications to AWS services. Support for:</p> <ul> <li>Amazon Cognito</li> <li>Amazon S3</li> <li>Other AWS services</li> </ul> <p>Front-end libraries for multiple frameworks:</p> <ul> <li>React.js</li> <li>Vue</li> <li>JavaScript</li> <li>iOS</li> <li>Android</li> <li>Flutter</li> </ul>"},{"location":"aws/computing/amplify/#amplify-cli","title":"Amplify CLI","text":"<ul> <li>Configure backend infrastructure</li> <li>Guided workflow for backend setup</li> <li>Streamlines AWS service configuration</li> </ul>"},{"location":"aws/computing/amplify/#amplify-hosting","title":"Amplify Hosting","text":"<ul> <li>Secure and reliable web app hosting</li> <li>Leverages AWS Content Delivery Network (CDN)</li> <li>Provides continuous deployment capabilities</li> </ul>"},{"location":"aws/computing/amplify/#key-features","title":"Key Features","text":""},{"location":"aws/computing/amplify/#authentication","title":"Authentication","text":"<p>Powered by Amazon Cognito. Features:</p> <ul> <li>User registration</li> <li>Authentication</li> <li>Account recovery</li> <li>Multi-factor authentication</li> <li>Social sign-in</li> <li>Pre-built UI components</li> <li>Fine-grained authorization</li> </ul>"},{"location":"aws/computing/amplify/#datastore","title":"DataStore","text":"<p>Utilizes Amazon AppSync and DynamoDB. Capabilities:</p> <ul> <li>Local data management</li> <li>Automatic cloud synchronization</li> <li>GraphQL-powered</li> <li>Offline and real-time data capabilities</li> <li>Visual data modeling</li> </ul>"},{"location":"aws/computing/amplify/#hosting-capabilities","title":"Hosting Capabilities","text":"<ul> <li>Continuous Integration/Continuous Deployment (CI/CD)</li> <li>Pull request previews</li> <li>Custom domain support</li> <li>Monitoring</li> <li>Redirect and custom header configurations</li> <li>Password protection</li> </ul>"},{"location":"aws/computing/amplify/#end-to-end-e2e-testing","title":"End-to-End (E2E) Testing","text":"<ul> <li>Integrated testing framework</li> <li>Supports Cypress testing</li> <li>Test phases:</li> <li>During build</li> <li>During deployment</li> <li>Generates UI test reports</li> </ul>"},{"location":"aws/computing/amplify/#benefits","title":"Benefits","text":"<ul> <li>Simplified full-stack development</li> <li>Incorporates AWS best practices</li> <li>Supports reliability, security, and scalability</li> <li>Rapid application development</li> <li>Seamless integration with AWS services</li> </ul>"},{"location":"aws/computing/amplify/#supported-development-workflows","title":"Supported Development Workflows","text":"<ul> <li>CLI-based development</li> <li>Visual studio development</li> <li>Hybrid approaches</li> </ul>"},{"location":"aws/computing/api_gateway/","title":"API Gateway","text":"<p>Amazon API Gateway is a fully managed service provided by AWS for creating, deploying, and managing APIs at any scale. It supports both RESTful APIs and WebSocket APIs, enabling real-time two-way communication between applications.</p>"},{"location":"aws/computing/api_gateway/#key-characteristics-of-api-gateway","title":"Key Characteristics of API Gateway","text":""},{"location":"aws/computing/api_gateway/#1-fully-managed-service","title":"1. Fully Managed Service","text":"<ul> <li>Simplifies API lifecycle management, including creation, deployment, monitoring, and versioning.</li> <li>No need for managing infrastructure.</li> </ul>"},{"location":"aws/computing/api_gateway/#2-multi-protocol-support","title":"2. Multi-Protocol Support","text":"<ul> <li>Supports both RESTful APIs and WebSocket APIs.</li> </ul>"},{"location":"aws/computing/api_gateway/#3-scalability","title":"3. Scalability","text":"<ul> <li>Automatically scales to handle thousands of concurrent API calls.</li> <li>Provides consistent performance regardless of traffic volume.</li> </ul>"},{"location":"aws/computing/api_gateway/#4-security","title":"4. Security","text":"<p>Supports multiple authentication mechanisms:</p> <ul> <li>AWS Identity and Access Management (IAM)</li> <li>API keys</li> <li>Amazon Cognito user pools</li> <li>Lambda authorizers for custom authentication</li> </ul> <p>Integration with AWS WAF for protection against DDoS attacks and common web exploits.</p>"},{"location":"aws/computing/api_gateway/#5-integration-with-aws-services","title":"5. Integration with AWS Services","text":"<ul> <li>Seamlessly integrates with AWS Lambda, DynamoDB, S3, Step Functions, and other AWS services.</li> <li>Acts as a front door for serverless and containerized applications.</li> </ul>"},{"location":"aws/computing/api_gateway/#6-monitoring-and-analytics","title":"6. Monitoring and Analytics","text":"<ul> <li>Provides built-in monitoring and logging via Amazon CloudWatch.</li> <li>Tracks metrics like latency, error rates, and request counts.</li> <li>Enables detailed request and response logging for debugging and analysis.</li> </ul>"},{"location":"aws/computing/api_gateway/#7-custom-domain-names","title":"7. Custom Domain Names","text":"<ul> <li>Supports custom domain names for APIs.</li> <li>Allows configuring HTTPS endpoints with custom certificates.</li> </ul>"},{"location":"aws/computing/api_gateway/#8-flexible-deployment-options","title":"8. Flexible Deployment Options","text":"<ul> <li>Supports multiple stages (e.g., dev, staging, production) for API deployment.</li> <li>Provides stage variables for dynamic configuration.</li> </ul>"},{"location":"aws/computing/api_gateway/#9-stages","title":"9. Stages","text":"<p>Definition: Stages represent different environments or versions of your API (e.g., development, testing, production). Key Features:</p> <ul> <li>Each stage has its own URL endpoint.</li> <li>Stages can be configured with unique settings, such as throttling limits, caching, and logging.</li> <li>Stage variables allow dynamic configuration of stage-specific values, such as Lambda function ARNs.</li> </ul> <p>Benefits:   - Simplifies versioning and environment management.   - Facilitates separate monitoring and troubleshooting for each stage.</p>"},{"location":"aws/computing/api_gateway/#10-throttling-and-quotas","title":"10. Throttling and Quotas","text":"<ul> <li>Allows setting rate limits and burst limits to protect APIs from being overwhelmed.</li> <li>Offers quota settings to manage usage by API consumers.</li> </ul>"},{"location":"aws/computing/api_gateway/#11-transformation-and-validation","title":"11. Transformation and Validation","text":"<ul> <li>Supports request and response transformation using Velocity Template Language (VTL).</li> <li>Validates incoming requests against defined schemas.</li> </ul>"},{"location":"aws/computing/api_gateway/#12-caching","title":"12. Caching","text":"<ul> <li>Provides in-built caching for reducing latency and improving API performance.</li> <li>Cache sizes range from 0.5 GB to 237 GB.</li> </ul>"},{"location":"aws/computing/api_gateway/#13-versioning","title":"13. Versioning","text":"<ul> <li>Allows managing multiple API versions simultaneously.</li> <li>Helps in seamless API transitions and backward compatibility.</li> </ul>"},{"location":"aws/computing/api_gateway/#14-pay-as-you-go-pricing","title":"14. Pay-As-You-Go Pricing","text":"<ul> <li>Pricing based on the number of API calls, data transfer out, and caching.</li> <li>No upfront costs or long-term commitments.</li> </ul>"},{"location":"aws/computing/api_gateway/#15-multi-region-deployment","title":"15. Multi-Region Deployment","text":"<ul> <li>Supports deploying APIs in multiple AWS regions.</li> <li>Ensures high availability and low latency for global users.</li> </ul>"},{"location":"aws/computing/api_gateway/#16-developer-portal","title":"16. Developer Portal","text":"<ul> <li>Provides an open-source developer portal for onboarding and managing API consumers.</li> <li>Enables API key generation, documentation browsing, and API testing.</li> </ul> <p>API Gateway simplifies API development by acting as a unified entry point for various backend systems. With its rich features, it is suitable for building scalable, secure, and performant APIs for modern applications.</p>"},{"location":"aws/computing/autoscaling_groups/","title":"Auto Scaling Groups (ASG)","text":""},{"location":"aws/computing/autoscaling_groups/#introduction","title":"Introduction","text":"<p>In real-world applications, website and application loads fluctuate constantly. AWS Auto Scaling Groups (ASG) provide a solution to dynamically manage compute resources in response to these changing demands. This service is provided at no additional cost beyond the underlying EC2 instances.</p>"},{"location":"aws/computing/autoscaling_groups/#core-functionality","title":"Core Functionality","text":"<p>Auto Scaling Groups enable automatic adjustment of EC2 instance capacity in response to application demands. The system scales out by adding instances during high load periods and scales in by removing instances when demand decreases. ASG maintains instance counts within defined minimum and maximum boundaries while automatically registering new instances with load balancers. When an instance becomes unhealthy, ASG automatically recreates it to maintain system reliability.</p>"},{"location":"aws/computing/autoscaling_groups/#architecture-components","title":"Architecture Components","text":""},{"location":"aws/computing/autoscaling_groups/#basic-structure","title":"Basic Structure","text":"<p>An Auto Scaling Group operates within defined capacity limits. The system maintains a minimum capacity to ensure service availability, adjusts the desired capacity based on demand, and enforces a maximum capacity to control costs. This flexible structure allows for dynamic resource allocation while maintaining operational control.</p>"},{"location":"aws/computing/autoscaling_groups/#integration-with-load-balancers","title":"Integration with Load Balancers","text":"<p>ASG seamlessly integrates with Elastic Load Balancers (ELB) to distribute traffic across instances. The ELB actively monitors instance health, enabling ASG to maintain service reliability by replacing unhealthy instances automatically.</p>"},{"location":"aws/computing/autoscaling_groups/#configuration-attributes","title":"Configuration Attributes","text":""},{"location":"aws/computing/autoscaling_groups/#launch-template","title":"Launch Template","text":"<p>The Launch Template (which replaces the deprecated Launch Configurations) defines the blueprint for new instances. This template includes:</p> <ul> <li>essential instance specifications including AMI and instance type</li> <li>initialization scripts through EC2 User Data</li> <li>storage configurations with EBS volumes</li> <li>security settings through Security Groups and SSH key pairs</li> <li>instance permissions via IAM roles</li> </ul> <p>The template also specifies network settings including VPC and subnet information, and load balancer configurations.</p>"},{"location":"aws/computing/autoscaling_groups/#scaling-parameters","title":"Scaling Parameters","text":"<p>ASG requires defined minimum and maximum size limits, along with an initial capacity setting. These parameters establish the operational boundaries for the scaling process. Scaling policies determine how and when the system adjusts capacity within these limits.</p>"},{"location":"aws/computing/autoscaling_groups/#scaling-mechanisms","title":"Scaling Mechanisms","text":""},{"location":"aws/computing/autoscaling_groups/#cloudwatch-integration","title":"CloudWatch Integration","text":"<p>ASG leverages CloudWatch alarms to trigger scaling actions. These alarms monitor metrics such as average CPU utilization or custom metrics across all ASG instances. When metrics cross defined thresholds, scaling policies execute to adjust instance counts appropriately.</p>"},{"location":"aws/computing/autoscaling_groups/#scaling-policy-types","title":"Scaling Policy Types","text":"<p>The system supports several scaling approaches:</p> <ul> <li>Dynamic Scaling - offers two main methods: </li> <li>Target Tracking Scaling provides straightforward setup for maintaining specific metric targets, such as keeping average CPU utilization at 40%</li> <li>Simple/Step Scaling executes specific capacity adjustments when CloudWatch alarms trigger, such as adding two instances when CPU exceeds 70%</li> <li>Scheduled Scaling enables proactive capacity adjustment based on known usage patterns, such as increasing minimum capacity during peak business hours.</li> <li>Predictive Scaling uses machine learning to forecast load patterns and schedule scaling activities in advance.</li> </ul>"},{"location":"aws/computing/autoscaling_groups/#scaling-metrics","title":"Scaling Metrics","text":"<p>Effective scaling relies on choosing appropriate metrics. Common metrics include:</p> <p>CPU Utilization provides insight into processing demands across instances. RequestCountPerTarget helps maintain consistent request distribution across instances. Network Input/Output monitoring supports scaling for network-bound applications. Custom metrics can be implemented through CloudWatch for specialized scaling requirements.</p>"},{"location":"aws/computing/autoscaling_groups/#operational-considerations","title":"Operational Considerations","text":""},{"location":"aws/computing/autoscaling_groups/#cooldown-periods","title":"Cooldown Periods","text":"<p>After each scaling activity, ASG implements a cooldown period (default 300 seconds) during which no additional scaling actions occur. This stabilization period prevents rapid scaling fluctuations and allows metric stabilization. Using pre-configured AMIs can reduce instance configuration time and enable shorter cooldown periods.</p>"},{"location":"aws/computing/autoscaling_groups/#instance-refresh","title":"Instance Refresh","text":"<p>When updating launch templates, ASG provides an Instance Refresh feature to systematically replace existing instances. This process maintains a specified minimum healthy percentage of instances while implementing updates. Administrators can define warm-up periods to ensure instances are fully operational before entering service.</p>"},{"location":"aws/computing/autoscaling_groups/#best-practices","title":"Best Practices","text":"<p>To optimize ASG operations:</p> <ul> <li>Implement appropriate monitoring metrics aligned with application characteristics</li> <li>Configure scaling policies that reflect actual application demands</li> <li>Use pre-configured AMIs to reduce instance initialization time</li> <li>Set appropriate cooldown periods to prevent scaling thrashing</li> <li>Regularly review and adjust scaling parameters based on performance data</li> <li>Implement predictive scaling for workloads with predictable patterns</li> <li>Maintain adequate minimum capacity for base load requirements</li> </ul> <p>Through careful configuration and monitoring of these components, Auto Scaling Groups provide robust, cost-effective management of compute resources in response to changing application demands.</p>"},{"location":"aws/computing/ec2/","title":"Elastic Compute Cloud (EC2)","text":""},{"location":"aws/computing/ec2/#introduction","title":"Introduction","text":"<p>Amazon Elastic Compute Cloud (EC2) is a fundamental web service providing scalable, flexible computing capacity in the AWS cloud. It enables users to launch, manage, and scale virtual server instances with complete control over computing resources, supporting a wide range of workloads from simple web hosting to complex enterprise applications.</p>"},{"location":"aws/computing/ec2/#core-architectural-components","title":"Core Architectural Components","text":""},{"location":"aws/computing/ec2/#instance-types","title":"Instance Types","text":"<p>EC2 offers diverse instance categories optimized for specific use cases:</p>"},{"location":"aws/computing/ec2/#general-purpose-instances","title":"General Purpose Instances","text":"<p>Balanced compute, memory, and networking resources suitable for a broad range of workloads. These instances provide an equilibrium between different computational resources, making them versatile for web servers, small databases, and development environments.</p>"},{"location":"aws/computing/ec2/#compute-optimized-instances","title":"Compute Optimized Instances","text":"<p>Designed for compute-intensive applications requiring high-performance processors. Ideal for batch processing, media transcoding, high-performance web servers, machine learning inference, and scientific modeling.</p>"},{"location":"aws/computing/ec2/#memory-optimized-instances","title":"Memory Optimized Instances","text":"<p>Engineered to deliver fast performance for workloads processing large datasets in memory. Critical for high-performance databases, distributed web scale cache, in-memory analytics, and real-time big data processing.</p>"},{"location":"aws/computing/ec2/#storage-optimized-instances","title":"Storage Optimized Instances","text":"<p>Provide high, sequential read/write access to large datasets. Optimal for distributed file systems, data warehousing, and high-frequency online transaction processing (OLTP) systems.</p>"},{"location":"aws/computing/ec2/#accelerated-computing-instances","title":"Accelerated Computing Instances","text":"<p>Incorporate hardware accelerators or co-processors for specialized computational tasks. Leveraged extensively in machine learning, graphics rendering, and cryptocurrency mining.</p>"},{"location":"aws/computing/ec2/#launch-and-management-strategies","title":"Launch and Management Strategies","text":""},{"location":"aws/computing/ec2/#instance-purchasing-options","title":"Instance Purchasing Options","text":""},{"location":"aws/computing/ec2/#on-demand-instances","title":"On-Demand Instances","text":"<p>Pay for compute capacity by the hour or second with no long-term commitments. Provides maximum flexibility for unpredictable workloads and short-term applications.</p>"},{"location":"aws/computing/ec2/#reserved-instances","title":"Reserved Instances","text":"<p>Offer significant cost savings by committing to specific instance configurations for 1-3 year terms. Ideal for predictable, steady-state workloads with consistent computational requirements.</p>"},{"location":"aws/computing/ec2/#spot-instances","title":"Spot Instances","text":"<p>Enable purchasing unused EC2 capacity at steep discounts, potentially up to 90% off on-demand pricing. Suitable for fault-tolerant, flexible workloads like batch processing and scientific computing.</p>"},{"location":"aws/computing/ec2/#dedicated-hosts","title":"Dedicated Hosts","text":"<p>Provide physical servers dedicated exclusively to a single customer, addressing complex licensing and compliance requirements.</p>"},{"location":"aws/computing/ec2/#networking-capabilities","title":"Networking Capabilities","text":""},{"location":"aws/computing/ec2/#network-configuration","title":"Network Configuration","text":"<p>EC2 instances operate within Virtual Private Clouds (VPCs), offering granular control over network environments. Users can configure:</p> <ul> <li>IP address ranges</li> <li>Subnet creation</li> <li>Route table management</li> <li>Network gateway configurations</li> </ul>"},{"location":"aws/computing/ec2/#best-practices","title":"Best Practices","text":"<ol> <li>Use Both: Combine Security Groups and NACLs for a layered defense approach.</li> <li>Least Privilege: Only allow traffic that is necessary for your application.</li> <li> <p>Organize Rule Sets:</p> </li> <li> <p>Use Security Groups for instance-level control.</p> </li> <li> <p>Use NACLs to block/allow traffic at the subnet level.</p> </li> <li> <p>Monitor and Audit:</p> </li> <li> <p>Regularly review and update rules to ensure compliance and avoid unnecessary exposure.</p> </li> <li> <p>Default Rules:</p> </li> <li> <p>Ensure custom NACLs deny all traffic by default until rules are explicitly added.</p> </li> </ol> <p>By leveraging Security Groups and NACLs together, you can implement a robust and secure network architecture in AWS.</p>"},{"location":"aws/computing/ec2/#storage-options","title":"Storage Options","text":""},{"location":"aws/computing/ec2/#amazon-ebs-elastic-block-store","title":"Amazon EBS (Elastic Block Store)","text":"<p>Persistent block-level storage volumes attachable to EC2 instances. Supports various volume types:</p> <ul> <li>General Purpose SSD</li> <li>Provisioned IOPS SSD</li> <li>Throughput Optimized HDD</li> <li>Cold HDD</li> </ul>"},{"location":"aws/computing/ec2/#instance-store","title":"Instance Store","text":"<p>Temporary block-level storage directly attached to the host computer, providing high I/O performance for temporary data.</p>"},{"location":"aws/computing/ec2/#monitoring-and-management","title":"Monitoring and Management","text":""},{"location":"aws/computing/ec2/#amazon-cloudwatch","title":"Amazon CloudWatch","text":"<p>Provides comprehensive monitoring capabilities:</p> <ul> <li>Performance metrics</li> <li>Resource utilization tracking</li> <li>Automated scaling</li> <li>Custom metric creation</li> </ul>"},{"location":"aws/computing/ec2/#aws-systems-manager","title":"AWS Systems Manager","text":"<p>Enables centralized operational management across AWS resources, facilitating:</p> <ul> <li>Configuration management</li> <li>Patch management</li> <li>Automated tasks</li> <li>Compliance tracking</li> </ul>"},{"location":"aws/computing/ec2/#pricing-model","title":"Pricing Model","text":""},{"location":"aws/computing/ec2/#factors-influencing-cost","title":"Factors Influencing Cost","text":"<ul> <li>Instance type</li> <li>Region</li> <li>Operating system</li> <li>Purchasing option</li> <li>Additional services and data transfer</li> </ul>"},{"location":"aws/computing/ec2/#use-cases","title":"Use Cases","text":""},{"location":"aws/computing/ec2/#enterprise-applications","title":"Enterprise Applications","text":"<ul> <li>Web hosting</li> <li>Enterprise application servers</li> <li>Development and testing environments</li> </ul>"},{"location":"aws/computing/ec2/#scientific-computing","title":"Scientific Computing","text":"<ul> <li>High-performance computing</li> <li>Genomic research</li> <li>Climate modeling</li> </ul>"},{"location":"aws/computing/ec2/#media-processing","title":"Media Processing","text":"<ul> <li>Video rendering</li> <li>Transcoding</li> <li>Content delivery</li> </ul>"},{"location":"aws/computing/ec2/#best-practices_1","title":"Best Practices","text":"<ul> <li>Right-size instances based on workload</li> <li>Implement auto-scaling</li> <li>Utilize multiple availability zones</li> <li>Leverage appropriate purchasing models</li> <li>Implement robust security configurations</li> <li>Continuously monitor performance metrics</li> </ul>"},{"location":"aws/computing/ec2/#limitations-and-considerations","title":"Limitations and Considerations","text":"<ul> <li>Maximum of 20 On-Demand instances per region</li> <li>Specific service quotas and limits</li> <li>Regional availability variations</li> <li>Potential data transfer costs</li> </ul>"},{"location":"aws/computing/ec2/#conclusion","title":"Conclusion","text":"<p>AWS EC2 represents a powerful, flexible computing platform enabling organizations to scale computational resources dynamically, efficiently, and cost-effectively. By understanding its comprehensive capabilities, users can design robust, scalable cloud architectures tailored to diverse computational requirements.</p>"},{"location":"aws/computing/ecs/","title":"Elastic Container Service (ECS)","text":""},{"location":"aws/computing/ecs/#overview","title":"Overview","text":"<p>Amazon Elastic Container Service (ECS) is a fully managed container orchestration service that enables you to run Docker containers on AWS infrastructure. This document provides a comprehensive overview of ECS components, features, and best practices.</p>"},{"location":"aws/computing/ecs/#launch-types","title":"Launch Types","text":""},{"location":"aws/computing/ecs/#ec2-launch-type","title":"EC2 Launch Type","text":"<p>ECS with EC2 launch type requires you to provision and maintain your own EC2 instances. Key characteristics include:</p> <ul> <li>You must provision and maintain the infrastructure (EC2 instances)</li> <li>Each EC2 instance must run the ECS Agent to register in the ECS Cluster</li> <li>AWS manages container starting and stopping</li> <li>Requires more management but offers more control</li> </ul>"},{"location":"aws/computing/ecs/#fargate-launch-type","title":"Fargate Launch Type","text":"<p>Fargate is a serverless compute engine for containers. Key features include:</p> <ul> <li>No infrastructure provisioning required</li> <li>Fully serverless operation</li> <li>Only requires task definitions</li> <li>AWS automatically runs ECS Tasks based on CPU/RAM requirements</li> <li>Scaling is simplified - just increase the number of tasks</li> </ul>"},{"location":"aws/computing/ecs/#iam-roles-for-ecs","title":"IAM Roles for ECS","text":""},{"location":"aws/computing/ecs/#ec2-instance-profile-ec2-launch-type-only","title":"EC2 Instance Profile (EC2 Launch Type only)","text":"<p>Used by the ECS agent for:</p> <ul> <li>Making API calls to ECS service</li> <li>Sending container logs to CloudWatch Logs</li> <li>Pulling Docker images from ECR</li> <li>Accessing sensitive data in Secrets Manager or SSM Parameter Store</li> </ul>"},{"location":"aws/computing/ecs/#ecs-task-role","title":"ECS Task Role","text":"<ul> <li>Allows each task to have a specific role</li> <li>Different roles can be used for different ECS Services</li> <li>Defined in the task definition</li> </ul>"},{"location":"aws/computing/ecs/#load-balancer-integration","title":"Load Balancer Integration","text":""},{"location":"aws/computing/ecs/#supported-load-balancers","title":"Supported Load Balancers","text":"<ul> <li>Application Load Balancer (ALB) - recommended for most use cases</li> <li>Network Load Balancer (NLB) - recommended for high throughput/performance cases or AWS Private Link</li> <li>Classic Load Balancer - supported but not recommended (no advanced features, no Fargate support)</li> </ul>"},{"location":"aws/computing/ecs/#data-volumes","title":"Data Volumes","text":""},{"location":"aws/computing/ecs/#efs-integration","title":"EFS Integration","text":"<ul> <li>Mount EFS file systems onto ECS tasks</li> <li>Compatible with both EC2 and Fargate launch types</li> <li>Tasks in any AZ share the same data</li> <li>Fargate + EFS provides fully serverless solution</li> <li>Note: Amazon S3 cannot be mounted as a file system</li> </ul>"},{"location":"aws/computing/ecs/#bind-mounts","title":"Bind Mounts","text":"<ul> <li>Share data between multiple containers in the same Task Definition</li> <li>Works for both EC2 and Fargate tasks</li> <li>EC2 Tasks: Uses EC2 instance storage (data tied to EC2 instance lifecycle)</li> <li>Fargate Tasks: Uses ephemeral storage (20-200 GiB, default 20 GiB)</li> </ul>"},{"location":"aws/computing/ecs/#auto-scaling","title":"Auto Scaling","text":""},{"location":"aws/computing/ecs/#service-auto-scaling","title":"Service Auto Scaling","text":"<ul> <li>Automatically adjusts the number of ECS tasks</li> <li>Uses AWS Application Auto Scaling</li> <li> <p>Scaling metrics:</p> </li> <li> <p>ECS Service Average CPU Utilization</p> </li> <li>ECS Service Average Memory Utilization</li> <li>ALB Request Count PerTarget</li> </ul>"},{"location":"aws/computing/ecs/#scaling-methods","title":"Scaling Methods","text":"<ul> <li>Target Tracking - based on CloudWatch metric target</li> <li>Step Scaling - based on CloudWatch Alarm</li> <li>Scheduled Scaling - based on date/time</li> </ul>"},{"location":"aws/computing/ecs/#ec2-launch-type-auto-scaling","title":"EC2 Launch Type Auto Scaling","text":"<ul> <li>Auto Scaling Group Scaling based on CPU Utilization</li> <li>ECS Cluster Capacity Provider for automatic infrastructure provisioning</li> </ul>"},{"location":"aws/computing/ecs/#task-definitions","title":"Task Definitions","text":"<p>Task definitions are JSON metadata that tell ECS how to run Docker containers. They include:</p> <ul> <li>Image Name</li> <li>Port Binding for Container and Host</li> <li>Memory and CPU requirements</li> <li>Environment variables</li> <li>Networking information</li> <li>IAM Role</li> <li>Logging configuration</li> <li>Up to 10 containers per Task Definition</li> </ul>"},{"location":"aws/computing/ecs/#environment-variables","title":"Environment Variables","text":"<p>Supported types:</p> <ul> <li>Hardcoded values (e.g., URLs)</li> <li>SSM Parameter Store (sensitive variables, API keys, configs)</li> <li>Secrets Manager (sensitive variables, DB passwords)</li> <li>Environment Files (bulk) from Amazon S3</li> </ul>"},{"location":"aws/computing/ecs/#task-placement-ec2-launch-type-only","title":"Task Placement (EC2 Launch Type Only)","text":""},{"location":"aws/computing/ecs/#placement-process","title":"Placement Process","text":"<ol> <li>Identify instances meeting CPU, memory, and port requirements</li> <li>Identify instances satisfying Task Placement Constraints</li> <li>Identify instances satisfying Task Placement Strategies</li> <li>Select instances</li> </ol>"},{"location":"aws/computing/ecs/#placement-strategies","title":"Placement Strategies","text":"<ul> <li>Binpack - optimizes resource usage by placing tasks on instances with least available CPU/memory</li> <li>Random - places tasks randomly across instances</li> <li>Spread - distributes tasks evenly based on specified values (e.g., availability zone)</li> </ul>"},{"location":"aws/computing/ecs/#placement-constraints","title":"Placement Constraints","text":"<ul> <li>distinctInstance - ensures tasks are placed on different EC2 instances</li> <li>memberOf - places tasks on instances meeting specified criteria using Cluster Query Language</li> </ul>"},{"location":"aws/computing/ecs/#rolling-updates","title":"Rolling Updates","text":"<p>Control task updates from v1 to v2 by specifying:</p> <ul> <li>Minimum Healthy Percent (0-100%)</li> <li>Maximum Percent (100-200%)</li> <li>Controls task termination and creation order during updates</li> </ul>"},{"location":"aws/computing/ecs/#load-balancing","title":"Load Balancing","text":""},{"location":"aws/computing/ecs/#ec2-launch-type_1","title":"EC2 Launch Type","text":"<ul> <li>Supports Dynamic Host Port Mapping</li> <li>ALB automatically finds correct ports on EC2 instances</li> <li>Security Group configuration required for EC2 instances</li> </ul>"},{"location":"aws/computing/ecs/#fargate","title":"Fargate","text":"<ul> <li>Each task gets unique private IP</li> <li>Only container port definition required</li> <li>Security Group configuration needed for ECS ENI and ALB</li> </ul>"},{"location":"aws/computing/elastic_beanstalk/","title":"Elastic Beanstalk","text":""},{"location":"aws/computing/elastic_beanstalk/#overview-of-aws-elastic-beanstalk","title":"Overview of AWS Elastic Beanstalk","text":"<p>AWS Elastic Beanstalk represents a sophisticated platform-as-a-service solution designed to simplify application deployment and management across multiple programming languages and web frameworks. By abstracting the underlying infrastructure complexities, Beanstalk enables developers to focus on writing code rather than managing complex cloud environments.</p>"},{"location":"aws/computing/elastic_beanstalk/#core-architectural-philosophy","title":"Core Architectural Philosophy","text":"<p>Elastic Beanstalk provides a comprehensive deployment ecosystem that automatically handles infrastructure provisioning, load balancing, auto-scaling, and application health monitoring. The service bridges the gap between manual infrastructure management and full platform abstraction, offering developers granular control while maintaining operational simplicity.</p>"},{"location":"aws/computing/elastic_beanstalk/#deployment-environment-mechanics","title":"Deployment Environment Mechanics","text":"<p>When an application is deployed through Elastic Beanstalk, the service creates a comprehensive infrastructure stack tailored to the specific application requirements. This includes selecting appropriate compute resources, configuring network settings, and establishing necessary communication pathways between different architectural components.</p>"},{"location":"aws/computing/elastic_beanstalk/#supported-platforms-and-runtime-environments","title":"Supported Platforms and Runtime Environments","text":"<p>Elastic Beanstalk supports a diverse range of programming languages and frameworks, providing native integration for:</p> <ul> <li>Java with Apache Tomcat</li> <li>.NET on Windows Server platform</li> <li>PHP applications</li> <li>Node.js web services</li> <li>Python with Django and Flask</li> <li>Ruby on Rails</li> <li>Go language web applications</li> <li>Docker containerized deployments</li> </ul>"},{"location":"aws/computing/elastic_beanstalk/#deployment-strategies","title":"Deployment Strategies","text":""},{"location":"aws/computing/elastic_beanstalk/#standard-application-deployment","title":"Standard Application Deployment","text":"<p>Developers can upload application code directly through the AWS Management Console, CLI, or SDK. Beanstalk automatically provisions the necessary infrastructure, configures environment variables, and manages application lifecycle.</p>"},{"location":"aws/computing/elastic_beanstalk/#container-based-deployments","title":"Container-Based Deployments","text":"<p>For more complex architectural requirements, Beanstalk supports Docker containerization. This approach allows developers to package applications with their dependencies, ensuring consistent behavior across different deployment environments.</p>"},{"location":"aws/computing/elastic_beanstalk/#environment-configuration","title":"Environment Configuration","text":""},{"location":"aws/computing/elastic_beanstalk/#environment-tiers","title":"Environment Tiers","text":"<p>Beanstalk offers two primary environment configurations:</p>"},{"location":"aws/computing/elastic_beanstalk/#web-server-environment","title":"Web Server Environment","text":"<p>Designed for hosting web applications and services with direct internet accessibility. These environments automatically configure load balancers and auto-scaling groups to manage incoming web traffic.</p>"},{"location":"aws/computing/elastic_beanstalk/#worker-environment","title":"Worker Environment","text":"<p>Optimized for background processing and asynchronous task execution. Worker environments integrate seamlessly with Amazon SQS for managing distributed computational workloads.</p>"},{"location":"aws/computing/elastic_beanstalk/#advanced-configuration-management","title":"Advanced Configuration Management","text":"<p>Elastic Beanstalk provides multiple mechanisms for customizing deployment environments:</p>"},{"location":"aws/computing/elastic_beanstalk/#configuration-files","title":"Configuration Files","text":"<p>Developers can include <code>.ebextensions</code> configuration files within their application package, enabling intricate environment customization without manual infrastructure modification.</p>"},{"location":"aws/computing/elastic_beanstalk/#environment-variables","title":"Environment Variables","text":"<p>Comprehensive support for dynamic configuration through environment-specific variables, allowing seamless transition between development, staging, and production environments.</p>"},{"location":"aws/computing/elastic_beanstalk/#monitoring-and-observability","title":"Monitoring and Observability","text":""},{"location":"aws/computing/elastic_beanstalk/#health-monitoring","title":"Health Monitoring","text":"<p>Beanstalk continuously monitors application and infrastructure health, automatically replacing failed instances and providing detailed diagnostic information through integrated CloudWatch metrics.</p>"},{"location":"aws/computing/elastic_beanstalk/#logging-mechanisms","title":"Logging Mechanisms","text":"<p>Comprehensive logging capabilities capture application and system-level events, facilitating efficient troubleshooting and performance optimization.</p>"},{"location":"aws/computing/elastic_beanstalk/#security-and-compliance","title":"Security and Compliance","text":""},{"location":"aws/computing/elastic_beanstalk/#iam-integration","title":"IAM Integration","text":"<p>Deep integration with AWS Identity and Access Management allows granular access control and role-based permissions for environment management.</p>"},{"location":"aws/computing/elastic_beanstalk/#network-isolation","title":"Network Isolation","text":"<p>Support for Amazon Virtual Private Cloud (VPC) enables secure, isolated network environments with customizable security group configurations.</p>"},{"location":"aws/computing/elastic_beanstalk/#scaling-and-performance","title":"Scaling and Performance","text":""},{"location":"aws/computing/elastic_beanstalk/#auto-scaling","title":"Auto Scaling","text":"<p>Intelligent auto-scaling mechanisms automatically adjust computational resources based on predefined performance metrics, ensuring optimal application responsiveness during variable traffic conditions.</p>"},{"location":"aws/computing/elastic_beanstalk/#load-balancing","title":"Load Balancing","text":"<p>Integrated elastic load balancing distributes incoming traffic across multiple instances, providing enhanced reliability and performance.</p>"},{"location":"aws/computing/elastic_beanstalk/#pricing-considerations","title":"Pricing Considerations","text":"<p>Elastic Beanstalk itself is a free service. Customers are charged only for the underlying AWS resources provisioned during application deployment, such as EC2 instances, load balancers, and data transfer.</p>"},{"location":"aws/computing/elastic_beanstalk/#use-case-scenarios","title":"Use Case Scenarios","text":"<ul> <li>Rapid web application deployment</li> <li>Microservices architecture</li> <li>Continuous integration and deployment pipelines</li> <li>Scalable enterprise applications</li> <li>Prototype and development environment management</li> </ul>"},{"location":"aws/computing/elastic_beanstalk/#conclusion","title":"Conclusion","text":"<p>AWS Elastic Beanstalk offers a powerful, flexible platform for application deployment, removing infrastructure complexity while providing developers comprehensive control over their computational environments.</p>"},{"location":"aws/computing/elb/","title":"Elastic Load Balancer (ELB)","text":""},{"location":"aws/computing/elb/#overview","title":"Overview","text":"<p>Elastic Load Balancer is a service that forwards traffic to multiple servers (such as EC2 instances) downstream. It acts as a single point of contact for all incoming web traffic to your applications.</p>"},{"location":"aws/computing/elb/#benefits-of-load-balancing","title":"Benefits of Load Balancing","text":"<ul> <li>Spreads load across multiple downstream instances</li> <li>Provides a single point of access (DNS) to your application</li> <li>Handles failures of downstream instances automatically</li> <li>Performs regular health checks on instances</li> <li>Provides SSL termination (HTTPS) for websites</li> <li>Enforces stickiness with cookies</li> <li>Ensures high availability across zones</li> <li>Separates public traffic from private traffic</li> </ul>"},{"location":"aws/computing/elb/#why-choose-elastic-load-balancer","title":"Why Choose Elastic Load Balancer?","text":"<ul> <li>Fully managed by AWS with guaranteed uptime</li> <li>AWS handles upgrades, maintenance, and high availability</li> <li>Simplified configuration options</li> <li>Cost-effective compared to setting up your own load balancer</li> <li>Integrated with various AWS services:</li> <li>EC2 and EC2 Auto Scaling Groups</li> <li>Amazon ECS</li> <li>AWS Certificate Manager (ACM)</li> <li>CloudWatch</li> <li>Route 53</li> <li>AWS WAF</li> <li>AWS Global Accelerator</li> </ul>"},{"location":"aws/computing/elb/#types-of-load-balancers","title":"Types of Load Balancers","text":""},{"location":"aws/computing/elb/#classic-load-balancer-clb-2009","title":"Classic Load Balancer (CLB) - 2009","text":"<ul> <li>Supports HTTP, HTTPS, TCP, and SSL (secure TCP)</li> <li>Legacy load balancer (first generation)</li> </ul>"},{"location":"aws/computing/elb/#application-load-balancer-alb-2016","title":"Application Load Balancer (ALB) - 2016","text":"<ul> <li>Operates at Layer 7 (HTTP)</li> <li>Supports HTTP, HTTPS, WebSocket</li> <li>Features:</li> <li>Path-based routing</li> <li>Host-based routing</li> <li>Query string/header-based routing</li> <li>Support for HTTP/2 and WebSocket</li> <li>Support for redirects</li> <li>Container-friendly with dynamic port mapping</li> </ul>"},{"location":"aws/computing/elb/#network-load-balancer-nlb-2017","title":"Network Load Balancer (NLB) - 2017","text":"<ul> <li>Operates at Layer 4 (TCP/UDP)</li> <li>Features:</li> <li>Handles millions of requests per second</li> <li>Ultra-low latency</li> <li>Static IP per AZ with Elastic IP support</li> <li>Ideal for extreme performance needs</li> </ul>"},{"location":"aws/computing/elb/#gateway-load-balancer-gwlb-2020","title":"Gateway Load Balancer (GWLB) - 2020","text":"<ul> <li>Operates at Layer 3 (Network layer)</li> <li>Used for deploying and managing third-party virtual appliances</li> </ul> <p>Combines:</p> <ul> <li>Transparent Network Gateway</li> <li>Load Balancer functionality</li> </ul>"},{"location":"aws/computing/elb/#health-checks","title":"Health Checks","text":"<ul> <li>Essential for load balancer operation</li> <li>Monitors instance availability</li> </ul> <p>Configured with:</p> <ul> <li>Protocol</li> <li>Port</li> <li>Endpoint path</li> <li>Response code expectations</li> </ul>"},{"location":"aws/computing/elb/#security-groups","title":"Security Groups","text":"<ul> <li> <p>Load Balancer Security Group:</p> </li> <li> <p>Allows inbound HTTPS/HTTP from anywhere</p> </li> <li> <p>Application Security Group:</p> </li> <li> <p>Allows traffic only from Load Balancer</p> </li> </ul>"},{"location":"aws/computing/elb/#target-groups","title":"Target Groups","text":"<p>Support various target types depending on the load balancer:</p> <ul> <li>EC2 instances</li> <li>IP addresses (must be private IPs)</li> <li>Lambda functions (ALB only)</li> <li>Application Load Balancers (NLB only)</li> <li>Container instances</li> </ul>"},{"location":"aws/computing/elb/#sticky-sessions","title":"Sticky Sessions","text":"<ul> <li>Ensures client requests route to the same instance</li> <li>Supported by all load balancer types</li> <li> <p>Cookie types:</p> </li> <li> <p>Application-based Cookies:</p> <ul> <li>Custom cookies (generated by target)</li> <li>Application cookies (generated by load balancer - AWSALBAPP)</li> </ul> </li> <li> <p>Duration-based Cookies:</p> <ul> <li>Generated by load balancer</li> <li>AWSALB for ALB, AWSELB for CLB</li> </ul> </li> </ul>"},{"location":"aws/computing/elb/#cross-zone-load-balancing","title":"Cross-Zone Load Balancing","text":"<ul> <li>ALB: Enabled by default, no inter-AZ charges</li> <li>NLB and GWLB: Disabled by default, charges apply for inter-AZ data if enabled</li> <li>CLB: Disabled by default, no inter-AZ charges if enabled</li> </ul>"},{"location":"aws/computing/elb/#ssltls-support","title":"SSL/TLS Support","text":"<ul> <li>Provides in-flight encryption</li> <li> <p>Certificate management through AWS Certificate Manager (ACM) Features:</p> </li> <li> <p>Support for multiple certificates (ALB and NLB)</p> </li> <li>Server Name Indication (SNI) support</li> <li>Customizable security policies</li> </ul> <p>Certificate handling varies by load balancer type:</p> <ul> <li>CLB: Single SSL certificate only</li> <li>ALB/NLB: Multiple listeners with multiple SSL certificates via SNI</li> </ul>"},{"location":"aws/computing/elb/#connection-draining","title":"Connection Draining","text":"<ul> <li>Named \u201cConnection Draining\u201d for CLB</li> <li>Named \u201cDeregistration Delay\u201d for ALB &amp; NLB</li> <li> <p>Allows completion of in-flight requests during instance deregistration Configuration options:</p> </li> <li> <p>Duration: 1-3600 seconds (default 300)</p> </li> <li>Can be disabled (set to 0)</li> <li>Recommended to set lower values for short-lived requests</li> </ul>"},{"location":"aws/computing/lambda/","title":"Lambda","text":""},{"location":"aws/computing/lambda/#overview","title":"Overview","text":"<p>AWS Lambda is a serverless compute service that enables you to run code without provisioning or managing servers. It executes your code only when needed and scales automatically to handle any number of requests simultaneously. This service is particularly useful for microservices architecture, data processing, and backend applications.</p>"},{"location":"aws/computing/lambda/#core-concepts","title":"Core Concepts","text":""},{"location":"aws/computing/lambda/#execution-model","title":"Execution Model","text":"<p>Lambda functions operate on an event-driven model where code executes in response to triggers. The execution environment is completely managed by AWS, handling all aspects of infrastructure, including:</p> <ul> <li>Serverless Execution: No server management required; AWS handles all infrastructure</li> <li>Event-Driven Architecture: Functions execute in response to events from various AWS services</li> <li>Automatic Scaling: Scales automatically from a few requests per day to thousands per second</li> <li>Pay-per-Use: Billing based on actual compute time used, calculated in milliseconds</li> <li>Built-in Fault Tolerance: Automatic replication across multiple Availability Zones</li> </ul>"},{"location":"aws/computing/lambda/#supported-runtimes","title":"Supported Runtimes","text":"<p>Lambda supports multiple programming languages through runtime environments:</p> <ul> <li>Node.js (18.x, 16.x, 14.x): JavaScript/TypeScript development with extensive NPM ecosystem</li> <li>Python (3.11, 3.10, 3.9, 3.8): Popular for data processing and scripting tasks</li> <li>Java (17, 11, 8): Enterprise-grade applications with full JVM support</li> <li>.NET Core (7.0, 6.0): C# and F# development with .NET ecosystem</li> <li>Ruby (3.2, 2.7): Ruby development with gem support</li> <li>Go (1.x): High-performance applications</li> <li>Custom Runtime: Support for any additional languages via container images</li> </ul>"},{"location":"aws/computing/lambda/#deployment-options","title":"Deployment Options","text":""},{"location":"aws/computing/lambda/#zip-file-archives","title":".zip File Archives","text":"<p>The traditional method of deploying Lambda functions using compressed archives:</p> <ul> <li> <p>Size Limits: </p> </li> <li> <p>Direct upload: 50 MB compressed</p> </li> <li> <p>S3 upload: 250 MB uncompressed</p> </li> <li> <p>Deployment Process: Upload directly via AWS Console, CLI, or SDK</p> </li> <li>Version Control: Integrated with AWS versioning system</li> <li>Cold Start Impact: Generally faster cold starts compared to containers</li> <li>Use Cases: Ideal for simpler functions with minimal dependencies</li> </ul>"},{"location":"aws/computing/lambda/#container-images","title":"Container Images","text":"<p>Deploy Lambda functions as container images, offering greater flexibility and consistency:</p> <ul> <li>Size Limit: Up to 10 GB</li> <li>Format Support: Compatible with OCI (Open Container Initiative) format</li> <li>Base Images: AWS-provided base images for each runtime</li> <li>Custom Runtimes: Support for any programming language via custom containers</li> <li> <p>Architecture Support:</p> </li> <li> <p>x86_64: Standard architecture, available in all regions</p> </li> <li>arm64: AWS Graviton2, offering better price/performance ratio</li> </ul>"},{"location":"aws/computing/lambda/#lambda-layers","title":"Lambda Layers","text":"<p>A mechanism to centrally manage code and dependencies:</p>"},{"location":"aws/computing/lambda/#purpose-and-benefits","title":"Purpose and Benefits","text":"<ul> <li>Code Reuse: Share common code across multiple functions</li> <li>Dependency Management: Centralize and version control dependencies</li> <li>Size Management: Reduce individual function size</li> <li>Updates: Easier updates of shared components</li> </ul>"},{"location":"aws/computing/lambda/#technical-specifications","title":"Technical Specifications","text":"<ul> <li>Layer Limit: Up to 5 layers per function</li> <li>Size Limit: 250 MB unzipped total size</li> <li>Sharing: Can be shared across accounts and regions</li> <li>Versioning: Each layer update creates a new version</li> </ul>"},{"location":"aws/computing/lambda/#testing-and-development","title":"Testing and Development","text":""},{"location":"aws/computing/lambda/#local-testing-methods","title":"Local Testing Methods","text":""},{"location":"aws/computing/lambda/#aws-sam-cli","title":"AWS SAM CLI","text":"<p>A command-line tool that provides a local development environment:</p> <ul> <li>Local Execution: Run Lambda functions locally</li> <li>API Testing: Test API Gateway integrations</li> <li>Debugging: Step through code using IDE integrations</li> <li>Event Simulation: Generate sample events for testing</li> </ul>"},{"location":"aws/computing/lambda/#runtime-interface-emulator-rie","title":"Runtime Interface Emulator (RIE)","text":"<p>A tool for testing container image-based functions:</p> <ul> <li>Container Testing: Test functions exactly as they\u2019ll run in AWS</li> <li>API Emulation: Simulates the Lambda Runtime API locally</li> <li>Integration: Works with standard Docker tools</li> </ul>"},{"location":"aws/computing/lambda/#localstack","title":"LocalStack","text":"<p>A local AWS cloud stack for testing:</p> <ul> <li>Service Emulation: Emulate AWS services locally</li> <li>Integration Testing: Test complete architectures</li> <li>Offline Development: Develop without AWS connectivity</li> </ul>"},{"location":"aws/computing/lambda/#lambda-runtime-api","title":"Lambda Runtime API","text":""},{"location":"aws/computing/lambda/#core-components","title":"Core Components","text":"<p>The Lambda Runtime API is an HTTP interface that custom runtimes must implement:</p>"},{"location":"aws/computing/lambda/#api-endpoints","title":"API Endpoints","text":"<ul> <li>Next Invocation: Polls for new function invocations</li> <li>Response Handling: Sends function results back to Lambda</li> <li>Error Management: Reports function and runtime errors</li> <li>Initialization: Handles runtime startup and initialization</li> </ul>"},{"location":"aws/computing/lambda/#implementation-requirements","title":"Implementation Requirements","text":"<ul> <li>Event Processing: Handle incoming events and context</li> <li>Error Handling: Proper error formatting and reporting</li> <li>Lifecycle Management: Manage function and runtime lifecycle</li> <li>Environment: Handle environment variables and configuration</li> </ul>"},{"location":"aws/computing/lambda/#monitoring-and-performance","title":"Monitoring and Performance","text":""},{"location":"aws/computing/lambda/#cloudwatch-integration","title":"CloudWatch Integration","text":"<p>Comprehensive monitoring and logging capabilities:</p>"},{"location":"aws/computing/lambda/#metrics","title":"Metrics","text":"<ul> <li>Invocations: Track function calls</li> <li>Duration: Monitor execution time</li> <li>Errors: Track function errors</li> <li>Throttling: Monitor concurrency limits</li> <li>Iterator Age: Track stream processing lag</li> </ul>"},{"location":"aws/computing/lambda/#logging","title":"Logging","text":"<ul> <li>Automatic Log Creation: Each invocation logged automatically</li> <li>Log Groups: Organized by function</li> <li>Log Retention: Configurable retention periods</li> <li>Log Insights: Query and analyze logs</li> </ul>"},{"location":"aws/computing/lambda/#x-ray-integration","title":"X-Ray Integration","text":"<p>Distributed tracing and performance analysis:</p>"},{"location":"aws/computing/lambda/#features","title":"Features","text":"<ul> <li>Trace Analysis: Track requests across services</li> <li>Performance Insights: Identify bottlenecks</li> <li>Error Tracking: Debug issues across services</li> <li>Service Maps: Visualize application architecture</li> </ul>"},{"location":"aws/computing/lambda/#limits-and-quotas","title":"Limits and Quotas","text":""},{"location":"aws/computing/lambda/#function-configuration","title":"Function Configuration","text":"<ul> <li>Memory: 128 MB to 10,240 MB, in 1 MB increments</li> <li>Timeout: Maximum of 900 seconds (15 minutes)</li> <li>Deployment Package: 50 MB (zipped) for direct upload</li> <li>Container Image: 10 GB maximum</li> <li>Environment Variables: 4 KB for all variables combined</li> </ul>"},{"location":"aws/computing/lambda/#execution","title":"Execution","text":"<ul> <li>Concurrent Executions: 1,000 per region (default)</li> <li>Burst Concurrency: 500-3000 depending on region</li> <li>Temporary Storage: 512 MB at /tmp</li> <li>Function Resource Limits: 1,000 versions per function</li> </ul>"},{"location":"aws/computing/lambda/#cost-optimization","title":"Cost Optimization","text":""},{"location":"aws/computing/lambda/#execution-costs","title":"Execution Costs","text":"<p>Understanding and optimizing Lambda costs:</p>"},{"location":"aws/computing/lambda/#billing-factors","title":"Billing Factors","text":"<ul> <li>Compute Time: Billed per millisecond</li> <li>Memory Allocation: Affects both performance and cost</li> <li>Requests: Number of function invocations</li> <li>Data Transfer: Network traffic costs</li> </ul>"},{"location":"aws/computing/lambda/#optimization-strategies","title":"Optimization Strategies","text":"<ul> <li>Memory Tuning: Balance between performance and cost</li> <li>Execution Time: Optimize code for faster execution</li> <li>Concurrent Execution: Manage concurrency limits</li> <li>Cold Start: Use provisioned concurrency when needed</li> </ul>"},{"location":"aws/database/athena/","title":"Athena","text":""},{"location":"aws/database/athena/#overview","title":"Overview","text":"<p>Amazon Athena is a powerful serverless query service that enables users to analyze data stored in Amazon S3 using standard SQL. Built on the Presto framework, Athena eliminates the need for complex ETL jobs when querying data, offering a simplified yet robust solution for data analysis.</p>"},{"location":"aws/database/athena/#core-features-and-functionality","title":"Core Features and Functionality","text":""},{"location":"aws/database/athena/#query-service-capabilities","title":"Query Service Capabilities","text":"<p>Athena operates as a serverless query engine, requiring no infrastructure management while providing the ability to analyze data directly from S3 storage. The service uses standard SQL language (built on Presto), making it accessible to users familiar with SQL syntax.</p>"},{"location":"aws/database/athena/#supported-file-formats","title":"Supported File Formats","text":"<p>Athena supports multiple data formats, including:</p> <ul> <li>CSV</li> <li>JSON</li> <li>ORC</li> <li>Avro</li> <li>Parquet</li> </ul>"},{"location":"aws/database/athena/#cost-structure","title":"Cost Structure","text":"<p>The pricing model is straightforward:</p> <ul> <li>$5.00 per TB of data scanned</li> <li>Users only pay for the data they query</li> </ul>"},{"location":"aws/database/athena/#integration-with-quicksight","title":"Integration with QuickSight","text":"<p>Athena seamlessly integrates with Amazon QuickSight, enabling:</p> <ul> <li>Creation of comprehensive reports</li> <li>Building interactive dashboards</li> <li>Visual data exploration and analysis</li> </ul>"},{"location":"aws/database/athena/#common-use-cases","title":"Common Use Cases","text":"<p>Athena serves various analytical needs, including:</p> <ul> <li>Business intelligence and analytics</li> <li>Reporting and data analysis</li> <li>VPC Flow Logs analysis</li> <li>ELB Logs examination</li> <li>CloudTrail trails investigation</li> </ul>"},{"location":"aws/database/athena/#performance-optimization","title":"Performance Optimization","text":""},{"location":"aws/database/athena/#data-format-recommendations","title":"Data Format Recommendations","text":"<p>To optimize performance and reduce costs, consider the following recommendations:</p> <ol> <li> <p>Use columnar data formats:</p> </li> <li> <p>Apache Parquet or ORC are highly recommended</p> </li> <li>These formats provide significant performance improvements</li> <li> <p>AWS Glue can be used to convert existing data to Parquet or ORC</p> </li> <li> <p>Implement data compression:</p> </li> <li> <p>Supported compression formats include:</p> <ul> <li>bzip2</li> <li>gzip</li> <li>lz4</li> <li>snappy</li> <li>zlib</li> <li>zstd</li> </ul> </li> </ol>"},{"location":"aws/database/athena/#data-organization-and-storage","title":"Data Organization and Storage","text":"<p>For optimal performance:</p> <ol> <li> <p>Implement partitioning:</p> </li> <li> <p>Organize datasets in S3 using partition columns</p> </li> <li>Follow the structure:      <pre><code>s3://yourBucket/pathToTable/&lt;PARTITION_COLUMN_NAME&gt;=&lt;VALUE&gt;/\n</code></pre></li> <li> <p>Example:      <pre><code>s3://athena-examples/flight/parquet/year=1991/month=1/day=1/\n</code></pre></p> </li> <li> <p>File size optimization:</p> </li> <li>Maintain file sizes larger than 128 MB</li> <li>This approach minimizes query overhead</li> </ol>"},{"location":"aws/database/athena/#federated-query-capabilities","title":"Federated Query Capabilities","text":""},{"location":"aws/database/athena/#overview_1","title":"Overview","text":"<p>Federated queries enable SQL query execution across diverse data sources, including:</p> <ul> <li>Relational databases</li> <li>Non-relational databases</li> <li>Object stores</li> <li>Custom data sources</li> </ul>"},{"location":"aws/database/athena/#architecture-components","title":"Architecture Components","text":"<p>The federated query system consists of: 1. Data Source Connectors running on AWS Lambda 2. Support for various data sources:</p> <ul> <li>CloudWatch Logs</li> <li>DynamoDB</li> <li>RDS</li> <li>ElastiCache</li> <li>DocumentDB</li> <li>Redshift</li> <li>HBase in EMR</li> <li>MySQL</li> <li>Aurora</li> <li>SQL Server</li> <li>On-premises databases</li> </ul>"},{"location":"aws/database/athena/#functionality","title":"Functionality","text":"<ul> <li>Enables querying across multiple data sources simultaneously</li> <li>Results can be stored back in Amazon S3</li> <li>Provides unified access to distributed data sources</li> </ul>"},{"location":"aws/database/athena/#best-practices","title":"Best Practices","text":"<ol> <li>Use columnar formats (Parquet or ORC) for cost optimization</li> <li>Implement appropriate compression methods</li> <li>Partition data effectively</li> <li>Optimize file sizes</li> <li>Utilize federated queries when dealing with multiple data sources</li> </ol>"},{"location":"aws/database/aurora/","title":"Aurora","text":""},{"location":"aws/database/aurora/#overview","title":"Overview","text":"<p>Amazon Aurora represents AWS\u2019s proprietary database technology, offering compatibility with both PostgreSQL and MySQL. This compatibility ensures existing database drivers work seamlessly with Aurora deployments. The service demonstrates significant performance improvements over traditional RDS implementations, showing up to 5x better performance compared to MySQL and 3x compared to PostgreSQL on RDS.</p>"},{"location":"aws/database/aurora/#technical-capabilities","title":"Technical Capabilities","text":"<p>Aurora\u2019s storage infrastructure automatically scales in 10GB increments, supporting databases up to 128TB. The service supports up to 15 replicas with industry-leading replication performance, maintaining sub-10 millisecond replica lag. Built with high availability as a core feature, Aurora provides instantaneous failover capabilities, though it comes at a 20% cost premium over standard RDS offerings.</p>"},{"location":"aws/database/aurora/#high-availability-architecture","title":"High Availability Architecture","text":"<p>Aurora implements a sophisticated high availability model through its distributed storage system. Data is automatically replicated across three Availability Zones with six copies maintained for redundancy. The system requires four copies for write operations and three copies for read operations, ensuring data durability and availability. Storage is distributed across hundreds of volumes with self-healing peer-to-peer replication.</p> <p>The primary instance handles write operations while up to 15 read replicas can serve read requests. Master failover occurs automatically within 30 seconds, and the service supports cross-region replication for global deployment scenarios.</p>"},{"location":"aws/database/aurora/#core-features","title":"Core Features","text":"<p>Aurora delivers enterprise-grade database capabilities including automated failover, comprehensive backup and recovery options, and robust security isolation. The service maintains industry compliance standards while offering push-button scaling capabilities. Operational tasks such as patching and maintenance occur without downtime, complemented by advanced monitoring capabilities.</p> <p>A distinctive feature called Backtrack allows point-in-time restoration without relying on traditional backups, offering flexible recovery options.</p>"},{"location":"aws/database/aurora/#security-framework","title":"Security Framework","text":""},{"location":"aws/database/aurora/#encryption-capabilities","title":"Encryption Capabilities","text":"<p>Aurora provides comprehensive encryption options both at rest and in transit. Database encryption uses AWS KMS and must be configured during instance launch. Important considerations include: - Read replicas can only be encrypted if the master database is encrypted - Encrypting an unencrypted database requires creating an encrypted snapshot and restoration - TLS encryption is enabled by default for data in transit</p>"},{"location":"aws/database/aurora/#access-control","title":"Access Control","text":"<p>The service integrates with AWS IAM for authentication, allowing database access through IAM roles instead of traditional username/password combinations. Network access is controlled through Security Groups, though direct SSH access is restricted except in RDS Custom deployments.</p>"},{"location":"aws/database/aurora/#audit-and-monitoring","title":"Audit and Monitoring","text":"<p>Aurora supports audit logging with CloudWatch Logs integration for extended retention periods, enabling comprehensive activity tracking and compliance monitoring.</p>"},{"location":"aws/database/aurora/#migration-and-backup-considerations","title":"Migration and Backup Considerations","text":"<p>When deploying Aurora, organizations should consider their backup strategy, migration paths, and replication requirements. The service\u2019s automatic storage scaling and backup capabilities simplify operational management, while its compatibility with existing MySQL and PostgreSQL applications facilitates smooth migrations from traditional database deployments.</p>"},{"location":"aws/database/dynamodb/","title":"DynamoDB","text":""},{"location":"aws/database/dynamodb/#introduction-to-dynamodb","title":"Introduction to DynamoDB","text":"<p>Amazon DynamoDB is a fully managed, serverless, key-value NoSQL database designed for high-performance, scalable applications. Developed by Amazon Web Services, it provides seamless and consistent single-digit millisecond latency at any scale, making it an ideal choice for modern cloud-native and distributed applications.</p>"},{"location":"aws/database/dynamodb/#core-architectural-concepts","title":"Core Architectural Concepts","text":""},{"location":"aws/database/dynamodb/#table-structure","title":"Table Structure","text":"<p>DynamoDB organizes data in tables, which are collections of items sharing a similar structure. Each item in a table is identified by a primary key, which can be simple (partition key) or composite (partition key and sort key). This design enables efficient data retrieval and supports complex querying strategies.</p>"},{"location":"aws/database/dynamodb/#primary-key-types","title":"Primary Key Types","text":"<ol> <li>Simple Primary Key: Consists of only a partition key, ensuring unique identification of items within the table.</li> <li>Composite Primary Key: Combines a partition key with a sort key, allowing multiple items to share the same partition key while maintaining unique identification through the sort key combination.</li> </ol>"},{"location":"aws/database/dynamodb/#capacity-unit-calculations","title":"Capacity Unit Calculations","text":""},{"location":"aws/database/dynamodb/#read-capacity-units-rcus","title":"Read Capacity Units (RCUs)","text":"<p>RCUs represent the number of reads per second for items up to 4 KB in size.</p> <p>Calculation Formula: <pre><code>Strongly Consistent RCUs = (Size of Item / 4 KB) \u00d7 Number of Reads per Second\nEventual Consistent RCUs = (Size of Item / 4 KB) \u00d7 Number of Reads per Second \u00d7 0.5\n</code></pre></p>"},{"location":"aws/database/dynamodb/#read-capacity-examples","title":"Read Capacity Examples:","text":"<ul> <li>4 KB item, 1 strongly consistent read/second: 1 RCU</li> <li>4 KB item, 1 eventual consistent read/second: 0.5 RCU</li> <li>8 KB item, 1 strongly consistent read/second: 2 RCUs</li> <li>8 KB item, 1 eventual consistent read/second: 1 RCU</li> <li>4 KB item, 10 strongly consistent reads/second: 10 RCUs</li> <li>4 KB item, 10 eventual consistent reads/second: 5 RCUs</li> </ul>"},{"location":"aws/database/dynamodb/#write-capacity-units-wcus","title":"Write Capacity Units (WCUs)","text":"<p>WCUs represent the number of writes per second for items up to 1 KB in size.</p> <p>Calculation Formula: <pre><code>WCUs = (Size of Item / 1 KB) \u00d7 Number of Writes per Second\n</code></pre></p>"},{"location":"aws/database/dynamodb/#write-capacity-examples","title":"Write Capacity Examples:","text":"<ul> <li>1 KB item, 1 write/second: 1 WCU</li> <li>2 KB item, 1 write/second: 2 WCUs</li> <li>1 KB item, 10 writes/second: 10 WCUs</li> </ul>"},{"location":"aws/database/dynamodb/#practical-capacity-planning","title":"Practical Capacity Planning","text":"<ol> <li>Estimate average item size</li> <li>Determine peak read/write requirements</li> <li>Calculate base RCUs and WCUs</li> <li>Add buffer for unexpected traffic</li> <li>Consider using auto-scaling</li> </ol>"},{"location":"aws/database/dynamodb/#data-consistency-and-pricing","title":"Data Consistency and Pricing","text":""},{"location":"aws/database/dynamodb/#consistency-models","title":"Consistency Models","text":""},{"location":"aws/database/dynamodb/#eventual-consistent-reads","title":"Eventual Consistent Reads","text":"<ul> <li>Default read model in DynamoDB</li> <li>Consumes 0.5 Read Capacity Units (RCUs) per 4 KB</li> <li>Typical cost: Approximately 50% cheaper than strong consistent reads</li> <li>Reflects changes within 1 second across database replicas</li> </ul>"},{"location":"aws/database/dynamodb/#strong-consistent-reads","title":"Strong Consistent Reads","text":"<ul> <li>Guarantees most recent write</li> <li>Consumes 1 Read Capacity Unit (RCU) per 4 KB</li> <li>Provides immediate data consistency</li> <li>Approximately double the cost of eventual consistent reads</li> </ul>"},{"location":"aws/database/dynamodb/#detailed-cost-breakdown","title":"Detailed Cost Breakdown","text":""},{"location":"aws/database/dynamodb/#read-capacity-unit-pricing","title":"Read Capacity Unit Pricing","text":"<ul> <li>Eventual Consistent Reads: $0.25 per million read request units</li> <li>Strong Consistent Reads: $0.50 per million read request units</li> <li>On-Demand Mode: Pricing varies by region and request volume</li> <li>Provisioned Mode: Predictable pricing based on pre-allocated capacity</li> </ul>"},{"location":"aws/database/dynamodb/#write-capacity-pricing","title":"Write Capacity Pricing","text":"<ul> <li>Standard Write Units: $0.47 per million write request units</li> <li>Pricing varies by region and specific AWS configuration</li> </ul>"},{"location":"aws/database/dynamodb/#storage-costs","title":"Storage Costs","text":"<ul> <li>First 25 TB per month: $0.25 per GB</li> <li>Over 25 TB: Reduced rates apply</li> <li>Incremental storage charges for backups and global tables</li> </ul>"},{"location":"aws/database/dynamodb/#data-model-and-attributes","title":"Data Model and Attributes","text":"<p>Items in DynamoDB can contain attributes of various types, including: - String - Number - Binary - Boolean - List - Map - String Set - Number Set - Binary Set</p> <p>Each attribute supports flexible schema design, enabling developers to adapt data structures without extensive migrations.</p>"},{"location":"aws/database/dynamodb/#performance-and-scaling","title":"Performance and Scaling","text":""},{"location":"aws/database/dynamodb/#readwrite-capacity-modes","title":"Read/Write Capacity Modes","text":"<p>DynamoDB offers two capacity modes to manage performance and cost:</p>"},{"location":"aws/database/dynamodb/#provisioned-mode","title":"Provisioned Mode","text":"<p>Developers specify expected read and write capacity units in advance. The system allocates dedicated resources to maintain performance, with options for manual or auto-scaling adjustments.</p>"},{"location":"aws/database/dynamodb/#on-demand-mode","title":"On-Demand Mode","text":"<p>Automatically scales to accommodate varying workloads without pre-planning capacity. Ideal for unpredictable traffic patterns and applications with sporadic access patterns.</p>"},{"location":"aws/database/dynamodb/#secondary-indexes","title":"Secondary Indexes","text":""},{"location":"aws/database/dynamodb/#global-secondary-indexes-gsi","title":"Global Secondary Indexes (GSI)","text":"<p>GSIs provide alternative query paths across the entire table, independent of the primary key. Key characteristics include: - Can be created on any table attribute - Support different partition and sort keys from the base table - Consume additional read capacity units - Enable complex querying strategies beyond the primary key</p>"},{"location":"aws/database/dynamodb/#local-secondary-indexes-lsi","title":"Local Secondary Indexes (LSI)","text":"<p>LSIs share the table\u2019s partition key but offer alternative sort key configurations. Distinguishing features: - Created during table creation - Limited to five per table - Use the same partition key as the base table - Consume storage from the base table\u2019s provisioned capacity</p>"},{"location":"aws/database/dynamodb/#data-consistency-and-replication","title":"Data Consistency and Replication","text":""},{"location":"aws/database/dynamodb/#consistency-models_1","title":"Consistency Models","text":"<ul> <li>Eventually Consistent Reads: Default mode with lower latency</li> <li>Strong Consistent Reads: Guarantees retrieval of the most recent write, with slightly higher latency</li> </ul>"},{"location":"aws/database/dynamodb/#global-tables","title":"Global Tables","text":"<p>Supports multi-region, multi-master replication, enabling: - Active-active database configurations - Low-latency global access - Automatic conflict resolution</p>"},{"location":"aws/database/dynamodb/#security-and-access-control","title":"Security and Access Control","text":""},{"location":"aws/database/dynamodb/#authentication-and-authorization","title":"Authentication and Authorization","text":"<ul> <li>Integrates with AWS Identity and Access Management (IAM)</li> <li>Granular access controls at table and item levels</li> <li>Support for encryption at rest using AWS Key Management Service</li> </ul>"},{"location":"aws/database/dynamodb/#use-cases","title":"Use Cases","text":"<p>DynamoDB excels in scenarios requiring: - High-velocity web and mobile applications - Real-time bidding platforms - Gaming leaderboards - IoT data storage - Session management - Metadata caching</p>"},{"location":"aws/database/dynamodb/#cost-optimization-strategies","title":"Cost Optimization Strategies","text":"<ul> <li>Utilize on-demand capacity for unpredictable workloads</li> <li>Implement Time-to-Live (TTL) for automatic data expiration</li> <li>Use compression and efficient indexing</li> <li>Monitor and adjust capacity settings regularly</li> </ul>"},{"location":"aws/database/dynamodb/#limitations-and-considerations","title":"Limitations and Considerations","text":"<ul> <li>Maximum item size: 400 KB</li> <li>Maximum attribute name length: 64 KB</li> <li>Complex joins not natively supported</li> <li>Scan operations can be costly for large datasets</li> </ul>"},{"location":"aws/database/dynamodb/#best-practices","title":"Best Practices","text":"<ul> <li>Design with access patterns in mind</li> <li>Minimize the number of secondary indexes</li> <li>Distribute partition key values evenly</li> <li>Use compression for large attributes</li> <li>Implement caching layers for read-heavy workloads</li> </ul>"},{"location":"aws/database/dynamodb/#conclusion","title":"Conclusion","text":"<p>AWS DynamoDB represents a powerful, flexible NoSQL database solution that combines scalability, performance, and ease of management. By understanding its architectural principles and leveraging its advanced features, developers can build robust, high-performance distributed applications.</p>"},{"location":"aws/database/elasticache/","title":"ElastiCache","text":""},{"location":"aws/database/elasticache/#overview","title":"Overview","text":"<p>AWS ElastiCache provides managed Redis or Memcached services, serving as in-memory databases that deliver high performance with low latency. Similar to how RDS manages relational databases, ElastiCache handles the operational complexities of caching solutions. AWS manages all aspects including OS maintenance, patching, optimization, configuration, monitoring, failure recovery, and backups.</p>"},{"location":"aws/database/elasticache/#redis-cluster-mode","title":"Redis Cluster mode","text":"<p>Redis Cluster Mode refers to a distributed implementation of Redis that automatically partitions data across multiple nodes in a cluster. It provides high availability and horizontal scaling while maintaining the simplicity and performance Redis is known for. While using Redis with cluster mode enabled, there are some limitations: - You cannot manually promote any of the replica nodes to primary. - Multi-AZ is required. - You can only change the structure of a cluster, the node type, and the number of nodes by restoring from a backup.</p> <p>All the nodes in a Redis cluster (cluster mode enabled or cluster mode disabled) must reside in the same region.</p>"},{"location":"aws/database/elasticache/#example-usages","title":"Example usages","text":""},{"location":"aws/database/elasticache/#database-cache-pattern","title":"Database Cache Pattern","text":"<p>In this architecture, applications first query ElastiCache for data. When cache misses occur, the application retrieves data from the primary database (typically RDS), then stores it in ElastiCache for future use. This pattern significantly reduces database load for read-intensive workloads. However, implementing an effective cache invalidation strategy becomes crucial to maintain data freshness.</p>"},{"location":"aws/database/elasticache/#user-session-store-pattern","title":"User Session Store Pattern","text":"<p>ElastiCache excels at managing user session data in distributed applications. When users authenticate with any application instance, their session data is written to ElastiCache. This allows other application instances to retrieve the session data, enabling seamless user experiences across multiple application servers and making applications truly stateless.</p>"},{"location":"aws/database/elasticache/#redis-vs-memcached-comparison","title":"Redis vs Memcached Comparison","text":""},{"location":"aws/database/elasticache/#redis-capabilities","title":"Redis Capabilities","text":"<p>Redis offers robust features for enterprise applications. It supports Multi-AZ deployments with automatic failover capabilities and read replicas for enhanced read scaling and high availability. Data durability is ensured through AOF (Append-Only File) persistence, complemented by comprehensive backup and restore functionality. Redis also provides advanced data structures including Sets and Sorted Sets.</p>"},{"location":"aws/database/elasticache/#memcached-features","title":"Memcached Features","text":"<p>Memcached focuses on simplicity and multi-threaded performance. It supports data partitioning through multi-node sharding but doesn\u2019t provide replication for high availability. Being non-persistent by design, it offers basic backup and restore capabilities through a serverless approach. Its multi-threaded architecture makes it particularly efficient for specific use cases.</p>"},{"location":"aws/database/elasticache/#caching-strategies","title":"Caching Strategies","text":""},{"location":"aws/database/elasticache/#lazy-loading-cache-aside","title":"Lazy Loading (Cache-Aside)","text":"<p>This strategy loads data into the cache only when necessary. Its advantages include efficient cache space utilization and resilience to node failures. However, it introduces additional latency on cache misses due to the required database roundtrip. Data staleness can occur when database updates aren\u2019t immediately reflected in the cache.</p>"},{"location":"aws/database/elasticache/#write-through-caching","title":"Write-Through Caching","text":"<p>Write-through caching updates both the database and cache simultaneously. This approach ensures cache consistency and enables quick reads. However, it introduces write latency due to the dual update requirement. New data remains unavailable until explicitly added to the database, though this limitation can be mitigated by combining with lazy loading.</p>"},{"location":"aws/database/elasticache/#cache-management","title":"Cache Management","text":""},{"location":"aws/database/elasticache/#eviction-policies-and-ttl","title":"Eviction Policies and TTL","text":"<p>Cache entries can be removed through:  - Explicit deletion - Memory pressure-based eviction (LRU) - Time-to-live (TTL) expiration TTL proves particularly valuable for time-sensitive data such as leaderboards, comments, and activity streams, with durations ranging from seconds to days. Memory pressure and frequent evictions indicate a need for cache scaling.</p>"},{"location":"aws/database/elasticache/#implementation-considerations","title":"Implementation Considerations","text":"<p>Implementing ElastiCache requires significant application code modifications to properly handle caching logic. Developers must carefully consider caching strategies, data consistency requirements, and failure scenarios. The chosen caching pattern should align with the application\u2019s specific needs regarding data freshness, read/write patterns, and performance requirements.</p> <p>The investment in proper cache implementation pays off through reduced database load, improved application response times, and enhanced scalability. However, this requires careful consideration of cache invalidation strategies, error handling, and monitoring to ensure optimal performance.</p>"},{"location":"aws/database/rds/","title":"Relational Database Service (RDS)","text":""},{"location":"aws/database/rds/#overview","title":"Overview","text":"<p>Amazon RDS (Relational Database Service) provides managed SQL databases in the cloud. It supports multiple database engines including PostgreSQL, MySQL, MariaDB, Oracle, Microsoft SQL Server, IBM DB2, and Amazon\u2019s proprietary Aurora database. This service enables organizations to operate and scale relational databases without managing the underlying infrastructure.</p>"},{"location":"aws/database/rds/#core-benefits-over-ec2-hosted-databases","title":"Core Benefits Over EC2-Hosted Databases","text":"<p>RDS provides significant advantages through its managed service model. AWS handles routine database administration tasks including automated provisioning, operating system patching, and continuous backups with point-in-time restore capabilities. The service includes comprehensive monitoring dashboards and supports both vertical and horizontal scaling options. Storage is provided through Amazon EBS, ensuring reliable persistence. While direct SSH access to database instances isn\u2019t available, this limitation supports enhanced security and consistent management.</p>"},{"location":"aws/database/rds/#storage-management","title":"Storage Management","text":""},{"location":"aws/database/rds/#auto-scaling-capabilities","title":"Auto Scaling Capabilities","text":"<p>RDS features dynamic storage scaling, automatically increasing storage capacity when free space becomes limited. This automation requires setting a maximum storage threshold and triggers when free storage drops below 10% for at least 5 minutes, with a minimum 6-hour interval between modifications. This feature particularly benefits applications with unpredictable storage requirements and supports all RDS database engines.</p>"},{"location":"aws/database/rds/#high-availability-and-replication","title":"High Availability and Replication","text":""},{"location":"aws/database/rds/#read-replicas","title":"Read Replicas","text":"<p>RDS supports up to 15 read replicas, which can be deployed within the same Availability Zone, across different AZs, or even across regions. These replicas use asynchronous replication, resulting in eventually consistent reads. While replicas can be promoted to standalone databases, applications must explicitly manage connection strings to utilize them effectively. Read replicas excel at handling read-heavy workloads, particularly for analytical queries and reporting functions that might otherwise impact production database performance.</p>"},{"location":"aws/database/rds/#network-cost-considerations","title":"Network Cost Considerations","text":"<p>AWS typically charges for data transfer between Availability Zones. However, RDS read replicas within the same region are exempt from these transfer fees, making them cost-effective for scaling read operations.</p>"},{"location":"aws/database/rds/#multi-az-deployment","title":"Multi-AZ Deployment","text":"<p>Multi-AZ deployments provide enhanced availability through synchronous replication to a standby instance in a different Availability Zone. This configuration uses a single DNS name, enabling automatic application failover. The setup protects against various failure scenarios including AZ outages, network issues, and instance or storage failures, all while requiring no application changes. Notably, Multi-AZ deployments focus on availability rather than scaling, though read replicas can be configured with Multi-AZ for comprehensive disaster recovery.</p>"},{"location":"aws/database/rds/#operational-flexibility","title":"Operational Flexibility","text":""},{"location":"aws/database/rds/#single-az-to-multi-az-migration","title":"Single-AZ to Multi-AZ Migration","text":"<p>Converting from Single-AZ to Multi-AZ deployment is a zero-downtime operation requiring no database shutdown. The process involves creating a snapshot, restoring it in a new Availability Zone, and establishing synchronization between instances. This seamless transition maintains database availability throughout the migration process.</p>"},{"location":"aws/database/rds/#performance-and-scaling","title":"Performance and Scaling","text":"<p>RDS provides both vertical and horizontal scaling options. Vertical scaling allows adjusting compute and memory resources, while horizontal scaling through read replicas distributes read workloads. The service automatically manages the underlying storage, supporting applications as they grow and their requirements evolve.</p>"},{"location":"aws/database/rds/#backup-and-recovery","title":"Backup and Recovery","text":"<p>The service includes automated backup capabilities with point-in-time recovery options. This feature enables restoration to any moment within the retention period, providing protection against data loss and corruption while maintaining business continuity.</p>"},{"location":"aws/database/rds/#amazon-rds-proxy","title":"Amazon RDS Proxy","text":"<p>Amazon RDS Proxy is a fully managed, highly available database proxy service that makes applications more scalable, more resilient to database failures, and more secure.</p>"},{"location":"aws/database/rds/#overview_1","title":"Overview","text":"<p>Amazon RDS Proxy acts as an intermediary layer between your applications and relational databases, enabling efficient connection management and improved database performance. This managed service helps applications maintain database connections, handle failovers more gracefully, and enhance security through IAM authentication and credentials management.</p>"},{"location":"aws/database/rds/#key-features-and-benefits","title":"Key Features and Benefits","text":""},{"location":"aws/database/rds/#connection-management","title":"Connection Management","text":"<p>RDS Proxy enables applications to pool and share database connections established with the database instance. Instead of each application instance maintaining its own database connections, the proxy manages a shared pool of connections, significantly reducing the connection management overhead.</p>"},{"location":"aws/database/rds/#performance-optimization","title":"Performance Optimization","text":"<p>By efficiently managing database connections, RDS Proxy substantially reduces the stress on database resources, including CPU and RAM utilization. The service minimizes the number of open connections and helps prevent connection timeouts, leading to better overall database performance and resource utilization.</p>"},{"location":"aws/database/rds/#high-availability-and-scalability","title":"High Availability and Scalability","text":"<p>The service is built with serverless architecture, automatically scaling to accommodate your application\u2019s needs without requiring manual intervention. RDS Proxy is designed for high availability with multi-AZ deployment support, ensuring continuous operation even during infrastructure failures.</p>"},{"location":"aws/database/rds/#enhanced-failover-support","title":"Enhanced Failover Support","text":"<p>One of the most significant advantages of RDS Proxy is its ability to reduce RDS and Aurora failover times by up to 66%. During failover events, the proxy manages connection handling, making the failover process more seamless for applications and reducing downtime.</p>"},{"location":"aws/database/rds/#database-compatibility","title":"Database Compatibility","text":"<p>RDS Proxy supports a wide range of popular database engines: Amazon RDS Databases: - MySQL - PostgreSQL - MariaDB - Microsoft SQL Server</p> <p>Amazon Aurora Databases: - Aurora MySQL - Aurora PostgreSQL</p>"},{"location":"aws/database/rds/#implementation-simplicity","title":"Implementation Simplicity","text":"<p>Most applications can implement RDS Proxy without requiring any code modifications. This seamless integration allows organizations to improve their database infrastructure without investing in application rewrites or extensive development efforts.</p>"},{"location":"aws/database/rds/#security-features","title":"Security Features","text":"<p>RDS Proxy incorporates robust security features to protect your database infrastructure. It enforces IAM Authentication for database access and integrates with AWS Secrets Manager for secure credential storage and management. This integration ensures that database credentials are securely stored and rotated according to your security policies.</p>"},{"location":"aws/database/rds/#network-security","title":"Network Security","text":"<p>RDS Proxy is designed with security in mind and is never publicly accessible. All access to the proxy must occur within your Amazon VPC, ensuring that your database connections remain secure and isolated within your private network infrastructure.</p>"},{"location":"aws/database/rds/#best-practices","title":"Best Practices","text":"<p>When implementing RDS Proxy, consider the following recommendations: - Configure appropriate IAM roles and permissions for secure access - Implement connection pooling strategies that align with your application\u2019s needs - Monitor proxy metrics to optimize performance and resource utilization - Review and adjust proxy settings based on your application\u2019s connection patterns</p>"},{"location":"aws/database/rds/#conclusion","title":"Conclusion","text":"<p>Amazon RDS Proxy provides a robust solution for managing database connections, improving application scalability, and enhancing database security. Its fully managed nature, combined with advanced features for connection pooling and security management, makes it an invaluable tool for organizations looking to optimize their database infrastructure while maintaining high availability and performance.</p>"},{"location":"aws/management/appconfig/","title":"AppConfig","text":""},{"location":"aws/management/appconfig/#overview","title":"Overview","text":"<p>AWS AppConfig is a dynamic configuration management service that enables you to deploy configuration changes to applications quickly, safely, and independently of code deployments.</p>"},{"location":"aws/management/appconfig/#key-features","title":"Key Features","text":""},{"location":"aws/management/appconfig/#dynamic-configuration-management","title":"Dynamic Configuration Management","text":"<ul> <li>Deploy configuration changes without application restarts</li> <li>Supports various use cases:</li> <li>Feature flags</li> <li>Application tuning</li> <li>Allow/block listing</li> <li>Dynamic parameter adjustments</li> </ul>"},{"location":"aws/management/appconfig/#broad-platform-support","title":"Broad Platform Support","text":"<p>Compatible with multiple AWS compute services: * EC2 instances * AWS Lambda * Amazon ECS * Amazon EKS</p>"},{"location":"aws/management/appconfig/#safe-deployment","title":"Safe Deployment","text":"<ul> <li>Gradual configuration rollout</li> <li>Built-in rollback mechanisms</li> <li>Prevents widespread issues from configuration changes</li> </ul>"},{"location":"aws/management/appconfig/#configuration-validation","title":"Configuration Validation","text":"<p>Two validation methods: 1. JSON Schema Validation    * Performs syntactic checks    * Ensures configuration structure meets defined requirements 2. Lambda Function Validation    * Enables custom semantic checks    * Allows running complex validation logic via custom code</p>"},{"location":"aws/management/appconfig/#benefits","title":"Benefits","text":"<ul> <li>Reduce deployment risks</li> <li>Enable dynamic application tuning</li> <li>Separate configuration management from code deployments</li> <li>Provide fine-grained control over configuration changes</li> </ul>"},{"location":"aws/management/billing/","title":"Billing and Cost","text":"<p>AWS Billing and Cost Management is a suite of tools that helps you monitor, manage, and optimize your AWS usage and costs. It provides features for budgeting, analyzing usage patterns, and forecasting future expenses to ensure cost-effective resource management.</p>"},{"location":"aws/management/billing/#key-features-and-characteristics","title":"Key Features and Characteristics","text":""},{"location":"aws/management/billing/#1-billing-dashboard","title":"1. Billing Dashboard","text":"<ul> <li>Provides a high-level overview of your AWS usage and charges.</li> <li>Displays current and forecasted month-to-date costs.</li> <li>Offers insights into service-specific expenditures.</li> </ul>"},{"location":"aws/management/billing/#2-cost-allocation-tags","title":"2. Cost Allocation Tags","text":"<ul> <li>User-Defined Tags: Tags you define and apply to categorize AWS resources (e.g., <code>Department: Finance</code>).</li> <li>AWS-Generated Tags: Automatically generated by AWS for certain resources.</li> <li>Use Case: Track costs by projects, departments, or teams.</li> </ul>"},{"location":"aws/management/billing/#3-cost-and-usage-reports-cur","title":"3. Cost and Usage Reports (CUR)","text":"<ul> <li>Comprehensive reports detailing AWS resource usage and costs.</li> <li>Delivered to an Amazon S3 bucket in CSV or Parquet format.</li> <li>Can be integrated with analytics tools like Amazon Athena or Amazon QuickSight for further analysis.</li> </ul>"},{"location":"aws/management/billing/#4-aws-budgets","title":"4. AWS Budgets","text":"<ul> <li>Set spending thresholds for your account or specific services.</li> <li>Types of budgets:</li> <li>Cost Budgets: Track total spending.</li> <li>Usage Budgets: Monitor service usage (e.g., EC2 instance hours).</li> <li>Savings Plans Budgets: Track Savings Plans utilization.</li> <li>Reservation Budgets: Monitor Reserved Instances (RIs).</li> <li>Configurable alerts via email or SNS notifications.</li> </ul>"},{"location":"aws/management/billing/#5-cost-explorer","title":"5. Cost Explorer","text":"<ul> <li>Interactive tool for visualizing and analyzing AWS costs and usage.</li> <li>Features include:</li> <li>Custom filtering by service, tag, or linked account.</li> <li>Granular breakdowns by hourly, daily, or monthly usage.</li> <li>Forecasting based on historical trends.</li> </ul>"},{"location":"aws/management/billing/#6-savings-plans-and-reserved-instances","title":"6. Savings Plans and Reserved Instances","text":"<ul> <li>Savings Plans: Flexible pricing models offering cost savings in exchange for a usage commitment (e.g., compute hours).</li> <li>Reserved Instances (RIs): Upfront or partial upfront payment options for EC2 instances with significant savings over On-Demand pricing.</li> </ul>"},{"location":"aws/management/billing/#7-free-tier-usage-tracking","title":"7. Free Tier Usage Tracking","text":"<ul> <li>Monitors free tier usage limits to avoid unexpected charges.</li> <li>Notifications when usage approaches or exceeds free tier limits.</li> </ul>"},{"location":"aws/management/billing/#8-consolidated-billing","title":"8. Consolidated Billing","text":"<ul> <li>Allows organizations to consolidate billing across multiple AWS accounts under a management account.</li> <li>Benefits:</li> <li>Single bill for all accounts.</li> <li>Cost-sharing and pooling of usage discounts (e.g., volume discounts).</li> </ul>"},{"location":"aws/management/billing/#9-service-quotas-integration","title":"9. Service Quotas Integration","text":"<ul> <li>Monitors service usage against AWS quotas.</li> <li>Helps prevent unexpected charges due to exceeding usage limits.</li> </ul>"},{"location":"aws/management/billing/#10-tax-settings-and-invoicing","title":"10. Tax Settings and Invoicing","text":"<ul> <li>Tax Settings: Configure tax information specific to your region (e.g., VAT/GST).</li> <li>Invoicing: Access detailed, downloadable invoices for each billing period.</li> </ul>"},{"location":"aws/management/billing/#11-currency-conversion","title":"11. Currency Conversion","text":"<ul> <li>Supports viewing charges in your preferred currency.</li> <li>AWS bills are always generated in USD, with conversions provided for reference.</li> </ul>"},{"location":"aws/management/billing/#12-anomaly-detection","title":"12. Anomaly Detection","text":"<ul> <li>Automatically identifies unusual spending patterns.</li> <li>Provides actionable insights to investigate cost anomalies.</li> </ul>"},{"location":"aws/management/billing/#13-integration-with-aws-organizations","title":"13. Integration with AWS Organizations","text":"<ul> <li>Enables centralized billing and cost management for multiple accounts.</li> <li>Linked accounts share discounts and usage data with the management account.</li> </ul>"},{"location":"aws/management/billing/#14-reports-and-notifications","title":"14. Reports and Notifications","text":"<ul> <li>Cost Reports: Auto-generated reports summarizing costs by service or account.</li> <li>Alerts and Notifications:</li> <li>Triggered when budgets or thresholds are exceeded.</li> <li>Configurable for email and SNS delivery.</li> </ul>"},{"location":"aws/management/billing/#15-third-party-tool-compatibility","title":"15. Third-Party Tool Compatibility","text":"<ul> <li>Compatible with external cost management tools via APIs.</li> <li>Examples include tools for advanced cost analytics or enterprise reporting.</li> </ul>"},{"location":"aws/management/billing/#iam-access-to-billing-and-cost-management","title":"IAM Access to Billing and Cost Management","text":"<ul> <li>By default, the root user has full access to all billing and cost management tools.</li> <li>IAM User Access Activation:</li> <li>By default, IAM user access to the Billing and Cost Management console is disabled.</li> <li>The root user or an authorized IAM user must enable this access in the Billing and Cost Management preferences.</li> <li>Once activated, IAM users with appropriate permissions can access the Billing and Cost Management console.</li> <li>Common Permissions:</li> <li><code>aws-portal:*</code>: Grants full access to the Billing and Cost Management console.</li> <li><code>ce:*</code>: Provides access to Cost Explorer API and related tools.</li> <li><code>cur:*</code>: Grants access to Cost and Usage Reports.</li> <li><code>budgets:*</code>: Allows management of AWS Budgets.</li> <li>Best Practices:</li> <li>Use IAM policies to grant least privilege.</li> <li>Restrict access to sensitive billing data to authorized users only.</li> </ul> <p>AWS Billing and Cost Management tools are essential for ensuring efficient resource utilization and cost control, making them integral to managing AWS accounts effectively.</p>"},{"location":"aws/management/cloudformation/","title":"CloudFormation","text":""},{"location":"aws/management/cloudformation/#introduction","title":"Introduction","text":"<p>AWS CloudFormation is a service that enables you to model, provision, and manage AWS resources by treating infrastructure as code. It allows you to create and manage a collection of AWS resources using templates written in JSON or YAML format. Instead of manually creating and configuring resources through the AWS Console, you can describe your desired infrastructure in a template file, and CloudFormation handles the rest.</p>"},{"location":"aws/management/cloudformation/#core-concepts","title":"Core Concepts","text":""},{"location":"aws/management/cloudformation/#templates","title":"Templates","text":"<p>Templates are JSON or YAML files that serve as blueprints for building AWS environments. They define all the AWS resources and their properties. A template can include:</p> <ul> <li>Resources</li> <li>Parameters</li> <li>Mappings</li> <li>Conditions</li> <li>Outputs</li> <li>Metadata</li> </ul>"},{"location":"aws/management/cloudformation/#stacks","title":"Stacks","text":"<p>A stack is a collection of AWS resources that you manage as a single unit. All resources described in a template are managed as part of the stack. Key aspects include:</p> <ul> <li>Creation, updating, and deletion of resources as a group</li> <li>Stack policies for resource protection</li> <li>Role-based access control</li> <li>Change sets for reviewing modifications</li> </ul>"},{"location":"aws/management/cloudformation/#change-sets","title":"Change Sets","text":"<p>Change sets let you preview how proposed changes to a stack might impact your running resources before implementing them.</p>"},{"location":"aws/management/cloudformation/#template-structure","title":"Template Structure","text":"<pre><code>AWSTemplateFormatVersion: \"2010-09-09\"\nDescription: \"A sample template\"\n\nParameters:\n  EnvironmentType:\n    Type: String\n    AllowedValues: \n      - prod\n      - dev\n\nMappings:\n  RegionMap:\n    us-east-1:\n      AMI: \"ami-0123456789\"\n\nResources:\n  MyEC2Instance:\n    Type: \"AWS::EC2::Instance\"\n    Properties:\n      InstanceType: t2.micro\n      ImageId: !FindInMap [RegionMap, !Ref \"AWS::Region\", AMI]\n\nOutputs:\n  InstanceID:\n    Description: \"Instance ID\"\n    Value: !Ref MyEC2Instance\n</code></pre>"},{"location":"aws/management/cloudformation/#template-components","title":"Template Components","text":""},{"location":"aws/management/cloudformation/#parameters","title":"Parameters","text":"<p>Parameters enable you to input custom values to your template each time you create or update a stack.</p> <pre><code>Parameters:\n  InstanceType:\n    Description: EC2 instance type\n    Type: String\n    Default: t2.micro\n    AllowedValues:\n      - t2.micro\n      - t2.small\n      - t2.medium\n</code></pre>"},{"location":"aws/management/cloudformation/#mappings","title":"Mappings","text":"<p>Mappings are fixed key-value pairs that you can use to specify conditional parameter values.</p> <pre><code>Mappings:\n  EnvironmentToInstanceType:\n    dev:\n      instanceType: t2.micro\n    prod:\n      instanceType: t2.small\n</code></pre>"},{"location":"aws/management/cloudformation/#resources","title":"Resources","text":"<p>Resources are the AWS components that will be created and configured.</p> <pre><code>Resources:\n  MyS3Bucket:\n    Type: AWS::S3::Bucket\n    Properties:\n      BucketName: !Sub \"${AWS::StackName}-bucket\"\n      VersioningConfiguration:\n        Status: Enabled\n</code></pre>"},{"location":"aws/management/cloudformation/#outputs","title":"Outputs","text":"<p>Outputs declare values that you can import into other stacks or view in the AWS Console.</p> <pre><code>Outputs:\n  BucketName:\n    Description: Name of the created bucket\n    Value: !Ref MyS3Bucket\n    Export:\n      Name: !Sub \"${AWS::StackName}-BucketName\"\n</code></pre>"},{"location":"aws/management/cloudformation/#intrinsic-functions","title":"Intrinsic Functions","text":"<p>CloudFormation provides several built-in functions for template management:</p> <ul> <li><code>!Ref</code> - References parameters or resources</li> <li><code>!GetAtt</code> - Gets an attribute from a resource</li> <li><code>!Sub</code> - Substitutes variables in a string</li> <li><code>!Join</code> - Joins values with a delimiter</li> <li><code>!Split</code> - Splits a string into a list</li> <li><code>!Select</code> - Selects an item from a list</li> <li><code>!FindInMap</code> - Returns a named value from a mapping</li> </ul>"},{"location":"aws/management/cloudformation/#best-practices","title":"Best Practices","text":""},{"location":"aws/management/cloudformation/#template-design","title":"Template Design","text":"<ul> <li>Use descriptive names for resources</li> <li>Implement proper tagging strategy</li> <li>Use parameters for values that change</li> <li>Implement proper error handling</li> <li>Use nested stacks for reusable components</li> </ul>"},{"location":"aws/management/cloudformation/#security","title":"Security","text":"<ul> <li>Use IAM roles and policies</li> <li>Implement stack policies</li> <li>Enable encryption where possible</li> <li>Use VPC endpoints</li> <li>Follow the principle of least privilege</li> </ul>"},{"location":"aws/management/cloudformation/#cost-management","title":"Cost Management","text":"<ul> <li>Use cost allocation tags</li> <li>Implement lifecycle policies</li> <li>Consider reserved instances</li> <li>Monitor resource usage</li> </ul>"},{"location":"aws/management/cloudformation/#common-operations","title":"Common Operations","text":""},{"location":"aws/management/cloudformation/#creating-a-stack","title":"Creating a Stack","text":"<pre><code>aws cloudformation create-stack \\\n  --stack-name my-stack \\\n  --template-body file://template.yaml \\\n  --parameters ParameterKey=EnvironmentType,ParameterValue=prod\n</code></pre>"},{"location":"aws/management/cloudformation/#updating-a-stack","title":"Updating a Stack","text":"<pre><code>aws cloudformation update-stack \\\n  --stack-name my-stack \\\n  --template-body file://template.yaml\n</code></pre>"},{"location":"aws/management/cloudformation/#deleting-a-stack","title":"Deleting a Stack","text":"<pre><code>aws cloudformation delete-stack \\\n  --stack-name my-stack\n</code></pre>"},{"location":"aws/management/cloudformation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"aws/management/cloudformation/#common-issues","title":"Common Issues","text":"<ol> <li>Stack Creation Failures</li> <li>Check resource limits</li> <li>Verify IAM permissions</li> <li> <p>Review dependency order</p> </li> <li> <p>Update Failures</p> </li> <li>Use change sets to preview changes</li> <li>Check for protected resources</li> <li> <p>Verify resource properties</p> </li> <li> <p>Deletion Failures</p> </li> <li>Check for deletion policies</li> <li>Verify termination protection</li> <li>Review stack dependencies</li> </ol>"},{"location":"aws/management/cloudformation/#integration-with-other-aws-services","title":"Integration with Other AWS Services","text":"<p>CloudFormation integrates with numerous AWS services:</p> <ul> <li>AWS Organizations</li> <li>AWS Config</li> <li>AWS Service Catalog</li> <li>AWS Systems Manager</li> <li>AWS CodePipeline</li> <li>AWS CodeBuild</li> <li>AWS CodeDeploy</li> </ul>"},{"location":"aws/management/cloudformation/#tools-and-resources","title":"Tools and Resources","text":""},{"location":"aws/management/cloudformation/#development-tools","title":"Development Tools","text":"<ul> <li>AWS CloudFormation Designer</li> <li>AWS CLI</li> <li>AWS SDK</li> <li>IDE plugins</li> </ul>"},{"location":"aws/management/cloudformation/#validation-tools","title":"Validation Tools","text":"<ul> <li>cfn-lint</li> <li>CloudFormation Guard</li> <li>TaskCat</li> </ul>"},{"location":"aws/management/cloudformation/#best-practices-for-cicd","title":"Best Practices for CI/CD","text":"<ul> <li>Use version control for templates</li> <li>Implement automated testing</li> <li>Use change sets in deployment pipeline</li> <li>Maintain separate stacks for different environments</li> <li>Implement proper rollback strategies</li> </ul>"},{"location":"aws/management/cloudformation/#conclusion","title":"Conclusion","text":"<p>AWS CloudFormation is a powerful service for infrastructure as code that enables consistent and repeatable deployments of AWS resources. By following best practices and utilizing its features effectively, you can manage complex infrastructure efficiently and reliably.</p>"},{"location":"aws/management/cloudformation/#additional-resources","title":"Additional Resources","text":"<ul> <li>Official AWS CloudFormation Documentation</li> <li>AWS CloudFormation Sample Templates</li> <li>AWS CloudFormation Workshop</li> <li>CloudFormation Registry</li> </ul>"},{"location":"aws/management/sam/","title":"Serverless Application Model (SAM)","text":""},{"location":"aws/management/sam/#overview","title":"Overview","text":"<p>AWS SAM is a framework for developing and deploying serverless applications, providing a simplified approach to creating serverless infrastructure.</p>"},{"location":"aws/management/sam/#key-characteristics","title":"Key Characteristics","text":"<ul> <li>Configuration-as-Code using YAML</li> <li>Generates complex CloudFormation templates from simple SAM templates</li> <li>Full CloudFormation compatibility</li> <li>Supports comprehensive resource configuration</li> </ul>"},{"location":"aws/management/sam/#core-components","title":"Core Components","text":""},{"location":"aws/management/sam/#template-structure","title":"Template Structure","text":"<ul> <li>Uses special transform header: <code>Transform: 'AWS::Serverless-2016-10-31'</code></li> <li>Supports key serverless resources:</li> <li><code>AWS::Serverless::Function</code></li> <li><code>AWS::Serverless::Api</code></li> <li><code>AWS::Serverless::SimpleTable</code></li> </ul>"},{"location":"aws/management/sam/#deployment-process","title":"Deployment Process","text":"<ol> <li>Build: <code>sam build</code></li> <li>Prepares application locally</li> <li>Package: <code>sam package</code></li> <li>Transforms and uploads application code to S3</li> <li>Deploy: <code>sam deploy</code></li> <li>Creates/updates CloudFormation stack</li> <li>Provisions serverless resources</li> </ol>"},{"location":"aws/management/sam/#advanced-features","title":"Advanced Features","text":""},{"location":"aws/management/sam/#sam-accelerate","title":"SAM Accelerate","text":"<ul> <li>Reduces deployment latency</li> <li>Synchronization options:</li> <li><code>sam sync</code>: Full resource synchronization</li> <li><code>sam sync --code</code>: Quick code updates</li> <li><code>sam sync --watch</code>: Automatic file change detection</li> </ul>"},{"location":"aws/management/sam/#local-development-capabilities","title":"Local Development Capabilities","text":"<ul> <li><code>sam local start-lambda</code>: Local Lambda endpoint</li> <li><code>sam local invoke</code>: Invoke Lambda functions locally</li> <li><code>sam local start-api</code>: Local API Gateway simulation</li> <li><code>sam local generate-event</code>: Generate sample event payloads</li> </ul>"},{"location":"aws/management/sam/#codedeploy-integration","title":"CodeDeploy Integration","text":"<ul> <li>Native traffic shifting for Lambda functions</li> <li>Deployment strategies:</li> <li>Canary</li> <li>Linear</li> <li>All At Once</li> <li>Support for:</li> <li>Pre/Post traffic hooks</li> <li>Automated rollback</li> <li>CloudWatch Alarm triggers</li> </ul>"},{"location":"aws/management/sam/#policy-templates","title":"Policy Templates","text":"<p>Predefined IAM permission templates for Lambda functions: * <code>S3ReadPolicy</code>: S3 read permissions * <code>SQSPollerPolicy</code>: SQS queue polling * <code>DynamoDBCrudPolicy</code>: Database CRUD operations</p>"},{"location":"aws/management/sam/#multi-environment-support","title":"Multi-Environment Support","text":"<ul> <li>Configuration via <code>samconfig.toml</code></li> <li>Environment-specific deployments</li> <li>Example: <code>sam deploy --config-env dev</code></li> </ul>"},{"location":"aws/management/sam/#benefits","title":"Benefits","text":"<ul> <li>Simplified serverless development</li> <li>Rapid local testing</li> <li>Consistent deployment patterns</li> <li>Reduced infrastructure configuration complexity</li> <li>Seamless AWS service integration</li> </ul>"},{"location":"aws/management/ssm/","title":"Systems Manager Parameter Store (SSM)","text":""},{"location":"aws/management/ssm/#overview","title":"Overview","text":"<p>AWS Systems Manager Parameter Store offers a robust and secure service for configuration and secrets management. This centralized service provides a comprehensive solution for storing and managing configuration data, secrets, and other operational parameters within your AWS infrastructure.</p>"},{"location":"aws/management/ssm/#core-features","title":"Core Features","text":""},{"location":"aws/management/ssm/#secure-storage-and-encryption","title":"Secure Storage and Encryption","text":"<p>Parameter Store provides secure storage capabilities for both configuration data and sensitive information. The service seamlessly integrates with AWS Key Management Service (KMS), offering optional encryption capabilities to ensure the security of your stored parameters. This encryption integration allows organizations to maintain strict security standards while managing their configuration data.</p>"},{"location":"aws/management/ssm/#architecture-and-implementation","title":"Architecture and Implementation","text":"<p>The service is built on a serverless architecture, eliminating the need for infrastructure management. It offers excellent scalability to accommodate growing parameter storage needs and ensures durability through AWS\u2019s reliable infrastructure. Developers can easily interact with Parameter Store through a well-designed SDK, making implementation straightforward across various applications and services.</p>"},{"location":"aws/management/ssm/#version-control","title":"Version Control","text":"<p>Parameter Store maintains version tracking for all stored configurations and secrets. This versioning capability enables organizations to maintain a history of parameter changes, roll back to previous versions when needed, and audit configuration modifications over time.</p>"},{"location":"aws/management/ssm/#security-framework","title":"Security Framework","text":"<p>Security in Parameter Store is managed through AWS Identity and Access Management (IAM). This integration allows organizations to implement fine-grained access controls, ensuring that only authorized users and applications can access specific parameters. The IAM integration provides a robust security framework for managing access to sensitive configuration data.</p>"},{"location":"aws/management/ssm/#integration-capabilities","title":"Integration Capabilities","text":"<p>The service features comprehensive integration with Amazon EventBridge, enabling automated notifications based on parameter changes or specific events. This integration allows organizations to build event-driven architectures and automated workflows around their configuration management processes.</p> <p>Parameter Store also integrates seamlessly with AWS CloudFormation, enabling infrastructure as code practices and automated resource management. This integration allows organizations to include parameter management as part of their infrastructure deployment and management processes.</p>"},{"location":"aws/management/ssm/#service-tiers","title":"Service Tiers","text":"<p>Parameter Store offers two distinct service tiers to accommodate different organizational needs:</p>"},{"location":"aws/management/ssm/#standard-tier","title":"Standard Tier","text":"<p>The standard tier provides essential parameter management capabilities suitable for many applications and use cases.</p>"},{"location":"aws/management/ssm/#advanced-tier","title":"Advanced Tier","text":"<p>The advanced tier offers enhanced features and capabilities, including support for parameter policies and larger parameter values.</p> <p></p>"},{"location":"aws/management/ssm/#parameter-policies","title":"Parameter Policies","text":"<p>Advanced parameters in Parameter Store can leverage parameter policies, which provide additional control and automation capabilities. These policies enable organizations to manage their parameters more effectively:</p>"},{"location":"aws/management/ssm/#time-to-live-ttl-management","title":"Time-to-Live (TTL) Management","text":"<p>Parameter policies allow the assignment of expiration dates (TTL) to parameters. This capability is particularly valuable for managing sensitive data such as passwords, ensuring that such information is regularly updated or removed. The TTL functionality helps organizations maintain security compliance by forcing the update or deletion of sensitive parameters after a specified period.</p>"},{"location":"aws/management/ssm/#policy-flexibility","title":"Policy Flexibility","text":"<p>Parameter Store supports the concurrent assignment of multiple policies to a single parameter. This flexibility allows organizations to implement complex parameter management strategies that combine different policy types to meet specific security and operational requirements.</p>"},{"location":"aws/management/ssm/#best-practices","title":"Best Practices","text":"<p>When implementing Parameter Store, consider these recommended practices: - Implement appropriate encryption for sensitive parameters using KMS - Establish clear naming conventions for parameters to maintain organization - Utilize parameter policies for sensitive data management - Configure EventBridge rules for critical parameter changes - Implement proper IAM policies following the principle of least privilege</p>"},{"location":"aws/management/ssm/#conclusion","title":"Conclusion","text":"<p>AWS Systems Manager Parameter Store provides a comprehensive solution for configuration and secrets management, combining secure storage, version control, and integration capabilities with other AWS services. Its flexible tier structure and policy management features make it suitable for organizations of all sizes seeking to implement robust configuration management practices.</p>"},{"location":"aws/messaging/firehose/","title":"Firehose","text":"<p>Previously known as Kinesis Data Firehose, this fully managed service provides a seamless solution for streaming data delivery and transformation. The service enables organizations to efficiently route streaming data to multiple destinations with minimal operational overhead.</p>"},{"location":"aws/messaging/firehose/#destination-capabilities","title":"Destination Capabilities","text":"<p>Amazon Data Firehose supports comprehensive data routing to various destinations, including native AWS services like Amazon Redshift, Amazon S3, and Amazon OpenSearch Service. The platform also facilitates integration with third-party platforms such as Splunk, MongoDB, Datadog, and New Relic, along with support for custom HTTP endpoints.</p>"},{"location":"aws/messaging/firehose/#service-characteristics","title":"Service Characteristics","text":"<p>The service operates as a serverless infrastructure with automatic scaling, allowing organizations to pay only for consumed resources. Data processing occurs with near real-time capabilities, utilizing intelligent buffering mechanisms based on data size and time intervals.</p>"},{"location":"aws/messaging/firehose/#data-format-flexibility","title":"Data Format Flexibility","text":"<p>Firehose demonstrates remarkable versatility in data format handling, supporting multiple input and output formats. Supported input formats include CSV, JSON, Parquet, Avro, Raw Text, and Binary data. The service offers advanced conversion capabilities, transforming data into Parquet or ORC formats and implementing compression using gzip or snappy algorithms.</p>"},{"location":"aws/messaging/firehose/#advanced-transformation-capabilities","title":"Advanced Transformation Capabilities","text":"<p>Organizations can leverage AWS Lambda for custom data transformations, enabling sophisticated data processing workflows. This feature allows complex data manipulation tasks, such as converting CSV data to JSON format, directly within the streaming pipeline.</p>"},{"location":"aws/messaging/firehose/#comparative-analysis-with-kinesis-data-streams","title":"Comparative Analysis with Kinesis Data Streams","text":"<p>While Kinesis Data Streams focuses on real-time streaming data collection with producer and consumer code management, Amazon Data Firehose provides a more streamlined approach. Key differentiations include:</p> <p>Firehose operates with near real-time processing, offers automatic scaling, and does not maintain long-term data storage. Unlike Kinesis Data Streams, Firehose lacks data replay capabilities, emphasizing immediate data routing and transformation.</p>"},{"location":"aws/messaging/firehose/#use-case-scenarios","title":"Use Case Scenarios","text":"<p>The service is particularly valuable for organizations requiring efficient, scalable data streaming solutions. Typical applications include log analytics, real-time business intelligence, website clickstream analysis, and comprehensive data warehouse loading.</p>"},{"location":"aws/messaging/firehose/#technical-architecture","title":"Technical Architecture","text":"<p>By abstracting complex streaming infrastructure challenges, Amazon Data Firehose enables developers to focus on data processing logic rather than managing underlying streaming mechanics. The serverless nature ensures seamless scalability and minimal operational complexity.</p>"},{"location":"aws/messaging/kinesis_data_analytics/","title":"Kinesis Data Analytics","text":"<p>Amazon Kinesis Data Analytics provides a powerful serverless solution for processing and analyzing streaming data in real-time using standard SQL queries. The service enables organizations to transform and derive insights from data streams without managing complex infrastructure.</p>"},{"location":"aws/messaging/kinesis_data_analytics/#processing-capabilities","title":"Processing Capabilities","text":"<p>The service supports multiple input streams from Kinesis Data Streams and Kinesis Data Firehose, allowing sophisticated data processing and analysis. Developers can write SQL queries to perform real-time transformations, aggregations, and complex analytical operations on streaming data.</p>"},{"location":"aws/messaging/kinesis_data_analytics/#input-and-output-flexibility","title":"Input and Output Flexibility","text":"<p>Kinesis Data Analytics supports diverse data sources and destinations. Input streams can originate from Kinesis Data Streams or Kinesis Data Firehose, while output can be directed to various endpoints including AWS Lambda, Kinesis Data Streams, and Kinesis Data Firehose.</p>"},{"location":"aws/messaging/kinesis_data_analytics/#data-processing-features","title":"Data Processing Features","text":"<p>The platform enables sophisticated stream processing through in-application SQL transformations. Users can perform complex operations like windowing, filtering, joining streams, and generating time-series aggregations without managing underlying computational infrastructure.</p>"},{"location":"aws/messaging/kinesis_data_analytics/#language-and-reference-data-support","title":"Language and Reference Data Support","text":"<p>Beyond standard SQL, the service provides advanced capabilities for referencing external data sources. Users can incorporate reference data from S3 to enrich stream processing, enabling more comprehensive analytical scenarios.</p>"},{"location":"aws/messaging/kinesis_data_analytics/#performance-and-scaling","title":"Performance and Scaling","text":"<p>Kinesis Data Analytics automatically scales computational resources based on streaming data volume and complexity. The serverless architecture ensures efficient resource utilization and eliminates manual capacity management requirements.</p>"},{"location":"aws/messaging/kinesis_data_analytics/#use-case-scenarios","title":"Use Case Scenarios","text":"<p>Typical applications include real-time analytics for IoT sensor data, log analysis, financial transaction monitoring, clickstream analysis, and dynamic business intelligence reporting. The service\u2019s flexibility supports diverse industry-specific streaming analysis requirements.</p>"},{"location":"aws/messaging/kinesis_data_analytics/#security-and-compliance","title":"Security and Compliance","text":"<p>The service integrates seamlessly with AWS security mechanisms, supporting encryption, access controls, and comprehensive compliance frameworks. Data processing occurs within secure, isolated environments managed by AWS infrastructure.</p>"},{"location":"aws/messaging/kinesis_data_streams/","title":"Kinesis Data Streams","text":"<p>Amazon Kinesis Data Streams provides a powerful real-time data collection and storage solution for streaming information across complex distributed systems. The service enables organizations to capture and process large volumes of streaming data with exceptional performance and flexibility.</p>"},{"location":"aws/messaging/kinesis_data_streams/#data-retention-and-processing-capabilities","title":"Data Retention and Processing Capabilities","text":"<p>Kinesis Data Streams offers extensive data retention capabilities, supporting streaming data storage for up to 365 days. This extended retention allows consumers to reprocess and replay data multiple times, providing remarkable flexibility in data analysis and recovery. Once data is ingested, it cannot be manually deleted and will automatically expire according to the configured retention period.</p>"},{"location":"aws/messaging/kinesis_data_streams/#data-characteristics-and-limitations","title":"Data Characteristics and Limitations","text":"<p>The service supports streaming data with individual record sizes up to 1 megabyte, making it ideal for processing numerous small real-time data points. Kinesis ensures data ordering guarantees for records sharing the same partition identifier, maintaining critical sequencing requirements for complex streaming scenarios.</p>"},{"location":"aws/messaging/kinesis_data_streams/#security-and-encryption","title":"Security and Encryption","text":"<p>Robust security mechanisms are integrated into Kinesis Data Streams. The service implements comprehensive encryption strategies, including at-rest encryption through AWS Key Management Service and in-flight encryption via HTTPS protocols. These security measures ensure data protection throughout the streaming lifecycle.</p>"},{"location":"aws/messaging/kinesis_data_streams/#optimization-libraries","title":"Optimization Libraries","text":"<p>To enhance development efficiency, AWS provides specialized libraries for producers and consumers. The Kinesis Producer Library enables developers to create optimized producer applications, while the Kinesis Client Library facilitates efficient consumer application development.</p>"},{"location":"aws/messaging/kinesis_data_streams/#capacity-management-modes","title":"Capacity Management Modes","text":""},{"location":"aws/messaging/kinesis_data_streams/#provisioned-mode","title":"Provisioned Mode","text":"<p>In the provisioned mode, organizations manually manage stream capacity by selecting specific shard configurations. Each shard supports 1 megabyte per second input (or 1000 records per second) and 2 megabytes per second output. Users pay for provisioned shards on an hourly basis and must manually scale infrastructure to meet changing throughput requirements.</p>"},{"location":"aws/messaging/kinesis_data_streams/#on-demand-mode","title":"On-Demand Mode","text":"<p>The on-demand mode eliminates complex capacity planning by automatically scaling stream infrastructure. With a default capacity of 4 megabytes per second input (or 4000 records per second), the service dynamically adjusts based on observed throughput peaks from the previous 30 days. Billing occurs per stream hour and per gigabyte of data transferred.</p>"},{"location":"aws/messaging/kinesis_data_streams/#use-cases-and-applications","title":"Use Cases and Applications","text":"<p>Kinesis Data Streams serves diverse real-time data processing scenarios, including log and event data streaming, website clickstream analysis, IoT device data processing, and complex event-driven architectural patterns. The service\u2019s flexible architecture supports rapid, scalable data ingestion across multiple industries and technological domains.</p>"},{"location":"aws/messaging/ses/","title":"Simple Email Service (SES)","text":"<p>Amazon Simple Email Service represents a comprehensive cloud-based email communication platform designed for secure, global email delivery at massive scale. This fully managed service empowers organizations to send and receive emails with unprecedented flexibility and reliability.</p>"},{"location":"aws/messaging/ses/#service-capabilities","title":"Service Capabilities","text":"<p>SES offers robust capabilities for both inbound and outbound email communications, supporting diverse business communication needs. Organizations can leverage the service for transactional, marketing, and bulk email communications across global infrastructures.</p>"},{"location":"aws/messaging/ses/#email-delivery-and-performance-management","title":"Email Delivery and Performance Management","text":"<p>The service provides extensive performance insights through a comprehensive reputation dashboard. Users gain access to detailed statistics tracking email deliveries, bounce rates, feedback loop results, and email open rates. This granular visibility enables organizations to monitor and optimize their email communication strategies effectively.</p>"},{"location":"aws/messaging/ses/#security-and-authentication-mechanisms","title":"Security and Authentication Mechanisms","text":"<p>SES implements sophisticated email authentication protocols to ensure message integrity and prevent potential spoofing. The service supports industry-standard authentication frameworks including DomainKeys Identified Mail (DKIM) and Sender Policy Framework (SPF), which enhance email deliverability and protect sender reputation.</p>"},{"location":"aws/messaging/ses/#ip-deployment-strategies","title":"IP Deployment Strategies","text":"<p>Email sending infrastructure offers remarkable flexibility through multiple IP deployment options. Organizations can choose between shared, dedicated, or customer-owned IP addresses, allowing tailored solutions that match specific compliance, performance, and scalability requirements.</p>"},{"location":"aws/messaging/ses/#sending-mechanisms","title":"Sending Mechanisms","text":"<p>Users can dispatch emails through multiple interfaces, including the AWS Console, programmatic APIs, or traditional SMTP protocols. This versatility ensures seamless integration with existing application architectures and development workflows.</p>"},{"location":"aws/messaging/ses/#communication-use-cases","title":"Communication Use Cases","text":"<p>The service accommodates diverse communication scenarios, supporting: - Transactional email communications - Marketing email campaigns - Bulk email distribution - Automated notification systems - Customer engagement communications</p>"},{"location":"aws/messaging/ses/#technical-integration","title":"Technical Integration","text":"<p>Developers can easily integrate SES into their applications, leveraging AWS\u2019s comprehensive cloud infrastructure to manage complex email communication requirements. The service\u2019s scalable architecture ensures reliable message transmission across global networks.</p>"},{"location":"aws/messaging/ses/#performance-and-reliability","title":"Performance and Reliability","text":"<p>By providing a fully managed email sending platform, SES abstracts complex email infrastructure challenges. Organizations can focus on crafting compelling content while AWS handles the intricate details of global email delivery, reputation management, and performance optimization.</p>"},{"location":"aws/messaging/sns/","title":"Simple Notification Service (SNS)","text":"<p>Amazon Simple Notification Service (SNS) is a robust pub/sub messaging service that enables distributed systems to communicate efficiently. The service allows an event producer to publish messages to a single topic, which can then be distributed to multiple subscribers across various AWS services and platforms.</p>"},{"location":"aws/messaging/sns/#core-characteristics","title":"Core Characteristics","text":"<p>SNS provides remarkable scalability, supporting up to 12.5 million subscriptions per topic with a limit of 100,000 topics per account. Every subscriber receives messages published to the topic, with recent enhancements introducing sophisticated message filtering capabilities.</p>"},{"location":"aws/messaging/sns/#publishing-methods","title":"Publishing Methods","text":"<p>Topic publishing utilizes AWS SDK, involving creating topics, establishing subscriptions, and publishing messages. For mobile applications, direct publish offers a specialized mechanism supporting integration with major mobile platforms like Google Cloud Messaging, Apple Push Notification Service, and Amazon Device Messaging.</p>"},{"location":"aws/messaging/sns/#security-and-encryption","title":"Security and Encryption","text":"<p>Security is deeply embedded in SNS\u2019s architecture. The service implements comprehensive encryption strategies, including in-flight encryption through HTTPS API, at-rest encryption via AWS Key Management Service, and optional client-side encryption for custom security requirements.</p>"},{"location":"aws/messaging/sns/#fan-out-architecture-with-sqs","title":"Fan-Out Architecture with SQS","text":"<p>SNS supports a powerful fan-out pattern that allows pushing a single message to multiple Amazon SQS queues. This approach creates a decoupled system with enhanced capabilities:</p> <ul> <li>Prevents data loss</li> <li>Enables data persistence</li> <li>Supports delayed processing</li> <li>Allows dynamic addition of queue subscribers</li> <li>Facilitates cross-region message delivery</li> </ul>"},{"location":"aws/messaging/sns/#fifo-topics","title":"FIFO Topics","text":"<p>First-In-First-Out topics provide advanced message management capabilities. This feature ensures precise message ordering within message groups, supports deduplication through multiple strategies, and maintains compatibility with both SQS Standard and FIFO queues.</p>"},{"location":"aws/messaging/sns/#message-filtering-strategies","title":"Message Filtering Strategies","text":"<p>SNS introduces sophisticated message filtering using JSON policies. This approach allows granular control over message routing, enabling subscriptions to selectively consume messages based on complex filtering rules.</p>"},{"location":"aws/messaging/sns/#specific-application-scenarios","title":"Specific Application Scenarios","text":"<p>The service offers unique solutions for various architectural challenges. For instance, it overcomes S3 event notification limitations by enabling fan-out distribution of identical events to multiple queues. Additionally, SNS can seamlessly route messages through Kinesis Data Firehose, providing flexible solution architectures.</p>"},{"location":"aws/messaging/sns/#service-ecosystem-integration","title":"Service Ecosystem Integration","text":"<p>SNS seamlessly connects with numerous AWS services, transforming it from a simple notification service into a robust event-driven architecture enabler. Whether processing cloud events, managing mobile push notifications, or coordinating distributed system communications, SNS provides a flexible, secure, and scalable messaging solution.</p>"},{"location":"aws/messaging/sns/#supported-platforms-and-endpoints","title":"Supported Platforms and Endpoints","text":"<p>The service supports a wide range of platforms, including mobile notification services like Google Cloud Messaging, Apple Push Notification Service, and Amazon Device Messaging. This extensive platform support ensures broad compatibility and easy integration across different technological ecosystems.</p>"},{"location":"aws/messaging/sqs/","title":"Simple Queue Service (SQS)","text":""},{"location":"aws/messaging/sqs/#overview-of-amazon-sqs","title":"Overview of Amazon SQS","text":"<p>Amazon Simple Queue Service (SQS) is AWS\u2019s oldest messaging service, designed to decouple applications by providing a fully managed message queuing system. As a foundational AWS service with over a decade of existence, SQS offers robust messaging capabilities with specific characteristics that make it versatile for various architectural needs.</p>"},{"location":"aws/messaging/sqs/#standard-queue-characteristics","title":"Standard Queue Characteristics","text":"<p>The Standard Queue provides unlimited throughput and message storage, allowing applications to handle massive message volumes with exceptional flexibility. Messages persist in the queue for a default retention period of 4 days, extendable up to 14 days. The service guarantees low latency, with message publishing and receiving typically occurring within 10 milliseconds.</p> <p>Key Standard Queue Attributes: - Messages are limited to 256KB in size - Supports at least once delivery mechanism - Offers best-effort message ordering - Provides unlimited throughput - Maintains messages for 4-14 days</p>"},{"location":"aws/messaging/sqs/#message-production-and-consumption","title":"Message Production and Consumption","text":""},{"location":"aws/messaging/sqs/#message-production","title":"Message Production","text":"<p>Applications produce messages using the SDK\u2019s SendMessage API, with messages persisting in the queue until explicitly deleted by a consumer. Messages can include diverse attributes such as order identifiers, customer details, or any custom metadata.</p>"},{"location":"aws/messaging/sqs/#message-consumption","title":"Message Consumption","text":"<p>Consumers, which can run on EC2 instances, servers, or AWS Lambda, interact with SQS through polling mechanisms: - Can receive up to 10 messages simultaneously - Process messages according to application logic - Delete processed messages using the DeleteMessage API</p>"},{"location":"aws/messaging/sqs/#multi-consumer-scenarios","title":"Multi-Consumer Scenarios","text":"<p>SQS supports parallel message processing across multiple EC2 instances. This approach enables horizontal scaling of consumers, improving overall message processing throughput. However, the service provides at least once delivery with best-effort ordering, which means applications must be designed to handle potential message duplicates.</p>"},{"location":"aws/messaging/sqs/#security-mechanisms","title":"Security Mechanisms","text":"<p>SQS implements comprehensive security through multiple encryption and access control strategies:</p> <p>Encryption Options: - In-flight encryption via HTTPS API - At-rest encryption using AWS KMS keys - Client-side encryption for custom encryption requirements</p> <p>Access Control: - IAM policies regulating SQS API access - SQS Access Policies enabling cross-account queue access - Permissions for service integrations with SNS, S3, and others</p>"},{"location":"aws/messaging/sqs/#advanced-message-management-features","title":"Advanced Message Management Features","text":""},{"location":"aws/messaging/sqs/#message-visibility-timeout","title":"Message Visibility Timeout","text":"<p>When a consumer polls a message, it becomes temporarily invisible to other consumers. The default visibility timeout is 30 seconds, during which the message must be processed. If processing fails within this window, the message becomes available again, potentially causing duplicate processing.</p>"},{"location":"aws/messaging/sqs/#dead-letter-queue-dlq","title":"Dead Letter Queue (DLQ)","text":"<p>DLQs provide a mechanism for managing problematic messages: - Track messages that repeatedly fail processing - Configurable maximum receive threshold - Separate queues for Standard and FIFO queue types - Supports message redriving for debugging and recovery</p>"},{"location":"aws/messaging/sqs/#delay-queues","title":"Delay Queues","text":"<p>Messages can be delayed up to 15 minutes before becoming visible to consumers, configurable at queue or message level.</p>"},{"location":"aws/messaging/sqs/#long-polling","title":"Long Polling","text":"<p>Long polling reduces API calls by allowing consumers to wait for messages, improving application efficiency and reducing latency. Wait times range from 1 to 20 seconds, with 20 seconds recommended.</p>"},{"location":"aws/messaging/sqs/#fifo-first-in-first-out-queue","title":"FIFO (First-In-First-Out) Queue","text":"<p>FIFO queues offer strict message ordering with unique capabilities: - Limited throughput (300 messages/second without batching) - Exactly-once send capability - Mandatory message group ID for ordering</p>"},{"location":"aws/messaging/sqs/#deduplication-mechanism","title":"Deduplication Mechanism","text":"<p>Deduplication is a critical feature of FIFO queues that prevents message duplication through two primary methods:</p> <ol> <li>Content-Based Deduplication:</li> <li>Automatically generates a SHA-256 hash of the message body</li> <li>If an identical message is sent within the 5-minute deduplication interval, it is rejected</li> <li> <p>Useful when message content serves as a natural unique identifier</p> </li> <li> <p>Explicit Deduplication:</p> </li> <li>Requires manually providing a Message Deduplication ID</li> <li>Developers specify a unique identifier for each message</li> <li>Allows more granular control over duplicate detection</li> <li>Useful when message content might be similar but logically distinct</li> </ol> <p>The deduplication process operates within a 5-minute interval, ensuring that duplicate messages are efficiently identified and eliminated. This approach guarantees that each unique message is processed exactly once, addressing a common challenge in distributed messaging systems.</p>"},{"location":"aws/messaging/sqs/#message-grouping","title":"Message Grouping","text":"<p>FIFO queues support message grouping through the Message Group ID: - Messages with the same Group ID are processed in order - Different Group IDs can have separate consumers - Enables parallel processing while maintaining local message sequence - Ordering is guaranteed within each group, but not across groups</p>"},{"location":"aws/messaging/sqs/#essential-sqs-apis","title":"Essential SQS APIs","text":"<p>Critical APIs include: - CreateQueue and DeleteQueue for queue management - PurgeQueue for complete message deletion - SendMessage and ReceiveMessage for message handling - ChangeMessageVisibility for timeout adjustments - Batch APIs for cost-efficient message processing</p>"},{"location":"aws/messaging/sqs/#extended-client-capabilities","title":"Extended Client Capabilities","text":"<p>For handling large messages exceeding the 256KB limit, the SQS Extended Client (Java Library) provides solutions for transmitting substantially larger payloads.</p>"},{"location":"aws/monitoring/cloudtrail/","title":"CloudTrail","text":""},{"location":"aws/monitoring/cloudtrail/#core-purpose","title":"Core Purpose","text":"<p>AWS CloudTrail serves as a critical governance, compliance, and auditing tool for AWS accounts. Enabled by default, it provides a comprehensive tracking mechanism for all activities within an AWS environment.</p> <p></p>"},{"location":"aws/monitoring/cloudtrail/#event-tracking-capabilities","title":"Event Tracking Capabilities","text":"<p>CloudTrail captures a detailed history of events and API calls across multiple interaction channels, including the AWS Console, SDKs, CLI, and various AWS services. This extensive tracking allows organizations to maintain a complete record of account-level interactions and resource modifications.</p>"},{"location":"aws/monitoring/cloudtrail/#event-types-and-logging","title":"Event Types and Logging","text":""},{"location":"aws/monitoring/cloudtrail/#management-events","title":"Management Events","text":"<p>Management events encompass operations performed on AWS resources, including critical activities such as: - Security configuration modifications - Data routing rule establishments - Logging setup processes</p> <p>These events are logged by default, with the ability to distinguish between read and write events. Read events represent non-modifying interactions, while write events capture potentially resource-altering actions.</p>"},{"location":"aws/monitoring/cloudtrail/#data-events","title":"Data Events","text":"<p>By default, data events are not logged due to their high-volume nature. However, organizations can enable specific data event tracking for services like Amazon S3 and AWS Lambda. S3 data events can track object-level activities such as GetObject, DeleteObject, and PutObject, with options to separate read and write events.</p>"},{"location":"aws/monitoring/cloudtrail/#cloudtrail-insights","title":"CloudTrail Insights","text":"<p>CloudTrail Insights provides advanced anomaly detection capabilities by: - Analyzing normal management event patterns - Continuously monitoring write events for unusual activities - Detecting potential issues like inaccurate resource provisioning - Identifying service limit breaches - Tracking sudden bursts of IAM actions - Recognizing gaps in periodic maintenance activities</p> <p>When anomalies are detected, they appear in the CloudTrail console, generate events in Amazon S3, and create EventBridge events for potential automation. It needs to be enabled and it\u2019s a paid service.</p>"},{"location":"aws/monitoring/cloudtrail/#event-storage-and-retention","title":"Event Storage and Retention","text":"<p>CloudTrail maintains event records for 90 days within its native storage. For long-term preservation, organizations can configure logging to Amazon S3 and utilize Amazon Athena for extended event analysis.</p>"},{"location":"aws/monitoring/cloudtrail/#practical-applications","title":"Practical Applications","text":"<p>CloudTrail is invaluable for forensic investigations, particularly when resources are unexpectedly deleted. By providing a comprehensive event trail, it enables detailed tracking and understanding of account-level activities.</p>"},{"location":"aws/monitoring/cloudtrail/#flexible-configuration","title":"Flexible Configuration","text":"<p>Users can configure trails to cover all AWS regions or focus on specific regional activities, providing flexible monitoring options tailored to organizational needs.</p>"},{"location":"aws/monitoring/cloudwatch/","title":"CloudWatch","text":""},{"location":"aws/monitoring/cloudwatch/#introduction-to-cloudwatch","title":"Introduction to CloudWatch","text":"<p>AWS CloudWatch serves as a comprehensive monitoring and observability service designed to provide insights into AWS resources and applications. The platform enables detailed tracking of performance metrics, log analysis, and proactive system management across the AWS ecosystem.</p>"},{"location":"aws/monitoring/cloudwatch/#metrics-and-monitoring","title":"Metrics and Monitoring","text":""},{"location":"aws/monitoring/cloudwatch/#metric-fundamentals","title":"Metric Fundamentals","text":"<p>CloudWatch tracks metrics as variables representing system performance and health. These metrics encompass various attributes such as CPU utilization, network traffic, and resource consumption. Each metric belongs to a specific namespace and can be associated with up to 30 dimensions, allowing granular performance tracking.</p>"},{"location":"aws/monitoring/cloudwatch/#ec2-monitoring-strategies","title":"EC2 Monitoring Strategies","text":"<p>For EC2 instances, CloudWatch offers standard monitoring at five-minute intervals and detailed monitoring at one-minute intervals. Detailed monitoring, available for an additional cost, provides more frequent data collection critical for rapid auto-scaling scenarios. While the AWS Free Tier supports ten detailed monitoring metrics, users should note that memory usage requires manual configuration as a custom metric.</p>"},{"location":"aws/monitoring/cloudwatch/#custom-metrics-and-advanced-tracking","title":"Custom Metrics and Advanced Tracking","text":""},{"location":"aws/monitoring/cloudwatch/#creating-custom-metrics","title":"Creating Custom Metrics","text":"<p>CloudWatch empowers users to define and send personalized metrics beyond standard AWS offerings. Developers can track specialized performance indicators like memory usage, disk space, or user authentication events using the PutMetricData API call. Custom metrics support flexible dimensioning and offer two resolution options: standard (1-minute) and high-resolution (1-30 seconds) with corresponding cost implications.</p>"},{"location":"aws/monitoring/cloudwatch/#log-management","title":"Log Management","text":""},{"location":"aws/monitoring/cloudwatch/#log-collection-and-processing","title":"Log Collection and Processing","text":"<p>CloudWatch Logs provides a robust platform for collecting, storing, and analyzing system and application logs. Users can define log groups representing applications and log streams representing specific instances or containers. The service supports comprehensive log management, including configurable retention policies and export capabilities to various AWS services like S3, Kinesis, and Lambda.</p>"},{"location":"aws/monitoring/cloudwatch/#log-insights-and-analysis","title":"Log Insights and Analysis","text":"<p>The CloudWatch Logs Insights feature enables advanced log analysis through a specialized query language. Users can search across multiple log groups, perform complex filtering, and extract specific event details. While not a real-time engine, Logs Insights facilitates deep log investigation and troubleshooting.</p>"},{"location":"aws/monitoring/cloudwatch/#monitoring-agents","title":"Monitoring Agents","text":""},{"location":"aws/monitoring/cloudwatch/#cloudwatch-agents","title":"CloudWatch Agents","text":"<p>AWS offers two primary agents for log and metric collection: the traditional CloudWatch Logs Agent and the more advanced CloudWatch Unified Agent. The Unified Agent provides comprehensive system-level metric collection, including CPU, disk, RAM, network, and process statistics, supporting both EC2 and on-premises environments.</p>"},{"location":"aws/monitoring/cloudwatch/#alarm-and-notification-system","title":"Alarm and Notification System","text":""},{"location":"aws/monitoring/cloudwatch/#cloudwatch-alarms","title":"CloudWatch Alarms","text":"<p>The alarm system allows users to define notification and response mechanisms based on specific metric thresholds. Alarms can trigger various actions such as stopping or recovering EC2 instances, initiating auto-scaling processes, or sending notifications through SNS. Composite alarms enable complex monitoring scenarios by evaluating multiple alarm states simultaneously.</p>"},{"location":"aws/monitoring/cloudwatch/#security-and-encryption","title":"Security and Encryption","text":""},{"location":"aws/monitoring/cloudwatch/#log-security","title":"Log Security","text":"<p>CloudWatch Logs are encrypted by default, with options for additional KMS-based encryption using custom keys. The service supports integration with various AWS security mechanisms, ensuring comprehensive data protection.</p>"},{"location":"aws/monitoring/cloudwatch/#use-cases-and-applications","title":"Use Cases and Applications","text":"<p>CloudWatch serves diverse monitoring needs across different domains: - Performance tracking for cloud infrastructure - Application health monitoring - Security and compliance tracking - Resource optimization - Automated system response and scaling</p>"},{"location":"aws/monitoring/cloudwatch/#best-practices","title":"Best Practices","text":"<p>Effective CloudWatch utilization involves: - Configuring appropriate metric resolutions - Implementing custom metrics for critical systems - Establishing comprehensive log retention policies - Creating intelligent alarm configurations - Regularly reviewing and optimizing monitoring strategies</p>"},{"location":"aws/monitoring/cloudwatch/#conclusion","title":"Conclusion","text":"<p>AWS CloudWatch represents a powerful, flexible monitoring solution that provides deep insights into AWS resources and applications. By offering comprehensive metrics, log management, and automated response capabilities, CloudWatch enables organizations to maintain robust, efficient cloud environments.</p>"},{"location":"aws/monitoring/eventbridge/","title":"EventBridge","text":""},{"location":"aws/monitoring/eventbridge/#core-functionality","title":"Core Functionality","text":"<p>Amazon EventBridge serves as a powerful serverless event bus that enables seamless integration and automation across AWS services and applications. It provides three primary mechanisms for event handling: scheduled jobs, event pattern matching, and cross-account event routing.</p>"},{"location":"aws/monitoring/eventbridge/#scheduling-and-event-patterns","title":"Scheduling and Event Patterns","text":"<p>EventBridge supports sophisticated scheduling through cron jobs, allowing precise timing of script executions. Beyond scheduling, it excels at event pattern recognition, enabling real-time reactions to service-level activities. Organizations can trigger diverse actions like invoking Lambda functions, dispatching messages to SQS or SNS, and orchestrating complex workflows based on specific event conditions.</p>"},{"location":"aws/monitoring/eventbridge/#advanced-event-management","title":"Advanced Event Management","text":""},{"location":"aws/monitoring/eventbridge/#event-bus-capabilities","title":"Event Bus Capabilities","text":"<p>Event buses in EventBridge offer remarkable flexibility. They can be configured with resource-based policies, permitting access from external AWS accounts. This feature enables centralized event aggregation and cross-account event sharing with granular permission controls.</p>"},{"location":"aws/monitoring/eventbridge/#event-archiving-and-replay","title":"Event Archiving and Replay","text":"<p>A standout feature of EventBridge is its comprehensive event archiving mechanism. Users can archive events comprehensively or apply sophisticated filtering, storing event data either indefinitely or for specified durations. The ability to replay archived events provides powerful debugging and recovery capabilities.</p>"},{"location":"aws/monitoring/eventbridge/#schema-registry-intelligent-event-understanding","title":"Schema Registry: Intelligent Event Understanding","text":"<p>EventBridge introduces an intelligent Schema Registry that automatically analyzes and infers schemas from events traversing the event bus. This capability allows applications to:</p> <ul> <li>Automatically generate code reflecting event structures</li> <li>Maintain versioned schema definitions</li> <li>Enhance predictability and type safety in event-driven architectures</li> </ul>"},{"location":"aws/monitoring/eventbridge/#resource-based-policy-management","title":"Resource-Based Policy Management","text":"<p>EventBridge\u2019s resource-based policies enable precise control over event bus permissions. Organizations can:</p> <ul> <li>Define granular access controls for specific event buses</li> <li>Manage cross-account and cross-region event interactions</li> <li>Implement complex event aggregation strategies</li> </ul> <p>By consolidating events from entire AWS Organizations into centralized accounts or regions, EventBridge simplifies complex event management and monitoring processes.</p>"},{"location":"aws/monitoring/xray/","title":"X-Ray","text":""},{"location":"aws/monitoring/xray/#introduction","title":"Introduction","text":"<p>AWS X-Ray provides robust troubleshooting capabilities for complex distributed systems. It enables developers to understand microservice architectures by tracing requests across different services, identifying performance bottlenecks, and pinpointing service issues.</p>"},{"location":"aws/monitoring/xray/#architecture","title":"Architecture","text":"<pre><code>Application \u2192 X-Ray SDK \u2192 X-Ray Daemon \u2192 X-Ray API\n    (2000/UDP)         (443/HTTPS)\n</code></pre>"},{"location":"aws/monitoring/xray/#compatibility-and-supported-platforms","title":"Compatibility and Supported Platforms","text":"<p>X-Ray integrates seamlessly with multiple AWS services and platforms, including: - AWS Lambda - Elastic Beanstalk - Amazon ECS - Elastic Load Balancers - API Gateway - EC2 Instances - On-premise application servers</p>"},{"location":"aws/monitoring/xray/#tracing-mechanism","title":"Tracing Mechanism","text":"<p>Tracing in X-Ray represents an end-to-end method of following a request through complex systems. Each component adds its own trace, composed of segments and subsegments. Developers can enhance traces with annotations to provide additional contextual information.</p>"},{"location":"aws/monitoring/xray/#tracing-strategies","title":"Tracing Strategies","text":"<p>X-Ray supports flexible tracing approaches: - Comprehensive tracing of every request - Sampling requests based on percentage or rate per minute</p>"},{"location":"aws/monitoring/xray/#security-features","title":"Security Features","text":"<p>The service implements robust security measures: - IAM for authorization - AWS KMS for encryption at rest</p>"},{"location":"aws/monitoring/xray/#enabling-x-ray","title":"Enabling X-Ray","text":"<p>Implementing X-Ray requires two primary steps:</p> <ol> <li>Code Instrumentation</li> <li>Import AWS X-Ray SDK in supported languages (Java, Python, Go, Node.js, .NET)</li> <li>Minimal code modification needed</li> <li> <p>Automatic capture of AWS service calls, HTTP/HTTPS requests, database interactions, and queue calls</p> </li> <li> <p>Daemon Configuration</p> </li> <li>Install X-Ray daemon or enable AWS integration</li> <li>Daemon functions as a low-level UDP packet interceptor</li> <li>AWS Lambda and other services automatically run the X-Ray daemon</li> </ol>"},{"location":"aws/monitoring/xray/#key-concepts","title":"Key Concepts","text":"<ul> <li>Segments: Performance data from each application/service</li> <li>Subsegments: Detailed breakdown of segments</li> <li>Traces: Collected segments forming end-to-end request tracking</li> <li>Sampling: Mechanism to reduce trace volume and control costs</li> <li>Annotations: Indexed key-value pairs for trace filtering</li> <li>Metadata: Non-indexed key-value pairs</li> </ul>"},{"location":"aws/monitoring/xray/#sampling-rules","title":"Sampling Rules","text":"<p>X-Ray provides sophisticated sampling control: - Default: First request per second, five percent of additional requests - Configurable rules without code changes - Reservoir ensures at least one trace per second - Customizable sampling rates and rules</p>"},{"location":"aws/monitoring/xray/#apis-and-integrations","title":"APIs and Integrations","text":"<p>X-Ray offers comprehensive APIs: - Write APIs for uploading trace segments - Read APIs for retrieving trace information - GetServiceGraph for generating service maps - BatchGetTraces for detailed trace retrieval</p>"},{"location":"aws/monitoring/xray/#platform-specific-implementation","title":"Platform-Specific Implementation","text":"<p>For platforms like Elastic Beanstalk: - X-Ray daemon included in platform - Configurable through console or configuration files - Requires proper IAM instance profile permissions</p>"},{"location":"aws/monitoring/xray/#cross-account-tracing","title":"Cross-Account Tracing","text":"<p>X-Ray supports cross-account tracing by: - Configuring daemon to send traces between accounts - Requiring correct IAM role assumptions - Enabling centralized application performance monitoring</p>"},{"location":"aws/monitoring/xray/#visualization-and-troubleshooting","title":"Visualization and Troubleshooting","text":"<p>X-Ray generates graphical service maps that transform complex trace data into intuitive visualizations, making performance analysis accessible to both technical and non-technical team members.</p>"},{"location":"aws/network/cloudfront/","title":"CloudFront","text":""},{"location":"aws/network/cloudfront/#service-definition","title":"Service Definition","text":"<p>AWS CloudFront is a Content Delivery Network (CDN) service that dramatically improves content delivery performance. With 216 global points of presence (edge locations), it offers enhanced read performance by caching content strategically worldwide.</p>"},{"location":"aws/network/cloudfront/#key-security-and-performance-features","title":"Key Security and Performance Features","text":"<p>CloudFront provides robust DDoS protection through integration with AWS Shield and Web Application Firewall. Its global infrastructure ensures content is cached close to end-users, significantly improving access speeds and user experience.</p>"},{"location":"aws/network/cloudfront/#origin-types","title":"Origin Types","text":"<p>CloudFront supports multiple origin types for content delivery:</p> <ol> <li>Enables file distribution and edge caching them. </li> <li>Implements enhanced security through Origin Access Control (OAC), guaranteeing that only Cloudfront can access them. </li> <li> <p>Can be used as an ingress point for S3 uploads</p> </li> <li> </li> <li>Supports HTTP-based origins including:</li> <li>Application Load Balancers</li> <li>EC2 instances</li> <li>Static S3 websites</li> <li>Any HTTP backend system</li> </ol>"},{"location":"aws/network/cloudfront/#s3-bucket-origins","title":"S3 Bucket Origins","text":""},{"location":"aws/network/cloudfront/#custom-origins-http","title":"Custom Origins (HTTP)","text":""},{"location":"aws/network/cloudfront/#cloudfront-vs-s3-cross-region-replication","title":"CloudFront vs S3 Cross Region Replication","text":"<ul> <li>CloudFront:</li> <li>Global Edge network</li> <li>Files are cached for a TTL (maybe a day)</li> <li>Great for static content that must be available everywhere</li> <li>S3 Cross Region Replication:</li> <li>Must be setup for each region you want replication to happen</li> <li>Files are updated in near real-time</li> <li>Read only</li> <li>Great for dynamic content that needs to be available at low-latency in few regions</li> </ul>"},{"location":"aws/network/cloudfront/#caching","title":"Caching","text":"<ul> <li>The cache lives at each CloudFront Edge Location</li> <li>CloudFront identifies each object in the cache using the Cache Key</li> <li>To maximize the Cache Hit ratio you need to minimize requests to the origin</li> <li>It\u2019s possible to invalidate part of the cache using the <code>CreateInvalidation</code> API</li> </ul>"},{"location":"aws/network/cloudfront/#cache-key","title":"Cache Key","text":"<p>The cache key is a unique identifier for each cached object, typically comprising the hostname and URL\u2019s resource portion. CloudFront allows sophisticated cache key customization by incorporating: - HTTP headers - Cookies - Query strings - Device information - User location</p>"},{"location":"aws/network/cloudfront/#cache-policies","title":"Cache Policies","text":"<p>CloudFront offers extensive cache policy configurations:</p> <ul> <li>HTTP Headers</li> <li>None: Excludes headers from cache key</li> <li> <p>Whitelist: Includes specific headers in cache key</p> </li> <li> <p>Cookies</p> </li> <li>None: Excludes cookie from cache key</li> <li>Whitelist: Includes specific cookie</li> <li>Include All-Except: Includes all cookie except specified ones</li> <li> <p>All: Includes all cookie (lowest caching performance)</p> </li> <li> <p>Query Strings</p> </li> <li>None: Excludes query strings from cache key</li> <li>Whitelist: Includes specific query strings</li> <li>Include All-Except: Includes all query strings except specified ones</li> <li>All: Includes all query strings (lowest caching performance)</li> </ul> <p>It\u2019s possible to control the TTL (0 seconds to 1 year), can be set by the origin using the <code>Cache-Control</code> header, <code>Expires</code> header or others.</p> <p>All HTTP headers, cookies, and query strings that you include in the Cache Key are automatically included in origin requests.</p>"},{"location":"aws/network/cloudfront/#origin-request-policy","title":"Origin Request Policy","text":"<p>Enables including values in origin requests without duplicating cached content, supporting: - HTTP headers configuration - Cookie management - Query string handling - Custom header addition It\u2019s possible to create custom ORP or use Predefined Managed Policies.</p>"},{"location":"aws/network/cloudfront/#cache-invalidation","title":"Cache Invalidation","text":"<p>When backend origins update, CloudFront allows forced cache refresh through: - Full cache invalidation - Partial path invalidation (e.g., /images/*)</p>"},{"location":"aws/network/cloudfront/#cache-behaviors","title":"Cache Behaviors","text":"<p>Configures distinct settings for specific URL path patterns, enabling: - Different origin routing based on content type - Customized caching for specific file types - Prioritized processing of cache behaviors</p>"},{"location":"aws/network/cloudfront/#geographical-restrictions","title":"Geographical Restrictions","text":"<p>Implements content access control based on the region users are trying to access through: - Allowlist: Permit access from specific countries - Blocklist: Prevent access from specified countries - Uses third-party Geo-IP database for determination</p>"},{"location":"aws/network/cloudfront/#cloudfront-signed-url-signed-cookies","title":"CloudFront Signed URL / Signed Cookies","text":"<p>IN case you want to distribute paid shared content to premium users over the world, Cloudfront provides secure content distribution mechanisms: - URL/cookie expiration control - IP range access restrictions - Trusted signer management - Supports individual file (Signed URL) and multiple file (Signed Cookies) access</p>"},{"location":"aws/network/cloudfront/#field-level-encryption","title":"Field-Level Encryption","text":"<p>Offers additional security layer using HTTPS by: - Encrypting sensitive information at edge locations - Supporting up to 10 encrypted fields in POST requests - Utilizing asymmetric encryption</p>"},{"location":"aws/network/cloudfront/#real-time-logging","title":"Real-Time Logging","text":"<p>Streams CloudFront requests to Kinesis Data Streams, enabling: - Real-time performance monitoring - Configurable sampling rates - Selective field and path pattern tracking</p>"},{"location":"aws/network/cloudfront/#origin-groups","title":"Origin Groups","text":"<p>Enhances availability through: - Primary and secondary origin configuration - Automatic failover mechanisms</p>"},{"location":"aws/network/cloudfront/#conclusion","title":"Conclusion","text":"<p>AWS CloudFront provides a comprehensive, flexible content delivery solution with robust security, performance optimization, and global reach.</p>"},{"location":"aws/network/elb/","title":"Elastic Load Balancer","text":""},{"location":"aws/network/elb/#introduction-to-elastic-load-balancer","title":"Introduction to Elastic Load Balancer","text":"<p>An Elastic Load Balancer (ELB) is AWS\u2019s managed load balancing solution that provides guaranteed operational reliability through AWS\u2019s maintenance, upgrades, and high availability management. While setting up a custom load balancer might be more cost-effective initially, ELB offers significant advantages through its deep integration with AWS services including EC2, EC2 Auto Scaling Groups, Amazon ECS, AWS Certificate Manager (ACM), CloudWatch, Route 53, AWS WAF, and AWS Global Accelerator.</p>"},{"location":"aws/network/elb/#health-check-mechanisms","title":"Health Check Mechanisms","text":"<p>Health checks are fundamental to load balancer operations, enabling the service to verify the availability of instances receiving traffic. These checks typically monitor a specific port and route (commonly /health), with instances being marked as unhealthy if they fail to return a 200 (OK) response.</p>"},{"location":"aws/network/elb/#load-balancer-types","title":"Load Balancer Types","text":"<p>AWS offers four distinct managed load balancer types, each designed for specific use cases:</p>"},{"location":"aws/network/elb/#classic-load-balancer-clb-2009","title":"Classic Load Balancer (CLB - 2009)","text":"<p>The first-generation load balancer supports HTTP, HTTPS, TCP, and SSL protocols. It provides TCP (Layer 4) and HTTP/HTTPS (Layer 7) load balancing with TCP or HTTP-based health checks. CLBs are assigned a fixed hostname in the format XXX.region.elb.amazonaws.com.</p>"},{"location":"aws/network/elb/#application-load-balancer-alb-2016","title":"Application Load Balancer (ALB - 2016)","text":"<p>Operating at Layer 7, ALB specializes in HTTP-based traffic management with support for HTTP/2 and WebSocket. It enables sophisticated routing to multiple applications through target groups, supporting path-based routing (/users, /posts), hostname-based routing (one.example.com, other.example.com), and query string/header-based routing (example.com/users?id=123&amp;order=false).</p> <p>ALB is particularly well-suited for microservices and container-based applications, offering dynamic port mapping for ECS. This represents a significant advantage over Classic Load Balancers, which would require multiple instances for similar functionality.</p> <p>ALB Target Groups can include:</p> <ul> <li>EC2 instances managed by Auto Scaling Groups (HTTP)</li> <li>ECS tasks (HTTP)</li> <li>Lambda functions (HTTP requests are converted to JSON events)</li> <li>Private IP addresses</li> </ul> <p>Additional ALB features include:</p> <ul> <li>Fixed hostname (XXX.region.elb.amazonaws.com)</li> <li>Client IP preservation through X-Forwarded-For header</li> <li>Port and protocol information through X-Forwarded-Port and X-Forwarded-Proto headers</li> </ul>"},{"location":"aws/network/elb/#network-load-balancer-nlb-2017","title":"Network Load Balancer (NLB - 2017)","text":"<p>NLB operates at Layer 4, handling TCP, TLS, and UDP traffic. It excels in high-performance scenarios, processing millions of requests per second with ultra-low latency. NLB provides static IP addresses per Availability Zone and supports Elastic IP assignment, making it ideal for IP whitelisting scenarios. Note that NLB is not included in the AWS free tier.</p> <p>NLB Target Groups can include:</p> <ul> <li>EC2 instances</li> <li>Private IP addresses</li> <li>Application Load Balancers</li> </ul> <p>Health checks support TCP, HTTP, and HTTPS protocols.</p>"},{"location":"aws/network/elb/#gateway-load-balancer-gwlb-2020","title":"Gateway Load Balancer (GWLB - 2020)","text":"<p>GWLB operates at Layer 3 (Network layer) using the GENEVE protocol on port 6081. It\u2019s designed for managing third-party network virtual appliances such as firewalls, intrusion detection systems, and deep packet inspection systems. GWLB combines transparent network gateway functionality with load balancing capabilities.</p> <p>GWLB Target Groups support:</p> <ul> <li>EC2 instances</li> <li>Private IP addresses</li> </ul>"},{"location":"aws/network/elb/#advanced-load-balancer-features","title":"Advanced Load Balancer Features","text":""},{"location":"aws/network/elb/#sticky-sessions","title":"Sticky Sessions","text":"<p>Sticky sessions (session affinity) ensure that clients are consistently directed to the same backend instance. This feature is available across all load balancer types, with cookie-based stickiness control for CLB and ALB. While useful for session maintenance, sticky sessions may create uneven load distribution.</p> <p>Cookie types and naming conventions include:</p> <ol> <li> <p>Application-based Cookies:</p> </li> <li> <p>Custom cookie:</p> <ul> <li>Generated by the target</li> <li>Can include any custom attributes required by the application</li> <li>Cookie name must be specified individually for each target group</li> <li>Must avoid reserved names: AWSALB, AWSALBAPP, or AWSALBTG (reserved for ELB use)</li> </ul> </li> <li> <p>Application cookie:</p> <ul> <li>Generated by the load balancer</li> <li>Uses the cookie name AWSALBAPP</li> </ul> </li> <li> <p>Duration-based Cookies:</p> </li> <li> <p>Generated by the load balancer</p> </li> <li>Uses specific names depending on the load balancer type:<ul> <li>AWSALB for Application Load Balancer</li> <li>AWSELB for Classic Load Balancer</li> </ul> </li> </ol>"},{"location":"aws/network/elb/#cross-zone-load-balancing","title":"Cross-Zone Load Balancing","text":"<p>Cross-zone load balancing enables even distribution of traffic across all registered instances in all Availability Zones.  </p> <p>Without cross-zone load balancing, requests are distributed only to instances within the load balancer node\u2019s Availability Zone.  </p> <p>The feature\u2019s default state and associated costs vary by load balancer type:</p> <ul> <li>ALB: Enabled by default, no inter-AZ charges</li> <li>NLB and GWLB: Disabled by default, inter-AZ charges apply when enabled</li> <li>CLB: Disabled by default, no inter-AZ charges when enabled</li> </ul>"},{"location":"aws/network/elb/#ssltls-certification","title":"SSL/TLS Certification","text":"<p>Load balancers support SSL/TLS certificates for in-transit encryption, managed through AWS Certificate Manager (ACM) or manual upload. SSL certificates are issued by Certificate Authorities (CA) such as Comodo, Symantec, GoDaddy, GlobalSign, Digicert, and Letsencrypt. Certificates follow the X.509 standard and must be renewed before expiration.</p> <p>HTTPS listeners require:</p> <ul> <li>A default certificate</li> <li>Optional additional certificates for multiple domains</li> <li>A security policy to support older versions of SSL/TLS for legacy clients</li> </ul> <p>Server Name Indication (SNI) allows loading multiple SSL certificates onto one web server. This newer protocol requires clients to specify the target hostname during the initial SSL handshake. SNI is supported by ALB, NLB, and CloudFront, but not by CLB.</p> <p>SSL Certificate support by load balancer type:</p> <ul> <li>CLB: Supports only one SSL certificate</li> <li>ALB and NLB: Support multiple listeners with multiple SSL certificates using SNI</li> </ul>"},{"location":"aws/network/elb/#connection-draining","title":"Connection Draining","text":"<p>Connection draining (known as Deregistration Delay for ALB and NLB) allows in-flight requests to complete while an instance is being de-registered or marked unhealthy. This feature can be configured from 1 to 3600 seconds (default 300) or disabled completely. It\u2019s recommended to use shorter values for applications with quick request processing times.</p>"},{"location":"aws/network/elb/#security-considerations","title":"Security Considerations","text":"<p>Load balancer security groups play a crucial role in controlling traffic flow. They should be configured to allow only necessary incoming traffic to the load balancer while permitting all outbound traffic to instance security groups. Instance security groups should be configured to accept traffic only from the load balancer\u2019s security group, creating a secure chain of trust.</p>"},{"location":"aws/network/route53/","title":"Route 53","text":""},{"location":"aws/network/route53/#introduction-to-dns-and-route-53","title":"Introduction to DNS and Route 53","text":"<p>Domain Name System (DNS) serves as the backbone of the Internet, functioning as a crucial translation mechanism that converts human-readable hostnames into machine-readable IP addresses. For instance, when you type www.google.com into your browser, DNS translates this into an IP address like 172.217.18.36.</p> <p></p> <p>The DNS hierarchical naming structure includes:</p> <ul> <li>.com</li> <li>example.com</li> <li>www.example.com</li> <li>api.example.com</li> </ul> <p>Essential DNS terminology includes:</p> <ul> <li>Domain Registrar: Services like Amazon Route 53, GoDaddy</li> <li>DNS Records: A, AAAA, CNAME, NS, and others</li> <li>Zone File: Contains DNS records</li> <li>Name Server: Resolves DNS queries (Authoritative or Non-Authoritative)</li> <li>Top Level Domain (TLD): .com, .us, .in, .gov, .org</li> <li>Second Level Domain (SLD): amazon.com, google.com</li> </ul>"},{"location":"aws/network/route53/#understanding-route-53","title":"Understanding Route 53","text":"<p>Amazon Route 53 is AWS\u2019s premier DNS service, characterized by:</p> <ul> <li>Highly available, scalable, and fully managed Authoritative DNS</li> <li>Authoritative nature allowing customers to update DNS records</li> <li>Domain Registrar capabilities</li> <li>Resource health checking functionality</li> <li>The only AWS service with 100% availability SLA</li> <li>Name reference to the traditional DNS port (53)</li> </ul>"},{"location":"aws/network/route53/#dns-records-in-route-53","title":"DNS Records in Route 53","text":"<p>Records define how traffic is routed for a domain. Each record contains:</p> <ul> <li>Domain/subdomain Name (e.g., example.com)</li> <li>Record Type (e.g., A or AAAA)</li> <li>Value (e.g., 12.34.56.78)</li> <li>Routing Policy</li> <li>TTL (Time to Live)</li> </ul> <p>Supported DNS record types:</p> <ul> <li>Must-know types: A, AAAA, CNAME, NS</li> <li>Advanced types: CAA, DS, MX, NAPTR, PTR, SOA, TXT, SPF, SRV</li> </ul> <p>Record Types in detail:</p> <ul> <li>A \u2013 maps a hostname to IPv4</li> <li>AAAA \u2013 maps a hostname to IPv6</li> <li>CNAME \u2013 maps a hostname to another hostname (cannot be used for zone apex)</li> <li>NS \u2013 Name Servers for the Hosted Zone</li> </ul>"},{"location":"aws/network/route53/#hosted-zones","title":"Hosted Zones","text":"<p>Route 53 uses hosted zones as containers for records, costing $0.50 per month per hosted zone. Types include: - Public Hosted Zones: For Internet traffic routing (e.g., application1.mypublicdomain.com) - Private Hosted Zones: For VPC traffic routing (e.g., application1.company.internal)</p> <p></p>"},{"location":"aws/network/route53/#ttl-considerations","title":"TTL Considerations","text":"<p>High TTL (e.g., 24 hr):</p> <ul> <li>Reduces Route 53 traffic</li> <li>May result in outdated records</li> </ul> <p>Low TTL (e.g., 60 sec):</p> <ul> <li>Increases Route 53 traffic (higher cost)</li> <li>Records outdated for less time</li> <li>Facilitates easier record changes</li> </ul> <p>TTL is mandatory for all DNS records except Alias records.</p>"},{"location":"aws/network/route53/#cname-versus-alias-records","title":"CNAME versus Alias Records","text":"<p>For AWS Resources (Load Balancer, CloudFront) with AWS hostnames:</p> <p>CNAME:</p> <ul> <li>Points hostname to any other hostname</li> <li>Only for non-root domain</li> <li>Cannot be used for zone apex</li> </ul> <p>Alias:</p> <ul> <li>Points hostname to AWS Resource</li> <li>Works for both root and non-root domains</li> <li>Free of charge</li> <li>Includes native health check</li> <li>Always type A/AAAA for AWS resources</li> <li>No TTL setting required</li> </ul> <p>Supported Alias Record Targets:</p> <ul> <li>Elastic Load Balancers</li> <li>CloudFront Distributions</li> <li>API Gateway</li> <li>Elastic Beanstalk environments</li> <li>S3 Websites</li> <li>VPC Interface Endpoints</li> <li>Global Accelerator</li> <li>Route 53 record in the same hosted zone</li> <li>Cannot target EC2 DNS names</li> </ul>"},{"location":"aws/network/route53/#routing-policies","title":"Routing Policies","text":"<p>Route 53 routing policies define DNS query responses, distinct from load balancer routing. Available policies include:</p> <ul> <li>Simple</li> <li>Weighted</li> <li>Failover</li> <li>Latency based</li> <li>Geolocation</li> <li>Multi-Value Answer</li> <li>Geoproximity (using Route 53 Traffic Flow)</li> </ul>"},{"location":"aws/network/route53/#simple-routing","title":"Simple Routing","text":"<ul> <li>Routes traffic to a single resource</li> <li>Supports multiple values in one record</li> <li>Random value selection by client</li> <li>Single AWS resource when Alias enabled</li> <li>No Health Check association</li> </ul>"},{"location":"aws/network/route53/#weighted-routing","title":"Weighted Routing","text":"<ul> <li>Controls request percentage per resource</li> <li>Uses relative weight calculation: traffic % = (Weight for a specific record)/(Sum of all weights for all records)</li> <li>Weights don\u2019t need to sum to 100</li> <li>Requires same name and type for DNS records</li> <li>Supports Health Checks</li> <li>Useful for regional load balancing and application version testing</li> <li>Zero weight stops traffic to a resource</li> </ul>"},{"location":"aws/network/route53/#latency-based-routing","title":"Latency-based Routing","text":"<ul> <li>Directs to lowest-latency resource</li> <li>Based on AWS Region-user latency</li> <li>Supports Health Checks with failover</li> <li>May direct users to unexpected regions based on latency</li> </ul>"},{"location":"aws/network/route53/#health-checks","title":"Health Checks","text":"<p>Health Check types:</p> <ol> <li>Endpoint monitoring</li> <li>Calculated Health Checks (monitoring other health checks)</li> <li>CloudWatch Alarm monitoring</li> </ol> <p>Endpoint Monitoring Details:</p> <ul> <li>15 global health checkers</li> <li>Default threshold: 3 (Healthy/Unhealthy)</li> <li>30-second interval (10-second option available)</li> <li>Supports HTTP, HTTPS, TCP</li> <li>18% healthy checker threshold</li> <li>Response must be 2xx or 3xx</li> <li>First 5120 bytes can determine pass/fail</li> <li>Requires firewall configuration for checker access</li> </ul> <p>Calculated Health Checks:</p> <ul> <li>Combines multiple check results</li> <li>Supports OR, AND, NOT operations</li> <li>Up to 256 Child Health Checks</li> <li>Customizable pass threshold</li> <li>Useful for maintenance windows</li> </ul> <p>Private Hosted Zone Health Checks:</p> <ul> <li>Checkers cannot access private endpoints</li> <li>Requires CloudWatch Metric and Alarm integration</li> </ul>"},{"location":"aws/network/route53/#routing-policies-continued","title":"Routing Policies (Continued)","text":""},{"location":"aws/network/route53/#geolocation","title":"Geolocation","text":"<ul> <li>Location-based (not latency-based)</li> <li>Supports Continent, Country, US State targeting</li> <li>Requires default record</li> <li>Ideal for content localization</li> <li>Supports Health Checks</li> </ul>"},{"location":"aws/network/route53/#geoproximity","title":"Geoproximity","text":"<ul> <li>Based on resource and user locations</li> <li>Adjustable bias values (-99 to +99)</li> <li>Supports AWS and non-AWS resources</li> <li>Requires Route 53 Traffic Flow</li> </ul>"},{"location":"aws/network/route53/#ip-based-routing","title":"IP-based Routing","text":"<ul> <li>Routes based on client IP addresses</li> <li>Requires CIDR list and endpoint mappings</li> <li>Optimizes performance and costs</li> </ul>"},{"location":"aws/network/route53/#multi-value","title":"Multi-Value","text":"<ul> <li>Routes to multiple resources</li> <li>Returns up to 8 healthy records</li> <li>Supports Health Checks</li> <li>Not a load balancer replacement</li> </ul>"},{"location":"aws/network/route53/#domain-registration-and-dns-service","title":"Domain Registration and DNS Service","text":"<p>Domain registration and DNS service can be separated:</p> <ul> <li>Annual domain registration with registrars</li> <li>DNS service choice is independent</li> <li>Route 53 can manage DNS for third-party registered domains</li> </ul> <p>Third-party domain integration steps:</p> <ol> <li>Create Route 53 Hosted Zone</li> <li>Update NS Records on third-party site</li> <li>Use Route 53\u2019s name servers</li> </ol>"},{"location":"aws/network/route53/#traffic-flow","title":"Traffic Flow","text":"<p>Traffic flow features:</p> <ul> <li>Visual editor for complex configurations</li> <li>Saveable Traffic Flow Policies</li> <li>Cross-zone policy application</li> <li>Version control support</li> </ul>"},{"location":"aws/network/vpc/","title":"Virtual Private Cloud (VPC)","text":""},{"location":"aws/network/vpc/#overview","title":"Overview","text":"<p>Amazon Virtual Private Cloud (VPC) represents a fundamental networking service in AWS that enables you to create isolated private networks for your cloud resources. This comprehensive guide covers essential VPC concepts crucial for AWS certifications, particularly the Solutions Architect Associate and SysOps Administrator certifications.</p>"},{"location":"aws/network/vpc/#core-components","title":"Core Components","text":""},{"location":"aws/network/vpc/#vpc-and-subnet-architecture","title":"VPC and Subnet Architecture","text":"<p>A VPC functions as a private network infrastructure within AWS, operating as a regional resource. Within each VPC, subnets serve as network partitions that operate at the Availability Zone level. These subnets can be categorized as either public or private, with public subnets having internet accessibility and private subnets remaining isolated from direct internet access. Route Tables govern the traffic flow between subnets and determine internet access permissions.</p>"},{"location":"aws/network/vpc/#internet-connectivity","title":"Internet Connectivity","text":"<p>Internet Gateways serve as the primary component enabling VPC resources to connect to the internet. Public subnets maintain direct routes to the Internet Gateway, facilitating outbound and inbound internet communication. For private subnets, NAT (Network Address Translation) Gateways, managed by AWS, or NAT Instances, managed by users, enable outbound internet access while maintaining the subnet\u2019s private nature.</p>"},{"location":"aws/network/vpc/#security-components","title":"Security Components","text":""},{"location":"aws/network/vpc/#network-access-control-lists-nacls","title":"Network Access Control Lists (NACLs)","text":""},{"location":"aws/network/vpc/#what-are-nacls","title":"What are NACLs?","text":"<ul> <li>Stateless firewalls that operate at the subnet level within a VPC.</li> <li>Used to control traffic entering and leaving subnets.</li> </ul>"},{"location":"aws/network/vpc/#key-characteristics","title":"Key Characteristics:","text":"<ol> <li>Subnet-Level: NACLs apply to all resources within the associated subnet(s).</li> <li>Stateless:</li> <li>Both inbound and outbound rules must be defined explicitly. Return traffic is not automatically allowed.</li> <li>Explicit Rules for Allow and Deny:</li> <li>Unlike Security Groups, NACLs support both <code>allow</code> and <code>deny</code> rules.</li> <li>Rules are Evaluated in Order:</li> <li>Each rule is evaluated in numerical order, starting from the lowest rule number.</li> <li>Traffic is allowed or denied based on the first matching rule.</li> <li>Default Behavior:</li> <li>Default NACL allows all inbound and outbound traffic.</li> <li>Custom NACLs deny all traffic unless rules are explicitly added.</li> </ol>"},{"location":"aws/network/vpc/#common-use-cases","title":"Common Use Cases:","text":"<ul> <li>Apply coarse-grained security controls at the subnet level.</li> <li>Use as an additional layer of security alongside Security Groups.</li> <li>Block specific IP addresses or ranges.</li> </ul>"},{"location":"aws/network/vpc/#example-rules","title":"Example Rules:","text":"Rule # Type Protocol Port Range Source Allow/Deny 100 HTTP TCP 80 0.0.0.0/0 Allow 200 SSH TCP 22 203.0.113.0/24 Allow 300 All Traffic All All 0.0.0.0/0 Deny"},{"location":"aws/network/vpc/#security-groups","title":"Security Groups","text":""},{"location":"aws/network/vpc/#what-are-security-groups","title":"What are Security Groups?","text":"<ul> <li>Stateful firewalls that operate at the instance level.</li> <li>Act as virtual firewalls to control inbound and outbound traffic to and from Amazon EC2 instances.</li> <li>Rules can allow or deny traffic based on IP ranges, protocols, and ports.</li> </ul>"},{"location":"aws/network/vpc/#key-characteristics_1","title":"Key Characteristics:","text":"<ol> <li>Instance-Level: Security Groups are attached directly to EC2 instances.</li> <li>Stateful:</li> <li>If you allow inbound traffic, the corresponding outbound traffic is automatically allowed, and vice versa.</li> <li>Implicit Deny:</li> <li>By default, all inbound traffic is denied, and all outbound traffic is allowed unless explicitly specified otherwise.</li> <li>Rule-Based Configuration:</li> <li>You define rules to allow traffic. Deny rules cannot be explicitly created.</li> <li>Supports Allow Only: </li> <li>Rules define what traffic is permitted, with no option to explicitly block traffic.</li> <li>Dynamic Updates:</li> <li>Modifications to a Security Group are automatically applied to all associated resources.</li> </ol>"},{"location":"aws/network/vpc/#common-use-cases_1","title":"Common Use Cases:","text":"<ul> <li>Control access to EC2 instances based on port (e.g., 22 for SSH, 80/443 for HTTP/HTTPS).</li> <li>Restrict traffic to specific IP ranges or other resources within the same VPC.</li> </ul>"},{"location":"aws/network/vpc/#example-rules_1","title":"Example Rules:","text":"Type Protocol Port Range Source SSH TCP 22 198.51.100.1/32 HTTP TCP 80 0.0.0.0/0 HTTPS TCP 443 0.0.0.0/0"},{"location":"aws/network/vpc/#comparison-between-security-groups-and-nacls","title":"Comparison Between Security Groups and NACLs","text":"Aspect Security Groups NACLs Level of Operation Instance Level Subnet Level Statefulness Stateful Stateless Allow/Deny Rules Allow only Allow and Deny Evaluation of Rules All rules evaluated equally Rules evaluated in order Direction of Traffic Separate inbound and outbound Separate inbound and outbound Default Behavior Inbound: Deny, Outbound: Allow Inbound &amp; Outbound: Allow all (default NACL)"},{"location":"aws/network/vpc/#vpc-flow-logs","title":"VPC Flow Logs","text":"<p>VPC Flow Logs capture detailed information about IP traffic traversing network interfaces within your VPC. This monitoring capability extends to various levels including VPC-wide, subnet-specific, and individual network interfaces. The logs prove invaluable for troubleshooting connectivity issues between subnets, internet communication, and internal network traffic. The service also monitors AWS-managed interfaces for services like Elastic Load Balancers, ElastiCache, RDS, and Aurora. Flow log data can be directed to multiple destinations including Amazon S3, CloudWatch Logs, and Kinesis Data Firehose.</p>"},{"location":"aws/network/vpc/#vpc-peering","title":"VPC Peering","text":"<p>VPC Peering enables private connectivity between two VPCs using AWS\u2019s internal network infrastructure. This connection allows VPCs to interact as if they existed within the same network, though they must maintain non-overlapping CIDR ranges. Importantly, peering connections are not transitive, requiring explicit establishment between each pair of VPCs that need to communicate.</p>"},{"location":"aws/network/vpc/#vpc-endpoints","title":"VPC Endpoints","text":"<p>VPC Endpoints provide private access to AWS services without requiring internet connectivity. This approach enhances security and reduces latency when accessing AWS services. Two types of endpoints exist: - Gateway Endpoints, specifically designed for Amazon S3 and DynamoDB - Interface Endpoints, supporting all other compatible AWS services</p>"},{"location":"aws/network/vpc/#hybrid-connectivity","title":"Hybrid Connectivity","text":""},{"location":"aws/network/vpc/#site-to-site-vpn","title":"Site-to-Site VPN","text":"<p>This service enables secure connections between on-premises networks and AWS using encrypted tunnels over the public internet. It provides a relatively quick way to establish hybrid connectivity.</p>"},{"location":"aws/network/vpc/#aws-direct-connect","title":"AWS Direct Connect","text":"<p>Direct Connect establishes dedicated physical connections between on-premises facilities and AWS. While requiring longer setup times (typically a month or more), it offers private, secure, and high-performance connectivity through a private network infrastructure.</p>"},{"location":"aws/network/vpc/#certification-focus","title":"Certification Focus","text":"<p>For the AWS Certified Developer examination, candidates should focus on understanding: - VPC and subnet fundamentals - Internet and NAT Gateway functionality - Security Groups and NACLs - VPC Peering and Endpoints - Hybrid connectivity options including Site-to-Site VPN and Direct Connect</p> <p>Throughout the certification preparation, various scenarios will highlight the practical applications of these VPC concepts in real-world architectures.</p>"},{"location":"aws/network/vpc/#best-practices","title":"Best Practices","text":"<p>When implementing VPC architecture, consider: - Proper CIDR range planning to avoid overlapping IP addresses - Implementation of both public and private subnets for appropriate resource isolation - Effective use of security groups and NACLs for defense in depth - Regular monitoring of VPC Flow Logs for security and troubleshooting - Strategic placement of VPC endpoints to optimize AWS service access</p>"},{"location":"aws/security/cognito/","title":"Cognito","text":""},{"location":"aws/security/cognito/#introduction","title":"Introduction","text":"<p>Amazon Cognito is a service that enables user identity and access management for web and mobile applications. It provides two main components: Cognito User Pools (CUP) and Cognito Identity Pools (Federated Identities), each serving distinct purposes in the authentication and authorization workflow.</p>"},{"location":"aws/security/cognito/#cognito-user-pools-cup","title":"Cognito User Pools (CUP)","text":""},{"location":"aws/security/cognito/#overview","title":"Overview","text":"<p>Cognito User Pools provide a serverless database solution for managing user identities in web and mobile applications. As the primary authentication mechanism, CUP handles user sign-up, sign-in, and integrates seamlessly with various identity providers to create a comprehensive identity management system.</p>"},{"location":"aws/security/cognito/#core-features","title":"Core Features","text":"<p>The User Authentication system in Cognito User Pools offers a comprehensive set of features for secure user management. Users can sign in using either username or email combined with their password. The system includes built-in password reset functionality and supports both email and phone number verification to ensure user authenticity. Multi-factor authentication (MFA) adds an additional layer of security, while federated identity support enables integration with providers such as Facebook, Google, and SAML. The system actively protects against compromised credentials and issues JSON Web Tokens (JWT) upon successful authentication.</p>"},{"location":"aws/security/cognito/#lambda-triggers-integration","title":"Lambda Triggers Integration","text":"<p>Cognito User Pools can invoke Lambda functions at various stages of the authentication process. During authentication events, pre-authentication triggers enable custom validation of sign-in requests, while post-authentication triggers facilitate event logging for analytics purposes. The pre-token generation phase allows for token claim modification.</p> <p>For sign-up operations, the system provides pre-sign-up validation capabilities, post-confirmation processing for welcome messages, and user migration support from existing directories. Additional customization options include message personalization and token attribute modification through pre-token generation triggers.</p>"},{"location":"aws/security/cognito/#hosted-authentication-ui","title":"Hosted Authentication UI","text":"<p>Cognito provides a hosted authentication interface that includes pre-built sign-up and sign-in workflows. This UI seamlessly integrates with social logins, OIDC, and SAML providers. Organizations can customize the interface with their own logos and CSS styles. Custom domain support is available, though it requires an ACM certificate in the us-east-1 region.</p>"},{"location":"aws/security/cognito/#adaptive-authentication","title":"Adaptive Authentication","text":"<p>The adaptive authentication system provides sophisticated security measures by analyzing login attempts and assigning risk scores (low, medium, high) based on various factors. The system monitors device usage, location patterns, and IP addresses to detect suspicious activities. It includes protection against compromised credentials and account takeover attempts. For monitoring and analysis, the system integrates with CloudWatch Logs to track sign-in attempts, risk scores, and authentication challenges.</p>"},{"location":"aws/security/cognito/#jwt-token-structure","title":"JWT Token Structure","text":"<p>The JSON Web Token issued by Cognito consists of three main components: a header, payload, and signature. The payload contains essential user information, including the user UUID (sub), given name, email, phone number, and any additional custom attributes. The signature verification ensures the token\u2019s authenticity and trustworthiness.</p>"},{"location":"aws/security/cognito/#application-load-balancer-integration","title":"Application Load Balancer Integration","text":""},{"location":"aws/security/cognito/#authentication-capabilities","title":"Authentication Capabilities","text":"<p>The Application Load Balancer can handle user authentication, removing this burden from application servers. It supports various identity providers that are OIDC compliant and integrates directly with Cognito User Pools. Authentication rules require HTTPS listeners, and administrators can configure how the system handles unauthenticated requests through authenticate, deny, or allow options.</p>"},{"location":"aws/security/cognito/#implementation-requirements","title":"Implementation Requirements","text":"<p>Implementing ALB authentication requires proper setup of the Cognito User Pool, Client, and Domain. The configuration must ensure proper ID token return and handle URL redirections appropriately. When integrating social or corporate identity providers, careful attention must be paid to callback URL configuration.</p>"},{"location":"aws/security/cognito/#cognito-identity-pools-federated-identities","title":"Cognito Identity Pools (Federated Identities)","text":""},{"location":"aws/security/cognito/#core-functionality","title":"Core Functionality","text":"<p>Identity Pools extend Cognito\u2019s capabilities by providing temporary AWS credentials to users. This system supports multiple identity sources and enables both authenticated and guest access to AWS services. Users can interact directly with AWS services or through API Gateway, with access controlled by customizable IAM policies based on user identity.</p>"},{"location":"aws/security/cognito/#identity-source-support","title":"Identity Source Support","text":"<p>The system accepts identities from various sources, including public providers like Amazon, Facebook, Google, and Apple. It integrates with Cognito User Pools and supports OpenID Connect and SAML identity providers. Organizations can also implement custom login servers through Developer Authenticated Identities.</p>"},{"location":"aws/security/cognito/#iam-role-management","title":"IAM Role Management","text":"<p>Identity Pools manage AWS access through IAM roles, with separate default roles for authenticated and guest users. Role selection can be customized based on user attributes, and access can be partitioned using policy variables. The system obtains credentials through AWS STS and requires appropriate trust policies.</p>"},{"location":"aws/security/cognito/#comparing-user-pools-and-identity-pools","title":"Comparing User Pools and Identity Pools","text":"<p>Understanding the distinction between User Pools and Identity Pools is crucial for implementing effective authentication and authorization strategies. User Pools focus on authentication, providing user directory management, federated login support, and customizable interfaces. Identity Pools handle authorization by managing AWS credentials and access control through IAM policies.</p>"},{"location":"aws/security/cognito/#security-best-practices","title":"Security Best Practices","text":"<p>Security in Cognito requires a comprehensive approach that includes proper IAM policy implementation, MFA enablement for sensitive operations, and regular monitoring through CloudWatch. Organizations should carefully configure trust relationships, implement least privilege access principles, and conduct regular security assessments to maintain a robust security posture.</p>"},{"location":"aws/security/iam/","title":"Identity and Access Management (IAM)","text":""},{"location":"aws/security/iam/#introduction","title":"Introduction","text":"<p>AWS Identity and Access Management (IAM) is a global service that provides secure control over access to AWS resources. This guide outlines the core components and best practices for implementing IAM effectively in your AWS environment.</p>"},{"location":"aws/security/iam/#users-and-groups","title":"Users and Groups","text":"<p>IAM begins with the root account, which is created by default for every AWS account. However, the root account shouldn\u2019t be used for daily operations or shared among users. Instead, organizations should create individual IAM users for people within their organization.</p> <p>Users can be organized into groups for easier management. While groups can contain multiple users, they cannot contain other groups. It\u2019s important to note that users have flexibility in group membership - they can belong to multiple groups or no group at all, depending on organizational needs.</p>"},{"location":"aws/security/iam/#iam-permissions","title":"IAM Permissions","text":"<p>Permission management in IAM is handled through JSON documents called policies. These policies define the specific permissions granted to users or groups. AWS emphasizes the principle of least privilege, meaning users should only receive permissions necessary for their tasks.</p> <p>The policy structure consists of several key components. The Version field specifies the policy language version, typically \u201c2012-10-17\u201d. An optional Id field provides a policy identifier. The Statement section, which is required, contains one or more individual statements that define specific permissions.</p> <p>Each statement includes several elements: - A Sid (Statement ID) for optional identification - An Effect that specifies whether to Allow or Deny access - A Principal that defines the account, user, or role affected - Action fields listing permitted or denied actions - Resource specifications indicating which AWS resources are affected - Optional Condition elements that define when the policy takes effect</p>"},{"location":"aws/security/iam/#password-policy","title":"Password Policy","text":"<p>IAM enables robust password security through customizable password policies. Organizations can establish requirements for password complexity, including minimum length and character type requirements such as uppercase letters, lowercase letters, numbers, and special characters. The system can enforce password expiration periods, prevent password reuse, and allow IAM users to manage their own passwords.</p>"},{"location":"aws/security/iam/#multi-factor-authentication-mfa","title":"Multi-Factor Authentication (MFA)","text":"<p>Multi-Factor Authentication adds an essential security layer to AWS accounts. It combines something users know (their password) with something they own (an MFA device). This protection is crucial for both root accounts and IAM users, as it prevents unauthorized access even if passwords are compromised.</p> <p>AWS supports several MFA device options. Virtual MFA devices include Google Authenticator and Authy for mobile devices. Physical security keys include Universal 2<sup>nd</sup> Factor (U2F) devices like YubiKey. Hardware key fob options are available from providers like Gemalto, with special versions for AWS GovCloud from SurePassID.</p>"},{"location":"aws/security/iam/#aws-access-methods","title":"AWS Access Methods","text":"<p>Users can interact with AWS through three primary methods: the AWS Management Console, Command Line Interface (CLI), and Software Development Kit (SDK). The Console requires password and MFA protection, while CLI and SDK access is secured through access keys.</p> <p>Access keys consist of an Access Key ID (similar to a username) and a Secret Access Key (similar to a password). These credentials must be carefully managed and never shared. Users are responsible for managing their own access keys through the AWS Console.</p>"},{"location":"aws/security/iam/#aws-cli-and-sdk","title":"AWS CLI and SDK","text":"<p>The AWS Command Line Interface provides a tool for interacting with AWS services through command-line shells. It enables direct access to AWS service APIs and supports script development for resource management. The CLI is open-source and serves as an alternative to the AWS Management Console.</p> <p>The AWS Software Development Kit provides language-specific APIs for programmatic AWS access. It supports multiple programming languages including JavaScript, Python, PHP, .NET, Ruby, Java, Go, Node.js, and C++. The SDK also includes support for mobile development (Android, iOS) and IoT devices (Embedded C, Arduino).</p>"},{"location":"aws/security/iam/#iam-roles-for-services","title":"IAM Roles for Services","text":"<p>AWS services often need to perform actions on behalf of users. IAM Roles facilitate this by providing temporary credentials to AWS services. Common implementations include EC2 Instance Roles, Lambda Function Roles, and Roles for CloudFormation.</p>"},{"location":"aws/security/iam/#security-tools","title":"Security Tools","text":"<p>IAM provides two primary security assessment tools. The IAM Credentials Report offers account-level insights by listing all users and their credential status. The IAM Access Advisor provides user-level analysis of service permissions and their usage, helping administrators refine access policies.</p>"},{"location":"aws/security/iam/#best-practices","title":"Best Practices","text":"<p>AWS recommends several IAM best practices: - Reserve root account use for initial AWS account setup only - Create individual AWS users for each physical user - Manage permissions through groups rather than individual users - Implement and enforce strong password policies - Require MFA for all users - Use roles for AWS service permissions - Utilize access keys for programmatic access - Regular auditing using IAM security tools - Maintain strict control over IAM credentials</p>"},{"location":"aws/security/iam/#shared-responsibility-model","title":"Shared Responsibility Model","text":"<p>The IAM service operates under AWS\u2019s shared responsibility model. AWS maintains infrastructure security, performs configuration and vulnerability analysis, and ensures compliance validation. Customers are responsible for managing users, groups, roles, and policies, enabling MFA, rotating access keys, implementing appropriate permissions, and monitoring access patterns.</p> <p>Through proper implementation of these IAM components and best practices, organizations can maintain secure and efficient access to their AWS resources while minimizing security risks.</p>"},{"location":"aws/security/inspector/","title":"Inspector","text":""},{"location":"aws/security/inspector/#overview","title":"Overview","text":"<p>AWS Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS by identifying potential security vulnerabilities and deviations from best practices.</p>"},{"location":"aws/security/inspector/#key-features","title":"Key Features","text":"<ul> <li>Automated security assessment</li> <li>Continuous monitoring of AWS resources</li> <li>Detailed security findings and recommendations</li> <li>Integration with AWS security ecosystem</li> </ul>"},{"location":"aws/security/inspector/#assessment-types","title":"Assessment Types","text":"<ul> <li>Network accessibility assessments</li> <li>Host vulnerability assessments</li> <li>Runtime behavior analysis</li> <li>Configuration compliance checks</li> </ul>"},{"location":"aws/security/inspector/#supported-resources","title":"Supported Resources","text":"<ul> <li>Amazon EC2 instances</li> <li>Container images</li> <li>Lambda functions</li> <li>Amazon ECR repositories</li> </ul>"},{"location":"aws/security/inspector/#core-capabilities","title":"Core Capabilities","text":"<ul> <li>Automated vulnerability scanning</li> <li>Security benchmarking</li> <li>Risk prioritization</li> <li>Comprehensive reporting</li> <li>Continuous monitoring</li> </ul>"},{"location":"aws/security/inspector/#benefits","title":"Benefits","text":"<ul> <li>Proactive security management</li> <li>Reduced manual security assessment effort</li> <li>Detailed security insights</li> <li>Compliance support</li> <li>Integration with AWS security tools</li> </ul>"},{"location":"aws/security/inspector/#compliance-standards","title":"Compliance Standards","text":"<ul> <li>NIST</li> <li>PCI DSS</li> <li>HIPAA</li> <li>SOC</li> <li>ISO</li> </ul>"},{"location":"aws/security/inspector/#assessment-workflow","title":"Assessment Workflow","text":"<ol> <li>Define assessment target</li> <li>Configure assessment rules</li> <li>Run security assessment</li> <li>Review and prioritize findings</li> <li>Remediate identified vulnerabilities</li> </ol>"},{"location":"aws/security/kms/","title":"Key Management Service (KMS)","text":""},{"location":"aws/security/kms/#overview","title":"Overview","text":"<p>AWS Key Management Service (KMS) represents a cornerstone of cloud security, providing a robust, centralized system for creating, managing, and controlling cryptographic keys across AWS services and applications. At its core, KMS transforms the complex landscape of encryption key management into a streamlined, secure, and highly integrated solution.</p> <p>Modern digital infrastructure demands sophisticated encryption strategies that balance security, compliance, and operational efficiency. KMS emerges as a critical solution, offering granular control over cryptographic processes while abstracting away the intricate complexities of key generation, rotation, and lifecycle management.</p>"},{"location":"aws/security/kms/#fundamental-concepts-of-cryptographic-key-management","title":"Fundamental Concepts of Cryptographic Key Management","text":""},{"location":"aws/security/kms/#key-types-and-their-purposes","title":"Key Types and Their Purposes","text":""},{"location":"aws/security/kms/#customer-master-keys-cmks","title":"Customer Master Keys (CMKs)","text":"<p>Customer Master Keys form the foundation of AWS KMS, serving as the primary resources for cryptographic operations. These keys can be categorized into two primary classifications:</p> <ol> <li> <p>AWS Managed Keys AWS automatically creates and manages these keys for specific AWS services. They provide a baseline level of encryption with minimal configuration overhead. Services like Amazon S3, Amazon EBS, and AWS CloudTrail leverage these keys by default.</p> </li> <li> <p>Customer Managed Keys These keys offer the highest degree of customization and control. Customers can define precise policies, enable or disable key capabilities, and implement sophisticated rotation strategies. Customer managed keys support more advanced use cases requiring nuanced encryption requirements.</p> </li> </ol>"},{"location":"aws/security/kms/#data-keys","title":"Data Keys","text":"<p>Data keys represent a critical mechanism for envelope encryption. Unlike master keys, data keys are used to encrypt actual data payloads. KMS generates these keys dynamically, allowing for efficient and secure large-scale data encryption scenarios.</p>"},{"location":"aws/security/kms/#encryption-capabilities","title":"Encryption Capabilities","text":""},{"location":"aws/security/kms/#encryption-context-and-additional-authentication","title":"Encryption Context and Additional Authentication","text":"<p>KMS introduces the powerful concept of encryption context, a mechanism that provides additional authentication and audit capabilities. This feature allows associating additional metadata with encryption operations, enhancing security and providing rich contextual information for monitoring and compliance purposes.</p> <p>An encryption context acts as an additional layer of authentication, ensuring that decryption can only occur with the exact metadata that was present during encryption. This approach significantly reduces the risk of unauthorized decryption attempts.</p>"},{"location":"aws/security/kms/#envelope-encryption-strategy","title":"Envelope Encryption Strategy","text":"<p>The envelope encryption approach represents a sophisticated method of securing data. Instead of directly encrypting large datasets with master keys, KMS generates unique data keys for each encryption task. The data key encrypts the actual content, while the master key protects the data key itself.</p> <p>This strategy offers multiple advantages: - Improved performance for large-scale encryption - Reduced computational complexity - Enhanced key rotation and management flexibility</p>"},{"location":"aws/security/kms/#types-of-encryption-in-aws-s3","title":"Types of Encryption in AWS S3","text":""},{"location":"aws/security/kms/#sse-s3-server-side-encryption-with-amazon-s3-managed-keys","title":"SSE-S3 (Server-Side Encryption with Amazon S3-Managed Keys)","text":"<p>SSE-S3 represents a straightforward encryption method managed entirely by Amazon S3. It uses 256-bit Advanced Encryption Standard (AES-256), with encryption keys automatically generated and managed by AWS. Each object receives a unique data key, encrypted with a master key controlled by Amazon.</p>"},{"location":"aws/security/kms/#sse-c-server-side-encryption-with-customer-provided-keys","title":"SSE-C (Server-Side Encryption with Customer-Provided Keys)","text":"<p>SSE-C provides maximum encryption control to customers. Users must provide their own 32-byte encryption keys for each API request. Unlike SSE-S3, AWS does not store these keys, placing complete key management responsibility on the customer. This approach supports AES-256 encryption and requires key transmission over HTTPS.</p>"},{"location":"aws/security/kms/#access-control-and-governance","title":"Access Control and Governance","text":""},{"location":"aws/security/kms/#iam-integration","title":"IAM Integration","text":"<p>AWS Identity and Access Management (IAM) provides granular control over KMS key usage. Administrators can define precise policies determining which users, roles, and services can perform specific cryptographic operations.</p> <p>The integration allows for extremely fine-grained access controls, such as limiting key usage to specific AWS services, restricting decryption operations, or implementing time-based access restrictions.</p>"},{"location":"aws/security/kms/#auditing-and-compliance","title":"Auditing and Compliance","text":"<p>AWS CloudTrail integration enables comprehensive logging of all KMS-related activities. Every key generation, encryption, decryption, and administrative action can be meticulously tracked, providing an immutable audit trail crucial for regulatory compliance and security investigations.</p>"},{"location":"aws/security/kms/#advanced-security-features","title":"Advanced Security Features","text":""},{"location":"aws/security/kms/#key-rotation-mechanisms","title":"Key Rotation Mechanisms","text":"<p>KMS supports automatic and manual key rotation strategies. Automatic rotation can occur annually, ensuring that cryptographic keys are regularly refreshed without manual intervention. Manual rotation provides additional flexibility for organizations with specific compliance requirements.</p>"},{"location":"aws/security/kms/#multi-region-keys","title":"Multi-Region Keys","text":"<p>For global organizations requiring consistent encryption across multiple geographic regions, KMS offers multi-region key capabilities. These keys can be replicated across AWS regions, maintaining cryptographic consistency while adhering to data residency requirements.</p>"},{"location":"aws/security/kms/#practical-implementation-scenarios","title":"Practical Implementation Scenarios","text":""},{"location":"aws/security/kms/#secure-storage-encryption","title":"Secure Storage Encryption","text":"<p>Amazon S3 buckets can leverage KMS for transparent, server-side encryption. By associating a KMS key with a storage bucket, all objects are automatically encrypted at rest, with decryption handled seamlessly during access.</p>"},{"location":"aws/security/kms/#database-encryption","title":"Database Encryption","text":"<p>Databases like Amazon RDS can integrate KMS for column-level or full-disk encryption. This approach ensures that sensitive information remains protected, even if underlying storage systems are compromised.</p>"},{"location":"aws/security/kms/#application-level-encryption","title":"Application-Level Encryption","text":"<p>Developers can directly integrate KMS into applications using AWS SDKs, enabling sophisticated encryption workflows that extend beyond infrastructure-level protection.</p>"},{"location":"aws/security/kms/#performance-and-scalability-considerations","title":"Performance and Scalability Considerations","text":"<p>While providing robust security, KMS is designed with performance in mind. The service can handle millions of cryptographic requests per second, with minimal latency. Cryptographic operations are offloaded to specialized hardware security modules, ensuring both speed and security.</p>"},{"location":"aws/security/kms/#cost-management","title":"Cost Management","text":"<p>KMS follows a usage-based pricing model. Costs are primarily associated with key storage, key usage, and the number of cryptographic operations. The service offers a free tier for basic usage, making it accessible for both small and large-scale deployments.</p>"},{"location":"aws/security/kms/#emerging-trends-and-future-directions","title":"Emerging Trends and Future Directions","text":"<p>As cloud security evolves, KMS continues to expand its capabilities. Emerging trends include enhanced machine learning-driven anomaly detection, more sophisticated key lifecycle management, and deeper integration with emerging compliance frameworks.</p>"},{"location":"aws/security/kms/#conclusion","title":"Conclusion","text":"<p>AWS Key Management Service represents more than a technical solution\u2014it\u2019s a comprehensive approach to cryptographic governance in cloud environments. By providing a flexible, secure, and integrated key management platform, KMS empowers organizations to implement robust security strategies without sacrificing operational efficiency.</p> <p>The true power of KMS lies not just in its technical capabilities, but in its ability to transform complex security challenges into manageable, transparent processes.</p>"},{"location":"aws/security/macie/","title":"Macie","text":""},{"location":"aws/security/macie/#overview","title":"Overview","text":"<p>AWS Macie is a fully managed data security and privacy service that leverages machine learning and pattern matching to discover and protect sensitive data in AWS environments.</p>"},{"location":"aws/security/macie/#key-features","title":"Key Features","text":""},{"location":"aws/security/macie/#data-discovery","title":"Data Discovery","text":"<ul> <li>Identifies sensitive information across AWS storage services</li> <li>Specializes in detecting Personally Identifiable Information (PII)</li> <li>Focuses primarily on Amazon S3 bucket content analysis</li> </ul>"},{"location":"aws/security/macie/#machine-learning-capabilities","title":"Machine Learning Capabilities","text":"<ul> <li>Uses advanced pattern matching techniques</li> <li>Automatically identifies potential data privacy risks</li> <li>Continuously learns and improves detection accuracy</li> </ul>"},{"location":"aws/security/macie/#integration-ecosystem","title":"Integration Ecosystem","text":"<ul> <li>Seamless integration with Amazon EventBridge</li> <li>Provides notification mechanisms for detected sensitive data</li> <li>Enables automated response and alerting workflows</li> </ul>"},{"location":"aws/security/macie/#primary-use-cases","title":"Primary Use Cases","text":"<ul> <li>Data privacy compliance</li> <li>Sensitive information protection</li> <li>Risk assessment and mitigation</li> <li>Regulatory compliance support</li> </ul>"},{"location":"aws/security/macie/#benefits","title":"Benefits","text":"<ul> <li>Automated sensitive data detection</li> <li>Reduced manual security monitoring</li> <li>Enhanced data protection</li> <li>Proactive risk identification</li> <li>Supports various compliance frameworks</li> </ul>"},{"location":"aws/security/macie/#supported-data-types","title":"Supported Data Types","text":"<ul> <li>Personally Identifiable Information (PII)</li> <li>Financial data</li> <li>Personal health information</li> <li>Confidential corporate data</li> </ul>"},{"location":"aws/security/macie/#alerting-and-reporting","title":"Alerting and Reporting","text":"<ul> <li>Real-time sensitive data discovery alerts</li> <li>Detailed findings and risk assessments</li> <li>Configurable notification mechanisms</li> </ul>"},{"location":"aws/security/secrets_manager/","title":"Secrets Manager","text":""},{"location":"aws/security/secrets_manager/#overview","title":"Overview","text":"<p>AWS Secrets Manager is a specialized service designed for storing and managing sensitive information in AWS cloud environments. As a newer addition to AWS\u2019s security services portfolio, it provides enhanced capabilities specifically focused on secrets management and rotation.</p>"},{"location":"aws/security/secrets_manager/#core-features","title":"Core Features","text":""},{"location":"aws/security/secrets_manager/#secret-storage-and-security","title":"Secret Storage and Security","text":"<p>Secrets Manager provides robust encryption for all stored secrets using AWS Key Management Service (KMS). This mandatory encryption ensures that sensitive information remains secure at rest and during transmission. The service is particularly optimized for managing database credentials, especially for Amazon RDS instances.</p>"},{"location":"aws/security/secrets_manager/#automated-secret-rotation","title":"Automated Secret Rotation","text":"<p>One of the most powerful features of Secrets Manager is its ability to force automatic rotation of secrets at defined intervals. Organizations can configure the service to rotate secrets every specified number of days, enhancing security through regular credential updates. This rotation process is automated through AWS Lambda functions, which handle the generation and distribution of new secrets.</p>"},{"location":"aws/security/secrets_manager/#database-integration","title":"Database Integration","text":"<p>Secrets Manager offers seamless integration with various Amazon RDS database engines, including MySQL, PostgreSQL, and Aurora. This integration simplifies the management of database credentials and supports automated rotation of database access credentials, making it particularly valuable for organizations using RDS services.</p>"},{"location":"aws/security/secrets_manager/#multi-region-secrets-management","title":"Multi-Region Secrets Management","text":""},{"location":"aws/security/secrets_manager/#secret-replication","title":"Secret Replication","text":"<p>Secrets Manager supports the replication of secrets across multiple AWS regions. This capability enables organizations to maintain synchronized copies of their secrets across different geographical locations, ensuring high availability and disaster recovery readiness.</p>"},{"location":"aws/security/secrets_manager/#synchronization-and-management","title":"Synchronization and Management","text":"<p>The service maintains automatic synchronization between the primary secret and its read replicas across regions. When the primary secret is updated, Secrets Manager automatically propagates these changes to all replica secrets, ensuring consistency across regions.</p>"},{"location":"aws/security/secrets_manager/#replica-promotion","title":"Replica Promotion","text":"<p>Organizations have the flexibility to promote a read replica secret to a standalone secret when needed. This feature provides additional management options and supports various architectural patterns.</p>"},{"location":"aws/security/secrets_manager/#use-cases","title":"Use Cases","text":"<p>Multi-region secrets management supports several critical scenarios: - Multi-region application deployments - Disaster recovery planning and implementation - Multi-region database deployments - Global application architecture requirements</p>"},{"location":"aws/security/secrets_manager/#comparison-with-ssm-parameter-store","title":"Comparison with SSM Parameter Store","text":""},{"location":"aws/security/secrets_manager/#cost-considerations","title":"Cost Considerations","text":"<p>Secrets Manager is positioned as a premium service with higher associated costs, reflecting its specialized features and capabilities. While more expensive than Parameter Store, it offers advanced functionality specifically designed for secrets management.</p>"},{"location":"aws/security/secrets_manager/#functionality-differences","title":"Functionality Differences","text":""},{"location":"aws/security/secrets_manager/#secrets-manager-advantages","title":"Secrets Manager Advantages","text":"<p>Secrets Manager provides built-in secret rotation capabilities through AWS Lambda, with pre-configured Lambda functions available for RDS, Redshift, and DocumentDB. The service mandates KMS encryption for all secrets and offers native integration with CloudFormation for infrastructure as code implementations.</p>"},{"location":"aws/security/secrets_manager/#parameter-store-features","title":"Parameter Store Features","text":"<p>Parameter Store offers a simpler API and more cost-effective solution for basic parameter management. While it doesn\u2019t include built-in secret rotation, organizations can implement rotation using Lambda functions triggered by EventBridge. KMS encryption remains optional in Parameter Store, providing flexibility in security implementation. The service supports CloudFormation integration and can access Secrets Manager secrets through its API.</p>"},{"location":"aws/security/secrets_manager/#best-practices","title":"Best Practices","text":"<p>When implementing Secrets Manager, consider these recommended practices: - Implement appropriate secret rotation schedules based on security requirements - Utilize multi-region replication for globally distributed applications - Configure proper IAM policies for secret access - Monitor secret rotation events and failures - Regularly audit secret access and usage</p>"},{"location":"aws/security/secrets_manager/#conclusion","title":"Conclusion","text":"<p>AWS Secrets Manager provides a comprehensive solution for organizations requiring robust secrets management with automated rotation capabilities. Its strong integration with RDS services and support for multi-region deployments makes it particularly valuable for organizations with complex database management requirements or global application architectures. While more costly than Parameter Store, its specialized features justify the investment for use cases requiring advanced secrets management capabilities.</p>"},{"location":"aws/security/shield/","title":"Shield","text":""},{"location":"aws/security/shield/#overview","title":"Overview","text":"<p>AWS Shield is a managed Distributed Denial of Service (DDoS) protection service designed to safeguard web applications running on AWS infrastructure.</p>"},{"location":"aws/security/shield/#protection-levels","title":"Protection Levels","text":"<ul> <li>AWS Shield Standard: </li> <li>Automatic protection for all AWS customers</li> <li>Free service for AWS resources</li> <li> <p>Protects against common DDoS attacks</p> </li> <li> <p>AWS Shield Advanced: </p> </li> <li>Paid service with comprehensive protection</li> <li>Detailed attack diagnostics</li> <li>Custom incident response team</li> </ul>"},{"location":"aws/security/shield/#supported-resources","title":"Supported Resources","text":"<ul> <li>Amazon CloudFront distributions</li> <li>Amazon Route 53 hosted zones</li> <li>Elastic Load Balancers</li> <li>AWS Global Accelerator</li> <li>Amazon EC2 instances</li> </ul>"},{"location":"aws/security/shield/#key-features","title":"Key Features","text":"<ul> <li>Real-time attack detection</li> <li>Traffic filtering</li> <li>Automatic traffic rate limiting</li> <li>Layer \u00be and Layer 7 protection</li> <li>Comprehensive threat mitigation</li> </ul>"},{"location":"aws/security/shield/#attack-types-mitigated","title":"Attack Types Mitigated","text":"<ul> <li>SYN floods</li> <li>UDP reflection attacks</li> <li>HTTP/HTTPS floods</li> <li>Volumetric attacks</li> <li>Protocol attacks</li> <li>Application-layer attacks</li> </ul>"},{"location":"aws/security/shield/#benefits","title":"Benefits","text":"<ul> <li>Minimizes application downtime</li> <li>Reduces infrastructure vulnerability</li> <li>Scalable protection</li> <li>Seamless integration with AWS services</li> <li>Continuous monitoring and protection</li> </ul>"},{"location":"aws/security/shield/#threat-detection-mechanisms","title":"Threat Detection Mechanisms","text":"<ul> <li>Machine learning algorithms</li> <li>Behavioral analysis</li> <li>Traffic pattern recognition</li> <li>Anomaly detection</li> </ul>"},{"location":"aws/security/sso/","title":"Single Sign-On (SSO)","text":""},{"location":"aws/security/sso/#overview","title":"Overview","text":"<p>AWS Single Sign-On (SSO) is a cloud-based service that simplifies user access management across multiple AWS accounts and business applications, enabling centralized access control and authentication.</p>"},{"location":"aws/security/sso/#key-features","title":"Key Features","text":"<ul> <li>Centralized access management</li> <li>Single login for multiple AWS accounts</li> <li>Integration with corporate directories</li> <li>Simplified user provisioning and deprovisioning</li> <li>Role-based access control</li> </ul>"},{"location":"aws/security/sso/#authentication-methods","title":"Authentication Methods","text":"<ul> <li>Active Directory integration</li> <li>SAML 2.0 identity providers</li> <li>AWS Organizations support</li> <li>Multi-factor authentication</li> <li>Custom identity source</li> </ul>"},{"location":"aws/security/sso/#access-management-capabilities","title":"Access Management Capabilities","text":"<ul> <li>Manage user permissions</li> <li>Assign application access</li> <li>Control account-level access</li> <li>Automated user provisioning</li> <li>Granular permission settings</li> </ul>"},{"location":"aws/security/sso/#supported-integrations","title":"Supported Integrations","text":"<ul> <li>Microsoft Active Directory</li> <li>Okta</li> <li>Azure AD</li> <li>Google Workspace</li> <li>SAML-compatible identity providers</li> </ul>"},{"location":"aws/security/sso/#use-cases","title":"Use Cases","text":"<ul> <li>Enterprise access management</li> <li>Simplified cloud account administration</li> <li>Secure application access</li> <li>Centralized identity governance</li> <li>Multi-account AWS environment management</li> </ul>"},{"location":"aws/security/sso/#security-benefits","title":"Security Benefits","text":"<ul> <li>Reduced credential management overhead</li> <li>Centralized access policy enforcement</li> <li>Enhanced authentication security</li> <li>Simplified compliance management</li> <li>Consistent access controls</li> </ul>"},{"location":"aws/storage/ebs/","title":"Elastic Block Store (EBS)","text":"<p>Amazon Elastic Block Store (EBS) is a scalable, high-performance block storage service designed for Amazon Elastic Compute Cloud (EC2) instances. It provides persistent storage that can be attached to EC2 instances to store data in a highly available and durable manner.</p>"},{"location":"aws/storage/ebs/#key-features","title":"Key Features","text":""},{"location":"aws/storage/ebs/#1-persistence-and-durability","title":"1. Persistence and Durability","text":"<p>EBS volumes are designed to be highly durable, with data replicated within the same Availability Zone (AZ) to prevent data loss from hardware failures.</p>"},{"location":"aws/storage/ebs/#2-scalability","title":"2. Scalability","text":"<p>You can dynamically scale the storage capacity of EBS volumes without disrupting operations, allowing your applications to adapt to changing workloads.</p>"},{"location":"aws/storage/ebs/#3-performance-options","title":"3. Performance Options","text":"<p>EBS offers a range of volume types with varying performance characteristics: - General Purpose SSD (gp2, gp3): Balanced price and performance for general workloads. - Provisioned IOPS SSD (io1, io2): High performance for mission-critical applications. - Throughput Optimized HDD (st1): Optimized for large, sequential workloads. - Cold HDD (sc1): Low-cost storage for infrequent access. - Magnetic Volumes (standard): Legacy storage option (deprecated in many regions).</p>"},{"location":"aws/storage/ebs/#4-snapshots","title":"4. Snapshots","text":"<p>EBS supports incremental snapshots to Amazon S3, allowing you to back up volumes at any point in time.</p>"},{"location":"aws/storage/ebs/#5-encryption","title":"5. Encryption","text":"<p>Data stored on EBS volumes can be encrypted using AWS Key Management Service (KMS). Encryption includes data at rest, data in transit, and volume snapshots.</p>"},{"location":"aws/storage/ebs/#6-multi-attach","title":"6. Multi-Attach","text":"<p>EBS io2 volumes support Multi-Attach, allowing a single volume to be attached to multiple EC2 instances within the same AZ.</p>"},{"location":"aws/storage/ebs/#7-integration-with-ec2-auto-scaling","title":"7. Integration with EC2 Auto Scaling","text":"<p>EBS integrates seamlessly with EC2 Auto Scaling, ensuring that storage scales along with compute resources.</p>"},{"location":"aws/storage/ebs/#volume-types","title":"Volume Types","text":"Volume Type Use Case Performance gp2 (General Purpose SSD) General-purpose workloads Baseline: 3 IOPS/GB, burst up to 16,000 IOPS gp3 General-purpose workloads Baseline: 3,000 IOPS, adjustable up to 16,000 IOPS io1 High-performance applications Provisioned up to 64,000 IOPS io2 Mission-critical, high-durability apps Provisioned up to 64,000 IOPS with durability of 99.999% st1 (Throughput Optimized HDD) Large, sequential I/O workloads Max throughput: 500 MiB/s sc1 (Cold HDD) Infrequent data access Max throughput: 250 MiB/s"},{"location":"aws/storage/ebs/#common-use-cases","title":"Common Use Cases","text":"<ol> <li>Relational Databases: Store transactional data for applications like MySQL, PostgreSQL, and Oracle.</li> <li>Big Data Analytics: Provide high throughput for processing large datasets in Hadoop and Spark.</li> <li>Content Management Systems: Host media files or application data.</li> <li>Backup and Recovery: Use snapshots to create point-in-time backups and ensure business continuity.</li> <li>Boot Volumes: Serve as boot devices for EC2 instances.</li> <li>Shared File Systems: Support shared storage with Multi-Attach-enabled volumes.</li> </ol>"},{"location":"aws/storage/ebs/#snapshots","title":"Snapshots","text":""},{"location":"aws/storage/ebs/#key-features-of-snapshots","title":"Key Features of Snapshots:","text":"<ol> <li>Incremental Backups: Only changes since the last snapshot are saved, reducing storage costs.</li> <li>Cross-Region Copy: Snapshots can be copied to other regions for disaster recovery.</li> <li>Lifecycle Policies: Automate snapshot creation and retention using Amazon Data Lifecycle Manager.</li> </ol>"},{"location":"aws/storage/ebs/#creating-a-snapshot","title":"Creating a Snapshot:","text":"<p>Using AWS CLI: <pre><code>aws ec2 create-snapshot \\\n    --volume-id vol-0abcd1234efgh5678 \\\n    --description \"My EBS Snapshot\"\n</code></pre></p>"},{"location":"aws/storage/ebs/#restoring-from-a-snapshot","title":"Restoring from a Snapshot:","text":"<p>Using AWS CLI: <pre><code>aws ec2 create-volume \\\n    --snapshot-id snap-0abcd1234efgh5678 \\\n    --availability-zone us-east-1a\n</code></pre></p>"},{"location":"aws/storage/ebs/#encryption","title":"Encryption","text":""},{"location":"aws/storage/ebs/#how-ebs-encryption-works","title":"How EBS Encryption Works","text":"<ol> <li>Encryption uses AWS KMS-managed keys or customer-managed keys.</li> <li>Encryption applies to data at rest, data in transit between the volume and the instance, and all backups.</li> </ol>"},{"location":"aws/storage/ebs/#enabling-encryption","title":"Enabling Encryption:","text":"<ol> <li>Default Encryption: Enable default encryption for all new volumes in your AWS account.</li> <li>Volume-Specific Encryption: Specify encryption when creating a volume.</li> </ol> <p>Using AWS CLI: <pre><code>aws ec2 create-volume \\\n    --size 10 \\\n    --availability-zone us-east-1a \\\n    --encrypted\n</code></pre></p>"},{"location":"aws/storage/ebs/#performance-optimization","title":"Performance Optimization","text":""},{"location":"aws/storage/ebs/#1-choose-the-right-volume-type","title":"1. Choose the Right Volume Type","text":"<ul> <li>Match volume type to workload characteristics (e.g., SSD for random I/O, HDD for sequential workloads).</li> </ul>"},{"location":"aws/storage/ebs/#2-monitor-performance-metrics","title":"2. Monitor Performance Metrics","text":"<ul> <li>Use Amazon CloudWatch to monitor IOPS, throughput, and latency.</li> </ul>"},{"location":"aws/storage/ebs/#3-use-elastic-volumes","title":"3. Use Elastic Volumes","text":"<ul> <li>Modify volume size, type, or IOPS without downtime.</li> </ul>"},{"location":"aws/storage/ebs/#pricing","title":"Pricing","text":"<p>EBS costs depend on several factors: 1. Volume Type: Cost per GB varies by type (e.g., gp3 vs. io2). 2. Provisioned IOPS: Additional charges for provisioned IOPS for io1/io2 volumes. 3. Snapshots: Charged based on the storage used by the snapshot. 4. Data Transfer: Charges may apply for data transfer across regions.</p> <p>Refer to the EBS Pricing Page for detailed information.</p>"},{"location":"aws/storage/ebs/#limits-and-considerations","title":"Limits and Considerations","text":""},{"location":"aws/storage/ebs/#limits","title":"Limits:","text":"<ul> <li>Volume Size: Up to 16 TiB.</li> <li>IOPS: Up to 64,000 IOPS (for io1/io2).</li> <li>Throughput: Up to 1,000 MiB/s.</li> <li>Provisioned IOPS Ratio: The maximum ratio of provisioned IOPS to volume size is 50:1 for io1 and io2 volumes.</li> </ul>"},{"location":"aws/storage/ebs/#considerations","title":"Considerations:","text":"<ol> <li>AZ-Specific: Volumes are tied to a specific AZ; cross-AZ usage requires snapshots or replication.</li> <li>Performance Throttling: Exceeding IOPS/throughput limits can lead to throttling.</li> <li>Data Durability: Snapshots are critical for long-term data durability.</li> </ol>"},{"location":"aws/storage/ebs/#best-practices","title":"Best Practices","text":"<ol> <li>Backup Regularly: Use snapshots for periodic backups.</li> <li>Use Tags: Organize EBS volumes and snapshots with meaningful tags.</li> <li>Monitor Costs: Regularly analyze usage and optimize volume types to control costs.</li> <li>Use Elastic Volumes: Resize or change volume types as workloads evolve.</li> <li>Enable Encryption: Protect sensitive data with EBS encryption.</li> </ol> <p>For more details, refer to the EBS Documentation.</p>"},{"location":"aws/storage/efs/","title":"Elastic File System (EFS)","text":""},{"location":"aws/storage/efs/#introduction","title":"Introduction","text":"<p>Amazon Elastic File System (EFS) is a managed Network File System (NFS) designed to provide scalable, shared storage for EC2 instances. The service enables multiple EC2 instances to simultaneously access a common file system across multiple Availability Zones (AZs).</p>"},{"location":"aws/storage/efs/#core-functionality","title":"Core Functionality","text":"<p>EFS operates as a fully managed NFS service that supports concurrent mounting from multiple EC2 instances. The system delivers high availability by working across multiple AZs within a region. While it comes at a higher cost point (approximately three times that of gp2), its scalability and pay-per-use pricing model make it suitable for various enterprise applications.</p>"},{"location":"aws/storage/efs/#use-cases-and-compatibility","title":"Use Cases and Compatibility","text":"<p>The service excels in several key applications, including content management systems, web serving, data sharing, and WordPress hosting. EFS implements the NFSv4.1 protocol and is specifically designed for Linux-based AMIs, making it incompatible with Windows environments. The system provides POSIX-compliant file system access with standard file APIs, enabling seamless integration with existing Linux-based applications.</p> <p>Security is managed through AWS security groups, which control access to EFS resources. Data security is enhanced through KMS-based encryption at rest. The file system\u2019s automatic scaling capability eliminates the need for capacity planning, allowing users to pay only for the storage they consume.</p>"},{"location":"aws/storage/efs/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"aws/storage/efs/#scaling-capabilities","title":"Scaling Capabilities","text":"<p>EFS demonstrates impressive scaling capabilities, supporting thousands of concurrent NFS clients with throughput exceeding 10 GB/s. The system can automatically expand to petabyte-scale, requiring no manual intervention for growth management.</p>"},{"location":"aws/storage/efs/#performance-modes","title":"Performance Modes","text":"<p>EFS offers two performance modes, which must be selected during creation:</p> <p>The General Purpose mode serves as the default option, optimized for latency-sensitive applications such as web servers and content management systems. The Max I/O mode caters to high-throughput requirements and parallel processing scenarios, commonly used in big data analytics and media processing, though it may introduce higher latency.</p>"},{"location":"aws/storage/efs/#throughput-options","title":"Throughput Options","text":"<p>EFS provides three throughput modes to match different workload requirements:</p> <p>The Bursting mode provides baseline throughput of 50MiB/s per TB of storage, with burst capability up to 100MiB/s. Provisioned mode allows setting specific throughput requirements independent of storage size, supporting configurations up to 1 GiB/s per TB of storage. The Elastic mode automatically adjusts throughput based on workload demands, supporting up to 3GiB/s for reads and 1GiB/s for writes, making it ideal for unpredictable workload patterns.</p>"},{"location":"aws/storage/efs/#storage-classes-and-management","title":"Storage Classes and Management","text":""},{"location":"aws/storage/efs/#tiered-storage","title":"Tiered Storage","text":"<p>EFS implements a lifecycle management feature that enables automatic file movement between storage tiers based on access patterns:</p> <p>The Standard tier serves frequently accessed files with optimal performance. The Infrequent Access tier (EFS-IA) offers lower storage costs but includes retrieval fees. The Archive tier provides the most cost-effective storage for rarely accessed data, offering up to 50% cost savings for files accessed only a few times annually.</p>"},{"location":"aws/storage/efs/#availability-options","title":"Availability Options","text":"<p>EFS offers two availability configurations:</p> <p>The Standard configuration provides Multi-AZ redundancy, making it suitable for production environments. The One Zone configuration limits storage to a single AZ, offering a more cost-effective solution for development environments while maintaining backup capabilities by default. One Zone storage is compatible with IA storage (EFS One Zone-IA) and can provide over 90% cost savings compared to standard configurations.</p>"},{"location":"aws/storage/efs/#ebs-vs-efs-comparison","title":"EBS vs EFS Comparison","text":""},{"location":"aws/storage/efs/#storage-characteristics","title":"Storage Characteristics","text":"<p>EFS differs significantly from EBS in several key aspects. While EBS volumes are typically limited to single instance attachment (except for multi-attach io1/io2) and confined to specific AZs, EFS provides simultaneous access to hundreds of instances across multiple AZs. This makes EFS particularly suitable for shared file system requirements, such as WordPress installations.</p>"},{"location":"aws/storage/efs/#cost-and-performance-considerations","title":"Cost and Performance Considerations","text":"<p>Though EFS commands a higher price point than EBS, it offers cost optimization through storage tiers. The decision between EFS, EBS, and Instance Store should consider factors such as access patterns, performance requirements, and budget constraints. EFS\u2019s ability to serve as a centralized storage solution can often justify its higher cost through reduced operational complexity and improved data consistency across instances.</p> <p>The choice between these storage solutions should be guided by specific application requirements, considering factors such as access patterns, performance needs, sharing requirements, and cost constraints.</p>"},{"location":"aws/storage/s3/","title":"Simple Storage Service (S3)","text":""},{"location":"aws/storage/s3/#introduction","title":"Introduction","text":"<p>Amazon S3 (Simple Storage Service) is an object storage service offering industry-leading scalability, data availability, security, and performance. It can store and retrieve any amount of data from anywhere on the web, making it a versatile solution for backup, archiving, content distribution, and data lakes.</p>"},{"location":"aws/storage/s3/#core-concepts","title":"Core Concepts","text":""},{"location":"aws/storage/s3/#buckets","title":"Buckets","text":"<ul> <li>Containers for objects stored in S3</li> <li>Must have a globally unique name</li> <li>Region-specific</li> <li>No limit to objects in a bucket</li> <li>Flat structure (no real hierarchy)</li> </ul>"},{"location":"aws/storage/s3/#objects","title":"Objects","text":"<ul> <li>Basic storage unit in S3</li> <li>Consists of:</li> <li>Data (the file)</li> <li>Key (the file name)</li> <li>Metadata (data about the data)</li> <li>Version ID (if versioning enabled)</li> <li>Size limits:</li> <li>Single object: 5TB</li> <li>Single PUT: 5GB</li> </ul>"},{"location":"aws/storage/s3/#storage-classes","title":"Storage Classes","text":""},{"location":"aws/storage/s3/#s3-standard","title":"S3 Standard","text":"<ul> <li>Default storage class</li> <li>99.99% availability</li> <li>11 9\u2019s durability</li> <li>Multiple AZ replication</li> <li>Best for: Frequently accessed data</li> </ul>"},{"location":"aws/storage/s3/#s3-intelligent-tiering","title":"S3 Intelligent-Tiering","text":"<ul> <li>Automatic cost optimization</li> <li>Moves objects between access tiers</li> <li>No retrieval fees</li> <li>Small monthly monitoring fee</li> <li>Best for: Unknown or changing access patterns</li> </ul>"},{"location":"aws/storage/s3/#s3-standard-ia-infrequent-access","title":"S3 Standard-IA (Infrequent Access)","text":"<ul> <li>Lower storage cost than Standard</li> <li>Higher retrieval cost</li> <li>99.9% availability</li> <li>Best for: Less frequently accessed data</li> </ul>"},{"location":"aws/storage/s3/#s3-one-zone-ia","title":"S3 One Zone-IA","text":"<ul> <li>Single AZ storage</li> <li>Lower cost than Standard-IA</li> <li>99.5% availability</li> <li>Best for: Reproducible, infrequently accessed data</li> </ul>"},{"location":"aws/storage/s3/#s3-glacier","title":"S3 Glacier","text":"<ul> <li>Long-term archival storage</li> <li>Retrieval times: minutes to hours</li> <li>Significantly lower storage cost</li> <li>Best for: Long-term backups and archives</li> </ul>"},{"location":"aws/storage/s3/#s3-glacier-deep-archive","title":"S3 Glacier Deep Archive","text":"<ul> <li>Lowest cost storage option</li> <li>Retrieval time: 12 hours</li> <li>Best for: Long-term data retention (7-10 years)</li> </ul>"},{"location":"aws/storage/s3/#data-protection","title":"Data Protection","text":""},{"location":"aws/storage/s3/#versioning","title":"Versioning","text":"<p><pre><code>{\n    \"VersioningConfiguration\": {\n        \"Status\": \"Enabled\"\n    }\n}\n</code></pre> - Maintains multiple versions of objects - Protects against accidental deletions - Can be suspended but not disabled - Increases storage costs</p>"},{"location":"aws/storage/s3/#replication","title":"Replication","text":"<p>Types: 1. Cross-Region Replication (CRR) 2. Same-Region Replication (SRR)</p> <pre><code>{\n    \"ReplicationConfiguration\": {\n        \"Role\": \"arn:aws:iam::account-id:role/role-name\",\n        \"Rules\": [\n            {\n                \"Status\": \"Enabled\",\n                \"Destination\": {\n                    \"Bucket\": \"arn:aws:s3:::destination-bucket\"\n                }\n            }\n        ]\n    }\n}\n</code></pre>"},{"location":"aws/storage/s3/#object-lock","title":"Object Lock","text":"<ul> <li>Write-once-read-many (WORM)</li> <li>Retention periods</li> <li>Legal holds</li> <li>Compliance mode</li> </ul>"},{"location":"aws/storage/s3/#security","title":"Security","text":""},{"location":"aws/storage/s3/#access-control","title":"Access Control","text":"<ol> <li> <p>IAM Policies <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:PutObject\"\n            ],\n            \"Resource\": \"arn:aws:s3:::bucket-name/*\"\n        }\n    ]\n}\n</code></pre></p> </li> <li> <p>Bucket Policies <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"PublicRead\",\n            \"Effect\": \"Allow\",\n            \"Principal\": \"*\",\n            \"Action\": \"s3:GetObject\",\n            \"Resource\": \"arn:aws:s3:::bucket-name/*\"\n        }\n    ]\n}\n</code></pre></p> </li> <li> <p>Access Control Lists (ACLs)</p> </li> <li>Legacy access control mechanism</li> <li>Granular permissions at object level</li> </ol>"},{"location":"aws/storage/s3/#s3-encryption","title":"S3 Encryption","text":"<p>Amazon S3 provides robust object encryption capabilities through multiple approaches. Encryption ensures data confidentiality and protection at rest, with users having flexibility in key management and encryption strategies.</p>"},{"location":"aws/storage/s3/#server-side-encryption-sse-encryption-at-rest","title":"Server-Side Encryption (SSE) - Encryption at rest","text":"<p>Limitation: - If you use SSE-KMS, you may be impacted by the KMS limits: when you upload, it calls the <code>GenerateDataKey</code> KMS API, when you download, it calls the <code>Decrypt</code> KMS API - Count towards the KMS quota per second (5500, 10000, 30000 req/s based on region) - You can request a quota increase using the Service Quotas Console</p>"},{"location":"aws/storage/s3/#sse-s3-aws-managed-encryption","title":"SSE-S3 (AWS-Managed Encryption)","text":"<p>AWS provides default server-side encryption using keys entirely managed by Amazon. This method uses AES-256 encryption and is automatically enabled for new buckets and objects. When using SSE-S3, AWS handles all aspects of key management, simplifying the encryption process for users. Must set header <code>\"x-amz-server-side-encryption\": \"AES256\"</code></p>"},{"location":"aws/storage/s3/#sse-kms-aws-key-management-service","title":"SSE-KMS (AWS Key Management Service)","text":"<p>This encryption method leverages AWS Key Management Service for enhanced control and auditability. Users can manage encryption keys through KMS, enabling granular control and the ability to track key usage via CloudTrail. However, users should be aware of KMS request quotas, which vary by region and may require service quota increases for high-volume operations. Must set header <code>\"x-amz-server-side-encryption\": \"aws:kms\"</code></p>"},{"location":"aws/storage/s3/#sse-c-customer-provided-keys","title":"SSE-C (Customer-Provided Keys)","text":"For organizations requiring complete key management control, SSE-C allows customers to manage their encryption keys externally from AWS. Critical requirements include using HTTPS and providing encryption keys with every HTTP request. Importantly, Amazon S3 does not store the provided encryption keys, maintaining maximum customer control."},{"location":"aws/storage/s3/#client-side-encryption","title":"Client-Side Encryption","text":"<p>In client-side encryption, data is encrypted by the client before transmission to Amazon S3. Clients use specialized libraries like the Amazon S3 Client-Side Encryption Library, managing the entire encryption lifecycle independently. This approach provides maximum control but requires more complex implementation.</p>"},{"location":"aws/storage/s3/#encryption-in-transit","title":"Encryption in Transit","text":"<p>Amazon S3 supports encryption during data transfer through SSL/TLS. While HTTP endpoints exist, HTTPS is strongly recommended and mandatory for certain encryption methods like SSE-C. Most clients default to the encrypted HTTPS endpoint, ensuring secure data transmission.</p>"},{"location":"aws/storage/s3/#access-control-and-security-mechanisms","title":"Access Control and Security Mechanisms","text":""},{"location":"aws/storage/s3/#bucket-policies-and-encryption-enforcement","title":"Bucket Policies and Encryption Enforcement","text":"<p>S3 allows enforcing encryption through bucket policies. Administrators can configure policies to reject API calls that do not include proper encryption headers, ensuring all stored objects meet security requirements. These policies are evaluated before default encryption settings.</p>"},{"location":"aws/storage/s3/#multi-factor-authentication-mfa-delete","title":"Multi-Factor Authentication (MFA) Delete","text":"<p>For critical buckets, MFA Delete provides an additional security layer. Bucket owners can require multi-factor authentication for sensitive operations like permanently deleting object versions or suspending versioning.</p>"},{"location":"aws/storage/s3/#access-points","title":"Access Points","text":"<p>S3 Access Points simplify security management by providing: - Unique DNS names - Granular access policies - VPC-origin configurations for enhanced network security</p>"},{"location":"aws/storage/s3/#logging-and-auditing","title":"Logging and Auditing","text":"<p>S3 supports comprehensive access logging, recording all bucket access attempts regardless of authorization status. These logs can be stored in a separate S3 bucket and analyzed using various data tools, enabling thorough security monitoring.</p>"},{"location":"aws/storage/s3/#cross-origin-resource-sharing-cors","title":"Cross-Origin Resource Sharing (CORS)","text":"<p>S3 supports CORS, allowing controlled cross-origin web browser requests. By configuring CORS headers, administrators can specify which origins can interact with S3 resources, enhancing web application security.</p>"},{"location":"aws/storage/s3/#pre-signed-urls","title":"Pre-Signed URLs","text":"<p>For temporary, controlled access, S3 generates pre-signed URLs with configurable expiration. These URLs inherit the permissions of the generating user, enabling secure, time-limited access to specific objects.</p>"},{"location":"aws/storage/s3/#advanced-security-features-s3-object-lambda","title":"Advanced Security Features: S3 Object Lambda","text":"<p>S3 Object Lambda introduces dynamic object transformation using AWS Lambda functions. This feature allows real-time modifications like: - Redacting sensitive information - Converting data formats - Dynamically modifying object content before retrieval</p> <p>By integrating these security strategies, AWS S3 provides a comprehensive, flexible approach to data protection, giving users multiple options to secure their cloud storage infrastructure.</p>"},{"location":"aws/storage/s3/#best-practices-for-encryption","title":"Best Practices for Encryption","text":"<ol> <li>Key Management</li> <li>Regularly rotate encryption keys</li> <li>Use different keys for different environments</li> <li>Implement key backup and recovery procedures</li> <li> <p>Monitor key usage with CloudTrail</p> </li> <li> <p>Policy Enforcement</p> </li> <li>Use bucket policies to enforce encryption</li> <li>Implement default encryption at bucket level</li> <li>Regular audit of encryption settings</li> <li> <p>Monitor for encryption-related events</p> </li> <li> <p>Compliance</p> </li> <li>Document encryption procedures</li> <li>Regular compliance audits</li> <li>Maintain encryption configuration inventory</li> <li>Test key rotation procedures</li> </ol>"},{"location":"aws/storage/s3/#data-management","title":"Data Management","text":""},{"location":"aws/storage/s3/#lifecycle-rules","title":"Lifecycle Rules","text":"<pre><code>{\n    \"Rules\": [\n        {\n            \"Status\": \"Enabled\",\n            \"Transition\": {\n                \"Days\": 30,\n                \"StorageClass\": \"STANDARD_IA\"\n            }\n        }\n    ]\n}\n</code></pre>"},{"location":"aws/storage/s3/#event-notifications","title":"Event Notifications","text":"<ul> <li>Triggers on object operations</li> <li>Destinations:</li> <li>SNS</li> <li>SQS</li> <li>Lambda</li> </ul>"},{"location":"aws/storage/s3/#performance-optimization","title":"Performance Optimization","text":""},{"location":"aws/storage/s3/#best-practices","title":"Best Practices","text":"<ol> <li>Prefix Naming</li> <li>Use random prefixes for high throughput</li> <li> <p>Example: <code>hex-hash/filename</code> instead of <code>date/filename</code></p> </li> <li> <p>Multipart Upload</p> </li> <li>Recommended for files &gt; 100MB</li> <li>Required for files &gt; 5GB</li> <li> <p>Parallel upload capability</p> </li> <li> <p>Transfer Acceleration</p> </li> <li>Uses CloudFront edge locations</li> <li>Faster long-distance transfers</li> <li>Additional cost per GB</li> </ol>"},{"location":"aws/storage/s3/#monitoring-and-analytics","title":"Monitoring and Analytics","text":""},{"location":"aws/storage/s3/#cloudwatch-metrics","title":"CloudWatch Metrics","text":"<ul> <li>Request metrics</li> <li>Replication metrics</li> <li>Storage metrics</li> </ul>"},{"location":"aws/storage/s3/#s3-analytics","title":"S3 Analytics","text":"<ul> <li>Storage class analysis</li> <li>Access pattern insights</li> <li>Lifecycle optimization recommendations</li> </ul>"},{"location":"aws/storage/s3/#storage-lens","title":"Storage Lens","text":"<ul> <li>Organization-wide visibility</li> <li>Usage and activity metrics</li> <li>Recommendations for optimization</li> </ul>"},{"location":"aws/storage/s3/#common-operations","title":"Common Operations","text":""},{"location":"aws/storage/s3/#basic-operations","title":"Basic Operations","text":"<pre><code># Upload file\naws s3 cp file.txt s3://bucket-name/\n\n# Download file\naws s3 cp s3://bucket-name/file.txt .\n\n# List objects\naws s3 ls s3://bucket-name/\n\n# Delete object\naws s3 rm s3://bucket-name/file.txt\n</code></pre>"},{"location":"aws/storage/s3/#bucket-operations","title":"Bucket Operations","text":"<pre><code># Create bucket\naws s3 mb s3://bucket-name\n\n# Delete bucket\naws s3 rb s3://bucket-name\n\n# Sync directories\naws s3 sync local-dir s3://bucket-name/remote-dir\n</code></pre>"},{"location":"aws/storage/s3/#cost-optimization","title":"Cost Optimization","text":""},{"location":"aws/storage/s3/#cost-components","title":"Cost Components","text":"<ol> <li>Storage pricing</li> <li>Per GB-month rates</li> <li> <p>Varies by storage class</p> </li> <li> <p>Request pricing</p> </li> <li>PUT, COPY, POST, LIST</li> <li> <p>GET, SELECT, and retrieval</p> </li> <li> <p>Data transfer</p> </li> <li>Inbound: usually free</li> <li>Outbound: charged per GB</li> </ol>"},{"location":"aws/storage/s3/#cost-reduction-strategies","title":"Cost Reduction Strategies","text":"<ol> <li>Use appropriate storage classes</li> <li>Implement lifecycle policies</li> <li>Enable compression</li> <li>Monitor usage with Cost Explorer</li> <li>Use S3 Analytics for optimization</li> </ol>"},{"location":"aws/storage/s3/#integration-with-other-aws-services","title":"Integration with Other AWS Services","text":"<ul> <li>CloudFront (Content Distribution)</li> <li>Lambda (Serverless Computing)</li> <li>Athena (SQL Queries)</li> <li>EMR (Big Data Processing)</li> <li>Redshift (Data Warehousing)</li> <li>CloudTrail (Audit Logging)</li> </ul>"},{"location":"aws/storage/s3/#best-practices_1","title":"Best Practices","text":""},{"location":"aws/storage/s3/#security_1","title":"Security","text":"<ol> <li>Enable encryption at rest</li> <li>Use VPC endpoints</li> <li>Implement least privilege access</li> <li>Enable access logging</li> <li>Regular security audits</li> </ol>"},{"location":"aws/storage/s3/#performance","title":"Performance","text":"<ol> <li>Use multipart upload</li> <li>Implement retry mechanism</li> <li>Use appropriate prefix strategy</li> <li>Enable transfer acceleration</li> <li>Implement caching where appropriate</li> </ol>"},{"location":"aws/storage/s3/#durability","title":"Durability","text":"<ol> <li>Enable versioning</li> <li>Implement cross-region replication</li> <li>Regular backup validation</li> <li>Monitor data integrity</li> <li>Test restore procedures</li> </ol>"},{"location":"aws/storage/s3/#troubleshooting","title":"Troubleshooting","text":""},{"location":"aws/storage/s3/#common-issues","title":"Common Issues","text":"<ol> <li>Access Denied</li> <li>Check IAM permissions</li> <li>Verify bucket policy</li> <li> <p>Check object ACLs</p> </li> <li> <p>Slow Performance</p> </li> <li>Review prefix strategy</li> <li>Check multipart upload usage</li> <li> <p>Verify network configuration</p> </li> <li> <p>Error Responses</p> </li> <li>403: Permission issues</li> <li>404: Object not found</li> <li>503: Service unavailable</li> </ol>"},{"location":"aws/storage/s3/#resources","title":"Resources","text":"<ul> <li>Official S3 Documentation</li> <li>S3 Best Practices</li> <li>S3 Pricing</li> <li>S3 FAQ</li> </ul>"},{"location":"aws/tools/cloud9/","title":"Cloud9","text":""},{"location":"aws/tools/cloud9/#overview","title":"Overview","text":"<p>AWS Cloud9 is a cloud-based integrated development environment (IDE) that allows developers to write, run, and debug code directly from a web browser.</p>"},{"location":"aws/tools/cloud9/#key-features","title":"Key Features","text":""},{"location":"aws/tools/cloud9/#development-environment","title":"Development Environment","text":"<ul> <li>Fully cloud-hosted IDE</li> <li>Accessible from any device with a web browser</li> <li>Eliminates local development environment setup</li> <li>Supports multiple programming languages</li> </ul>"},{"location":"aws/tools/cloud9/#collaboration-capabilities","title":"Collaboration Capabilities","text":"<ul> <li>Real-time code editing</li> <li>Shared development environments</li> <li>Live pair programming</li> <li>Built-in chat and communication tools</li> </ul>"},{"location":"aws/tools/cloud9/#development-tools","title":"Development Tools","text":"<ul> <li>Terminal access</li> <li>Code editor with syntax highlighting</li> <li>Integrated debugger</li> <li>Built-in package manager</li> <li>Version control integration</li> </ul>"},{"location":"aws/tools/cloud9/#language-and-runtime-support","title":"Language and Runtime Support","text":"<ul> <li>Multiple programming language support:</li> <li>Python</li> <li>JavaScript</li> <li>Node.js</li> <li>C++</li> <li>PHP</li> <li>And more</li> </ul>"},{"location":"aws/tools/cloud9/#aws-integration","title":"AWS Integration","text":"<ul> <li>Direct access to AWS services</li> <li>Pre-configured AWS CLI</li> <li>Simple environment management</li> <li>Seamless interaction with AWS resources</li> </ul>"},{"location":"aws/tools/cloud9/#benefits","title":"Benefits","text":"<ul> <li>No local software installation required</li> <li>Consistent development environment</li> <li>Easy team collaboration</li> <li>Reduced setup complexity</li> <li>Quick start for development projects</li> </ul>"},{"location":"aws/tools/cloud9/#use-cases","title":"Use Cases","text":"<ul> <li>Web application development</li> <li>Serverless application testing</li> <li>Cloud-native development</li> <li>Remote coding</li> <li>Quick prototyping</li> <li>Learning and training</li> </ul>"},{"location":"aws/tools/cloud9/#security","title":"Security","text":"<ul> <li>Isolated development environments</li> <li>AWS Identity and Access Management (IAM) integration</li> <li>Secure access controls</li> <li>Managed by AWS infrastructure</li> </ul>"},{"location":"aws/tools/codeartifact/","title":"CodeArtifact","text":""},{"location":"aws/tools/codeartifact/#overview","title":"Overview","text":"<p>Modern software development relies heavily on code dependencies, where software packages interconnect and depend on each other for successful builds. As development progresses, new dependencies are continuously created, making artifact management - the process of storing and retrieving these dependencies - a crucial aspect of the development lifecycle.</p> <p>Traditionally, organizations needed to invest significant resources in setting up and maintaining their own artifact management systems. AWS CodeArtifact eliminates this overhead by providing a secure, scalable, and cost-effective artifact management solution specifically designed for software development teams.</p>"},{"location":"aws/tools/codeartifact/#integration-with-package-managers","title":"Integration with Package Managers","text":"<p>CodeArtifact offers seamless integration with popular dependency management tools that developers use in their daily workflows. The service supports a wide range of package managers including:</p> <ul> <li>Maven and Gradle for Java dependencies</li> <li>npm and yarn for JavaScript and Node.js packages</li> <li>twine and pip for Python packages</li> <li>NuGet for .NET dependencies</li> </ul> <p>This broad compatibility ensures that developers can continue using their preferred tools while leveraging CodeArtifact\u2019s enterprise-grade capabilities. Furthermore, AWS CodeBuild can directly retrieve dependencies from CodeArtifact, streamlining the continuous integration process.</p>"},{"location":"aws/tools/codeartifact/#resource-policy-and-access-management","title":"Resource Policy and Access Management","text":"<p>CodeArtifact implements a straightforward but powerful access control mechanism through its resource policy feature. This policy framework enables cross-account access to CodeArtifact repositories, facilitating collaboration across different AWS accounts within an organization.</p> <p>A notable characteristic of CodeArtifact\u2019s access control is its all-or-nothing approach to package access: when a principal (user or role) is granted access to a repository, they can either read all packages within that repository or none at all. This simplifies access management while ensuring consistent security controls across all packages within a repository.</p>"},{"location":"aws/tools/codebuild/","title":"CodeBuild","text":""},{"location":"aws/tools/codebuild/#overview","title":"Overview","text":"<p>AWS CodeBuild represents Amazon\u2019s fully managed continuous integration service, designed to eliminate the operational overhead of managing build servers. Unlike traditional CI tools such as Jenkins that require server provisioning and maintenance, CodeBuild automatically scales to meet your build requirements without managing infrastructure or build queues.</p> <p>The service handles essential development tasks including source code compilation, test execution, and software package creation. By charging only for the compute time consumed during builds, CodeBuild offers a cost-effective solution for development teams of any size.</p>"},{"location":"aws/tools/codebuild/#technical-architecture","title":"Technical Architecture","text":"<p>At its foundation, CodeBuild utilizes Docker containers to ensure consistent and reproducible builds across different environments. Developers can choose from AWS\u2019s prepackaged Docker images or create custom images to meet specific build requirements. This containerized approach guarantees that builds remain consistent across different environments and team members.</p> <p></p>"},{"location":"aws/tools/codebuild/#security-framework","title":"Security Framework","text":"<p>CodeBuild implements a comprehensive security model that integrates with multiple AWS services. Build artifacts are protected through AWS Key Management Service (KMS) encryption, while AWS Identity and Access Management (IAM) handles access controls and permissions. For network security, CodeBuild can operate within Virtual Private Clouds (VPCs). All API interactions are logged through AWS CloudTrail, providing a complete audit trail of build activities.</p>"},{"location":"aws/tools/codebuild/#source-control-integration","title":"Source Control Integration","text":"<p>CodeBuild seamlessly integrates with various source code repositories including AWS CodeCommit, Amazon S3, Bitbucket, and GitHub. This flexibility allows teams to maintain their existing version control workflows while leveraging CodeBuild\u2019s capabilities.</p>"},{"location":"aws/tools/codebuild/#build-configuration-and-monitoring","title":"Build Configuration and Monitoring","text":"<p>Build instructions can be defined either through a buildspec.yml file in the source code repository or manually through the AWS Console. Build outputs and logs are automatically stored in Amazon S3 and CloudWatch Logs for easy access and archival.</p> <p>The service provides comprehensive monitoring capabilities through CloudWatch Metrics, enabling teams to track build statistics and performance metrics. Amazon EventBridge integration allows for automated notifications on build failures, while CloudWatch Alarms can be configured to alert based on specific failure thresholds.</p> <p>CodeBuild projects can be managed independently or integrated into broader CI/CD workflows through AWS CodePipeline, offering flexibility in pipeline design and implementation.</p>"},{"location":"aws/tools/codebuild/#supported-development-environments","title":"Supported Development Environments","text":"<p>CodeBuild provides native support for numerous programming languages and frameworks including: Java, Ruby, Python, Go, Node.js, Android, .NET Core, and PHP. The Docker support enables teams to extend these environments or create entirely custom build environments as needed.</p>"},{"location":"aws/tools/codebuild/#buildspec-configuration","title":"BuildSpec Configuration","text":"<p>The buildspec.yml file serves as the blueprint for your build process and must be located at the root of your source code repository. This file contains several key sections:</p>"},{"location":"aws/tools/codebuild/#environment-variables","title":"Environment Variables","text":"<p>CodeBuild supports various types of environment variables, from plaintext variables for basic configuration to more secure options using AWS Systems Manager Parameter Store or AWS Secrets Manager for sensitive information.</p>"},{"location":"aws/tools/codebuild/#build-phases","title":"Build Phases","text":"<p>The build process is organized into distinct phases:</p> <ul> <li>The <code>install</code> phase handles dependency installation and initial setup requirements. </li> <li>During the <code>pre-build</code> phase, final preparations and validations occur before the main build process. </li> <li>The <code>build</code> phase executes the primary build commands</li> <li>The <code>post-build</code> phase handles final tasks such as packaging and preparing artifacts for deployment.</li> </ul>"},{"location":"aws/tools/codebuild/#artifacts-and-caching","title":"Artifacts and Caching","text":"<p>Build outputs designated as artifacts are automatically uploaded to S3 with KMS encryption. To optimize build performance, CodeBuild supports caching of specified files (typically dependencies) to S3, significantly reducing build times for subsequent executions by reusing cached components.</p> <p>This caching mechanism is particularly valuable for projects with substantial dependencies, as it can dramatically improve build efficiency by eliminating the need to repeatedly download and process unchanged dependencies.</p>"},{"location":"aws/tools/codecommit/","title":"CodeCommit","text":""},{"location":"aws/tools/codecommit/#introduction-to-aws-codecommit","title":"Introduction to AWS CodeCommit","text":"<p>AWS CodeCommit is a fully managed source control service provided by Amazon Web Services that enables organizations to host secure and scalable private Git repositories. Designed as a cloud-based version control system, CodeCommit facilitates collaborative software development by offering robust repository management without the need for self-hosted infrastructure.</p>"},{"location":"aws/tools/codecommit/#core-architectural-features","title":"Core Architectural Features","text":""},{"location":"aws/tools/codecommit/#repository-management","title":"Repository Management","text":"<p>CodeCommit provides a comprehensive platform for creating, managing, and interacting with Git repositories. Developers can seamlessly store and version their source code, documentation, and binary files within a secure, highly available cloud environment. The service supports standard Git commands and integrates natively with existing development workflows.</p>"},{"location":"aws/tools/codecommit/#security-and-access-control","title":"Security and Access Control","text":"<p>Security represents a fundamental design principle of CodeCommit. The service leverages AWS Identity and Access Management (IAM) to implement granular access controls. Organizations can define precise repository permissions, controlling who can view, modify, or delete repository contents.  Multi-factor authentication and encryption at rest and in transit ensure comprehensive data protection.</p> <ul> <li>Repositories are automatically encrypted at rest using AWS KMS.</li> <li>Encryption in transit is guaranteed by using HTTPS or SSH.</li> </ul> <p>For cross-account access sharing use a IAM Role and STS AssumeRole</p>"},{"location":"aws/tools/codecommit/#integration-capabilities","title":"Integration Capabilities","text":""},{"location":"aws/tools/codecommit/#aws-development-ecosystem","title":"AWS Development Ecosystem","text":"<p>CodeCommit seamlessly integrates with other AWS development and deployment services, creating a cohesive software development lifecycle. Developers can easily connect CodeCommit repositories with services like AWS CodeBuild, CodePipeline, and CodeDeploy, enabling streamlined continuous integration and continuous deployment (CI/CD) workflows.</p>"},{"location":"aws/tools/codecommit/#development-tool-compatibility","title":"Development Tool Compatibility","text":"<p>The service supports standard Git client tools, including command-line interfaces, desktop applications, and integrated development environments. Developers can utilize familiar Git workflows without requiring significant tool modifications or learning new interfaces.</p>"},{"location":"aws/tools/codecommit/#authentication-mechanisms","title":"Authentication Mechanisms","text":""},{"location":"aws/tools/codecommit/#iam-user-authentication","title":"IAM User Authentication","text":"<p>AWS provides multiple authentication methods for accessing CodeCommit repositories. IAM users can generate:</p> <ul> <li>SSH Keys: User can generate SSH Keys in the IAM console</li> <li>HTTPS: with AWS CLI Credentials helper or Git Credentials for IAM User</li> </ul> <p>The credential management system allows for easy rotation and revocation of access keys.</p>"},{"location":"aws/tools/codecommit/#federated-access","title":"Federated Access","text":"<p>Organizations using corporate directory services can implement federated access through AWS Single Sign-On (SSO) or third-party identity providers. This approach simplifies authentication while maintaining robust security standards.</p>"},{"location":"aws/tools/codecommit/#repository-management-features","title":"Repository Management Features","text":""},{"location":"aws/tools/codecommit/#branch-protection","title":"Branch Protection","text":"<p>CodeCommit enables sophisticated branch management strategies. Administrators can implement branch protection rules, requiring pull request reviews before merging code into critical branches. These governance mechanisms help maintain code quality and enforce collaborative development practices.</p>"},{"location":"aws/tools/codecommit/#metadata-and-tagging","title":"Metadata and Tagging","text":"<p>Repositories support comprehensive metadata management. Developers can attach tags and annotations to commits, facilitating better tracking and documentation of code changes. These metadata features enhance traceability and support advanced repository management strategies.</p>"},{"location":"aws/tools/codecommit/#performance-and-scalability","title":"Performance and Scalability","text":""},{"location":"aws/tools/codecommit/#storage-and-performance","title":"Storage and Performance","text":"<p>CodeCommit automatically scales to accommodate repositories of varying sizes. The service supports repositories containing large files and complex version histories while maintaining high performance. AWS manages the underlying infrastructure, ensuring consistent repository access and minimal latency.</p>"},{"location":"aws/tools/codecommit/#global-accessibility","title":"Global Accessibility","text":"<p>Repositories are designed with global accessibility in mind. Distributed teams can collaborate effectively, with AWS providing low-latency access across multiple geographic regions.</p>"},{"location":"aws/tools/codecommit/#pricing-and-cost-management","title":"Pricing and Cost Management","text":""},{"location":"aws/tools/codecommit/#pricing-structure","title":"Pricing Structure","text":"<p>AWS CodeCommit offers a flexible pricing model based on active repository storage and data transfer. The service provides a generous free tier, allowing small teams and individual developers to leverage its capabilities without immediate financial commitment.</p>"},{"location":"aws/tools/codecommit/#use-cases","title":"Use Cases","text":""},{"location":"aws/tools/codecommit/#enterprise-software-development","title":"Enterprise Software Development","text":"<p>CodeCommit serves diverse software development scenarios, from small startup projects to large enterprise applications. Its robust security, scalability, and integration capabilities make it suitable for complex software development environments.</p>"},{"location":"aws/tools/codecommit/#open-source-project-management","title":"Open Source Project Management","text":"<p>While primarily designed for private repositories, CodeCommit can support open-source project management strategies, providing a secure and reliable version control platform.</p>"},{"location":"aws/tools/codecommit/#best-practices","title":"Best Practices","text":""},{"location":"aws/tools/codecommit/#repository-design","title":"Repository Design","text":"<p>Implement clear branching strategies, utilize meaningful commit messages, and leverage CodeCommit\u2019s advanced features like branch protection and pull request reviews.</p>"},{"location":"aws/tools/codecommit/#security-configuration","title":"Security Configuration","text":"<p>Regularly audit IAM permissions, implement least-privilege access models, and utilize multi-factor authentication to enhance repository security.</p>"},{"location":"aws/tools/codecommit/#limitations-and-considerations","title":"Limitations and Considerations","text":""},{"location":"aws/tools/codecommit/#service-constraints","title":"Service Constraints","text":"<p>CodeCommit imposes certain limitations on repository size, number of branches, and data transfer. Organizations should review these constraints during architectural planning.</p>"},{"location":"aws/tools/codecommit/#conclusion","title":"Conclusion","text":"<p>AWS CodeCommit represents a sophisticated, secure, and scalable source control solution integrated deeply within the AWS ecosystem. By providing a robust, managed Git repository service, AWS empowers development teams to collaborate effectively while maintaining high security and performance standards.</p>"},{"location":"aws/tools/codedeploy/","title":"CodeDeploy","text":""},{"location":"aws/tools/codedeploy/#overview","title":"Overview","text":"<p>AWS CodeDeploy is an automated deployment service that streamlines the process of deploying applications across various AWS compute platforms. The service supports deployments to Amazon EC2 instances, on-premises servers, AWS Lambda functions, and Amazon ECS services. CodeDeploy provides sophisticated deployment control features, including automated rollback capabilities triggered by deployment failures or CloudWatch alarms. The entire deployment process is defined in an appspec.yml file.</p> <p></p>"},{"location":"aws/tools/codedeploy/#platform-support","title":"Platform Support","text":""},{"location":"aws/tools/codedeploy/#ec2-and-on-premises-platform","title":"EC2 and On-premises Platform","text":"<p>CodeDeploy provides comprehensive support for deploying applications to EC2 instances and on-premises servers. The service supports both in-place and blue/green deployment strategies, with the requirement that target instances run the CodeDeploy Agent.</p> <p>Deployment speeds can be customized through various options:</p> <ul> <li>AllAtOnce: Fastest deployment with maximum downtime</li> <li>HalfAtATime: Balanced approach with 50% capacity reduction</li> <li>OneAtATime: Minimal availability impact with longest deployment time</li> <li>Custom: User-defined percentage-based deployment</li> </ul>"},{"location":"aws/tools/codedeploy/#lambda-platform","title":"Lambda Platform","text":"<p>For Lambda deployments, CodeDeploy automates traffic shifting for Lambda aliases, featuring tight integration with the AWS Serverless Application Model (SAM) framework. Traffic shifting patterns include:</p> <p>Linear deployments:</p> <ul> <li>LambdaLinear10PercentEvery3Minutes</li> <li>LambdaLinear10PercentEvery10Minutes</li> </ul> <p>Canary deployments:</p> <ul> <li>LambdaCanary10Percent5Minutes</li> <li>LambdaCanary10Percent30Minutes</li> </ul> <p>AllAtOnce deployment for immediate traffic shifting</p>"},{"location":"aws/tools/codedeploy/#ecs-platform","title":"ECS Platform","text":"<p>CodeDeploy automates the deployment of new ECS Task Definitions exclusively through blue/green deployments. Traffic shifting patterns include:</p> <p>Linear deployments:</p> <ul> <li>ECSLinear10PercentEvery3Minutes</li> <li>ECSLinear10PercentEvery10Minutes</li> </ul> <p>Canary deployments:</p> <ul> <li>ECSCanary10Percent5Minutes</li> <li>ECSCanary10Percent30Minutes</li> </ul> <p>AllAtOnce deployment for immediate updates</p>"},{"location":"aws/tools/codedeploy/#codedeploy-agent","title":"CodeDeploy Agent","text":"<p>The CodeDeploy Agent is a crucial component that must be running on target EC2 instances prior to deployment. The agent can be automatically installed and updated using AWS Systems Manager. Instances must have appropriate IAM permissions to access deployment bundles stored in Amazon S3.</p>"},{"location":"aws/tools/codedeploy/#deployment-configurations","title":"Deployment Configurations","text":""},{"location":"aws/tools/codedeploy/#ec2-deployment-process","title":"EC2 Deployment Process","text":"<p>Deployments to EC2 instances are governed by the appspec.yml file and the chosen deployment strategy. The process supports deployment hooks for verification at various phases of the deployment lifecycle.</p>"},{"location":"aws/tools/codedeploy/#auto-scaling-group-integration","title":"Auto Scaling Group Integration","text":"<p>In-place Deployments:</p> <ul> <li>Updates existing EC2 instances</li> <li>Automatically includes newly created instances in the deployment process</li> </ul> <p>Blue/Green Deployments:</p> <ul> <li>Creates a new Auto Scaling Group with copied settings</li> <li>Requires an Elastic Load Balancer</li> <li>Allows customization of instance retention period for the old ASG</li> </ul>"},{"location":"aws/tools/codedeploy/#rollback-management","title":"Rollback Management","text":"<p>CodeDeploy offers flexible rollback capabilities to maintain application reliability:</p> <p>Automatic Rollbacks:</p> <ul> <li>Triggered by deployment failures</li> <li>Initiated when CloudWatch Alarm thresholds are exceeded</li> </ul> <p>Manual Rollbacks:</p> <ul> <li>User-initiated rollback to previous version</li> <li>Option to disable rollbacks for specific deployments</li> </ul> <p>When a rollback occurs, CodeDeploy creates a new deployment using the last known good revision rather than restoring a previous version. This approach ensures consistent deployment processes and maintains deployment history.</p>"},{"location":"aws/tools/codedeploy/#best-practices","title":"Best Practices","text":"<ul> <li>Thoroughly test deployment configurations in non-production environments</li> <li>Implement appropriate CloudWatch Alarms for automated rollbacks</li> <li>Maintain proper version control of your appspec.yml file</li> <li>Regular monitoring and maintenance of the CodeDeploy Agent</li> <li>Implement appropriate security controls and IAM permissions</li> <li>Use deployment hooks effectively for validation</li> <li>Maintain comprehensive documentation of deployment configurations</li> </ul>"},{"location":"aws/tools/codeguru/","title":"CodeGuru","text":""},{"location":"aws/tools/codeguru/#overview","title":"Overview","text":"<p>Amazon CodeGuru is a machine learning-powered service that provides automated code reviews and application performance monitoring. The service consists of two main components: CodeGuru Reviewer for static code analysis during development, and CodeGuru Profiler for runtime performance analysis in production environments.</p>"},{"location":"aws/tools/codeguru/#codeguru-reviewer","title":"CodeGuru Reviewer","text":"<p>CodeGuru Reviewer leverages machine learning and automated reasoning to perform sophisticated code analysis. The service has been trained on millions of code reviews from thousands of open-source and Amazon repositories, incorporating extensive real-world experience into its analysis capabilities.</p> <p>The reviewer excels at identifying critical issues that might otherwise go unnoticed, including:</p> <ul> <li>Security vulnerabilities</li> <li>Resource leaks</li> <li>Input validation problems</li> <li>Deviations from coding best practices</li> <li>Hard-to-detect bugs</li> </ul> <p>Currently, CodeGuru Reviewer supports Java and Python codebases and integrates seamlessly with popular version control systems including GitHub and Bitbucket.</p>"},{"location":"aws/tools/codeguru/#codeguru-profiler","title":"CodeGuru Profiler","text":"<p>CodeGuru Profiler provides deep insights into application runtime behavior, helping developers understand and optimize their applications\u2019 performance characteristics. This component is particularly valuable for identifying resource-intensive operations, such as excessive CPU usage in routine operations like logging.</p>"},{"location":"aws/tools/codeguru/#key-profiler-features","title":"Key Profiler Features","text":"<p>The profiler offers comprehensive performance optimization capabilities by helping developers identify and eliminate code inefficiencies. This leads to improved application performance, particularly in areas of CPU utilization, ultimately resulting in decreased compute costs.</p> <p>Memory management is another crucial aspect of the profiler, which provides detailed heap summaries to help developers identify objects consuming excessive memory. The service also includes anomaly detection capabilities to identify unusual behavior patterns.</p> <p>The profiler supports applications running both on AWS infrastructure and on-premise environments, with minimal performance impact on the monitored applications.</p>"},{"location":"aws/tools/codeguru/#agent-configuration-parameters","title":"Agent Configuration Parameters","text":"<p>CodeGuru Profiler\u2019s agent can be fine-tuned through several important configuration parameters:</p> <p>MaxStackDepth: This parameter controls the maximum depth of stack traces that the profiler analyzes. For instance, in a call chain where method A calls B, which calls C, which then calls D (depth of 4), setting MaxStackDepth to 2 would limit analysis to methods A and B only.</p> <p>MemoryUsageLimitPercent: Defines the maximum percentage of system memory that the profiler can utilize during its operation.</p> <p>MinimumTimeForReportingInMilliseconds: Establishes the minimum interval between successive profiling reports, measured in milliseconds.</p> <p>ReportingIntervalInMilliseconds: Determines how frequently the profiler sends its collected data, specified in milliseconds.</p> <p>SamplingIntervalInMilliseconds: Controls how often the profiler collects sample data, measured in milliseconds. Reducing this value increases the sampling rate, providing more detailed profiling data at the cost of increased overhead.</p>"},{"location":"aws/tools/codeguru/#performance-considerations","title":"Performance Considerations","text":"<p>The profiler is designed to maintain minimal overhead on application performance, making it suitable for use in production environments. However, careful configuration of the sampling and reporting intervals is recommended to balance between detailed profiling data and system performance impact.</p>"},{"location":"aws/tools/codeguru/#integration-capabilities","title":"Integration Capabilities","text":"<p>CodeGuru works alongside existing development tools and processes, making it an adaptable addition to various development environments. Its insights can be integrated into continuous integration/continuous deployment (CI/CD) pipelines to automate performance monitoring and code quality checks.</p>"},{"location":"aws/tools/codepipeline/","title":"CodePipeline","text":""},{"location":"aws/tools/codepipeline/#overview","title":"Overview","text":"<p>AWS CodePipeline serves as a fully managed continuous integration and continuous delivery (CI/CD) service that enables users to create visual workflows for their software release process. This service orchestrates the entire development pipeline from source code through deployment.</p>"},{"location":"aws/tools/codepipeline/#integration-capabilities","title":"Integration Capabilities","text":"<p>CodePipeline offers extensive integration with various services across different stages of the development process:</p>"},{"location":"aws/tools/codepipeline/#source-control-integration","title":"Source Control Integration","text":"<p>CodePipeline can pull source code from multiple repositories including AWS CodeCommit, Amazon ECR, Amazon S3, Bitbucket, and GitHub. This flexibility allows teams to maintain their preferred source control solutions while leveraging CodePipeline\u2019s capabilities.</p>"},{"location":"aws/tools/codepipeline/#build-service-integration","title":"Build Service Integration","text":"<p>The service seamlessly connects with various build tools including AWS CodeBuild, Jenkins, CloudBees, and TeamCity. This enables teams to maintain their existing build processes while incorporating them into an automated pipeline.</p>"},{"location":"aws/tools/codepipeline/#testing-integration","title":"Testing Integration","text":"<p>For testing purposes, CodePipeline integrates with AWS CodeBuild, AWS Device Farm, and various third-party testing tools. This ensures comprehensive testing coverage across different aspects of the application.</p>"},{"location":"aws/tools/codepipeline/#deployment-options","title":"Deployment Options","text":"<p>CodePipeline supports multiple deployment targets including AWS CodeDeploy, Elastic Beanstalk, CloudFormation, Amazon ECS, and S3. This variety of deployment options accommodates different application architectures and hosting requirements.</p>"},{"location":"aws/tools/codepipeline/#advanced-workflows","title":"Advanced Workflows","text":"<p>For complex operations, CodePipeline can invoke AWS Lambda functions and AWS Step Functions, enabling sophisticated automation and orchestration capabilities.</p>"},{"location":"aws/tools/codepipeline/#pipeline-structure","title":"Pipeline Structure","text":"<p>CodePipeline organizes workflows into stages, with each stage capable of containing both sequential and parallel actions. A typical pipeline might flow from build to test to deploy, and then to load testing, with each stage performing specific actions in the software delivery process.</p> <p>The service also supports manual approval stages, which can be inserted at any point in the pipeline. This feature is particularly useful for controlling deployments to production environments or when human verification is required.</p>"},{"location":"aws/tools/codepipeline/#artifact-management","title":"Artifact Management","text":"<p>CodePipeline implements a robust artifact management system where each pipeline stage can generate artifacts. These artifacts are automatically stored in an Amazon S3 bucket and passed to subsequent stages, ensuring proper version control and traceability throughout the pipeline.</p>"},{"location":"aws/tools/codepipeline/#troubleshooting-and-monitoring","title":"Troubleshooting and Monitoring","text":""},{"location":"aws/tools/codepipeline/#event-monitoring","title":"Event Monitoring","text":"<p>CodePipeline integrates with Amazon EventBridge (formerly CloudWatch Events) to monitor pipeline, action, and stage execution state changes. This enables teams to:</p> <ul> <li>Create alerts for failed pipelines</li> <li>Monitor cancelled stages</li> <li>Track pipeline execution progress</li> <li>Set up automated responses to pipeline events</li> </ul>"},{"location":"aws/tools/codepipeline/#error-handling","title":"Error Handling","text":"<p>When a stage fails, the pipeline automatically stops, and detailed information is available in the AWS Management Console. Common issues often relate to IAM permissions, where the pipeline\u2019s service role may need additional permissions to perform certain actions.</p>"},{"location":"aws/tools/codepipeline/#audit-and-compliance","title":"Audit and Compliance","text":"<p>AWS CloudTrail integration provides comprehensive audit logging of all API calls made to CodePipeline, supporting security analysis, resource change tracking, and compliance auditing requirements.</p>"},{"location":"aws/tools/codepipeline/#best-practices","title":"Best Practices","text":"<p>To ensure optimal pipeline operation:</p> <ul> <li>Regularly review and update IAM roles and permissions</li> <li>Implement appropriate monitoring and alerting through EventBridge</li> <li>Use manual approval stages strategically in sensitive environments</li> <li>Maintain clear stage and action naming conventions</li> <li>Regularly clean up unused artifacts to manage storage costs</li> </ul>"},{"location":"gcp/","title":"Index","text":"<ul> <li>Connection (??)<ul> <li>Partner interconnect</li> <li>Dedicated interconnect</li> </ul> </li> <li>Databases</li> </ul>"},{"location":"gcp/cloud-storage/data-transfer/","title":"Cloud Storage Data Transfer Methods","text":""},{"location":"gcp/cloud-storage/data-transfer/#core-concepts","title":"Core Concepts","text":"<p>Transferring data to Cloud Storage requires selecting the appropriate method based on data size, location, bandwidth, timeline, and cost constraints. Understanding trade-offs between online and offline transfer, as well as optimization techniques, is critical for architecture decisions.</p> <p>Key Principle: Method selection depends primarily on data size and available bandwidth; optimize for time, cost, and operational complexity.</p>"},{"location":"gcp/cloud-storage/data-transfer/#transfer-method-comparison","title":"Transfer Method Comparison","text":"Method Data Size Bandwidth Required Timeline Cost Complexity Use Case gsutil/Console &lt;1 TB Good Hours-Days Free Low Small datasets Storage Transfer Service Any Good Continuous Free Medium Cloud-to-cloud, on-prem Transfer Appliance &gt;20 TB Limited Weeks Device fee High Offline bulk transfer Parallel Upload Large files Good Optimized Free Medium Single large files Composite Upload &gt;32 MB Good Optimized Free Medium Large file assembly"},{"location":"gcp/cloud-storage/data-transfer/#online-vs-offline-transfer","title":"Online vs Offline Transfer","text":""},{"location":"gcp/cloud-storage/data-transfer/#decision-criteria","title":"Decision Criteria","text":"<p>Online Transfer When:</p> <ul> <li>Good internet bandwidth (&gt;100 Mbps)</li> <li>Data size fits timeline (calculate transfer time)</li> <li>Continuous/scheduled transfers needed</li> <li>Source is another cloud provider</li> <li>Cost-sensitive (no hardware fees)</li> </ul> <p>Offline Transfer When:</p> <ul> <li>Limited bandwidth (&lt;10 Mbps)</li> <li>Massive datasets (&gt;20 TB)</li> <li>Network transfer time unacceptable</li> <li>One-time migration</li> <li>Remote locations with poor connectivity</li> </ul>"},{"location":"gcp/cloud-storage/data-transfer/#transfer-time-calculation","title":"Transfer Time Calculation","text":"<p>Formula: <pre><code>Transfer Time = Data Size / (Bandwidth \u00d7 Utilization \u00d7 0.125)\n</code></pre></p> <p>Example: 100 TB over 1 Gbps connection</p> <pre><code>100 TB = 100,000 GB\n1 Gbps = 1000 Mbps = 125 MB/s (\u00f78 for bytes)\nUtilization = 70% (realistic)\n\nTime = 100,000 GB / (125 MB/s \u00d7 0.7) = 1,142,857 seconds \u2248 13 days\n</code></pre> <p>Decision: If 13 days acceptable \u2192 Online; If not \u2192 Offline</p>"},{"location":"gcp/cloud-storage/data-transfer/#storage-transfer-service","title":"Storage Transfer Service","text":""},{"location":"gcp/cloud-storage/data-transfer/#overview","title":"Overview","text":"<p>Purpose: Managed service for transferring data from AWS S3, Azure Blob Storage, HTTP/HTTPS endpoints, or on-premises to Cloud Storage</p> <p>Key Characteristics:</p> <ul> <li>Managed, scalable transfer</li> <li>Scheduling and automation</li> <li>No infrastructure to manage</li> <li>Free (except source egress charges)</li> <li>Progress tracking and monitoring</li> </ul>"},{"location":"gcp/cloud-storage/data-transfer/#architecture","title":"Architecture","text":"<p>How It Works:</p> <ol> <li>Create transfer job with source and destination</li> <li>Service manages transfer execution</li> <li>Automatic retry and error handling</li> <li>Incremental transfers (only new/changed objects)</li> <li>Optional deletion of source objects</li> </ol> <p>Transfer Agents (for on-premises):</p> <ul> <li>Software agents run on-premises</li> <li>Pool of agents for parallel transfer</li> <li>Manage bandwidth and performance</li> <li>Required for on-premises sources</li> </ul>"},{"location":"gcp/cloud-storage/data-transfer/#when-to-use","title":"When to Use","text":"<p>\u2705 Appropriate for:</p> <p>Cloud-to-Cloud Migration:</p> <ul> <li>AWS S3 to Cloud Storage</li> <li>Azure Blob to Cloud Storage</li> <li>Cross-region Cloud Storage</li> <li>Multi-cloud strategy</li> </ul> <p>Continuous Synchronization:</p> <ul> <li>Scheduled daily/weekly transfers</li> <li>Keep buckets in sync</li> <li>Backup from other clouds</li> <li>Multi-cloud data replication</li> </ul> <p>Large-Scale Transfer:</p> <ul> <li>Terabytes to petabytes</li> <li>Many small files</li> <li>Need parallelization</li> <li>Automatic management preferred</li> </ul> <p>On-Premises Transfer (with agents):</p> <ul> <li>Good bandwidth available</li> <li>Continuous/scheduled uploads</li> <li>Multiple source locations</li> <li>Need progress monitoring</li> </ul>"},{"location":"gcp/cloud-storage/data-transfer/#when-not-to-use","title":"When NOT to Use","text":"<p>\u274c Inappropriate for:</p> <p>Small Datasets (&lt;100 GB):</p> <ul> <li>Overhead not justified</li> <li>gsutil simpler and faster</li> <li>No need for managed service</li> </ul> <p>Limited Bandwidth:</p> <ul> <li>Online transfer too slow</li> <li>Transfer Appliance better choice</li> <li>Network congestion concerns</li> </ul> <p>One-Time Small Transfer:</p> <ul> <li>gsutil more straightforward</li> <li>No need for job management</li> <li>Quick ad-hoc operation</li> </ul>"},{"location":"gcp/cloud-storage/data-transfer/#configuration-considerations","title":"Configuration Considerations","text":"<p>Scheduling Options:</p> <ul> <li>One-time transfer</li> <li>Daily recurring</li> <li>Custom schedule</li> <li>Start time specification</li> </ul> <p>Transfer Options:</p> <ul> <li>Overwrite existing objects: Yes/No</li> <li>Delete source objects: Yes/No (use cautiously)</li> <li>Transfer only modified objects</li> <li>Preserve metadata</li> </ul> <p>Bandwidth Management:</p> <ul> <li>Agent pool sizing (on-premises)</li> <li>Parallel transfer optimization</li> <li>Network impact control</li> </ul>"},{"location":"gcp/cloud-storage/data-transfer/#cost-implications","title":"Cost Implications","text":"<p>Free Transfer Service:</p> <ul> <li>No Google charges for the service</li> <li>Pay only for storage and operations</li> </ul> <p>Source Costs:</p> <ul> <li>AWS S3 egress: ~$0.09/GB (to internet)</li> <li>Azure egress: ~$0.087/GB (varies by region)</li> <li>On-premises: ISP charges</li> </ul> <p>Architecture Decision: Factor in source egress costs for cloud-to-cloud</p>"},{"location":"gcp/cloud-storage/data-transfer/#transfer-appliance","title":"Transfer Appliance","text":""},{"location":"gcp/cloud-storage/data-transfer/#overview_1","title":"Overview","text":"<p>Purpose: Physical device shipped to customer location for offline data transfer when network transfer is impractical</p> <p>Key Characteristics:</p> <ul> <li>Ruggedized, secure storage device</li> <li>40 TB or 300 TB capacity</li> <li>Encrypted at rest</li> <li>Shipped both ways</li> <li>Offline transfer solution</li> </ul>"},{"location":"gcp/cloud-storage/data-transfer/#process-flow","title":"Process Flow","text":"<p>Workflow:</p> <ol> <li>Request appliance from Google</li> <li>Receive device at location (1-2 weeks)</li> <li>Connect to network, copy data</li> <li>Ship device back to Google</li> <li>Google uploads to Cloud Storage</li> <li>Verify data and release device</li> </ol> <p>Timeline:</p> <ul> <li>Shipping to customer: 1-2 weeks</li> <li>Data copy: Depends on local network</li> <li>Shipping to Google: 1-2 weeks</li> <li>Upload to Cloud Storage: 1-2 weeks</li> <li>Total: 4-8 weeks typically</li> </ul>"},{"location":"gcp/cloud-storage/data-transfer/#when-to-use_1","title":"When to Use","text":"<p>\u2705 Appropriate for:</p> <p>Limited Bandwidth Scenarios:</p> <ul> <li>Poor internet connectivity (&lt;10 Mbps)</li> <li>Remote locations</li> <li>Network transfer time &gt; 1 week</li> <li>Expensive bandwidth costs</li> </ul> <p>Large Datasets:</p> <ul> <li> <p>20 TB recommended minimum</p> </li> <li>Hundreds of TB</li> <li>Petabyte-scale migration</li> <li>One-time bulk transfer</li> </ul> <p>Cost-Effective Alternative:</p> <ul> <li>Network transfer cost &gt; appliance cost</li> <li>Limited transfer windows</li> <li>Bandwidth caps/throttling</li> <li>ISP restrictions</li> </ul> <p>Time-Sensitive Migration:</p> <ul> <li>Network too slow for deadline</li> <li>Large dataset, short timeline</li> <li>Predictable shipping time preferred</li> <li>Parallel work during data copy</li> </ul>"},{"location":"gcp/cloud-storage/data-transfer/#when-not-to-use_1","title":"When NOT to Use","text":"<p>\u274c Inappropriate for:</p> <p>Small Datasets (&lt;20 TB):</p> <ul> <li>Appliance overkill</li> <li>Online transfer faster</li> <li>Not cost-effective</li> <li>Unnecessary complexity</li> </ul> <p>Good Bandwidth:</p> <ul> <li>Fast internet available</li> <li>Online transfer reasonable time</li> <li>Continuous access to data needed</li> <li>No shipping delays acceptable</li> </ul> <p>Continuous Sync:</p> <ul> <li>Ongoing transfers required</li> <li>Regular updates needed</li> <li>Not one-time migration</li> <li>Use Transfer Service instead</li> </ul> <p>Frequent Access Required:</p> <ul> <li>Data needed during transfer</li> <li>Cannot be offline for weeks</li> <li>Business continuity concerns</li> </ul>"},{"location":"gcp/cloud-storage/data-transfer/#cost-considerations","title":"Cost Considerations","text":"<p>Appliance Fees:</p> <ul> <li>40 TB appliance: ~$300 fee</li> <li>300 TB appliance: ~$2,500 fee</li> <li>Shipping included in fee</li> <li>Storage (after ingestion) charged separately</li> </ul> <p>Cost Comparison:</p> <p>Example: 100 TB transfer over 10 Mbps connection</p> <p>Online Transfer:</p> <ul> <li>Time: ~100 days</li> <li>Cost: ISP charges only</li> </ul> <p>Transfer Appliance:</p> <ul> <li>Time: 4-8 weeks</li> <li>Cost: $2,500 + shipping (if not included)</li> </ul> <p>Decision: Appliance worth cost if time savings critical</p>"},{"location":"gcp/cloud-storage/data-transfer/#security-and-compliance","title":"Security and Compliance","text":"<p>Encryption:</p> <ul> <li>AES-256 encryption at rest</li> <li>Encryption keys managed by Google</li> <li>Secure data in transit (physical shipping)</li> </ul> <p>Chain of Custody:</p> <ul> <li>Tracked shipping</li> <li>Tamper-evident seals</li> <li>Audit trail</li> <li>Secure Google data centers</li> </ul> <p>Compliance:</p> <ul> <li>HIPAA compliant</li> <li>Suitable for regulated data</li> <li>Physical security controls</li> </ul>"},{"location":"gcp/cloud-storage/data-transfer/#parallel-uploads","title":"Parallel Uploads","text":""},{"location":"gcp/cloud-storage/data-transfer/#concept","title":"Concept","text":"<p>Purpose: Split large files into chunks and upload in parallel for faster transfer</p> <p>How It Works:</p> <ul> <li>Break file into parts</li> <li>Upload parts concurrently</li> <li>Reassemble in Cloud Storage</li> <li>Utilize bandwidth efficiently</li> </ul> <p>Automatic in gsutil:</p> <ul> <li>gsutil -m (multi-threading)</li> <li>Automatically parallelizes large files</li> <li>Optimizes based on file size</li> <li>No manual configuration needed</li> </ul>"},{"location":"gcp/cloud-storage/data-transfer/#when-to-use_2","title":"When to Use","text":"<p>\u2705 Appropriate for:</p> <p>Large Files:</p> <ul> <li>Files &gt;100 MB</li> <li>Maximum bandwidth utilization</li> <li>Faster upload times</li> <li>Better throughput</li> </ul> <p>Good Bandwidth:</p> <ul> <li>High-speed connection</li> <li>Underutilized bandwidth</li> <li>Can handle parallel streams</li> <li>Network not bottleneck</li> </ul>"},{"location":"gcp/cloud-storage/data-transfer/#performance-impact","title":"Performance Impact","text":"<p>Benefits:</p> <ul> <li>5-10x faster for large files</li> <li>Better bandwidth utilization</li> <li>Reduced total transfer time</li> <li>Optimized network usage</li> </ul> <p>Considerations:</p> <ul> <li>CPU overhead for chunking</li> <li>Memory usage for buffers</li> <li>Network congestion possible</li> <li>Optimal chunk size matters</li> </ul>"},{"location":"gcp/cloud-storage/data-transfer/#architecture-implications","title":"Architecture Implications","text":"<p>Design Patterns:</p> <ul> <li>Use for bulk data loads</li> <li>Initial data migration</li> <li>Large media file uploads</li> <li>Database backup uploads</li> </ul> <p>Not Beneficial For:</p> <ul> <li>Small files (&lt;10 MB)</li> <li>Slow network connections</li> <li>Many concurrent uploads already</li> <li>CPU/memory constrained systems</li> </ul>"},{"location":"gcp/cloud-storage/data-transfer/#composite-uploads","title":"Composite Uploads","text":""},{"location":"gcp/cloud-storage/data-transfer/#concept_1","title":"Concept","text":"<p>Purpose: Upload parts of large file separately, then compose into single object</p> <p>Difference from Parallel Upload:</p> <ul> <li>Parallel Upload: Splits during upload, single API call series</li> <li>Composite Upload: Manual part uploads, explicit compose operation</li> </ul>"},{"location":"gcp/cloud-storage/data-transfer/#how-it-works","title":"How It Works","text":"<p>Process:</p> <ol> <li>Split file into components (max 32)</li> <li>Upload each component separately</li> <li>Compose components into final object</li> <li>Delete temporary components</li> </ol> <p>Use Cases:</p> <ul> <li>Resume interrupted uploads</li> <li>Upload from multiple sources</li> <li>Distributed upload systems</li> <li>Custom upload logic</li> </ul>"},{"location":"gcp/cloud-storage/data-transfer/#when-to-use_3","title":"When to Use","text":"<p>\u2705 Appropriate for:</p> <p>Resumable Large File Uploads:</p> <ul> <li>Unreliable connections</li> <li>Very large files (&gt;5 GB)</li> <li>Risk of interruption</li> <li>Need checkpoint capability</li> </ul> <p>Distributed Upload:</p> <ul> <li>Multiple sources for single file</li> <li>Parallel processing systems</li> <li>Map-reduce style uploads</li> <li>Custom upload orchestration</li> </ul> <p>Failure Recovery:</p> <ul> <li>Only re-upload failed parts</li> <li>Avoid full re-transfer</li> <li>Save time and bandwidth</li> <li>Production reliability</li> </ul>"},{"location":"gcp/cloud-storage/data-transfer/#limitations","title":"Limitations","text":"<p>Constraints:</p> <ul> <li>Maximum 32 components per composition</li> <li>Each component min 5 MB (except last)</li> <li>No additional composition of composites (1 level only)</li> <li>Temporary storage of components</li> </ul> <p>Architecture Implication: Design chunking strategy within limits</p>"},{"location":"gcp/cloud-storage/data-transfer/#streaming-uploads","title":"Streaming Uploads","text":""},{"location":"gcp/cloud-storage/data-transfer/#concept_2","title":"Concept","text":"<p>Purpose: Upload data without knowing size in advance (streaming data)</p> <p>Characteristics:</p> <ul> <li>No Content-Length header</li> <li>Chunked transfer encoding</li> <li>Indeterminate size</li> <li>Real-time data upload</li> </ul>"},{"location":"gcp/cloud-storage/data-transfer/#when-to-use_4","title":"When to Use","text":"<p>\u2705 Appropriate for:</p> <p>Streaming Data:</p> <ul> <li>Live data feeds</li> <li>Real-time processing</li> <li>Log streaming</li> <li>IoT sensor data</li> </ul> <p>Unknown Size:</p> <ul> <li>Generated content</li> <li>Compressed streams</li> <li>Encrypted data</li> <li>Dynamic content</li> </ul> <p>Immediate Upload:</p> <ul> <li>No buffering desired</li> <li>Low latency requirement</li> <li>Storage as generated</li> <li>Streaming pipelines</li> </ul>"},{"location":"gcp/cloud-storage/data-transfer/#limitations_1","title":"Limitations","text":"<p>Considerations:</p> <ul> <li>Cannot use parallel upload</li> <li>No resume capability</li> <li>Single-stream only</li> <li>Error requires full retry</li> </ul>"},{"location":"gcp/cloud-storage/data-transfer/#signed-urls-for-upload","title":"Signed URLs for Upload","text":""},{"location":"gcp/cloud-storage/data-transfer/#concept_3","title":"Concept","text":"<p>Purpose: Allow clients to upload directly to Cloud Storage without credentials</p> <p>Architecture Pattern:</p> <pre><code>Application (with creds) \u2192 Generate signed URL \u2192 Client \u2192 Upload directly to GCS\n</code></pre> <p>Benefits:</p> <ul> <li>No proxy through application</li> <li>Reduced server load</li> <li>Better performance</li> <li>Scalability</li> </ul>"},{"location":"gcp/cloud-storage/data-transfer/#when-to-use_5","title":"When to Use","text":"<p>\u2705 Appropriate for:</p> <p>User Upload Scenarios:</p> <ul> <li>User file uploads</li> <li>Mobile app uploads</li> <li>Browser-based uploads</li> <li>No backend proxy needed</li> </ul> <p>Temporary Access:</p> <ul> <li>Time-limited upload capability</li> <li>Specific object/location</li> <li>No permanent credentials</li> <li>Security through expiration</li> </ul>"},{"location":"gcp/cloud-storage/data-transfer/#configuration","title":"Configuration","text":"<p>Parameters:</p> <ul> <li>Expiration time (max 7 days with service account key)</li> <li>HTTP method (PUT, POST)</li> <li>Content-Type restrictions</li> <li>Size limits</li> </ul> <p>Security Considerations:</p> <ul> <li>Short expiration times</li> <li>Specific object names</li> <li>Content-Type validation</li> <li>Size restrictions</li> </ul>"},{"location":"gcp/cloud-storage/data-transfer/#transfer-optimization-strategies","title":"Transfer Optimization Strategies","text":""},{"location":"gcp/cloud-storage/data-transfer/#network-optimization","title":"Network Optimization","text":"<p>Bandwidth Utilization:</p> <ul> <li>Parallel transfers for throughput</li> <li>Avoid peak network times</li> <li>QoS configuration</li> <li>Bandwidth reservation</li> </ul> <p>Compression:</p> <ul> <li>gzip before transfer (if not already compressed)</li> <li>Reduce transfer size</li> <li>CPU trade-off</li> <li>Not beneficial for already compressed (images, video)</li> </ul>"},{"location":"gcp/cloud-storage/data-transfer/#transfer-validation","title":"Transfer Validation","text":"<p>Checksums:</p> <ul> <li>MD5 hash verification</li> <li>CRC32c checksums</li> <li>Automatic validation in gsutil</li> <li>Detect corruption</li> </ul> <p>Retry Logic:</p> <ul> <li>Automatic retry on failure</li> <li>Exponential backoff</li> <li>Transient error handling</li> <li>Progress preservation</li> </ul>"},{"location":"gcp/cloud-storage/data-transfer/#monitoring","title":"Monitoring","text":"<p>Metrics to Track:</p> <ul> <li>Transfer progress (bytes/objects)</li> <li>Transfer rate (MB/s)</li> <li>Error rate</li> <li>Estimated completion time</li> </ul> <p>Alerting:</p> <ul> <li>Stalled transfers</li> <li>High error rates</li> <li>Bandwidth saturation</li> <li>Unexpected costs</li> </ul>"},{"location":"gcp/cloud-storage/data-transfer/#decision-framework","title":"Decision Framework","text":""},{"location":"gcp/cloud-storage/data-transfer/#data-size-based","title":"Data Size Based","text":"<p>&lt;1 GB: Console upload or gsutil 1-100 GB: gsutil with parallel upload 100 GB - 20 TB: Storage Transfer Service &gt;20 TB: Storage Transfer Service or Transfer Appliance</p>"},{"location":"gcp/cloud-storage/data-transfer/#bandwidth-based","title":"Bandwidth Based","text":"<p>&gt;100 Mbps: Online transfer (gsutil or Transfer Service) 10-100 Mbps: Online transfer with scheduling &lt;10 Mbps: Consider Transfer Appliance</p>"},{"location":"gcp/cloud-storage/data-transfer/#timeline-based","title":"Timeline Based","text":"<p>Hours: gsutil for small data Days: Transfer Service for medium data Weeks: Transfer Service for large data or Transfer Appliance Months: Transfer Appliance only option</p>"},{"location":"gcp/cloud-storage/data-transfer/#source-based","title":"Source Based","text":"<p>AWS/Azure: Storage Transfer Service On-premises (good bandwidth): Storage Transfer Service with agents On-premises (limited bandwidth): Transfer Appliance HTTP/HTTPS source: Storage Transfer Service</p>"},{"location":"gcp/cloud-storage/data-transfer/#cost-analysis","title":"Cost Analysis","text":""},{"location":"gcp/cloud-storage/data-transfer/#online-transfer-costs","title":"Online Transfer Costs","text":"<p>Google Cloud:</p> <ul> <li>Transfer Service: Free</li> <li>Ingress: Free</li> <li>Storage: Based on class</li> <li>Operations: Normal rates</li> </ul> <p>Source Costs:</p> <ul> <li>AWS S3 egress: ~$0.09/GB</li> <li>Azure egress: ~$0.087/GB</li> <li>On-premises: ISP charges</li> </ul>"},{"location":"gcp/cloud-storage/data-transfer/#offline-transfer-costs","title":"Offline Transfer Costs","text":"<p>Transfer Appliance:</p> <ul> <li>Device fee: $300-$2,500</li> <li>Shipping: Included</li> <li>Time cost: 4-8 weeks</li> </ul> <p>Break-Even Analysis:</p> <pre><code>Example: 100 TB from AWS\n\nOnline:\n\n- AWS egress: 100,000 GB \u00d7 $0.09 = $9,000\n- Timeline: 13 days (1 Gbps)\n\nOffline:\n\n- Appliance: $2,500\n- Timeline: 6 weeks\n\nDecision: Online faster, offline cheaper\n</code></pre>"},{"location":"gcp/cloud-storage/data-transfer/#exam-focus-areas","title":"Exam Focus Areas","text":""},{"location":"gcp/cloud-storage/data-transfer/#method-selection","title":"Method Selection","text":"<ul> <li>Data size and method mapping</li> <li>Bandwidth requirements</li> <li>Timeline constraints</li> <li>Cost optimization</li> </ul>"},{"location":"gcp/cloud-storage/data-transfer/#transfer-service","title":"Transfer Service","text":"<ul> <li>Cloud-to-cloud scenarios</li> <li>Scheduling and automation</li> <li>On-premises agents</li> <li>Incremental transfers</li> </ul>"},{"location":"gcp/cloud-storage/data-transfer/#transfer-appliance_1","title":"Transfer Appliance","text":"<ul> <li>When to use vs online transfer</li> <li>Capacity planning</li> <li>Timeline expectations</li> <li>Security and compliance</li> </ul>"},{"location":"gcp/cloud-storage/data-transfer/#optimization","title":"Optimization","text":"<ul> <li>Parallel upload benefits</li> <li>Composite upload use cases</li> <li>Streaming scenarios</li> <li>Signed URL patterns</li> </ul>"},{"location":"gcp/cloud-storage/data-transfer/#architecture-patterns","title":"Architecture Patterns","text":"<ul> <li>Migration strategies</li> <li>Continuous sync</li> <li>Multi-cloud data</li> <li>Cost-effective transfer</li> </ul>"},{"location":"gcp/cloud-storage/object-lifecycle/","title":"Object Lifecycle Management","text":""},{"location":"gcp/cloud-storage/object-lifecycle/#core-concepts","title":"Core Concepts","text":"<p>Object lifecycle management automatically transitions objects between storage classes or deletes them based on age, storage class, or other conditions. Understanding lifecycle policy design is critical for cost optimization without sacrificing data availability.</p> <p>Key Principle: Automate data aging to optimize costs; manually managing storage classes for billions of objects is impractical.</p>"},{"location":"gcp/cloud-storage/object-lifecycle/#lifecycle-policy-architecture","title":"Lifecycle Policy Architecture","text":""},{"location":"gcp/cloud-storage/object-lifecycle/#how-lifecycle-works","title":"How Lifecycle Works","text":"<p>Daily Processing:</p> <ul> <li>Google scans all objects daily</li> <li>Evaluates conditions against each object</li> <li>Applies first matching action</li> <li>Asynchronous execution (not immediate)</li> <li>No operation charges for lifecycle transitions</li> </ul> <p>Execution Timing:</p> <ul> <li>Runs once per day</li> <li>Not real-time (up to 24-hour delay)</li> <li>Actions applied in batch</li> <li>Eventual consistency</li> </ul> <p>Order of Evaluation:</p> <ul> <li>Policies evaluated top to bottom</li> <li>First matching condition wins</li> <li>No further evaluation after match</li> <li>Order matters in configuration</li> </ul>"},{"location":"gcp/cloud-storage/object-lifecycle/#lifecycle-actions","title":"Lifecycle Actions","text":"<p>Delete:</p> <ul> <li>Permanently delete object</li> <li>Cannot be recovered (unless versioning enabled)</li> <li>Frees storage space</li> <li>Stops storage charges</li> </ul> <p>SetStorageClass:</p> <ul> <li>Transition to different storage class</li> <li>Change from Standard \u2192 Nearline \u2192 Coldline \u2192 Archive</li> <li>Reduce storage costs</li> <li>Increase retrieval costs</li> </ul> <p>AbortIncompleteMultipartUpload:</p> <ul> <li>Delete incomplete multipart upload parts</li> <li>Clean up failed uploads</li> <li>Reduce storage waste</li> <li>Age-based deletion</li> </ul>"},{"location":"gcp/cloud-storage/object-lifecycle/#lifecycle-conditions","title":"Lifecycle Conditions","text":""},{"location":"gcp/cloud-storage/object-lifecycle/#age-condition","title":"Age Condition","text":"<p>Purpose: Match objects older than specified days</p> <p>Syntax: <code>age: &lt;days&gt;</code></p> <p>Examples:</p> <pre><code>age: 30    # Objects created &gt;30 days ago\nage: 90    # Objects created &gt;90 days ago\nage: 365   # Objects created &gt;365 days ago\n</code></pre> <p>Use Cases:</p> <ul> <li>Data aging policies</li> <li>Compliance retention</li> <li>Cost optimization</li> <li>Automatic cleanup</li> </ul> <p>Calculation: Days since object creation</p>"},{"location":"gcp/cloud-storage/object-lifecycle/#createdbefore-condition","title":"CreatedBefore Condition","text":"<p>Purpose: Match objects created before specific date</p> <p>Syntax: <code>createdBefore: \"YYYY-MM-DD\"</code></p> <p>Example:</p> <pre><code>createdBefore: \"2024-01-01\"  # Created before Jan 1, 2024\n</code></pre> <p>Use Cases:</p> <ul> <li>One-time cleanup of old data</li> <li>Compliance deadline</li> <li>Project end date</li> <li>Historical cutoff</li> </ul>"},{"location":"gcp/cloud-storage/object-lifecycle/#matchesstorageclass-condition","title":"MatchesStorageClass Condition","text":"<p>Purpose: Apply actions to specific storage classes</p> <p>Syntax: <code>matchesStorageClass: [classes]</code></p> <p>Examples:</p> <pre><code>matchesStorageClass: [\"STANDARD\"]      # Only Standard objects\nmatchesStorageClass: [\"NEARLINE\"]      # Only Nearline objects\nmatchesStorageClass: [\"STANDARD\", \"NEARLINE\"]  # Multiple classes\n</code></pre> <p>Use Cases:</p> <ul> <li>Storage class transitions</li> <li>Multi-tier aging</li> <li>Selective deletion</li> <li>Class-specific policies</li> </ul>"},{"location":"gcp/cloud-storage/object-lifecycle/#numberofnewerversions-condition","title":"NumberOfNewerVersions Condition","text":"<p>Purpose: Match non-current versions (with versioning enabled)</p> <p>Syntax: <code>numberOfNewerVersions: &lt;count&gt;</code></p> <p>Example:</p> <pre><code>numberOfNewerVersions: 3  # Keep 3 most recent versions\n</code></pre> <p>Use Cases:</p> <ul> <li>Version pruning</li> <li>Limit version accumulation</li> <li>Control versioning costs</li> <li>Compliance with retention limits</li> </ul>"},{"location":"gcp/cloud-storage/object-lifecycle/#dayssincenoncurrenttime-condition","title":"DaysSinceNoncurrentTime Condition","text":"<p>Purpose: Age of non-current versions</p> <p>Syntax: <code>daysSinceNoncurrentTime: &lt;days&gt;</code></p> <p>Example:</p> <pre><code>daysSinceNoncurrentTime: 30  # Versions older than 30 days\n</code></pre> <p>Use Cases:</p> <ul> <li>Version lifecycle</li> <li>Age out old versions</li> <li>Compliance retention for versions</li> </ul>"},{"location":"gcp/cloud-storage/object-lifecycle/#matchesprefix-condition","title":"MatchesPrefix Condition","text":"<p>Purpose: Match objects with specific prefix</p> <p>Syntax: <code>matchesPrefix: [prefixes]</code></p> <p>Example:</p> <pre><code>matchesPrefix: [\"logs/\"]           # Objects in logs/ prefix\nmatchesPrefix: [\"temp/\", \"tmp/\"]  # Multiple prefixes\n</code></pre> <p>Use Cases:</p> <ul> <li>Folder-based policies</li> <li>Application-specific rules</li> <li>Data organization alignment</li> </ul>"},{"location":"gcp/cloud-storage/object-lifecycle/#matchessuffix-condition","title":"MatchesSuffix Condition","text":"<p>Purpose: Match objects with specific suffix</p> <p>Syntax: <code>matchesSuffix: [suffixes]</code></p> <p>Example:</p> <pre><code>matchesSuffix: [\".tmp\", \".log\"]   # Temporary and log files\n</code></pre> <p>Use Cases:</p> <ul> <li>File type specific policies</li> <li>Temporary file cleanup</li> <li>Extension-based rules</li> </ul>"},{"location":"gcp/cloud-storage/object-lifecycle/#common-lifecycle-patterns","title":"Common Lifecycle Patterns","text":""},{"location":"gcp/cloud-storage/object-lifecycle/#pattern-1-multi-tier-data-aging","title":"Pattern 1: Multi-Tier Data Aging","text":"<p>Use Case: Automatically reduce storage costs as data ages</p> <p>Policy:</p> <pre><code># Conceptual representation\nRule 1: age 30 days + Standard \u2192 Nearline\nRule 2: age 90 days + Nearline \u2192 Coldline\nRule 3: age 365 days + Coldline \u2192 Archive\nRule 4: age 2555 days (7 years) \u2192 Delete\n</code></pre> <p>Cost Impact:</p> <ul> <li>0-30 days: $0.020/GB/month (Standard)</li> <li>30-90 days: $0.010/GB/month (Nearline)</li> <li>90-365 days: $0.004/GB/month (Coldline)</li> <li>365-2555 days: $0.0012/GB/month (Archive)</li> <li>After 2555 days: Deleted (zero cost)</li> </ul> <p>Use Cases:</p> <ul> <li>Log storage and aging</li> <li>Data lake with predictable access patterns</li> <li>Compliance with cost optimization</li> <li>Analytics data lifecycle</li> </ul>"},{"location":"gcp/cloud-storage/object-lifecycle/#pattern-2-temporary-data-cleanup","title":"Pattern 2: Temporary Data Cleanup","text":"<p>Use Case: Automatically delete temporary or staging data</p> <p>Policy:</p> <pre><code># Conceptual representation\nRule: age 7 days + matchesPrefix \"temp/\" \u2192 Delete\n</code></pre> <p>Benefits:</p> <ul> <li>Prevent storage waste</li> <li>Automatic cleanup</li> <li>No manual intervention</li> <li>Predictable costs</li> </ul> <p>Use Cases:</p> <ul> <li>Temporary processing data</li> <li>Build artifacts</li> <li>Cache storage</li> <li>Staging areas</li> </ul>"},{"location":"gcp/cloud-storage/object-lifecycle/#pattern-3-compliance-retention","title":"Pattern 3: Compliance Retention","text":"<p>Use Case: Retain for compliance period, then delete</p> <p>Policy:</p> <pre><code># Conceptual representation\nRule 1: age 30 days + Standard \u2192 Coldline (reduce cost immediately)\nRule 2: age 2555 days (7 years) \u2192 Delete (HIPAA, SOX)\n</code></pre> <p>Compliance Requirements:</p> <ul> <li>HIPAA: 6 years</li> <li>SOX: 7 years</li> <li>GDPR: As per agreement</li> <li>Industry-specific regulations</li> </ul>"},{"location":"gcp/cloud-storage/object-lifecycle/#pattern-4-version-pruning","title":"Pattern 4: Version Pruning","text":"<p>Use Case: Limit number of versions per object</p> <p>Policy:</p> <pre><code># Conceptual representation\nRule: numberOfNewerVersions 5 \u2192 Delete\n</code></pre> <p>Benefits:</p> <ul> <li>Control versioning costs</li> <li>Prevent unbounded growth</li> <li>Maintain recent history</li> <li>Balance protection and cost</li> </ul> <p>Consideration: Keep enough versions for recovery needs</p>"},{"location":"gcp/cloud-storage/object-lifecycle/#pattern-5-graduated-versioning","title":"Pattern 5: Graduated Versioning","text":"<p>Use Case: Age versions differently than current objects</p> <p>Policy:</p> <pre><code># Conceptual representation\n# Current object lifecycle\nRule 1: age 30 days + Standard \u2192 Nearline\n\n# Version lifecycle\nRule 2: daysSinceNoncurrentTime 7 days \u2192 Nearline\nRule 3: daysSinceNoncurrentTime 30 days \u2192 Delete\n</code></pre> <p>Benefits:</p> <ul> <li>Aggressive version cleanup</li> <li>Protect current objects longer</li> <li>Reduce versioning costs</li> <li>Maintain recent version history</li> </ul>"},{"location":"gcp/cloud-storage/object-lifecycle/#cost-optimization-strategies","title":"Cost Optimization Strategies","text":""},{"location":"gcp/cloud-storage/object-lifecycle/#early-transition-strategy","title":"Early Transition Strategy","text":"<p>Principle: Transition to lower class as soon as minimum duration met</p> <p>Pattern:</p> <pre><code>Standard \u2192 Nearline at 30 days\nNearline \u2192 Coldline at 90 days\nColdline \u2192 Archive at 365 days\n</code></pre> <p>When Appropriate:</p> <ul> <li>Predictable access decay</li> <li>Known access patterns</li> <li>Cost optimization priority</li> <li>Rare access after initial period</li> </ul> <p>When Inappropriate:</p> <ul> <li>Unpredictable access</li> <li>Frequent retrieval needed</li> <li>Performance critical</li> <li>Unknown usage patterns</li> </ul>"},{"location":"gcp/cloud-storage/object-lifecycle/#delayed-transition-strategy","title":"Delayed Transition Strategy","text":"<p>Principle: Keep in higher class longer for flexibility</p> <p>Pattern:</p> <pre><code>Standard for 90 days\nStandard \u2192 Coldline at 90 days\nColdline \u2192 Archive at 365 days\nSkip Nearline tier\n</code></pre> <p>When Appropriate:</p> <ul> <li>Occasional access within 90 days</li> <li>Avoid multiple transition costs</li> <li>Simpler policy</li> <li>Uncertain 30-90 day access</li> </ul>"},{"location":"gcp/cloud-storage/object-lifecycle/#archive-quickly-for-compliance","title":"Archive Quickly for Compliance","text":"<p>Principle: Move to Archive immediately for known cold data</p> <p>Pattern:</p> <pre><code>Standard \u2192 Archive at 30 days (or immediately via object class)\n</code></pre> <p>When Appropriate:</p> <ul> <li>Known rare access</li> <li>Compliance-only data</li> <li>Cost critical</li> <li>Retrieval cost acceptable</li> </ul> <p>Consideration: 365-day minimum retention in Archive</p>"},{"location":"gcp/cloud-storage/object-lifecycle/#selective-lifecycle-by-prefix","title":"Selective Lifecycle by Prefix","text":"<p>Principle: Different rules for different data types</p> <p>Pattern:</p> <pre><code>logs/ \u2192 Aggressive aging (Archive at 30 days, Delete at 365)\nbackups/ \u2192 Moderate aging (Nearline at 30, Coldline at 90, keep long-term)\ndata/ \u2192 Conservative (Standard for 365 days, then Archive)\n</code></pre> <p>Benefits:</p> <ul> <li>Optimize each data type</li> <li>Granular control</li> <li>Balance competing requirements</li> <li>Application-specific policies</li> </ul>"},{"location":"gcp/cloud-storage/object-lifecycle/#lifecycle-and-early-deletion-fees","title":"Lifecycle and Early Deletion Fees","text":""},{"location":"gcp/cloud-storage/object-lifecycle/#understanding-early-deletion","title":"Understanding Early Deletion","text":"<p>Storage Class Minimums:</p> <ul> <li>Standard: None</li> <li>Nearline: 30 days</li> <li>Coldline: 90 days</li> <li>Archive: 365 days</li> </ul> <p>Fee Calculation:</p> <pre><code>Early deletion fee = (Remaining days / Days in month) \u00d7 Monthly storage cost\n</code></pre> <p>Example: Delete Nearline object after 20 days</p> <pre><code>Minimum: 30 days\nUsed: 20 days\nRemaining: 10 days\n\nFee: (10/30) \u00d7 $0.010/GB = $0.0033/GB\n</code></pre>"},{"location":"gcp/cloud-storage/object-lifecycle/#lifecycle-transition-impact","title":"Lifecycle Transition Impact","text":"<p>Transition Timing:</p> <ul> <li>Lifecycle transition counts as delete + create</li> <li>Early deletion fees apply</li> <li>Must account for minimum durations</li> </ul> <p>Example: Standard \u2192 Nearline at 25 days \u2192 Coldline at 60 days</p> <pre><code>Nearline duration: 35 days (60 - 25)\nColdline minimum: 90 days\n\nProblem: Transitioned to Coldline before Nearline minimum\nSolution: Wait until day 115 (25 + 90) for Coldline transition\n</code></pre> <p>Architecture Decision: Plan transitions to avoid early deletion fees</p>"},{"location":"gcp/cloud-storage/object-lifecycle/#optimal-transition-timing","title":"Optimal Transition Timing","text":"<p>Formula:</p> <pre><code>Transition to Nearline: Day 30 minimum\nTransition to Coldline: Day 120 minimum (30 + 90)\nTransition to Archive: Day 485 minimum (30 + 90 + 365)\n</code></pre> <p>Policy Design:</p> <pre><code># Conceptual - Avoid early deletion fees\nRule 1: age 30 days + Standard \u2192 Nearline\nRule 2: age 120 days + Nearline \u2192 Coldline  # Not 90!\nRule 3: age 485 days + Coldline \u2192 Archive   # Not 365!\n</code></pre>"},{"location":"gcp/cloud-storage/object-lifecycle/#lifecycle-policy-limitations","title":"Lifecycle Policy Limitations","text":""},{"location":"gcp/cloud-storage/object-lifecycle/#constraints","title":"Constraints","text":"<p>Policy Limits:</p> <ul> <li>100 rules per bucket maximum</li> <li>Conditions combined with AND logic</li> <li>Actions mutually exclusive per rule</li> <li>Daily execution only (not real-time)</li> </ul> <p>Transition Restrictions:</p> <ul> <li>Can only transition to \u201ccolder\u201d classes</li> <li>Cannot transition Archive \u2192 Coldline \u2192 Nearline \u2192 Standard</li> <li>One-way transitions only</li> <li>No circular transitions</li> </ul> <p>Architecture Implication: Design forward-only aging policies</p>"},{"location":"gcp/cloud-storage/object-lifecycle/#performance-considerations","title":"Performance Considerations","text":"<p>Asynchronous Execution:</p> <ul> <li>Up to 24-hour delay</li> <li>Not for time-critical operations</li> <li>Eventual consistency</li> <li>Best-effort timing</li> </ul> <p>Large Bucket Impact:</p> <ul> <li>Billions of objects processed daily</li> <li>No performance impact on bucket operations</li> <li>Lifecycle runs independently</li> <li>Scalable to any size</li> </ul>"},{"location":"gcp/cloud-storage/object-lifecycle/#lifecycle-and-versioning-interaction","title":"Lifecycle and Versioning Interaction","text":""},{"location":"gcp/cloud-storage/object-lifecycle/#version-aware-policies","title":"Version-Aware Policies","text":"<p>Non-Current Versions:</p> <ul> <li>Lifecycle can target versions separately</li> <li>Different rules for current vs non-current</li> <li>Version-specific conditions</li> <li>Control version accumulation</li> </ul> <p>Pattern: Aggressive version cleanup</p> <pre><code># Conceptual\nCurrent objects:\n  age 30 days \u2192 Nearline\n\nNon-current versions:\n  numberOfNewerVersions 3 \u2192 Delete\n  daysSinceNoncurrentTime 30 \u2192 Delete\n</code></pre> <p>Benefits:</p> <ul> <li>Keep current objects longer</li> <li>Limit version storage costs</li> <li>Maintain recent version history</li> <li>Automatic version pruning</li> </ul>"},{"location":"gcp/cloud-storage/object-lifecycle/#monitoring-and-validation","title":"Monitoring and Validation","text":""},{"location":"gcp/cloud-storage/object-lifecycle/#metrics-to-track","title":"Metrics to Track","text":"<p>Lifecycle Operations:</p> <ul> <li>Objects transitioned per day</li> <li>Objects deleted per day</li> <li>Storage class distribution</li> <li>Cost trends</li> </ul> <p>Cost Impact:</p> <ul> <li>Storage cost reduction</li> <li>Early deletion fees incurred</li> <li>Net cost savings</li> <li>Return on investment</li> </ul>"},{"location":"gcp/cloud-storage/object-lifecycle/#testing-lifecycle-policies","title":"Testing Lifecycle Policies","text":"<p>Best Practices:</p> <ol> <li>Test on subset first: Create test bucket with sample data</li> <li>Verify timing: Confirm transitions occur as expected</li> <li>Monitor costs: Check for early deletion fees</li> <li>Review regularly: Access patterns change over time</li> <li>Adjust as needed: Refine based on actual usage</li> </ol> <p>Test Bucket Pattern:</p> <ul> <li>Create separate bucket</li> <li>Copy representative sample</li> <li>Apply lifecycle policy</li> <li>Monitor for 30-90 days</li> <li>Validate behavior</li> <li>Apply to production</li> </ul>"},{"location":"gcp/cloud-storage/object-lifecycle/#lifecycle-vs-autoclass","title":"Lifecycle vs Autoclass","text":""},{"location":"gcp/cloud-storage/object-lifecycle/#comparison","title":"Comparison","text":"Feature Lifecycle Policies Autoclass Control Explicit rules Automatic Flexibility High (custom rules) Limited (fixed algorithm) Complexity Medium-High Low Cost Free Small management fee Optimization Manual tuning Automatic Predictability High Variable"},{"location":"gcp/cloud-storage/object-lifecycle/#when-to-use-each","title":"When to Use Each","text":"<p>Lifecycle Policies:</p> <ul> <li>Known access patterns</li> <li>Specific compliance rules</li> <li>Complex multi-tier aging</li> <li>Cost optimization priority</li> <li>Need full control</li> </ul> <p>Autoclass:</p> <ul> <li>Unknown access patterns</li> <li>Simple automation preferred</li> <li>Variable object access</li> <li>Hands-off management</li> <li>Willing to pay management fee</li> </ul> <p>Hybrid Approach:</p> <ul> <li>Autoclass for main data (unknown patterns)</li> <li>Lifecycle for cleanup (delete old data)</li> <li>Lifecycle for compliance (retention rules)</li> </ul>"},{"location":"gcp/cloud-storage/object-lifecycle/#exam-focus-areas","title":"Exam Focus Areas","text":""},{"location":"gcp/cloud-storage/object-lifecycle/#policy-design","title":"Policy Design","text":"<ul> <li>Multi-tier aging strategies</li> <li>Transition timing to avoid early deletion fees</li> <li>Condition selection and combination</li> <li>Order of rule evaluation</li> </ul>"},{"location":"gcp/cloud-storage/object-lifecycle/#cost-optimization","title":"Cost Optimization","text":"<ul> <li>Storage class transition economics</li> <li>Early deletion fee calculation</li> <li>Break-even analysis</li> <li>Total cost of ownership</li> </ul>"},{"location":"gcp/cloud-storage/object-lifecycle/#use-case-matching","title":"Use Case Matching","text":"<ul> <li>Compliance retention policies</li> <li>Temporary data cleanup</li> <li>Data lake aging</li> <li>Version management</li> </ul>"},{"location":"gcp/cloud-storage/object-lifecycle/#architecture-patterns","title":"Architecture Patterns","text":"<ul> <li>Log lifecycle management</li> <li>Backup retention</li> <li>Data archival strategies</li> <li>Cost-optimized storage</li> </ul>"},{"location":"gcp/cloud-storage/object-lifecycle/#lifecycle-vs-alternatives","title":"Lifecycle vs Alternatives","text":"<ul> <li>When to use lifecycle vs Autoclass</li> <li>Manual class selection vs automatic</li> <li>Versioning interaction</li> <li>Performance implications</li> </ul>"},{"location":"gcp/cloud-storage/object-versioning/","title":"Object Versioning","text":""},{"location":"gcp/cloud-storage/object-versioning/#core-concepts","title":"Core Concepts","text":"<p>Object versioning maintains multiple variants of an object in the same bucket, protecting against accidental deletion or overwrites. Understanding when versioning provides value versus when backups or other strategies are more appropriate is critical for cost-effective data protection.</p> <p>Key Principle: Versioning is for protecting against accidental changes and deletions, not for backup/archival (use separate buckets or lifecycle policies instead).</p>"},{"location":"gcp/cloud-storage/object-versioning/#how-versioning-works","title":"How Versioning Works","text":""},{"location":"gcp/cloud-storage/object-versioning/#version-generation","title":"Version Generation","text":"<p>Without Versioning:</p> <pre><code>Upload object.txt \u2192 Replaces any existing object.txt\nDelete object.txt \u2192 Permanently deleted\n</code></pre> <p>With Versioning:</p> <pre><code>Upload object.txt v1 \u2192 Generation #1234567890 (current)\nUpload object.txt v2 \u2192 Generation #1234567891 (current), v1 becomes non-current\nDelete object.txt   \u2192 Generation #1234567892 (delete marker), v2 becomes non-current\n</code></pre> <p>Architecture Implications:</p> <ul> <li>Objects never truly deleted (delete marker created)</li> <li>All versions stored and charged</li> <li>Must explicitly delete versions to free space</li> <li>Cannot turn off versioning (only suspend)</li> </ul>"},{"location":"gcp/cloud-storage/object-versioning/#object-generations","title":"Object Generations","text":"<p>Generation Number:</p> <ul> <li>Unique identifier for each version</li> <li>Microsecond timestamp</li> <li>Automatically assigned</li> <li>Immutable</li> </ul> <p>Current vs Non-Current:</p> <ul> <li>Current: Latest version, returned by default</li> <li>Non-Current: Previous versions, must specify generation</li> <li>Delete Marker: Special version indicating deletion</li> </ul>"},{"location":"gcp/cloud-storage/object-versioning/#versioning-states","title":"Versioning States","text":"<p>Enabled:</p> <ul> <li>All new objects versioned</li> <li>Existing objects gain versions</li> <li>Cannot disable (only suspend)</li> </ul> <p>Suspended:</p> <ul> <li>New objects get null generation</li> <li>Existing versions retained</li> <li>New uploads overwrite null generation</li> <li>Can re-enable later</li> </ul> <p>Never Enabled:</p> <ul> <li>Default state</li> <li>Objects have generation (for consistency) but not versioned</li> <li>Cannot recover from deletion/overwrite</li> </ul> <p>Important: Once enabled, cannot return to \u201cnever enabled\u201d state</p>"},{"location":"gcp/cloud-storage/object-versioning/#when-to-use-versioning","title":"When to Use Versioning","text":""},{"location":"gcp/cloud-storage/object-versioning/#appropriate-uses","title":"\u2705 Appropriate Uses","text":"<p>Protection Against Accidental Deletion:</p> <ul> <li>User errors</li> <li>Application bugs</li> <li>Operator mistakes</li> <li>Malicious actions</li> </ul> <p>Protection Against Overwrites:</p> <ul> <li>Accidental file replacements</li> <li>Concurrent write conflicts</li> <li>Application errors</li> <li>Data corruption</li> </ul> <p>Compliance Requirements:</p> <ul> <li>Audit trails</li> <li>Change tracking</li> <li>Document retention</li> <li>Regulatory requirements</li> </ul> <p>Collaboration Scenarios:</p> <ul> <li>Multiple users editing</li> <li>Concurrent access</li> <li>Rollback capability</li> <li>Change history</li> </ul> <p>Critical Data Protection:</p> <ul> <li>Configuration files</li> <li>Important documents</li> <li>Application state</li> <li>Database backups</li> </ul>"},{"location":"gcp/cloud-storage/object-versioning/#inappropriate-uses","title":"\u274c Inappropriate Uses","text":"<p>Long-Term Backup:</p> <ul> <li>Use separate backup bucket instead</li> <li>Lifecycle policies to Archive class</li> <li>Cross-region replication</li> <li>Not cost-effective for long-term retention</li> </ul> <p>Version Control for Code:</p> <ul> <li>Use Git, SVN, or other VCS</li> <li>Cloud Storage not designed for code versioning</li> <li>Better tools available</li> <li>Wrong abstraction</li> </ul> <p>Frequent Changes:</p> <ul> <li>Rapidly changing files</li> <li>Many small updates</li> <li>High-frequency logging</li> <li>Append-only data</li> </ul> <p>Why: Cost accumulates quickly, version explosion</p> <p>Temporary Files:</p> <ul> <li>Scratch space</li> <li>Intermediate processing results</li> <li>Cache data</li> <li>Short-lived data</li> </ul> <p>Why: Versions of temporary data wasteful</p>"},{"location":"gcp/cloud-storage/object-versioning/#versioning-vs-alternatives","title":"Versioning vs Alternatives","text":""},{"location":"gcp/cloud-storage/object-versioning/#versioning-vs-separate-backup-bucket","title":"Versioning vs Separate Backup Bucket","text":"Feature Versioning Separate Backup Bucket Same Bucket Yes No (different bucket) Cost All versions charged at current class Can use cheaper classes Lifecycle Can age versions differently Full lifecycle control Protection Accidental deletion/overwrite Logical/physical separation Complexity Low Medium Best For Operational recovery Disaster recovery <p>Decision: Use versioning for operational protection, separate buckets for DR/backup</p>"},{"location":"gcp/cloud-storage/object-versioning/#versioning-vs-snapshots-compute-engine","title":"Versioning vs Snapshots (Compute Engine)","text":"Feature Object Versioning Disk Snapshots Granularity Per object Per disk Incremental No (full versions) Yes Use Case Object storage Block storage Cost Full object size per version Incremental only <p>Decision: Different services, different use cases</p>"},{"location":"gcp/cloud-storage/object-versioning/#versioning-vs-lifecycle-policies","title":"Versioning vs Lifecycle Policies","text":"<p>Complementary, Not Alternative:</p> <ul> <li>Versioning: Creates versions</li> <li>Lifecycle: Manages version aging</li> <li>Use together for cost optimization</li> <li>Lifecycle can delete old versions</li> </ul> <p>Pattern: Enable versioning + lifecycle policy for version cleanup</p> <pre><code># Conceptual\nVersioning: Enabled\nLifecycle: Delete versions older than 30 days\n</code></pre>"},{"location":"gcp/cloud-storage/object-versioning/#cost-implications","title":"Cost Implications","text":""},{"location":"gcp/cloud-storage/object-versioning/#storage-costs","title":"Storage Costs","text":"<p>Every Version Charged:</p> <ul> <li>Each version is full object size</li> <li>All versions in same storage class (unless lifecycle transitions)</li> <li>Costs accumulate with versions</li> </ul> <p>Example: 1 GB file, 10 versions</p> <pre><code>Without versioning: 1 GB \u00d7 $0.020 = $0.020/month\nWith versioning: 10 GB \u00d7 $0.020 = $0.200/month\nCost multiplier: 10x\n</code></pre> <p>Architecture Impact: Monitor version count to control costs</p>"},{"location":"gcp/cloud-storage/object-versioning/#early-deletion-fees-and-versions","title":"Early Deletion Fees and Versions","text":"<p>Version Age Calculation:</p> <ul> <li>Each version has its own creation time</li> <li>Minimum duration per version</li> <li>Lifecycle transitions affect each version</li> </ul> <p>Example: Nearline object with 5 versions</p> <pre><code>Current version: Created today (day 0)\nVersion 2: Created 10 days ago\nVersion 3: Created 20 days ago\nVersion 4: Created 25 days ago\nVersion 5: Created 35 days ago\n\nLifecycle: Delete versions older than 30 days\n\nVersion 5: Deleted at day 35 (no early deletion fee)\nVersions 1-4: Retained\n</code></pre>"},{"location":"gcp/cloud-storage/object-versioning/#cost-optimization-strategies","title":"Cost Optimization Strategies","text":"<p>Version Lifecycle Policies:</p> <pre><code># Conceptual\nRule 1: numberOfNewerVersions 3 \u2192 Delete  # Keep only 3 recent\nRule 2: daysSinceNoncurrentTime 30 \u2192 Delete  # Delete after 30 days non-current\n</code></pre> <p>Benefits:</p> <ul> <li>Control version accumulation</li> <li>Automatic cleanup</li> <li>Predictable costs</li> <li>Balance protection and cost</li> </ul> <p>Transition Non-Current Versions:</p> <pre><code># Conceptual\nCurrent objects: Keep in Standard\nNon-current versions: Move to Nearline after 7 days\n</code></pre> <p>Benefits:</p> <ul> <li>Reduce storage costs for old versions</li> <li>Maintain current object performance</li> <li>Cost-effective version retention</li> </ul>"},{"location":"gcp/cloud-storage/object-versioning/#versioning-patterns","title":"Versioning Patterns","text":""},{"location":"gcp/cloud-storage/object-versioning/#pattern-1-short-term-version-retention","title":"Pattern 1: Short-Term Version Retention","text":"<p>Use Case: Protect against recent mistakes, not long-term history</p> <p>Configuration:</p> <ul> <li>Versioning: Enabled</li> <li>Lifecycle: Delete non-current versions after 30 days</li> <li>Keep only 5 most recent versions</li> </ul> <p>Appropriate For:</p> <ul> <li>Application data</li> <li>Configuration files</li> <li>Working documents</li> <li>Development assets</li> </ul> <p>Cost Profile: Moderate (limited versions)</p>"},{"location":"gcp/cloud-storage/object-versioning/#pattern-2-compliance-version-retention","title":"Pattern 2: Compliance Version Retention","text":"<p>Use Case: Regulatory requirements for change history</p> <p>Configuration:</p> <ul> <li>Versioning: Enabled</li> <li>Lifecycle: Transition non-current to Archive after 30 days</li> <li>Retention lock: 7 years</li> <li>Keep all versions</li> </ul> <p>Appropriate For:</p> <ul> <li>Financial records</li> <li>Medical documents</li> <li>Legal documents</li> <li>Audit trails</li> </ul> <p>Cost Profile: High (all versions, long retention)</p>"},{"location":"gcp/cloud-storage/object-versioning/#pattern-3-recent-version-accessibility","title":"Pattern 3: Recent Version Accessibility","text":"<p>Use Case: Frequent rollback needs, recent changes only</p> <p>Configuration:</p> <ul> <li>Versioning: Enabled</li> <li>Lifecycle: Keep 10 recent versions</li> <li>Non-current to Nearline after 7 days</li> <li>Delete after 90 days non-current</li> </ul> <p>Appropriate For:</p> <ul> <li>Website content</li> <li>Application assets</li> <li>Content management</li> <li>Collaboration scenarios</li> </ul> <p>Cost Profile: Low-Moderate (limited versions, lower storage class)</p>"},{"location":"gcp/cloud-storage/object-versioning/#pattern-4-minimal-versioning","title":"Pattern 4: Minimal Versioning","text":"<p>Use Case: Protection only, minimize costs</p> <p>Configuration:</p> <ul> <li>Versioning: Enabled</li> <li>Lifecycle: Keep only 1-2 previous versions</li> <li>Delete non-current after 7 days</li> </ul> <p>Appropriate For:</p> <ul> <li>Large files</li> <li>Binary assets</li> <li>Media files</li> <li>Cost-sensitive workloads</li> </ul> <p>Cost Profile: Low (minimal versions)</p>"},{"location":"gcp/cloud-storage/object-versioning/#versioning-and-object-lifecycle","title":"Versioning and Object Lifecycle","text":""},{"location":"gcp/cloud-storage/object-versioning/#lifecycle-rules-for-versions","title":"Lifecycle Rules for Versions","text":"<p>Conditions for Versions:</p> <ul> <li><code>numberOfNewerVersions</code>: Limit version count</li> <li><code>daysSinceNoncurrentTime</code>: Age since becoming non-current</li> <li><code>isLive: false</code>: Target only non-current versions</li> </ul> <p>Common Patterns:</p> <pre><code># Conceptual\n# Keep 5 recent versions\nRule 1: numberOfNewerVersions 5 + isLive false \u2192 Delete\n\n# Age out old versions\nRule 2: daysSinceNoncurrentTime 90 + isLive false \u2192 Delete\n\n# Cheap storage for versions\nRule 3: daysSinceNoncurrentTime 7 + isLive false + Nearline \u2192 Coldline\n</code></pre>"},{"location":"gcp/cloud-storage/object-versioning/#version-specific-transitions","title":"Version-Specific Transitions","text":"<p>Pattern: Different lifecycle for current vs non-current</p> <pre><code># Conceptual\nCurrent object lifecycle:\n  age 30 days + Standard \u2192 Nearline\n\nNon-current version lifecycle:\n  daysSinceNoncurrentTime 7 \u2192 Nearline\n  daysSinceNoncurrentTime 30 \u2192 Delete\n</code></pre> <p>Benefits:</p> <ul> <li>Aggressive version cleanup</li> <li>Protect current object longer</li> <li>Cost optimization</li> <li>Flexibility</li> </ul>"},{"location":"gcp/cloud-storage/object-versioning/#deletion-behavior-with-versioning","title":"Deletion Behavior with Versioning","text":""},{"location":"gcp/cloud-storage/object-versioning/#soft-delete-delete-marker","title":"Soft Delete (Delete Marker)","text":"<p>Default Behavior:</p> <pre><code>Delete object \u2192 Create delete marker (special version)\nList bucket \u2192 Object not shown (appears deleted)\nGet object \u2192 404 Not Found\n</code></pre> <p>Recovery:</p> <pre><code>Delete the delete marker \u2192 Object reappears (previous version becomes current)\n</code></pre> <p>Use Case: Accidental deletion recovery</p>"},{"location":"gcp/cloud-storage/object-versioning/#permanent-delete","title":"Permanent Delete","text":"<p>Deleting Specific Version:</p> <pre><code>Delete object with generation number \u2192 Version permanently deleted\nDelete current version \u2192 Previous version becomes current\n</code></pre> <p>Cannot Be Recovered: Permanent deletion</p>"},{"location":"gcp/cloud-storage/object-versioning/#delete-marker-cleanup","title":"Delete Marker Cleanup","text":"<p>Problem: Delete markers accumulate</p> <p>Solution: Lifecycle policy</p> <pre><code># Conceptual\nRule: Delete marker with no other versions \u2192 Delete marker\n</code></pre> <p>Benefit: Clean up orphaned delete markers</p>"},{"location":"gcp/cloud-storage/object-versioning/#recovery-scenarios","title":"Recovery Scenarios","text":""},{"location":"gcp/cloud-storage/object-versioning/#scenario-1-accidental-file-overwrite","title":"Scenario 1: Accidental File Overwrite","text":"<p>Problem: Uploaded wrong version</p> <p>Recovery:</p> <ol> <li>List object versions</li> <li>Identify correct version (by timestamp)</li> <li>Copy correct version to new object or restore as current</li> </ol> <p>Timeline: Immediate (minutes)</p>"},{"location":"gcp/cloud-storage/object-versioning/#scenario-2-accidental-deletion","title":"Scenario 2: Accidental Deletion","text":"<p>Problem: Deleted important file</p> <p>Recovery:</p> <ol> <li>Identify delete marker</li> <li>Delete the delete marker</li> <li>Previous version becomes current</li> </ol> <p>Timeline: Immediate (seconds)</p>"},{"location":"gcp/cloud-storage/object-versioning/#scenario-3-ransomwaremalware","title":"Scenario 3: Ransomware/Malware","text":"<p>Problem: Files encrypted/corrupted</p> <p>Recovery:</p> <ol> <li>Identify last good version (before infection)</li> <li>Restore all objects to that generation</li> <li>Delete infected versions</li> </ol> <p>Timeline: Hours (depending on scale)</p> <p>Architecture Pattern: Versioning + retention lock prevents malicious deletion of versions</p>"},{"location":"gcp/cloud-storage/object-versioning/#retention-lock-and-versioning","title":"Retention Lock and Versioning","text":""},{"location":"gcp/cloud-storage/object-versioning/#bucket-level-retention","title":"Bucket-Level Retention","text":"<p>How It Works:</p> <ul> <li>Minimum retention period</li> <li>Objects/versions cannot be deleted before period</li> <li>Applies to all objects in bucket</li> <li>Lock can be permanent or temporary</li> </ul> <p>With Versioning:</p> <ul> <li>Applies to each version independently</li> <li>Version age starts when version created (not object)</li> <li>Protects against accidental and malicious deletion</li> </ul> <p>Use Case: Compliance (WORM - Write Once Read Many)</p>"},{"location":"gcp/cloud-storage/object-versioning/#object-hold","title":"Object Hold","text":"<p>Event-Based Hold:</p> <ul> <li>Hold until event occurs</li> <li>Manual release required</li> <li>Applies to object (all versions)</li> </ul> <p>Temporary Hold:</p> <ul> <li>Manual hold/release</li> <li>Investigation or legal purposes</li> <li>Can be removed any time</li> </ul> <p>With Versioning: Holds apply per version</p>"},{"location":"gcp/cloud-storage/object-versioning/#performance-considerations","title":"Performance Considerations","text":""},{"location":"gcp/cloud-storage/object-versioning/#list-operations","title":"List Operations","text":"<p>Impact of Versioning:</p> <ul> <li>Listing includes all versions (if requested)</li> <li>More versions = more data to return</li> <li>Can impact list performance</li> <li>Use prefix/delimiter to limit scope</li> </ul> <p>Optimization:</p> <ul> <li>List current versions only (default)</li> <li>Request versions only when needed</li> <li>Use pagination for large result sets</li> </ul>"},{"location":"gcp/cloud-storage/object-versioning/#storage-operations","title":"Storage Operations","text":"<p>No Performance Impact:</p> <ul> <li>Read/write performance unchanged</li> <li>Versioning handled transparently</li> <li>No latency increase</li> <li>Scales to any number of versions</li> </ul>"},{"location":"gcp/cloud-storage/object-versioning/#monitoring-and-management","title":"Monitoring and Management","text":""},{"location":"gcp/cloud-storage/object-versioning/#metrics-to-track","title":"Metrics to Track","text":"<p>Version Count:</p> <ul> <li>Total versions per object</li> <li>Non-current version count</li> <li>Delete markers</li> </ul> <p>Storage Usage:</p> <ul> <li>Versioned object storage</li> <li>Version storage breakdown</li> <li>Cost attribution</li> </ul> <p>Lifecycle Actions:</p> <ul> <li>Versions deleted per day</li> <li>Versions transitioned</li> <li>Delete markers cleaned</li> </ul>"},{"location":"gcp/cloud-storage/object-versioning/#alerting","title":"Alerting","text":"<p>Cost Alerts:</p> <ul> <li>Unexpected version accumulation</li> <li>Storage cost increase</li> <li>Budget exceeded</li> </ul> <p>Operational Alerts:</p> <ul> <li>High version count per object</li> <li>Rapid version creation</li> <li>Lifecycle failure</li> </ul>"},{"location":"gcp/cloud-storage/object-versioning/#versioning-best-practices","title":"Versioning Best Practices","text":""},{"location":"gcp/cloud-storage/object-versioning/#1-enable-with-lifecycle","title":"1. Enable with Lifecycle","text":"<p>Pattern: Always pair versioning with lifecycle policies</p> <pre><code># Conceptual\nVersioning: Enabled\nLifecycle: \n\n  - Keep 10 recent versions\n  - Delete versions &gt; 90 days non-current\n</code></pre> <p>Benefit: Automatic cost control</p>"},{"location":"gcp/cloud-storage/object-versioning/#2-monitor-version-count","title":"2. Monitor Version Count","text":"<p>Practice: Regular audits of version counts</p> <p>Tools: Cloud Monitoring, custom scripts</p> <p>Action: Adjust lifecycle if accumulation excessive</p>"},{"location":"gcp/cloud-storage/object-versioning/#3-test-recovery-procedures","title":"3. Test Recovery Procedures","text":"<p>Practice: Quarterly recovery drills</p> <p>Steps:</p> <ol> <li>Simulate deletion</li> <li>Recover from version</li> <li>Verify data integrity</li> <li>Document procedure</li> </ol>"},{"location":"gcp/cloud-storage/object-versioning/#4-use-for-appropriate-data","title":"4. Use for Appropriate Data","text":"<p>Enable For:</p> <ul> <li>Configuration files</li> <li>Important documents</li> <li>Critical application data</li> <li>Collaboration documents</li> </ul> <p>Don\u2019t Enable For:</p> <ul> <li>Large media files (high cost)</li> <li>Temporary data</li> <li>Log files (use lifecycle)</li> <li>Frequently changing data</li> </ul>"},{"location":"gcp/cloud-storage/object-versioning/#5-document-version-policy","title":"5. Document Version Policy","text":"<p>Include:</p> <ul> <li>Retention period</li> <li>Version count limits</li> <li>Recovery procedures</li> <li>Responsible parties</li> </ul>"},{"location":"gcp/cloud-storage/object-versioning/#exam-focus-areas","title":"Exam Focus Areas","text":""},{"location":"gcp/cloud-storage/object-versioning/#when-to-use-versioning_1","title":"When to Use Versioning","text":"<ul> <li>Accidental deletion protection</li> <li>Overwrite protection</li> <li>Compliance requirements</li> <li>Versioning vs other backup methods</li> </ul>"},{"location":"gcp/cloud-storage/object-versioning/#cost-management","title":"Cost Management","text":"<ul> <li>Version storage cost calculation</li> <li>Lifecycle policies for versions</li> <li>Version count control</li> <li>Cost optimization strategies</li> </ul>"},{"location":"gcp/cloud-storage/object-versioning/#lifecycle-integration","title":"Lifecycle Integration","text":"<ul> <li>Version-specific lifecycle rules</li> <li>Current vs non-current policies</li> <li>Delete marker cleanup</li> <li>Storage class transitions for versions</li> </ul>"},{"location":"gcp/cloud-storage/object-versioning/#recovery-scenarios_1","title":"Recovery Scenarios","text":"<ul> <li>Deletion recovery process</li> <li>Overwrite recovery</li> <li>Ransomware recovery</li> <li>Version selection and restoration</li> </ul>"},{"location":"gcp/cloud-storage/object-versioning/#architecture-patterns","title":"Architecture Patterns","text":"<ul> <li>Version retention strategies</li> <li>Compliance patterns</li> <li>Cost-optimized versioning</li> <li>Versioning vs separate backups</li> </ul>"},{"location":"gcp/cloud-storage/overview/","title":"Cloud Storage Overview","text":""},{"location":"gcp/cloud-storage/overview/#description","title":"Description","text":"<p>Cloud Storage is Google Cloud\u2019s object storage service for storing and retrieving unstructured data at any scale. Understanding when to use object storage versus block storage (Persistent Disks) or file storage (Filestore) is fundamental to cloud architecture.</p> <p>Key Principle: Object storage is for massive-scale, infrequently changing data accessed via HTTP/S; not for databases or file systems requiring POSIX operations.</p>"},{"location":"gcp/cloud-storage/overview/#core-concepts","title":"Core Concepts","text":""},{"location":"gcp/cloud-storage/overview/#object-storage-model","title":"Object Storage Model","text":"<p>Architecture:</p> <ul> <li>Buckets: Top-level containers for objects</li> <li>Objects: Individual files with metadata</li> <li>Flat namespace: No directory structure (though prefixes simulate folders)</li> <li>HTTP/S access: RESTful API, not filesystem mount</li> </ul> <p>Characteristics:</p> <ul> <li>Unlimited capacity</li> <li>Strongly consistent (read-after-write)</li> <li>Eventually consistent for IAM/ACL changes</li> <li>Atomic operations</li> <li>No minimum object size</li> <li>Maximum object size: 5 TB</li> </ul>"},{"location":"gcp/cloud-storage/overview/#buckets","title":"Buckets","text":"<p>Bucket Properties:</p> <ul> <li>Globally unique name</li> <li>Geographic location (region, dual-region, multi-region)</li> <li>Storage class (default for objects)</li> <li>Access control method (IAM, ACL, or both)</li> <li>Lifecycle policies</li> <li>Versioning settings</li> </ul> <p>Immutable Properties (cannot change after creation):</p> <ul> <li>Bucket name</li> <li>Location type (region/dual-region/multi-region)</li> <li>Specific location</li> </ul> <p>Architecture Impact: Location and name decisions are permanent; plan carefully</p>"},{"location":"gcp/cloud-storage/overview/#objects","title":"Objects","text":"<p>Object Components:</p> <ul> <li>Data (the file content)</li> <li>Metadata (key-value pairs)</li> <li>Access control (IAM, ACLs)</li> <li>Generation number (versioning)</li> </ul> <p>Object Metadata:</p> <ul> <li>System metadata (size, content-type, timestamps)</li> <li>Custom metadata (user-defined key-value pairs)</li> <li>Object generation (version identifier)</li> <li>Metageneration (metadata version)</li> </ul>"},{"location":"gcp/cloud-storage/overview/#durability-and-availability","title":"Durability and Availability","text":""},{"location":"gcp/cloud-storage/overview/#durability-99999999999-11-9s","title":"Durability: 99.999999999% (11 9\u2019s)","text":"<p>What This Means:</p> <ul> <li>Store 10 million objects</li> <li>Expect to lose 1 object every 10,000 years</li> <li>Designed for no data loss</li> <li>Automatic replication and erasure coding</li> </ul> <p>Architecture Implication: Cloud Storage is more durable than any self-managed solution</p>"},{"location":"gcp/cloud-storage/overview/#availability-varies-by-location","title":"Availability Varies by Location","text":"<p>Multi-Region: 99.95% SLA</p> <ul> <li>Data in at least two geographic locations</li> <li> <p>160 km apart</p> </li> <li>Automatic failover</li> <li>Use for: Global applications, highest availability</li> </ul> <p>Dual-Region: 99.95% SLA</p> <ul> <li>Data in two specific regions</li> <li>Choose regions for latency/compliance</li> <li>Balance between cost and availability</li> <li>Use for: Regional applications with HA requirements</li> </ul> <p>Region: 99.9% SLA</p> <ul> <li>Data in single region (across 3 zones)</li> <li>Lower cost than multi/dual-region</li> <li>Lower availability than multi/dual-region</li> <li>Use for: Regional data, cost optimization</li> </ul> <p>Architecture Decision: Balance availability requirements with cost</p>"},{"location":"gcp/cloud-storage/overview/#strong-consistency","title":"Strong Consistency","text":""},{"location":"gcp/cloud-storage/overview/#read-after-write-consistency","title":"Read-After-Write Consistency","text":"<p>Guarantees:</p> <ul> <li>Object immediately available after write</li> <li>Listing immediately shows new objects</li> <li>Deletes immediately reflected</li> <li>No eventual consistency delays</li> </ul> <p>Architectural Benefits:</p> <ul> <li>Simplifies application logic</li> <li>No need to handle stale reads</li> <li>Reliable for build artifacts and CI/CD</li> <li>Safe for concurrent access patterns</li> </ul> <p>Exceptions:</p> <ul> <li>IAM changes: Up to 60 seconds</li> <li>Bucket configuration: Up to 60 seconds</li> </ul>"},{"location":"gcp/cloud-storage/overview/#when-to-use-cloud-storage","title":"When to Use Cloud Storage","text":""},{"location":"gcp/cloud-storage/overview/#use-cloud-storage-when","title":"\u2705 Use Cloud Storage When:","text":"<p>Unstructured Data Storage:</p> <ul> <li>Media files (images, videos, audio)</li> <li>Document storage</li> <li>Backups and archives</li> <li>Log files and analytics data</li> <li>Build artifacts and binaries</li> </ul> <p>Static Website Hosting:</p> <ul> <li>HTML, CSS, JavaScript files</li> <li>Static assets</li> <li>Public documentation</li> <li>Download repositories</li> </ul> <p>Data Lake / Analytics:</p> <ul> <li>Raw data ingestion</li> <li>Data warehouse staging</li> <li>BigQuery external tables</li> <li>Dataflow input/output</li> </ul> <p>Backup and Archive:</p> <ul> <li>Database backups</li> <li>VM images and snapshots (backend storage)</li> <li>Long-term archival</li> <li>Compliance data retention</li> </ul> <p>Content Distribution:</p> <ul> <li>Cloud CDN origin</li> <li>Software distribution</li> <li>Global asset delivery</li> <li>User-generated content</li> </ul>"},{"location":"gcp/cloud-storage/overview/#dont-use-cloud-storage-when","title":"\u274c Don\u2019t Use Cloud Storage When:","text":"<p>Database Storage:</p> <ul> <li>Use Cloud SQL, Spanner, Firestore</li> <li>Object storage not optimized for transactional data</li> <li>No query language support</li> <li>Not ACID compliant</li> </ul> <p>File System Requirements:</p> <ul> <li>Applications expecting POSIX filesystem</li> <li>Random writes within files</li> <li>File locking mechanisms</li> <li>Use Filestore (NFS) instead</li> </ul> <p>Block Storage for VMs:</p> <ul> <li>VM boot disks</li> <li>Database data files</li> <li>High-IOPS applications</li> <li>Use Persistent Disks instead</li> </ul> <p>Real-Time Streaming:</p> <ul> <li>Message queues</li> <li>Real-time event streaming</li> <li>Use Pub/Sub instead</li> </ul> <p>Frequent Small Updates:</p> <ul> <li>Collaborative editing</li> <li>Append operations</li> <li>Partial object updates</li> <li>Use database or Firestore</li> </ul>"},{"location":"gcp/cloud-storage/overview/#cloud-storage-vs-alternatives","title":"Cloud Storage vs Alternatives","text":""},{"location":"gcp/cloud-storage/overview/#cloud-storage-vs-persistent-disks","title":"Cloud Storage vs Persistent Disks","text":"Feature Cloud Storage Persistent Disk Access Method HTTP/S API Block device (mounted) Use Case Unstructured objects VM storage, databases Performance High throughput High IOPS Capacity Unlimited Up to 64 TB per disk Cost Based on storage class Based on disk type Attachment Any number of clients Limited per VM Snapshot Object versioning Incremental snapshots <p>Decision: Use Cloud Storage for objects, Persistent Disks for databases and VMs</p>"},{"location":"gcp/cloud-storage/overview/#cloud-storage-vs-filestore","title":"Cloud Storage vs Filestore","text":"Feature Cloud Storage Filestore Protocol HTTP/S NFS Consistency Object-level POSIX filesystem Use Case Object storage Shared file storage Scale Unlimited Up to 100 TB Performance Throughput optimized IOPS optimized Mounting API/SDK NFS mount <p>Decision: Use Cloud Storage for objects, Filestore for POSIX filesystem requirements</p>"},{"location":"gcp/cloud-storage/overview/#cloud-storage-vs-cloud-sql","title":"Cloud Storage vs Cloud SQL","text":"Feature Cloud Storage Cloud SQL Data Model Objects Relational tables Query List/Get by name SQL queries Transactions None ACID compliant Use Case Unstructured data Structured data Consistency Strong per object Transactional <p>Decision: Use Cloud Storage for files, Cloud SQL for relational data</p>"},{"location":"gcp/cloud-storage/overview/#location-types","title":"Location Types","text":""},{"location":"gcp/cloud-storage/overview/#multi-region","title":"Multi-Region","text":"<p>Characteristics:</p> <ul> <li>At least two geographic areas (&gt;160 km apart)</li> <li>Highest availability (99.95%)</li> <li>Geo-redundant</li> <li>Higher cost</li> </ul> <p>Available Multi-Regions:</p> <ul> <li>US (multiple US locations)</li> <li>EU (multiple Europe locations)</li> <li>ASIA (multiple Asia locations)</li> </ul> <p>Use Cases:</p> <ul> <li>Global applications</li> <li>Highest availability requirements</li> <li>Content distribution</li> <li>Disaster recovery</li> </ul> <p>Cost: Highest storage cost, no egress within multi-region</p>"},{"location":"gcp/cloud-storage/overview/#dual-region","title":"Dual-Region","text":"<p>Characteristics:</p> <ul> <li>Two specific regions</li> <li>99.95% availability</li> <li>Geo-redundant</li> <li>Choose regions for compliance/latency</li> <li>Turbo replication option (async replication &lt;15 min)</li> </ul> <p>Examples:</p> <ul> <li>NAM4 (Iowa + South Carolina)</li> <li>EUR4 (Netherlands + Finland)</li> </ul> <p>Use Cases:</p> <ul> <li>Regional applications with HA</li> <li>Data residency requirements</li> <li>Balance cost and availability</li> <li>Specific latency requirements</li> </ul> <p>Cost: Between multi-region and region</p>"},{"location":"gcp/cloud-storage/overview/#region","title":"Region","text":"<p>Characteristics:</p> <ul> <li>Single region (3 zones)</li> <li>99.9% availability</li> <li>Zone-redundant within region</li> <li>Lowest cost</li> </ul> <p>Use Cases:</p> <ul> <li>Regional applications</li> <li>Cost optimization</li> <li>Data locality requirements</li> <li>Compute in same region (lower latency, no egress)</li> </ul> <p>Cost: Lowest storage cost</p>"},{"location":"gcp/cloud-storage/overview/#access-control","title":"Access Control","text":""},{"location":"gcp/cloud-storage/overview/#iam-recommended","title":"IAM (Recommended)","text":"<p>Characteristics:</p> <ul> <li>Bucket-level and project-level permissions</li> <li>Fine-grained roles</li> <li>Condition-based access</li> <li>Integration with organization policies</li> <li>Audit logging</li> </ul> <p>Roles:</p> <ul> <li><code>roles/storage.objectViewer</code>: Read objects</li> <li><code>roles/storage.objectCreator</code>: Create objects</li> <li><code>roles/storage.objectAdmin</code>: Full object control</li> <li><code>roles/storage.admin</code>: Full bucket control</li> </ul> <p>Use Cases:</p> <ul> <li>Modern applications</li> <li>Service-to-service access</li> <li>Centralized access control</li> <li>Conditional access policies</li> </ul>"},{"location":"gcp/cloud-storage/overview/#acls-legacy","title":"ACLs (Legacy)","text":"<p>Characteristics:</p> <ul> <li>Object-level and bucket-level</li> <li>Simpler but less flexible</li> <li>Compatible with S3 ACLs</li> <li>Being phased out</li> </ul> <p>Use Cases:</p> <ul> <li>Legacy applications</li> <li>S3 compatibility requirements</li> <li>Specific object-level permissions</li> <li>Not recommended for new applications</li> </ul>"},{"location":"gcp/cloud-storage/overview/#uniform-bucket-level-access","title":"Uniform Bucket-Level Access","text":"<p>Recommendation: Enable uniform bucket-level access (IAM only, no ACLs)</p> <p>Benefits:</p> <ul> <li>Simplified permission model</li> <li>Better security</li> <li>Easier auditing</li> <li>Organization policy enforcement</li> </ul> <p>Architecture Decision: Use IAM with uniform bucket-level access for all new buckets</p>"},{"location":"gcp/cloud-storage/overview/#signed-urls","title":"Signed URLs","text":""},{"location":"gcp/cloud-storage/overview/#use-cases","title":"Use Cases","text":"<p>Temporary Access:</p> <ul> <li>Time-limited access to private objects</li> <li>No authentication required</li> <li>Share with external users</li> <li>Download links with expiration</li> </ul> <p>Architecture Pattern:</p> <pre><code>Application (with credentials) \u2192 Generate signed URL \u2192 Share URL \u2192 User downloads directly\n</code></pre> <p>Benefits:</p> <ul> <li>No proxy through application server</li> <li>Direct download from Cloud Storage</li> <li>Reduced infrastructure costs</li> <li>Better performance</li> </ul> <p>Time Limits:</p> <ul> <li>Maximum 7 days (service account key)</li> <li>Maximum 12 hours (user credentials)</li> <li>Set appropriate expiration for use case</li> </ul>"},{"location":"gcp/cloud-storage/overview/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"gcp/cloud-storage/overview/#throughput","title":"Throughput","text":"<p>Capabilities:</p> <ul> <li>Multi-gigabit per second throughput</li> <li>Scales with parallel requests</li> <li>No bottleneck for large files</li> <li>Optimized for bandwidth</li> </ul> <p>Optimization:</p> <ul> <li>Parallel uploads for large files</li> <li>Multiple threads/workers</li> <li>Composite uploads for &gt;32 MB files</li> </ul>"},{"location":"gcp/cloud-storage/overview/#request-rate-limits","title":"Request Rate Limits","text":"<p>Bucket Limits (sustained):</p> <ul> <li>5,000 writes per second</li> <li>50,000 reads per second</li> </ul> <p>Object Limits:</p> <ul> <li>1,000 operations per second per object</li> </ul> <p>Architecture Impact:</p> <ul> <li>Design to avoid hotspots (single object)</li> <li>Use object prefixes for distribution</li> <li>Consider request rate in architecture</li> </ul>"},{"location":"gcp/cloud-storage/overview/#latency","title":"Latency","text":"<p>Typical Latency:</p> <ul> <li>Single-digit milliseconds (same region)</li> <li>Tens of milliseconds (cross-region)</li> <li>Sub-second for first byte</li> </ul> <p>Optimizations:</p> <ul> <li>Collocate compute and storage</li> <li>Use Cloud CDN for global access</li> <li>Minimize request overhead</li> </ul>"},{"location":"gcp/cloud-storage/overview/#cost-model","title":"Cost Model","text":""},{"location":"gcp/cloud-storage/overview/#storage-costs","title":"Storage Costs","text":"<p>Pricing by Class (per GB/month):</p> <ul> <li>Standard: ~$0.020</li> <li>Nearline: ~$0.010</li> <li>Coldline: ~$0.004</li> <li>Archive: ~$0.0012</li> </ul> <p>Variations:</p> <ul> <li>Multi-region: Higher than region</li> <li>Region: Lowest</li> <li>Dual-region: Between multi and region</li> </ul>"},{"location":"gcp/cloud-storage/overview/#operation-costs","title":"Operation Costs","text":"<p>Class A Operations (writes, lists): ~$0.05 per 10,000</p> <ul> <li>Insert, update, list</li> <li>Lifecycle transitions</li> <li>Composition operations</li> </ul> <p>Class B Operations (reads): ~$0.004 per 10,000</p> <ul> <li>Get object</li> <li>Get metadata</li> </ul> <p>Free Operations:</p> <ul> <li>Delete</li> <li>Get bucket metadata (non-object)</li> </ul>"},{"location":"gcp/cloud-storage/overview/#network-costs","title":"Network Costs","text":"<p>Egress (data out):</p> <ul> <li>Same location (region/multi-region): Free</li> <li>Cross-region: ~$0.01-$0.12 per GB</li> <li>To internet: ~$0.12 per GB (first GB free)</li> <li>GCP services same region: Free</li> <li>GCP services cross-region: Charged</li> </ul> <p>Ingress (data in): Free</p>"},{"location":"gcp/cloud-storage/overview/#retrieval-costs-non-standard-classes","title":"Retrieval Costs (Non-Standard Classes)","text":"<p>Nearline: ~$0.01 per GB retrieved Coldline: ~$0.02 per GB retrieved Archive: ~$0.05 per GB retrieved</p> <p>Architecture Impact: Factor retrieval costs into storage class selection</p>"},{"location":"gcp/cloud-storage/overview/#integration-patterns","title":"Integration Patterns","text":""},{"location":"gcp/cloud-storage/overview/#with-compute-services","title":"With Compute Services","text":"<p>Compute Engine: Upload/download via gsutil or API GKE: Mount via Cloud Storage FUSE or S3 API Cloud Functions: Triggered by object changes, direct access Cloud Run: Access via client libraries</p>"},{"location":"gcp/cloud-storage/overview/#with-data-services","title":"With Data Services","text":"<p>BigQuery: External tables, import/export Dataflow: Source/sink for pipelines Dataproc: HDFS replacement, job I/O Composer: DAG storage, data staging</p>"},{"location":"gcp/cloud-storage/overview/#with-mlai","title":"With ML/AI","text":"<p>Vertex AI: Training data, model storage AutoML: Dataset storage AI Platform: Job input/output</p>"},{"location":"gcp/cloud-storage/overview/#with-migration-services","title":"With Migration Services","text":"<p>Transfer Service: Online data transfer Transfer Appliance: Offline data transfer Database Migration Service: Backup storage</p>"},{"location":"gcp/cloud-storage/overview/#security-considerations","title":"Security Considerations","text":""},{"location":"gcp/cloud-storage/overview/#encryption","title":"Encryption","text":"<p>At Rest (default):</p> <ul> <li>Google-managed encryption keys</li> <li>Automatic for all data</li> <li>No configuration needed</li> </ul> <p>Customer-Managed Keys (CMEK):</p> <ul> <li>Cloud KMS integration</li> <li>Customer controls key lifecycle</li> <li>Compliance requirements</li> <li>Audit key usage</li> </ul> <p>Customer-Supplied Keys (CSEK):</p> <ul> <li>Customer provides keys</li> <li>Google doesn\u2019t store keys</li> <li>More operational overhead</li> <li>Enhanced security</li> </ul>"},{"location":"gcp/cloud-storage/overview/#in-transit","title":"In Transit","text":"<p>HTTPS Only:</p> <ul> <li>TLS encryption automatic</li> <li>No unencrypted option for API</li> <li>Best practice for all access</li> </ul>"},{"location":"gcp/cloud-storage/overview/#bucket-policies","title":"Bucket Policies","text":"<p>Organization Policies:</p> <ul> <li>Enforce public access prevention</li> <li>Require uniform access</li> <li>Location restrictions</li> <li>Domain restrictions</li> </ul> <p>VPC Service Controls:</p> <ul> <li>Perimeter-based access</li> <li>Prevent data exfiltration</li> <li>Additional security layer</li> </ul>"},{"location":"gcp/cloud-storage/overview/#compliance-and-governance","title":"Compliance and Governance","text":""},{"location":"gcp/cloud-storage/overview/#data-residency","title":"Data Residency","text":"<p>Control via Location:</p> <ul> <li>Choose specific region</li> <li>Dual-region for specific pairs</li> <li>Multi-region for broad geography</li> <li>Data stays in chosen location</li> </ul>"},{"location":"gcp/cloud-storage/overview/#retention-policies","title":"Retention Policies","text":"<p>Bucket-Level Retention:</p> <ul> <li>Minimum retention period</li> <li>Objects cannot be deleted before period</li> <li>Locked policies cannot be reduced</li> <li>Compliance use cases (regulatory requirements)</li> </ul>"},{"location":"gcp/cloud-storage/overview/#object-holds","title":"Object Holds","text":"<p>Types:</p> <ul> <li>Event-based hold: Until event clears</li> <li>Temporary hold: Manual hold/release</li> </ul> <p>Use Cases:</p> <ul> <li>Legal hold</li> <li>Investigation period</li> <li>Compliance requirements</li> </ul>"},{"location":"gcp/cloud-storage/overview/#exam-focus-areas","title":"Exam Focus Areas","text":""},{"location":"gcp/cloud-storage/overview/#design-decisions","title":"Design Decisions","text":"<ul> <li>When to use Cloud Storage vs alternatives</li> <li>Storage class selection criteria</li> <li>Location type selection (region/dual-region/multi-region)</li> <li>Access control method (IAM vs ACL)</li> </ul>"},{"location":"gcp/cloud-storage/overview/#cost-optimization","title":"Cost Optimization","text":"<ul> <li>Storage class economics</li> <li>Network egress patterns</li> <li>Operation cost implications</li> <li>Lifecycle management</li> </ul>"},{"location":"gcp/cloud-storage/overview/#performance","title":"Performance","text":"<ul> <li>Request rate limits</li> <li>Throughput optimization</li> <li>Latency considerations</li> <li>Parallel operations</li> </ul>"},{"location":"gcp/cloud-storage/overview/#security","title":"Security","text":"<ul> <li>Encryption options</li> <li>Access control patterns</li> <li>Signed URLs use cases</li> <li>VPC Service Controls</li> </ul>"},{"location":"gcp/cloud-storage/overview/#integration","title":"Integration","text":"<ul> <li>BigQuery external tables</li> <li>Dataflow pipelines</li> <li>Cloud Functions triggers</li> <li>Content delivery (CDN)</li> </ul>"},{"location":"gcp/cloud-storage/overview/#compliance","title":"Compliance","text":"<ul> <li>Data residency controls</li> <li>Retention policies</li> <li>Object holds</li> <li>Audit logging</li> </ul>"},{"location":"gcp/cloud-storage/storage-classes/","title":"Cloud Storage Classes","text":""},{"location":"gcp/cloud-storage/storage-classes/#core-concepts","title":"Core Concepts","text":"<p>Storage classes determine pricing and access characteristics for objects in Cloud Storage. Understanding the economic trade-offs between storage cost, retrieval cost, and minimum storage duration is critical for cost-effective architecture.</p> <p>Key Principle: Lower storage cost = higher retrieval cost + minimum duration penalties. Match storage class to actual access patterns.</p>"},{"location":"gcp/cloud-storage/storage-classes/#storage-class-comparison","title":"Storage Class Comparison","text":"Class Access Frequency Storage Cost Retrieval Cost Min Duration Availability Use Case Standard &gt;1/month Highest None None Highest Hot data Nearline ~1/month Medium Low 30 days High Warm data Coldline ~1/quarter Low Medium 90 days High Cool data Archive &lt;1/year Lowest Highest 365 days High Cold data Autoclass Unknown Automatic Varies None Varies Unpredictable"},{"location":"gcp/cloud-storage/storage-classes/#standard-storage-class","title":"Standard Storage Class","text":""},{"location":"gcp/cloud-storage/storage-classes/#characteristics","title":"Characteristics","text":"<p>Pricing (approximate, varies by location):</p> <ul> <li>Storage: $0.020 per GB/month (multi-region)</li> <li>Retrieval: No charge</li> <li>Operations: Class A ~$0.05/10K, Class B ~$0.004/10K</li> <li>Minimum duration: None</li> <li>Early deletion fee: None</li> </ul> <p>Availability:</p> <ul> <li>Multi-region: 99.95%</li> <li>Dual-region: 99.95%</li> <li>Region: 99.9%</li> </ul>"},{"location":"gcp/cloud-storage/storage-classes/#when-to-use","title":"When to Use","text":"<p>\u2705 Appropriate for:</p> <p>Frequently Accessed Data:</p> <ul> <li>Website content and assets</li> <li>Streaming media actively used</li> <li>Mobile/web application data</li> <li>Analytics data in active use</li> <li>Content being actively processed</li> </ul> <p>Short-Term Storage:</p> <ul> <li>Temporary processing data</li> <li>Build artifacts (recent builds)</li> <li>Staging data for pipelines</li> <li>Cache storage</li> </ul> <p>Performance-Critical:</p> <ul> <li>Low latency required</li> <li>High throughput needed</li> <li>Frequent access (&gt;1/month)</li> <li>No retrieval delay acceptable</li> </ul>"},{"location":"gcp/cloud-storage/storage-classes/#when-not-to-use","title":"When NOT to Use","text":"<p>\u274c Inappropriate for:</p> <ul> <li>Data accessed less than monthly</li> <li>Long-term archives</li> <li>Backup data rarely accessed</li> <li>Historical data for compliance only</li> <li>Aged logs and analytics</li> </ul> <p>Cost Impact: Paying premium storage cost for infrequent access wastes money</p>"},{"location":"gcp/cloud-storage/storage-classes/#nearline-storage-class","title":"Nearline Storage Class","text":""},{"location":"gcp/cloud-storage/storage-classes/#characteristics_1","title":"Characteristics","text":"<p>Pricing:</p> <ul> <li>Storage: $0.010 per GB/month (~50% of Standard)</li> <li>Retrieval: $0.01 per GB retrieved</li> <li>Minimum duration: 30 days</li> <li>Early deletion fee: Charged for remaining days</li> </ul> <p>Availability: Same as Standard</p>"},{"location":"gcp/cloud-storage/storage-classes/#economics","title":"Economics","text":"<p>Break-Even Analysis:</p> <pre><code>When is Nearline cheaper than Standard?\n\nStandard cost: $0.020/GB/month\nNearline storage: $0.010/GB/month\nNearline retrieval: $0.01/GB\n\nNearline is cheaper if:\nAccess frequency &lt; 1 full retrieval per month\n(0.010 storage + 0.01 retrieval) &lt; 0.020\n</code></pre> <p>Example: 1 TB data, accessed 50% monthly</p> <ul> <li>Standard: 1000 \u00d7 $0.020 = $20/month</li> <li>Nearline: (1000 \u00d7 $0.010) + (500 \u00d7 $0.01) = $15/month</li> <li>Savings: $5/month (25%)</li> </ul>"},{"location":"gcp/cloud-storage/storage-classes/#when-to-use_1","title":"When to Use","text":"<p>\u2705 Appropriate for:</p> <p>Monthly Access Patterns:</p> <ul> <li>Monthly reports and analytics</li> <li>Backup data accessed for recovery testing</li> <li>Aged data referenced occasionally</li> <li>Compliance data with periodic review</li> <li>Media assets for seasonal campaigns</li> </ul> <p>Data Retention Requirements:</p> <ul> <li>Regulatory data (30+ day retention)</li> <li>Application logs (older than 30 days)</li> <li>Database backups (recent history)</li> <li>Development artifacts (older versions)</li> </ul>"},{"location":"gcp/cloud-storage/storage-classes/#when-not-to-use_1","title":"When NOT to Use","text":"<p>\u274c Inappropriate for:</p> <p>Short-Term Storage (&lt;30 days):</p> <ul> <li>Temporary processing data</li> <li>Short-lived cache</li> <li>Data deleted within 30 days</li> <li>Rapid turnover data</li> </ul> <p>Why: Early deletion fees negate cost savings</p> <p>Frequent Access (&gt;1/month):</p> <ul> <li>Active application data</li> <li>Frequently accessed logs</li> <li>Primary datasets</li> <li>Live analytics data</li> </ul> <p>Why: Retrieval costs exceed Standard storage cost</p>"},{"location":"gcp/cloud-storage/storage-classes/#early-deletion-considerations","title":"Early Deletion Considerations","text":"<p>Scenario: Delete object after 20 days</p> <ul> <li>Charged for 30 days minimum</li> <li>Remaining 10 days \u00d7 storage rate</li> <li>No savings over Standard for short-term data</li> </ul> <p>Architecture Decision: Only use Nearline if data lives &gt;30 days</p>"},{"location":"gcp/cloud-storage/storage-classes/#coldline-storage-class","title":"Coldline Storage Class","text":""},{"location":"gcp/cloud-storage/storage-classes/#characteristics_2","title":"Characteristics","text":"<p>Pricing:</p> <ul> <li>Storage: $0.004 per GB/month (~20% of Standard)</li> <li>Retrieval: $0.02 per GB retrieved</li> <li>Minimum duration: 90 days</li> <li>Early deletion fee: Charged for remaining days</li> </ul> <p>Availability: Same as Standard</p>"},{"location":"gcp/cloud-storage/storage-classes/#economics_1","title":"Economics","text":"<p>Break-Even Analysis:</p> <pre><code>Coldline is cheaper than Standard if:\nAccess frequency &lt; 0.8 full retrievals per month\n\nColdline is cheaper than Nearline if:\nAccess frequency &lt; 0.3 full retrievals per month\n</code></pre> <p>Example: 1 TB data, accessed 10% quarterly (3.3% monthly)</p> <ul> <li>Standard: $20/month</li> <li>Nearline: $10 + $3.33 = $13.33/month</li> <li>Coldline: $4 + $0.67 = $4.67/month</li> <li>Savings: $15.33/month (77% vs Standard)</li> </ul>"},{"location":"gcp/cloud-storage/storage-classes/#when-to-use_2","title":"When to Use","text":"<p>\u2705 Appropriate for:</p> <p>Quarterly Access:</p> <ul> <li>Quarterly business reports</li> <li>Disaster recovery data (tested quarterly)</li> <li>Audit data (occasional review)</li> <li>Seasonal data archives</li> <li>Aged analytics data</li> </ul> <p>Compliance and Archival:</p> <ul> <li>Regulatory data (90+ day retention)</li> <li>Legal documents (occasional access)</li> <li>Historical records</li> <li>Long-term backups (recent years)</li> </ul> <p>Infrequently Accessed Backups:</p> <ul> <li>Database backups (older than 90 days)</li> <li>VM snapshots (disaster recovery)</li> <li>Application state backups</li> <li>Configuration backups</li> </ul>"},{"location":"gcp/cloud-storage/storage-classes/#when-not-to-use_2","title":"When NOT to Use","text":"<p>\u274c Inappropriate for:</p> <p>Short-Term Storage (&lt;90 days):</p> <ul> <li>Early deletion fees expensive</li> <li>No cost benefit</li> </ul> <p>Frequent Access (&gt;1/quarter):</p> <ul> <li>Retrieval costs accumulate</li> <li>Nearline or Standard cheaper</li> </ul> <p>Performance-Critical:</p> <ul> <li>Though retrieval is fast, cost implies rare access</li> <li>Use Standard for performance needs</li> </ul>"},{"location":"gcp/cloud-storage/storage-classes/#archive-storage-class","title":"Archive Storage Class","text":""},{"location":"gcp/cloud-storage/storage-classes/#characteristics_3","title":"Characteristics","text":"<p>Pricing:</p> <ul> <li>Storage: $0.0012 per GB/month (~6% of Standard)</li> <li>Retrieval: $0.05 per GB retrieved</li> <li>Minimum duration: 365 days</li> <li>Early deletion fee: Charged for remaining days</li> </ul> <p>Availability: Same as Standard (despite name suggesting slower access)</p>"},{"location":"gcp/cloud-storage/storage-classes/#economics_2","title":"Economics","text":"<p>Break-Even Analysis:</p> <pre><code>Archive is cheaper than Standard if:\nAccess frequency &lt; 0.38 full retrievals per month\n\nArchive is cheaper than Coldline if:\nAccess frequency &lt; 0.05 full retrievals per month\n</code></pre> <p>Example: 1 TB data, accessed once per year (8.3% monthly)</p> <ul> <li>Standard: $20/month \u00d7 12 = $240/year</li> <li>Coldline: $4/month \u00d7 12 + $16.6 = $64.6/year</li> <li>Archive: $1.20/month \u00d7 12 + $4.2 = $18.6/year</li> <li>Savings: $221.4/year (92% vs Standard)</li> </ul>"},{"location":"gcp/cloud-storage/storage-classes/#when-to-use_3","title":"When to Use","text":"<p>\u2705 Appropriate for:</p> <p>Annual or Rare Access:</p> <ul> <li>Compliance archives (multi-year retention)</li> <li>Historical data (rarely accessed)</li> <li>Long-term backups (disaster scenarios only)</li> <li>Legal archives</li> <li>Scientific data preservation</li> </ul> <p>Regulatory Requirements:</p> <ul> <li>7-year retention (SOX, HIPAA)</li> <li>Legal hold data</li> <li>Audit trails</li> <li>Medical records</li> </ul> <p>Cold Data:</p> <ul> <li>Data rarely if ever accessed</li> <li>\u201cJust in case\u201d storage</li> <li>Regulatory compliance only</li> <li>No business use but cannot delete</li> </ul>"},{"location":"gcp/cloud-storage/storage-classes/#when-not-to-use_3","title":"When NOT to Use","text":"<p>\u274c Inappropriate for:</p> <p>Any Regular Access:</p> <ul> <li>Monthly, quarterly, even semi-annual access expensive</li> <li>Very high retrieval costs</li> <li>Use Coldline instead</li> </ul> <p>Short-Term Storage (&lt;365 days):</p> <ul> <li>Massive early deletion fees</li> <li>Most expensive option for short-term</li> </ul> <p>Active Archives:</p> <ul> <li>\u201cArchive\u201d doesn\u2019t mean slower access</li> <li>Use based on access frequency, not retrieval speed</li> <li>If accessed regularly, use different class</li> </ul> <p>Common Misconception: \u201cArchive is for backups\u201d</p> <ul> <li>Reality: Archive is for RARELY accessed data</li> <li>Backups may be accessed frequently (use Nearline/Coldline)</li> <li>Archive is for compliance, not operational recovery</li> </ul>"},{"location":"gcp/cloud-storage/storage-classes/#autoclass","title":"Autoclass","text":""},{"location":"gcp/cloud-storage/storage-classes/#characteristics_4","title":"Characteristics","text":"<p>How It Works:</p> <ul> <li>Automatically transitions objects between classes</li> <li>Based on access patterns</li> <li>Starts as Standard</li> <li>Moves to Nearline after 30 days without access</li> <li>Moves to Coldline after 90 days without access</li> <li>Moves to Archive after 365 days without access (if enabled)</li> <li>Moves back to Standard when accessed</li> </ul> <p>Pricing:</p> <ul> <li>Storage cost based on class object is in</li> <li>No retrieval fees for automatic transitions</li> <li>Small management fee (~$0.0025 per 1000 objects/month)</li> <li>No minimum storage duration penalties</li> </ul>"},{"location":"gcp/cloud-storage/storage-classes/#when-to-use_4","title":"When to Use","text":"<p>\u2705 Appropriate for:</p> <p>Unknown Access Patterns:</p> <ul> <li>New data lake</li> <li>Uncertain usage patterns</li> <li>Variable access across objects</li> <li>Mixed workloads</li> </ul> <p>Simplification:</p> <ul> <li>Don\u2019t want to manage lifecycle policies</li> <li>Automated optimization preferred</li> <li>Mixed access patterns in bucket</li> <li>Hands-off cost optimization</li> </ul> <p>Testing and Development:</p> <ul> <li>Unpredictable access</li> <li>Changing requirements</li> <li>Experimental data</li> </ul>"},{"location":"gcp/cloud-storage/storage-classes/#when-not-to-use_4","title":"When NOT to Use","text":"<p>\u274c Inappropriate for:</p> <p>Known Access Patterns:</p> <ul> <li>Predictable usage (set specific class)</li> <li>Consistent access frequency</li> <li>Better cost control with manual selection</li> </ul> <p>Frequent Access:</p> <ul> <li>If all data accessed frequently, use Standard</li> <li>Autoclass overhead not needed</li> </ul> <p>Compliance Requirements:</p> <ul> <li>Need specific storage class guarantees</li> <li>Regulatory requirements for storage type</li> <li>Audit requirements</li> </ul> <p>Cost Optimization:</p> <ul> <li>Autoclass convenient but may not be cheapest</li> <li>Manual lifecycle policies can be more cost-effective</li> <li>Pay for management overhead</li> </ul>"},{"location":"gcp/cloud-storage/storage-classes/#trade-offs","title":"Trade-offs","text":"<p>Benefits:</p> <ul> <li>Automatic optimization</li> <li>No lifecycle policy management</li> <li>Adapts to changing patterns</li> <li>Simple to implement</li> </ul> <p>Drawbacks:</p> <ul> <li>Management fee</li> <li>Less control</li> <li>May not be optimal for all objects</li> <li>Harder to predict costs</li> </ul>"},{"location":"gcp/cloud-storage/storage-classes/#decision-framework","title":"Decision Framework","text":""},{"location":"gcp/cloud-storage/storage-classes/#access-pattern-analysis","title":"Access Pattern Analysis","text":"<p>Questions to Answer:</p> <ol> <li> <p>How often will data be accessed?</p> </li> <li> <p>Daily/weekly \u2192 Standard</p> </li> <li>Monthly \u2192 Nearline</li> <li>Quarterly \u2192 Coldline</li> <li> <p>Yearly/rare \u2192 Archive</p> </li> <li> <p>How long will data be stored?</p> </li> <li> <p>&lt;30 days \u2192 Standard only</p> </li> <li>30-90 days \u2192 Standard or Nearline</li> <li>90-365 days \u2192 Standard, Nearline, or Coldline</li> <li> <p>365 days \u2192 Any class</p> </li> <li> <p>What percentage retrieved when accessed?</p> </li> <li> <p>Full dataset \u2192 Factor full retrieval cost</p> </li> <li> <p>Partial \u2192 Lower effective retrieval cost</p> </li> <li> <p>Is access pattern predictable?</p> </li> <li> <p>Yes \u2192 Choose specific class</p> </li> <li> <p>No \u2192 Consider Autoclass</p> </li> <li> <p>Are there minimum storage requirements?</p> </li> <li> <p>Short-term data \u2192 Beware early deletion fees</p> </li> <li>Long-term data \u2192 All classes viable</li> </ol>"},{"location":"gcp/cloud-storage/storage-classes/#cost-calculation-formula","title":"Cost Calculation Formula","text":"<p>Total Monthly Cost = Storage Cost + Retrieval Cost + Operation Cost</p> <pre><code>Standard:    (GB \u00d7 $0.020) + (0 retrieval) + ops\nNearline:    (GB \u00d7 $0.010) + (GB_retrieved \u00d7 $0.01) + ops\nColdline:    (GB \u00d7 $0.004) + (GB_retrieved \u00d7 $0.02) + ops\nArchive:     (GB \u00d7 $0.0012) + (GB_retrieved \u00d7 $0.05) + ops\n</code></pre> <p>Example: 10 TB stored, 20% accessed monthly</p> <ul> <li>Standard: 10,000 \u00d7 0.020 = $200</li> <li>Nearline: (10,000 \u00d7 0.010) + (2,000 \u00d7 0.01) = $120</li> <li>Coldline: (10,000 \u00d7 0.004) + (2,000 \u00d7 0.02) = $80</li> <li>Archive: (10,000 \u00d7 0.0012) + (2,000 \u00d7 0.05) = $112</li> </ul> <p>Best choice: Coldline ($80/month)</p>"},{"location":"gcp/cloud-storage/storage-classes/#multi-class-bucket-strategy","title":"Multi-Class Bucket Strategy","text":""},{"location":"gcp/cloud-storage/storage-classes/#architectural-pattern","title":"Architectural Pattern","text":"<p>Single Bucket, Multiple Classes:</p> <ul> <li>Objects can have different storage classes in same bucket</li> <li>Bucket has default class for new objects</li> <li>Individual objects can override</li> <li>Lifecycle policies transition between classes</li> </ul> <p>Use Case: Data aging pattern</p> <pre><code>New objects \u2192 Standard (default)\nAfter 30 days \u2192 Nearline (lifecycle policy)\nAfter 90 days \u2192 Coldline (lifecycle policy)\nAfter 365 days \u2192 Archive (lifecycle policy)\nAfter 7 years \u2192 Delete (compliance)\n</code></pre>"},{"location":"gcp/cloud-storage/storage-classes/#common-mistakes","title":"Common Mistakes","text":""},{"location":"gcp/cloud-storage/storage-classes/#mistake-1-using-archive-for-backups","title":"Mistake 1: Using Archive for Backups","text":"<p>Problem: Backups often accessed for testing and recovery</p> <p>Solution: Use Nearline or Coldline based on access frequency</p>"},{"location":"gcp/cloud-storage/storage-classes/#mistake-2-ignoring-early-deletion-fees","title":"Mistake 2: Ignoring Early Deletion Fees","text":"<p>Problem: Delete Nearline object after 20 days, pay for 30 days</p> <p>Solution: Use Standard for short-lived data</p>"},{"location":"gcp/cloud-storage/storage-classes/#mistake-3-overusing-standard","title":"Mistake 3: Overusing Standard","text":"<p>Problem: Paying premium for data accessed monthly</p> <p>Solution: Analyze access patterns, use Nearline for monthly access</p>"},{"location":"gcp/cloud-storage/storage-classes/#mistake-4-underestimating-retrieval-costs","title":"Mistake 4: Underestimating Retrieval Costs","text":"<p>Problem: Archive looks cheap but retrieval costs accumulate</p> <p>Solution: Calculate total cost including retrieval frequency</p>"},{"location":"gcp/cloud-storage/storage-classes/#mistake-5-autoclass-for-everything","title":"Mistake 5: Autoclass for Everything","text":"<p>Problem: Management fees and less control</p> <p>Solution: Use Autoclass only when access patterns unknown</p>"},{"location":"gcp/cloud-storage/storage-classes/#exam-focus-areas","title":"Exam Focus Areas","text":""},{"location":"gcp/cloud-storage/storage-classes/#storage-class-selection","title":"Storage Class Selection","text":"<ul> <li>Economic trade-offs (storage vs retrieval vs minimum duration)</li> <li>Access pattern matching</li> <li>Break-even analysis</li> <li>Early deletion fee scenarios</li> </ul>"},{"location":"gcp/cloud-storage/storage-classes/#cost-optimization","title":"Cost Optimization","text":"<ul> <li>Lifecycle policies to transition classes</li> <li>Appropriate class for use case</li> <li>Total cost calculation (not just storage)</li> <li>Multi-class bucket strategies</li> </ul>"},{"location":"gcp/cloud-storage/storage-classes/#architecture-patterns","title":"Architecture Patterns","text":"<ul> <li>Data aging strategies</li> <li>Backup and archive distinction</li> <li>Compliance requirements</li> <li>Performance vs cost trade-offs</li> </ul>"},{"location":"gcp/cloud-storage/storage-classes/#common-scenarios","title":"Common Scenarios","text":"<ul> <li>Hot/warm/cold data patterns</li> <li>Compliance and retention</li> <li>Disaster recovery storage</li> <li>Data lake storage classes</li> </ul>"},{"location":"gcp/cloud-storage/storage-classes/#decision-making","title":"Decision Making","text":"<ul> <li>When to use each class</li> <li>When NOT to use each class</li> <li>Autoclass vs manual selection</li> <li>Multi-class bucket design</li> </ul>"},{"location":"gcp/compute-engine/backups/","title":"Backup and Disaster Recovery Strategy","text":""},{"location":"gcp/compute-engine/backups/#core-concepts","title":"Core Concepts","text":"<p>A comprehensive backup and disaster recovery strategy minimizes data loss (RPO) and downtime (RTO) through a combination of technologies and procedures. Understanding trade-offs between different approaches is critical for architecture decisions.</p> <p>Key Principle: Design for failure; assume resources will fail and plan accordingly.</p>"},{"location":"gcp/compute-engine/backups/#recovery-objectives","title":"Recovery Objectives","text":""},{"location":"gcp/compute-engine/backups/#rpo-recovery-point-objective","title":"RPO (Recovery Point Objective)","text":"<p>Definition: Maximum acceptable data loss (time between last backup and disaster)</p> <p>Common Targets:</p> <ul> <li>Tier 1 (Critical): RPO &lt; 1 hour</li> <li>Real-time replication or hourly snapshots</li> <li>Regional persistent disks (RPO near-zero)</li> <li> <p>Highest cost</p> </li> <li> <p>Tier 2 (Important): RPO &lt; 4 hours</p> </li> <li>4-hourly snapshots</li> <li> <p>Balanced cost/protection</p> </li> <li> <p>Tier 3 (Normal): RPO &lt; 24 hours</p> </li> <li>Daily snapshots</li> <li> <p>Standard for most workloads</p> </li> <li> <p>Tier 4 (Low Priority): RPO &lt; 7 days</p> </li> <li>Weekly backups</li> <li>Cost-optimized</li> </ul> <p>Architecture Decision: RPO directly impacts backup frequency and cost</p>"},{"location":"gcp/compute-engine/backups/#rto-recovery-time-objective","title":"RTO (Recovery Time Objective)","text":"<p>Definition: Maximum acceptable downtime (time to restore and resume operations)</p> <p>Common Targets:</p> <ul> <li>Tier 1 (Critical): RTO &lt; 1 hour</li> <li>Regional persistent disks with automatic failover</li> <li>Active-active architecture</li> <li> <p>Highest cost</p> </li> <li> <p>Tier 2 (Important): RTO &lt; 4 hours</p> </li> <li>Hot standby in DR region</li> <li> <p>Automated restore procedures</p> </li> <li> <p>Tier 3 (Normal): RTO &lt; 24 hours</p> </li> <li>Warm standby or documented procedures</li> <li> <p>Standard for most workloads</p> </li> <li> <p>Tier 4 (Low Priority): RTO &lt; 48 hours</p> </li> <li>Cold standby or manual procedures</li> <li>Cost-optimized</li> </ul> <p>Architecture Decision: RTO determines DR strategy (active-active, hot/warm/cold standby)</p>"},{"location":"gcp/compute-engine/backups/#backup-strategies-comparison","title":"Backup Strategies Comparison","text":""},{"location":"gcp/compute-engine/backups/#strategy-1-snapshot-only","title":"Strategy 1: Snapshot-Only","text":"<p>Architecture:</p> <ul> <li>Daily disk snapshots</li> <li>Automated snapshot schedules</li> <li>Retention based on requirements</li> <li>Multi-regional storage for DR</li> </ul> <p>Characteristics:</p> <ul> <li>RPO: 24 hours (daily snapshots)</li> <li>RTO: 30-60 minutes (disk restore + VM recreation)</li> <li>Cost: Low (incremental storage)</li> <li>Complexity: Low</li> </ul> <p>Appropriate for:</p> <ul> <li>Standard workloads (Tier 3)</li> <li>Cost-sensitive environments</li> <li>Predictable, documented recovery procedures</li> <li>Acceptable 24-hour data loss</li> </ul> <p>Limitations:</p> <ul> <li>VM configuration not captured</li> <li>Manual recreation required</li> <li>24-hour RPO minimum (daily snapshots)</li> </ul>"},{"location":"gcp/compute-engine/backups/#strategy-2-machine-image-only","title":"Strategy 2: Machine Image-Only","text":"<p>Architecture:</p> <ul> <li>Weekly complete VM backups</li> <li>All disks + configuration</li> <li>Multi-regional storage</li> </ul> <p>Characteristics:</p> <ul> <li>RPO: 7 days (weekly images)</li> <li>RTO: 15-30 minutes (fast VM restore)</li> <li>Cost: High (full copies)</li> <li>Complexity: Low</li> </ul> <p>Appropriate for:</p> <ul> <li>Infrequent changes</li> <li>Configuration preservation critical</li> <li>Fast recovery more important than RPO</li> <li>Pre-maintenance backups</li> </ul> <p>Limitations:</p> <ul> <li>Expensive for frequent backups</li> <li>7-day RPO not acceptable for most production</li> <li>Large storage footprint</li> </ul>"},{"location":"gcp/compute-engine/backups/#strategy-3-hybrid-approach-recommended","title":"Strategy 3: Hybrid Approach (Recommended)","text":"<p>Architecture:</p> <ul> <li>Daily snapshots (data protection, 24-hour RPO)</li> <li>Weekly machine images (full VM backup)</li> <li>Combines best of both</li> </ul> <p>Characteristics:</p> <ul> <li>RPO: 24 hours (daily snapshots)</li> <li>RTO: 30-45 minutes</li> <li>Cost: Medium (balanced)</li> <li>Complexity: Medium</li> </ul> <p>Appropriate for:</p> <ul> <li>Most production workloads</li> <li>Balance of cost and protection</li> <li>Flexible recovery options</li> <li>Standard enterprise practice</li> </ul> <p>Pattern: </p> <ul> <li>Snapshots for operational recovery (fast, frequent)</li> <li>Machine images for disaster recovery (complete, infrequent)</li> </ul>"},{"location":"gcp/compute-engine/backups/#strategy-4-regional-persistent-disks","title":"Strategy 4: Regional Persistent Disks","text":"<p>Architecture:</p> <ul> <li>Synchronous replication across zones</li> <li>Automatic failover</li> <li>No backup needed for zone failures</li> </ul> <p>Characteristics:</p> <ul> <li>RPO: Near-zero (synchronous replication)</li> <li>RTO: Minutes (automatic failover with MIG)</li> <li>Cost: High (2x disk cost)</li> <li>Complexity: Medium</li> </ul> <p>Appropriate for:</p> <ul> <li>Critical databases (Tier 1)</li> <li>Zero data loss requirement</li> <li>Zone failure protection</li> <li>HA applications</li> </ul> <p>Limitations:</p> <ul> <li>2x storage cost</li> <li>Regional only (not multi-region)</li> <li>Still need snapshots for data corruption/deletion</li> <li>Slight latency increase (cross-zone sync)</li> </ul> <p>Important: Regional disks protect against zone failure, not against data corruption or deletion - still need snapshots</p>"},{"location":"gcp/compute-engine/backups/#disaster-recovery-patterns","title":"Disaster Recovery Patterns","text":""},{"location":"gcp/compute-engine/backups/#dr-strategy-1-backup-and-restore-lowest-cost","title":"DR Strategy 1: Backup and Restore (Lowest Cost)","text":"<p>Architecture:</p> <ul> <li>Production in primary region</li> <li>Snapshots/images in multi-regional storage</li> <li>Restore on-demand in DR region</li> <li>Network pre-configured but inactive</li> </ul> <p>Characteristics:</p> <ul> <li>RTO: 2-4 hours</li> <li>RPO: 24 hours (daily snapshots)</li> <li>Cost: Lowest (pay for storage only)</li> <li>Complexity: Low</li> </ul> <p>Appropriate for:</p> <ul> <li>Tier 3-4 workloads</li> <li>Cost-sensitive environments</li> <li>Acceptable multi-hour outage</li> <li>Regional disaster only scenario</li> </ul>"},{"location":"gcp/compute-engine/backups/#dr-strategy-2-pilot-light","title":"DR Strategy 2: Pilot Light","text":"<p>Architecture:</p> <ul> <li>Minimal infrastructure in DR region (always running)</li> <li>Core components ready (networking, databases at small scale)</li> <li>Scale up on failover</li> <li>Regular snapshots for data sync</li> </ul> <p>Characteristics:</p> <ul> <li>RTO: 1-2 hours</li> <li>RPO: 4-24 hours</li> <li>Cost: Low-Medium</li> <li>Complexity: Medium</li> </ul> <p>Appropriate for:</p> <ul> <li>Tier 2 workloads</li> <li>Balance cost and recovery time</li> <li>Predictable scale-up process</li> <li>Partial availability acceptable during failover</li> </ul>"},{"location":"gcp/compute-engine/backups/#dr-strategy-3-warm-standby","title":"DR Strategy 3: Warm Standby","text":"<p>Architecture:</p> <ul> <li>Reduced capacity in DR region (always running)</li> <li>Database replication active</li> <li>Can handle reduced load immediately</li> <li>Scale up for full capacity</li> </ul> <p>Characteristics:</p> <ul> <li>RTO: 30-60 minutes</li> <li>RPO: 1-4 hours (replication lag)</li> <li>Cost: Medium-High</li> <li>Complexity: Medium-High</li> </ul> <p>Appropriate for:</p> <ul> <li>Tier 1-2 workloads</li> <li>Need fast recovery</li> <li>Can operate at reduced capacity</li> <li>Critical business applications</li> </ul>"},{"location":"gcp/compute-engine/backups/#dr-strategy-4-hot-standby-active-active","title":"DR Strategy 4: Hot Standby / Active-Active","text":"<p>Architecture:</p> <ul> <li>Full capacity in multiple regions</li> <li>Active-active or active-passive</li> <li>Real-time data replication</li> <li>Global load balancing</li> </ul> <p>Characteristics:</p> <ul> <li>RTO: &lt; 5 minutes (automatic failover)</li> <li>RPO: Near-zero (real-time replication)</li> <li>Cost: Highest (2x infrastructure)</li> <li>Complexity: Highest</li> </ul> <p>Appropriate for:</p> <ul> <li>Tier 1 workloads only</li> <li>No tolerance for downtime</li> <li>Global services</li> <li>High availability requirement</li> </ul> <p>Considerations:</p> <ul> <li>Data consistency challenges</li> <li>Application must handle multi-region</li> <li>Complex failover procedures</li> <li>Significant cost</li> </ul>"},{"location":"gcp/compute-engine/backups/#3-2-1-backup-rule","title":"3-2-1 Backup Rule","text":""},{"location":"gcp/compute-engine/backups/#rule-definition","title":"Rule Definition","text":"<p>3 Copies: Original data + 2 backups 2 Media Types: Different storage types 1 Off-Site: Geographic separation</p>"},{"location":"gcp/compute-engine/backups/#gcp-implementation","title":"GCP Implementation","text":"<p>3 Copies:</p> <ol> <li>Original: Data on persistent disk</li> <li>Backup 1: Regional snapshots (same region)</li> <li>Backup 2: Multi-regional machine images</li> </ol> <p>2 Media Types:</p> <ol> <li>Persistent disk (block storage)</li> <li>Snapshots in Cloud Storage (object storage)</li> </ol> <p>1 Off-Site:</p> <ul> <li>Multi-regional snapshot storage</li> <li>Cross-region machine images</li> <li>Geographic redundancy</li> </ul> <p>Architecture Pattern: Standard for critical data (Tier 1-2)</p>"},{"location":"gcp/compute-engine/backups/#backup-testing","title":"Backup Testing","text":""},{"location":"gcp/compute-engine/backups/#regular-dr-drills","title":"Regular DR Drills","text":"<p>Frequency:</p> <ul> <li>Critical systems (Tier 1): Monthly</li> <li>Important systems (Tier 2): Quarterly</li> <li>Normal systems (Tier 3): Semi-annually</li> </ul> <p>Test Scope:</p> <ul> <li>Complete restoration procedure</li> <li>Network connectivity verification</li> <li>Application functionality testing</li> <li>Performance validation</li> <li>Documentation update</li> </ul> <p>Automation:</p> <ul> <li>Scripted restoration</li> <li>Automated testing</li> <li>Monitoring and alerting</li> <li>Regular execution (CI/CD)</li> </ul>"},{"location":"gcp/compute-engine/backups/#validation-strategy","title":"Validation Strategy","text":"<p>Verification Points:</p> <ul> <li>Snapshot creation success</li> <li>Backup completeness</li> <li>Restoration time (actual RTO)</li> <li>Data integrity</li> <li>Application functionality</li> </ul> <p>Documentation:</p> <ul> <li>Runbooks for each scenario</li> <li>Contact information</li> <li>Escalation procedures</li> <li>Last test date and results</li> </ul>"},{"location":"gcp/compute-engine/backups/#cost-optimization","title":"Cost Optimization","text":""},{"location":"gcp/compute-engine/backups/#tiered-backup-strategy","title":"Tiered Backup Strategy","text":"<p>Pattern:</p> Tier Snapshot Frequency Retention Machine Image Total Cost Tier 1 Hourly 7 days Weekly, 30 days Highest Tier 2 Daily 30 days Weekly, 30 days High Tier 3 Daily 7 days Monthly, 90 days Medium Tier 4 Weekly 30 days Quarterly, 1 year Low <p>Architecture Decision: Match backup cost to data criticality</p>"},{"location":"gcp/compute-engine/backups/#storage-location-optimization","title":"Storage Location Optimization","text":"<p>Regional Snapshots:</p> <ul> <li>Use for non-critical data</li> <li>Same-region restore (free egress)</li> <li>Lower recovery time</li> <li>Single-region risk</li> </ul> <p>Multi-Regional Snapshots:</p> <ul> <li>Use for critical data</li> <li>DR capability</li> <li>Cross-region restore (egress charges)</li> <li>Geographic redundancy</li> </ul>"},{"location":"gcp/compute-engine/backups/#disaster-scenarios-and-recovery","title":"Disaster Scenarios and Recovery","text":""},{"location":"gcp/compute-engine/backups/#scenario-1-data-corruption","title":"Scenario 1: Data Corruption","text":"<p>Recovery Approach:</p> <ul> <li>Identify corruption time</li> <li>Restore from nearest snapshot before corruption</li> <li>Create new disk from snapshot</li> <li>Attach to VM or create new VM</li> <li>Verify data integrity</li> </ul> <p>RTO: 30-60 minutes Best Prevention: Frequent snapshots, application-level logging</p>"},{"location":"gcp/compute-engine/backups/#scenario-2-accidental-deletion","title":"Scenario 2: Accidental Deletion","text":"<p>Recovery Approach:</p> <ul> <li>Restore from machine image (complete VM)</li> <li>Or restore disk from snapshot + recreate VM</li> <li>Update external dependencies (DNS, load balancer)</li> <li>Verify functionality</li> </ul> <p>RTO: 30-60 minutes Best Prevention: Deletion protection, IAM controls, change management</p>"},{"location":"gcp/compute-engine/backups/#scenario-3-zone-failure","title":"Scenario 3: Zone Failure","text":"<p>Recovery Approach:</p> <ul> <li>Regional persistent disks: Automatic failover (minutes)</li> <li>Zonal disks: Restore from snapshot in different zone</li> <li>MIG autohealing creates new instances</li> <li>Load balancer reroutes traffic</li> </ul> <p>RTO: 5-30 minutes (depending on architecture) Best Prevention: Regional MIG, regional persistent disks, multi-zone architecture</p>"},{"location":"gcp/compute-engine/backups/#scenario-4-regional-disaster","title":"Scenario 4: Regional Disaster","text":"<p>Recovery Approach:</p> <ol> <li>Declare disaster</li> <li>Restore snapshots/images in DR region</li> <li>Update global load balancer</li> <li>Update DNS (if needed)</li> <li>Scale infrastructure</li> <li>Verify functionality</li> <li>Monitor and adjust</li> </ol> <p>RTO: 1-4 hours (depending on DR strategy) Best Prevention: Multi-region architecture, regular DR testing</p>"},{"location":"gcp/compute-engine/backups/#compliance-and-governance","title":"Compliance and Governance","text":""},{"location":"gcp/compute-engine/backups/#retention-requirements","title":"Retention Requirements","text":"<p>Regulatory Standards:</p> <ul> <li>HIPAA: 6 years minimum</li> <li>SOX: 7 years minimum</li> <li>GDPR: As per data processing agreement</li> <li>PCI-DSS: 3 months minimum, 1 year recommended</li> </ul> <p>Implementation:</p> <ul> <li>Snapshot schedules with appropriate retention</li> <li>Separate schedules for compliance backups</li> <li>Automated lifecycle management</li> <li>Regular audit</li> </ul>"},{"location":"gcp/compute-engine/backups/#audit-and-monitoring","title":"Audit and Monitoring","text":"<p>Audit Logs:</p> <ul> <li>Snapshot creation/deletion</li> <li>Machine image operations</li> <li>Recovery procedures</li> <li>Access to backups</li> </ul> <p>Monitoring:</p> <ul> <li>Backup success/failure rates</li> <li>Time to create backups</li> <li>Storage costs</li> <li>Age of last successful backup</li> <li>RTO/RPO compliance</li> </ul>"},{"location":"gcp/compute-engine/backups/#architecture-decision-framework","title":"Architecture Decision Framework","text":""},{"location":"gcp/compute-engine/backups/#questions-to-answer","title":"Questions to Answer","text":"<ol> <li>What is acceptable data loss? (Determines RPO, backup frequency)</li> <li>What is acceptable downtime? (Determines RTO, DR strategy)</li> <li>What is the budget? (Constrains solutions)</li> <li>What are compliance requirements? (Minimum retention, encryption)</li> <li>What is recovery complexity tolerance? (Affects automation needs)</li> <li>Is multi-region DR needed? (Geographic redundancy)</li> <li>What is change frequency? (Affects snapshot efficiency)</li> </ol>"},{"location":"gcp/compute-engine/backups/#decision-matrix","title":"Decision Matrix","text":"RPO Requirement RTO Requirement Recommended Strategy Cost Level &lt; 1 hour &lt; 1 hour Regional disks + Active-active Very High &lt; 4 hours &lt; 1 hour Hourly snapshots + Hot standby High &lt; 24 hours &lt; 4 hours Daily snapshots + Warm standby Medium-High &lt; 24 hours &lt; 24 hours Daily snapshots + Pilot light Medium &lt; 7 days &lt; 48 hours Weekly images + Backup/restore Low"},{"location":"gcp/compute-engine/backups/#exam-focus-areas","title":"Exam Focus Areas","text":""},{"location":"gcp/compute-engine/backups/#strategy-selection","title":"Strategy Selection","text":"<ul> <li>RPO/RTO requirements and implications</li> <li>Backup strategy trade-offs (cost, complexity, protection)</li> <li>DR pattern selection criteria</li> <li>Multi-region considerations</li> </ul>"},{"location":"gcp/compute-engine/backups/#cost-optimization_1","title":"Cost Optimization","text":"<ul> <li>Tiered backup strategies</li> <li>Snapshot vs machine image economics</li> <li>Storage location decisions</li> <li>Retention policy optimization</li> </ul>"},{"location":"gcp/compute-engine/backups/#architecture-patterns","title":"Architecture Patterns","text":"<ul> <li>3-2-1 backup rule implementation</li> <li>Hybrid backup approaches</li> <li>Regional disk HA patterns</li> <li>Multi-region DR architectures</li> </ul>"},{"location":"gcp/compute-engine/backups/#compliance","title":"Compliance","text":"<ul> <li>Retention requirements</li> <li>Encryption and security</li> <li>Audit logging</li> <li>Testing requirements</li> </ul>"},{"location":"gcp/compute-engine/backups/#operations","title":"Operations","text":"<ul> <li>Automation strategies</li> <li>Testing procedures</li> <li>Monitoring and alerting</li> <li>Documentation requirements</li> </ul>"},{"location":"gcp/compute-engine/disks/","title":"Persistent Disks","text":""},{"location":"gcp/compute-engine/disks/#core-concepts","title":"Core Concepts","text":"<p>Persistent Disks are durable, network-attached block storage independent of VM lifecycle. Understanding disk types and their performance characteristics is crucial for designing performant and cost-effective solutions.</p> <p>Key Principle: Performance scales with disk size; separate boot and data disks for flexibility.</p>"},{"location":"gcp/compute-engine/disks/#disk-types-comparison","title":"Disk Types Comparison","text":"Type IOPS/GB Max IOPS Throughput Use Case Cost pd-standard 0.75-1.5 7,500-15,000 Low Archives, backups Lowest pd-balanced 6 80,000 28 MB/s/GB General purpose Medium pd-ssd 30 100,000 48 MB/s/GB Databases, high I/O High pd-extreme Custom 120,000 2,400 MB/s Mission-critical Highest hyperdisk-balanced Configurable 160,000 Dynamic Next-gen balanced Medium-High hyperdisk-extreme Configurable 350,000 Dynamic Highest performance Highest Local SSD N/A 2,400,000 9,360 MB/s Ultra-high perf Medium (ephemeral)"},{"location":"gcp/compute-engine/disks/#architectural-decision-criteria","title":"Architectural Decision Criteria","text":""},{"location":"gcp/compute-engine/disks/#pd-standard-hdd","title":"pd-standard (HDD)","text":"<p>Appropriate for:</p> <ul> <li>Sequential access patterns (logs, media files)</li> <li>Large datasets with infrequent access</li> <li>Backup target storage</li> <li>Cost-sensitive workloads</li> <li>Data archiving</li> </ul> <p>Not appropriate for:</p> <ul> <li>Database storage</li> <li>Random I/O workloads</li> <li>Low-latency requirements</li> <li>Applications needing &gt;15,000 IOPS</li> </ul>"},{"location":"gcp/compute-engine/disks/#pd-balanced-ssd-recommended-default","title":"pd-balanced (SSD) - Recommended Default","text":"<p>Appropriate for:</p> <ul> <li>General-purpose workloads (should be default choice)</li> <li>Small to medium databases</li> <li>Boot disks</li> <li>Most enterprise applications</li> <li>Development environments</li> <li>Balance of price and performance</li> </ul> <p>Architecture Pattern: Start with pd-balanced, upgrade to pd-ssd only if I/O bottleneck identified</p>"},{"location":"gcp/compute-engine/disks/#pd-ssd","title":"pd-ssd","text":"<p>Appropriate for:</p> <ul> <li>High-performance databases</li> <li>I/O-intensive applications</li> <li>OLTP workloads</li> <li>Applications requiring low latency</li> <li>Random I/O patterns</li> </ul> <p>Cost Consideration: 2.5x cost of pd-balanced; verify I/O requirements justify cost</p>"},{"location":"gcp/compute-engine/disks/#pd-extreme","title":"pd-extreme","text":"<p>Appropriate for:</p> <ul> <li>Mission-critical databases (SAP HANA, Oracle)</li> <li>Very large databases requiring &gt;100,000 IOPS</li> <li>Real-time analytics</li> <li>High-frequency trading</li> <li>When performance is more important than cost</li> </ul> <p>Limitations:</p> <ul> <li>Requires N2, C2, or M-series VMs</li> <li>Minimum size: 500 GB</li> <li>Much higher cost than pd-ssd</li> <li>Custom IOPS configuration required</li> </ul>"},{"location":"gcp/compute-engine/disks/#local-ssds","title":"Local SSDs","text":"<p>Characteristics:</p> <ul> <li>Physically attached to server</li> <li>Ephemeral (data lost on VM stop/delete)</li> <li>Ultra-high performance</li> <li>375 GB per device, up to 24 devices (9 TB max)</li> <li>No persistent disk performance limits apply</li> </ul> <p>Appropriate for:</p> <ul> <li>Temporary cache</li> <li>Scratch space for computation</li> <li>Data that can be rebuilt (replicated databases)</li> <li>High-performance computing</li> <li>When highest IOPS/throughput needed</li> </ul> <p>Not appropriate for:</p> <ul> <li>Primary database storage without replication</li> <li>Data that must survive VM stop</li> <li>Critical data without backups</li> <li>Single point of failure scenarios</li> </ul> <p>Architecture Pattern: Use for performance, replicate critical data externally</p>"},{"location":"gcp/compute-engine/disks/#performance-scaling","title":"Performance Scaling","text":""},{"location":"gcp/compute-engine/disks/#size-based-performance","title":"Size-Based Performance","text":"<p>pd-balanced Example:</p> <ul> <li>100 GB: 600 read IOPS, 600 write IOPS</li> <li>500 GB: 3,000 IOPS</li> <li>13,334 GB: 80,000 IOPS (maximum)</li> </ul> <p>Design Implication: May need larger disk for performance, not just capacity</p>"},{"location":"gcp/compute-engine/disks/#vm-level-limits","title":"VM-Level Limits","text":"<p>Performance is limited by:</p> <ol> <li>Disk type and size</li> <li>VM machine type (CPU count)</li> <li>Number of vCPUs determines max I/O</li> </ol> <p>Example:</p> <ul> <li>n2-standard-2 (2 vCPUs): Max 15,000 read IOPS</li> <li>n2-standard-32 (32 vCPUs): Max 100,000 read IOPS</li> </ul> <p>Architecture Consideration: Right-size VM for I/O requirements, not just compute</p>"},{"location":"gcp/compute-engine/disks/#regional-persistent-disks","title":"Regional Persistent Disks","text":""},{"location":"gcp/compute-engine/disks/#synchronous-replication","title":"Synchronous Replication","text":"<p>Characteristics:</p> <ul> <li>Replicated across two zones in same region</li> <li>Synchronous writes (both zones acknowledge)</li> <li>Automatic failover on zone failure</li> <li>RPO: Near-zero (synchronous)</li> <li>RTO: Minutes (automatic failover)</li> <li>2x cost of zonal disks</li> </ul> <p>Appropriate for:</p> <ul> <li>High-availability databases</li> <li>Mission-critical applications</li> <li>Zone failure protection required</li> <li>Applications needing automatic failover</li> <li>When cost of 2x storage justified by availability</li> </ul> <p>Not appropriate for:</p> <ul> <li>Cost-sensitive workloads</li> <li>Single-zone deployments</li> <li>Applications with external replication</li> <li>Development/testing</li> </ul> <p>Architecture Pattern: Use for stateful tier in multi-zone deployment</p>"},{"location":"gcp/compute-engine/disks/#failover-behavior","title":"Failover Behavior","text":"<p>Automatic Failover:</p> <ul> <li>Regional disk automatically attaches to VM in surviving zone</li> <li>Application must handle brief I/O pause</li> <li>No data loss (synchronous replication)</li> <li>Requires regional MIG for automatic VM recreation</li> </ul> <p>Limitations:</p> <ul> <li>Both zones must be available for writes</li> <li>Slightly higher latency than zonal (cross-zone sync)</li> <li>Cannot span regions</li> <li>Requires zone failure, not VM failure (use MIG autohealing for VM failures)</li> </ul>"},{"location":"gcp/compute-engine/disks/#disk-architecture-patterns","title":"Disk Architecture Patterns","text":""},{"location":"gcp/compute-engine/disks/#separate-boot-and-data-disks","title":"Separate Boot and Data Disks","text":"<p>Benefits:</p> <ul> <li>Independent lifecycle management</li> <li>Different disk types (balanced boot, SSD data)</li> <li>Easier backups (snapshot data disk only)</li> <li>Flexibility to resize independently</li> <li>Replace boot disk without data loss</li> </ul> <p>Pattern:</p> <ul> <li>Boot disk: 20-50 GB pd-balanced</li> <li>Data disk(s): Size and type based on workload</li> <li>Attach data disks to multiple VMs (read-only)</li> </ul>"},{"location":"gcp/compute-engine/disks/#multi-disk-for-performance","title":"Multi-Disk for Performance","text":"<p>Stripe Multiple Disks:</p> <ul> <li>Combine multiple disks in RAID 0</li> <li>Aggregate IOPS and throughput</li> <li>Each disk contributes performance</li> <li>Better than single large disk for max performance</li> </ul> <p>Consideration: Local SSDs better choice for highest performance</p>"},{"location":"gcp/compute-engine/disks/#disk-quotas-and-limits","title":"Disk Quotas and Limits","text":"<p>Per VM Limits:</p> <ul> <li>128 persistent disks (including boot)</li> <li>257 TB total persistent disk size</li> <li>24 Local SSD devices (9 TB)</li> </ul> <p>Architecture Impact:</p> <ul> <li>Plan data architecture within limits</li> <li>Consider object storage (Cloud Storage) for &gt; 128 data sources</li> <li>Use Filestore for NFS requirements</li> </ul>"},{"location":"gcp/compute-engine/disks/#snapshot-architecture","title":"Snapshot Architecture","text":""},{"location":"gcp/compute-engine/disks/#incremental-backups","title":"Incremental Backups","text":"<p>How It Works:</p> <ul> <li>First snapshot: Full copy of disk</li> <li>Subsequent snapshots: Only changed blocks</li> <li>Snapshot chains maintained automatically</li> <li>Deleting intermediate snapshots safe (blocks preserved if needed)</li> </ul> <p>Cost Efficiency:</p> <ul> <li>Daily snapshots of 1 TB disk with 5% daily change</li> <li>Month 1: ~1,000 GB + 30 \u00d7 50 GB = ~2,500 GB storage</li> <li>Much cheaper than 30 full copies (30,000 GB)</li> </ul>"},{"location":"gcp/compute-engine/disks/#snapshot-storage-locations","title":"Snapshot Storage Locations","text":"<p>Regional:</p> <ul> <li>Stored in single region</li> <li>Lower cost</li> <li>Faster creation/restore (same region)</li> <li>Use for: Non-critical data, cost optimization</li> </ul> <p>Multi-Regional:</p> <ul> <li>Stored across multiple regions</li> <li>Higher cost</li> <li>Disaster recovery protection</li> <li>Slower initial creation</li> <li>Use for: Critical data, DR requirements</li> </ul> <p>Architecture Decision: Balance cost vs DR requirements</p>"},{"location":"gcp/compute-engine/disks/#encryption","title":"Encryption","text":""},{"location":"gcp/compute-engine/disks/#default-encryption","title":"Default Encryption","text":"<p>Google-Managed Keys:</p> <ul> <li>All persistent disks encrypted at rest</li> <li>Automatic, no configuration</li> <li>No performance impact</li> <li>Transparent to applications</li> </ul>"},{"location":"gcp/compute-engine/disks/#customer-managed-encryption-keys-cmek","title":"Customer-Managed Encryption Keys (CMEK)","text":"<p>Benefits:</p> <ul> <li>Control key lifecycle</li> <li>Regulatory compliance</li> <li>Audit key usage</li> <li>Revoke access immediately</li> </ul> <p>Considerations:</p> <ul> <li>Additional Cloud KMS costs</li> <li>Key management responsibility</li> <li>Availability dependency on Cloud KMS</li> <li>Slight performance impact</li> </ul> <p>Use Cases:</p> <ul> <li>Regulatory requirements (HIPAA, PCI-DSS)</li> <li>Enhanced security posture</li> <li>Key rotation policies</li> <li>Multi-cloud key management</li> </ul>"},{"location":"gcp/compute-engine/disks/#cost-optimization-strategies","title":"Cost Optimization Strategies","text":""},{"location":"gcp/compute-engine/disks/#right-sizing","title":"Right-Sizing","text":"<p>Approach:</p> <ul> <li>Start with pd-balanced (cost-effective default)</li> <li>Monitor I/O metrics</li> <li>Upgrade to pd-ssd only if bottleneck identified</li> <li>Downgrade to pd-standard for sequential-only workloads</li> </ul>"},{"location":"gcp/compute-engine/disks/#snapshot-management","title":"Snapshot Management","text":"<p>Best Practices:</p> <ul> <li>Implement retention policies (delete old snapshots)</li> <li>Use snapshot schedules (automated lifecycle)</li> <li>Regional storage for non-critical data</li> <li>Delete disk but keep snapshots (cheaper long-term storage)</li> </ul>"},{"location":"gcp/compute-engine/disks/#storage-tiering","title":"Storage Tiering","text":"<p>Pattern:</p> <ul> <li>Hot data: pd-ssd (frequent access)</li> <li>Warm data: pd-balanced (occasional access)</li> <li>Cold data: pd-standard or Cloud Storage (rare access)</li> <li>Archive: Cloud Storage Archive class (long-term retention)</li> </ul>"},{"location":"gcp/compute-engine/disks/#disaster-recovery-considerations","title":"Disaster Recovery Considerations","text":""},{"location":"gcp/compute-engine/disks/#backup-strategy","title":"Backup Strategy","text":"<p>Snapshot Frequency:</p> <ul> <li>Critical data: Hourly or continuous (regional disks)</li> <li>Important data: Daily</li> <li>Normal data: Weekly</li> <li>Test data: Monthly or manual</li> </ul> <p>Retention:</p> <ul> <li>Compliance requirements dictate minimum</li> <li>Balance cost vs recovery point options</li> <li>30 days common for production</li> <li>7 days for development</li> </ul>"},{"location":"gcp/compute-engine/disks/#cross-region-dr","title":"Cross-Region DR","text":"<p>Approach:</p> <ul> <li>Snapshots stored in multi-regional location</li> <li>Restore disks in DR region from snapshots</li> <li>Test DR procedures regularly</li> <li>Document recovery procedures</li> </ul> <p>RTO Considerations:</p> <ul> <li>Snapshot restore: 30-60 minutes</li> <li>Regional disk failover: Minutes</li> <li>Multi-region failover: 1-2 hours (manual)</li> </ul>"},{"location":"gcp/compute-engine/disks/#exam-focus-areas","title":"Exam Focus Areas","text":""},{"location":"gcp/compute-engine/disks/#design-decisions","title":"Design Decisions","text":"<ul> <li>Disk type selection based on workload</li> <li>Performance sizing (IOPS/throughput requirements)</li> <li>Regional vs zonal disks for HA</li> <li>Cost optimization strategies</li> </ul>"},{"location":"gcp/compute-engine/disks/#architecture-patterns","title":"Architecture Patterns","text":"<ul> <li>Separate boot and data disks</li> <li>Local SSD use cases and limitations</li> <li>Multi-disk configurations</li> <li>Snapshot strategies</li> </ul>"},{"location":"gcp/compute-engine/disks/#performance","title":"Performance","text":"<ul> <li>Size-based performance scaling</li> <li>VM-level performance limits</li> <li>When to use Local SSDs</li> <li>Multi-disk striping considerations</li> </ul>"},{"location":"gcp/compute-engine/disks/#disaster-recovery","title":"Disaster Recovery","text":"<ul> <li>Snapshot frequency and retention</li> <li>Regional disk RPO/RTO</li> <li>Cross-region DR strategies</li> <li>Backup architecture patterns</li> </ul>"},{"location":"gcp/compute-engine/disks/#cost-management","title":"Cost Management","text":"<ul> <li>Disk type cost comparison</li> <li>Snapshot storage optimization</li> <li>Unused disk identification</li> <li>Storage tiering strategies</li> </ul>"},{"location":"gcp/compute-engine/images/","title":"Machine Images and Disk Images","text":""},{"location":"gcp/compute-engine/images/#core-concepts","title":"Core Concepts","text":"<p>Images capture VM or disk state for replication, backup, and standardization. Understanding the differences between machine images, disk images, and snapshots is critical for architecture decisions.</p>"},{"location":"gcp/compute-engine/images/#machine-images-vs-disk-images-vs-snapshots","title":"Machine Images vs Disk Images vs Snapshots","text":"Feature Machine Image Disk Image Snapshot Scope Entire VM Single disk Single disk Configuration Yes (all settings) No No Multiple Disks Yes (all attached) No No Storage Method Full copy Full copy Incremental Cost Highest Medium Lowest (incremental) Creation Time Slowest Medium Fast (after first) Use Case VM cloning, DR OS templates Regular backups Frequency Weekly/monthly One-time Daily/hourly"},{"location":"gcp/compute-engine/images/#machine-images","title":"Machine Images","text":""},{"location":"gcp/compute-engine/images/#architecture-purpose","title":"Architecture Purpose","text":"<p>Complete VM Backup:</p> <ul> <li>All disk data preserved</li> <li>VM configuration captured (machine type, labels, network settings)</li> <li>Metadata and startup scripts included</li> <li>Service account configuration retained</li> </ul> <p>When to Use:</p> <ul> <li>Complete VM backup and restore</li> <li>VM migration between projects/regions</li> <li>Disaster recovery (weekly/monthly full backups)</li> <li>Creating identical VMs across environments</li> <li>Compliance snapshots (full system state)</li> </ul> <p>When Not to Use:</p> <ul> <li>Regular incremental backups (use snapshots instead)</li> <li>Single disk backup (use snapshots)</li> <li>OS templates (use disk images)</li> <li>Frequent backups (too expensive)</li> </ul>"},{"location":"gcp/compute-engine/images/#design-patterns","title":"Design Patterns","text":"<p>DR Strategy:</p> <ul> <li>Weekly machine images for complete recovery</li> <li>Daily snapshots for incremental backups</li> <li>Store in multi-regional location</li> <li>Test restoration quarterly</li> </ul> <p>VM Migration:</p> <ul> <li>Create machine image in source project</li> <li>Share with target project via IAM</li> <li>Restore in target project/region</li> <li>Update network and external integrations</li> </ul> <p>Environment Replication:</p> <ul> <li>Production machine image</li> <li>Restore as staging environment</li> <li>Modify labels and metadata</li> <li>Separate network configuration</li> </ul>"},{"location":"gcp/compute-engine/images/#disk-images","title":"Disk Images","text":""},{"location":"gcp/compute-engine/images/#architecture-purpose_1","title":"Architecture Purpose","text":"<p>OS and Software Templates:</p> <ul> <li>Base operating system</li> <li>Pre-installed software</li> <li>Security hardening applied</li> <li>Configuration management tools</li> <li>Company standards enforced</li> </ul> <p>When to Use:</p> <ul> <li>Creating standard VM configurations</li> <li>Instance templates for MIGs</li> <li>Golden images for teams</li> <li>Consistent deployments</li> <li>OS patching strategy (update image, redeploy VMs)</li> </ul> <p>When Not to Use:</p> <ul> <li>Backup purposes (use snapshots)</li> <li>Capturing running VM state (use machine images)</li> <li>Incremental updates (use configuration management)</li> </ul>"},{"location":"gcp/compute-engine/images/#image-families","title":"Image Families","text":"<p>Concept: Logical grouping where latest image used automatically</p> <p>Benefits:</p> <ul> <li>Version control for images</li> <li>Automatic use of latest</li> <li>Easy rollback to previous</li> <li>Staged rollouts</li> </ul> <p>Pattern:</p> <pre><code>Image Family: \"web-server\"\n\u251c\u2500\u2500 web-server-v1 (deprecated)\n\u251c\u2500\u2500 web-server-v2 (obsolete)\n\u2514\u2500\u2500 web-server-v3 (active, automatically used)\n</code></pre> <p>Architecture Decision: Always use image families in instance templates, never specific image names</p>"},{"location":"gcp/compute-engine/images/#public-images","title":"Public Images","text":"<p>Use Cases:</p> <ul> <li>Quick testing/development</li> <li>Standard OS deployments</li> <li>Community-maintained images</li> <li>Starting point for custom images</li> </ul> <p>Considerations:</p> <ul> <li>No customization</li> <li>Google or community maintained</li> <li>May not meet security standards</li> <li>Use as base for custom images</li> </ul> <p>Pattern: Public image \u2192 Customize \u2192 Save as custom image in family</p>"},{"location":"gcp/compute-engine/images/#image-lifecycle-management","title":"Image Lifecycle Management","text":""},{"location":"gcp/compute-engine/images/#deprecation-states","title":"Deprecation States","text":"<p>States:</p> <ul> <li>ACTIVE: Available for use</li> <li>DEPRECATED: Available but not recommended, shows warning</li> <li>OBSOLETE: Available but strongly discouraged, shows error</li> <li>DELETED: No longer available</li> </ul> <p>Strategy:</p> <ol> <li>Release new image (ACTIVE)</li> <li>Deprecate old image (after testing period)</li> <li>Mark obsolete (after migration period)</li> <li>Delete (after retention period)</li> </ol> <p>Architecture Pattern: Gradual deprecation allows staged migrations</p>"},{"location":"gcp/compute-engine/images/#cross-project-sharing","title":"Cross-Project Sharing","text":""},{"location":"gcp/compute-engine/images/#use-cases","title":"Use Cases","text":"<p>Organization Pattern:</p> <ul> <li>Central image repository project</li> <li>Shared across all projects</li> <li>Consistent base images</li> <li>Centralized maintenance</li> </ul> <p>Security Pattern:</p> <ul> <li>Share via IAM (not public)</li> <li>Principle of least privilege</li> <li>Service account access</li> <li>Audit image usage</li> </ul> <p>Multi-Project Architecture:</p> <ul> <li>Host project: Maintains images</li> <li>Service projects: Consume images</li> <li>Reduces duplication</li> <li>Single source of truth</li> </ul>"},{"location":"gcp/compute-engine/images/#image-creation-strategies","title":"Image Creation Strategies","text":""},{"location":"gcp/compute-engine/images/#build-pipeline-pattern","title":"Build Pipeline Pattern","text":"<p>Automated Image Creation:</p> <ol> <li>Base public image</li> <li>Packer or Cloud Build</li> <li>Apply configurations</li> <li>Security hardening</li> <li>Testing and validation</li> <li>Promote to image family</li> <li>Deprecate old version</li> </ol> <p>Benefits:</p> <ul> <li>Consistent, repeatable process</li> <li>Version controlled configurations</li> <li>Automated testing</li> <li>Audit trail</li> </ul>"},{"location":"gcp/compute-engine/images/#golden-image-approach","title":"Golden Image Approach","text":"<p>Single Source of Truth:</p> <ul> <li>Create \u201cperfect\u201d VM</li> <li>Install and configure software</li> <li>Harden and document</li> <li>Create image from disk</li> <li>Distribute to teams</li> </ul> <p>Considerations:</p> <ul> <li>Manual process initially</li> <li>Requires documentation</li> <li>Update process needed</li> <li>Drift over time</li> </ul> <p>Recommendation: Automate with Cloud Build</p>"},{"location":"gcp/compute-engine/images/#cost-considerations","title":"Cost Considerations","text":""},{"location":"gcp/compute-engine/images/#storage-costs","title":"Storage Costs","text":"<p>Machine Images:</p> <ul> <li>Charged for all disk data</li> <li>Multiple disks = higher cost</li> <li>Full copy each time</li> <li>Expensive for frequent backups</li> </ul> <p>Disk Images:</p> <ul> <li>Single disk storage</li> <li>Full copy</li> <li>Long-term storage</li> <li>Moderate cost</li> </ul> <p>Snapshots:</p> <ul> <li>Incremental storage</li> <li>First snapshot = full copy</li> <li>Subsequent = only changes</li> <li>Most cost-effective for frequent backups</li> </ul>"},{"location":"gcp/compute-engine/images/#optimization-strategies","title":"Optimization Strategies","text":"<p>Hybrid Approach:</p> <ul> <li>Machine images: Weekly/monthly</li> <li>Snapshots: Daily</li> <li>Balance cost vs recovery options</li> </ul> <p>Retention Policies:</p> <ul> <li>Keep recent machine images (30 days)</li> <li>Archive older snapshots</li> <li>Delete obsolete images</li> <li>Regular cleanup</li> </ul>"},{"location":"gcp/compute-engine/images/#compliance-and-security","title":"Compliance and Security","text":""},{"location":"gcp/compute-engine/images/#image-hardening","title":"Image Hardening","text":"<p>Security Baselines:</p> <ul> <li>CIS benchmarks applied</li> <li>Unnecessary services disabled</li> <li>Security patches current</li> <li>Logging configured</li> <li>Security agents installed</li> </ul> <p>Compliance Requirements:</p> <ul> <li>Approved software list</li> <li>Vulnerability scanning</li> <li>Configuration standards</li> <li>Audit logging</li> </ul>"},{"location":"gcp/compute-engine/images/#scanning-and-validation","title":"Scanning and Validation","text":"<p>Container Analysis:</p> <ul> <li>Scan images for vulnerabilities</li> <li>Block deployment if critical issues</li> <li>Continuous monitoring</li> <li>Compliance reporting</li> </ul> <p>Testing:</p> <ul> <li>Automated security tests</li> <li>Vulnerability assessments</li> <li>Penetration testing</li> <li>Regular updates</li> </ul>"},{"location":"gcp/compute-engine/images/#disaster-recovery","title":"Disaster Recovery","text":""},{"location":"gcp/compute-engine/images/#multi-region-strategy","title":"Multi-Region Strategy","text":"<p>Pattern:</p> <ul> <li>Machine images in multi-regional storage</li> <li>Snapshots in multiple regions</li> <li>Regular DR testing</li> <li>Documented procedures</li> </ul> <p>Considerations:</p> <ul> <li>Storage location affects cost</li> <li>Restore time varies by location</li> <li>Network egress charges</li> <li>Compliance requirements</li> </ul>"},{"location":"gcp/compute-engine/images/#recovery-time-objectives","title":"Recovery Time Objectives","text":"<p>Machine Images:</p> <ul> <li>RTO: 15-30 minutes</li> <li>Complete VM restore</li> <li>All configuration preserved</li> <li>Fastest recovery option</li> </ul> <p>Snapshots:</p> <ul> <li>RTO: 30-60 minutes</li> <li>Disk restoration</li> <li>VM recreation required</li> <li>Configuration must be reapplied</li> </ul>"},{"location":"gcp/compute-engine/images/#instance-templates","title":"Instance Templates","text":""},{"location":"gcp/compute-engine/images/#relationship-with-images","title":"Relationship with Images","text":"<p>Instance Template Contains:</p> <ul> <li>Reference to image (family preferred)</li> <li>Machine type specification</li> <li>Network configuration</li> <li>Metadata and labels</li> <li>Startup scripts</li> <li>NOT the actual disk data</li> </ul> <p>Architecture Pattern:</p> <pre><code>Disk Image (OS + Software)\n    \u2193\nImage Family (versioning)\n    \u2193\nInstance Template (configuration)\n    \u2193\nManaged Instance Group (scaling)\n</code></pre>"},{"location":"gcp/compute-engine/images/#versioning-strategy","title":"Versioning Strategy","text":"<p>Pattern:</p> <ul> <li>Update image in family</li> <li>Instance template references family</li> <li>MIG rolling update to new version</li> <li>Automated, consistent deployments</li> </ul> <p>Benefits:</p> <ul> <li>Decoupling of image and configuration</li> <li>Easy rollbacks</li> <li>Gradual rollouts</li> <li>Version control</li> </ul>"},{"location":"gcp/compute-engine/images/#exam-focus-areas","title":"Exam Focus Areas","text":""},{"location":"gcp/compute-engine/images/#decision-criteria","title":"Decision Criteria","text":"<ul> <li>Machine image vs disk image vs snapshot trade-offs</li> <li>When to use each type</li> <li>Cost comparison and optimization</li> <li>Storage location decisions</li> </ul>"},{"location":"gcp/compute-engine/images/#architecture-patterns","title":"Architecture Patterns","text":"<ul> <li>Image pipeline automation</li> <li>Golden image management</li> <li>Cross-project sharing</li> <li>Image family versioning</li> </ul>"},{"location":"gcp/compute-engine/images/#dr-and-backup","title":"DR and Backup","text":"<ul> <li>Hybrid backup strategies (images + snapshots)</li> <li>Multi-region DR with images</li> <li>RTO/RPO considerations</li> <li>Testing and validation</li> </ul>"},{"location":"gcp/compute-engine/images/#security","title":"Security","text":"<ul> <li>Image hardening strategies</li> <li>Vulnerability scanning</li> <li>Compliance baselines</li> <li>Access control (IAM)</li> </ul>"},{"location":"gcp/compute-engine/images/#operations","title":"Operations","text":"<ul> <li>Image lifecycle management</li> <li>Deprecation strategies</li> <li>Instance template integration</li> <li>Automated updates</li> </ul>"},{"location":"gcp/compute-engine/overview/","title":"Compute Engine Overview","text":""},{"location":"gcp/compute-engine/overview/#description","title":"Description","text":"<p>Google Compute Engine (GCE) provides Infrastructure as a Service (IaaS) with virtual machines running in Google\u2019s data centers. As an architect, understanding when to choose Compute Engine versus managed services is critical for designing scalable, cost-effective, and maintainable solutions.</p> <p>Key Architectural Consideration: Compute Engine offers maximum control but requires operational overhead. Evaluate managed alternatives (Cloud Run, GKE, App Engine, Cloud SQL) before defaulting to VMs.</p>"},{"location":"gcp/compute-engine/overview/#core-concepts","title":"Core Concepts","text":""},{"location":"gcp/compute-engine/overview/#virtual-machine-model","title":"Virtual Machine Model","text":"<ul> <li>Ephemeral vs Persistent: VM instance is ephemeral; only persistent disks survive deletion</li> <li>Live Migration: VMs migrate between hosts transparently during maintenance</li> <li>Preemptible/Spot VMs: Up to 91% discount but can be terminated any time (24-hour max runtime)</li> <li>Regional Persistent Disks: Synchronous replication across two zones for HA</li> </ul>"},{"location":"gcp/compute-engine/overview/#machine-families","title":"Machine Families","text":"<p>Decision Framework:</p> <ul> <li>E2 (Cost-Optimized): Development, testing, low-traffic applications</li> <li>N2/N2D (General Purpose): Balanced price/performance, most workloads</li> <li>C2/C2D/H3 (Compute-Optimized): CPU-intensive, HPC, high-performance computing</li> <li>M1/M2/M3 (Memory-Optimized): In-memory databases, SAP HANA, large caches</li> <li>A2/A3 (GPU-Accelerated): ML training, inference, scientific computing</li> </ul> <p>Custom Machine Types: Create VMs with specific vCPU-to-memory ratios when predefined types don\u2019t fit</p>"},{"location":"gcp/compute-engine/overview/#architectural-patterns","title":"Architectural Patterns","text":""},{"location":"gcp/compute-engine/overview/#high-availability","title":"High Availability","text":"<p>Single-Zone Deployment:</p> <ul> <li>Simplest architecture</li> <li>No SLA for VM availability</li> <li>Acceptable for dev/test, non-critical workloads</li> <li>Use Managed Instance Groups (MIGs) for autohealing</li> </ul> <p>Multi-Zone Deployment:</p> <ul> <li>Regional MIG distributes VMs across zones</li> <li>99.99% availability (with MIG and load balancer)</li> <li>Protects against zone failures</li> <li>Recommended for production</li> </ul> <p>Multi-Region Deployment:</p> <ul> <li>Global load balancer distributes traffic across regions</li> <li>Protects against regional failures</li> <li>Higher cost (cross-region egress)</li> <li>Required for global low-latency services</li> </ul>"},{"location":"gcp/compute-engine/overview/#disaster-recovery","title":"Disaster Recovery","text":"<p>RPO/RTO Targets:</p> <ul> <li>Snapshots: RPO 24 hours (daily), RTO 30-60 minutes</li> <li>Machine Images: RPO 7 days (weekly), RTO 15-30 minutes</li> <li>Regional Persistent Disks: RPO near-zero, RTO minutes</li> <li>Cross-Region Replication: Manual via snapshots/images</li> </ul> <p>DR Strategies:</p> <ul> <li>Backup and Restore: Snapshots/images, slowest RTO, lowest cost</li> <li>Pilot Light: Minimal infrastructure in DR region, scale on failover</li> <li>Warm Standby: Running infrastructure at reduced capacity</li> <li>Hot Standby: Full capacity active-active, highest cost</li> </ul>"},{"location":"gcp/compute-engine/overview/#important-limits-architecture-impact","title":"Important Limits (Architecture Impact)","text":"Limit Value Architectural Impact VMs per project 24 (default) per region Request increases proactively; use multiple projects CPUs per project 24 (default) per region Affects scalability; plan quotas Persistent disks per VM 128 Consider object storage for &gt; 128 data sources Network interfaces per VM 8 Limits multi-VPC connectivity options Local SSD per VM 24 devices (9 TB) Ephemeral; design for data loss Snapshots per project 10,000 Implement retention policies"},{"location":"gcp/compute-engine/overview/#decision-criteria-compute-engine-vs-alternatives","title":"Decision Criteria: Compute Engine vs Alternatives","text":""},{"location":"gcp/compute-engine/overview/#use-compute-engine-when","title":"Use Compute Engine When:","text":"<p>Full OS Control Required:</p> <ul> <li>Custom kernel modules</li> <li>Specific OS versions/configurations</li> <li>Legacy applications requiring full VM</li> <li>Windows Server workloads</li> </ul> <p>Lift-and-Shift Migrations:</p> <ul> <li>Rehosting existing on-premises VMs</li> <li>Minimal application changes acceptable</li> <li>Time-to-cloud more important than optimization</li> </ul> <p>Hybrid Cloud:</p> <ul> <li>Consistent VM experience on-premises and cloud</li> <li>VPN/Interconnect integration</li> <li>Active Directory integration</li> </ul> <p>Stateful Applications:</p> <ul> <li>Self-managed databases (when Cloud SQL doesn\u2019t fit)</li> <li>Traditional enterprise applications</li> <li>Licensing constraints (BYOL)</li> </ul>"},{"location":"gcp/compute-engine/overview/#dont-use-compute-engine-when","title":"Don\u2019t Use Compute Engine When:","text":"<p>Managed Services Available:</p> <ul> <li>Databases \u2192 Cloud SQL, Spanner, Firestore</li> <li>Containers \u2192 GKE, Cloud Run</li> <li>Batch processing \u2192 Dataflow, Dataproc</li> <li>Message queues \u2192 Pub/Sub</li> </ul> <p>Serverless Fits:</p> <ul> <li>HTTP-only services \u2192 Cloud Run</li> <li>Event-driven functions \u2192 Cloud Functions</li> <li>Stateless applications with variable load</li> <li>Want zero operational overhead</li> </ul> <p>Simple Requirements:</p> <ul> <li>Static websites \u2192 Cloud Storage + CDN</li> <li>Simple APIs \u2192 Cloud Run</li> <li>No persistent state needed</li> </ul>"},{"location":"gcp/compute-engine/overview/#cost-optimization-strategies","title":"Cost Optimization Strategies","text":""},{"location":"gcp/compute-engine/overview/#pricing-models","title":"Pricing Models","text":"<p>On-Demand:</p> <ul> <li>Per-second billing (1-minute minimum)</li> <li>Sustained use discounts (automatic 20-30%)</li> <li>Most flexible, no commitment</li> </ul> <p>Spot VMs (Preemptible):</p> <ul> <li>Up to 91% discount</li> <li>Can terminate any time (30-second warning)</li> <li>Use for: batch jobs, CI/CD, fault-tolerant workloads</li> <li>Not for: databases, stateful apps, critical services</li> </ul> <p>Committed Use Discounts (CUD):</p> <ul> <li>1-year or 3-year commitments</li> <li>Up to 57% discount</li> <li>Resource-based or spend-based</li> <li>Use for: predictable, long-running workloads</li> </ul> <p>Sole-Tenant Nodes:</p> <ul> <li>Dedicated physical servers</li> <li>Per-node pricing</li> <li>Use for: compliance, licensing, isolation requirements</li> </ul>"},{"location":"gcp/compute-engine/overview/#cost-architecture-decisions","title":"Cost Architecture Decisions","text":"<p>Right-Sizing:</p> <ul> <li>Use Recommender API for optimization suggestions</li> <li>Start small, scale up based on metrics</li> <li>Custom machine types for exact fit</li> <li>Avoid over-provisioning</li> </ul> <p>Scheduling:</p> <ul> <li>Stop VMs during off-hours (dev/test)</li> <li>Only pay for disk storage when stopped</li> <li>Use Instance Schedules or Cloud Scheduler</li> <li>60-70% savings for non-24/7 workloads</li> </ul> <p>Storage Optimization:</p> <ul> <li>Use standard disks for non-critical data</li> <li>Delete unused snapshots/images</li> <li>Regional vs multi-regional storage</li> <li>Consider object storage for large datasets</li> </ul>"},{"location":"gcp/compute-engine/overview/#networking-considerations","title":"Networking Considerations","text":""},{"location":"gcp/compute-engine/overview/#vpc-integration","title":"VPC Integration","text":"<p>Network Design:</p> <ul> <li>VMs exist in VPC subnets</li> <li>Each VM can have up to 8 network interfaces</li> <li>Internal IP (private) required, external IP (public) optional</li> <li>Use Cloud NAT for outbound internet without public IPs</li> </ul> <p>Connectivity Options:</p> <ul> <li>Internal: VPC, VPC Peering, Shared VPC</li> <li>Hybrid: Cloud VPN, Cloud Interconnect</li> <li>Internet: Cloud NAT, External IPs, Load Balancers</li> </ul>"},{"location":"gcp/compute-engine/overview/#load-balancing-architecture","title":"Load Balancing Architecture","text":"<p>External Application Load Balancer:</p> <ul> <li>Global HTTP(S) load balancing</li> <li>SSL termination</li> <li>URL-based routing</li> <li>Cloud CDN integration</li> </ul> <p>Internal Load Balancer:</p> <ul> <li>Private load balancing within VPC</li> <li>TCP/UDP support</li> <li>Regional or cross-region</li> </ul> <p>Network Load Balancer:</p> <ul> <li>Layer 4 (TCP/UDP)</li> <li>Preserves client IP</li> <li>Regional or global</li> </ul>"},{"location":"gcp/compute-engine/overview/#security-architecture","title":"Security Architecture","text":""},{"location":"gcp/compute-engine/overview/#security-layers","title":"Security Layers","text":"<p>VM-Level Security:</p> <ul> <li>OS hardening and patching</li> <li>OS Login (IAM-based SSH)</li> <li>Shielded VMs (Secure Boot, vTPM, integrity monitoring)</li> <li>Confidential VMs (memory encryption)</li> </ul> <p>Network Security:</p> <ul> <li>VPC firewall rules (stateful)</li> <li>Hierarchical firewall policies</li> <li>Private Google Access</li> <li>VPC Service Controls</li> </ul> <p>Identity and Access:</p> <ul> <li>Service accounts with least privilege</li> <li>Workload Identity for GKE integration</li> <li>IAM roles for VM management</li> <li>Separate admin duties</li> </ul> <p>Data Protection:</p> <ul> <li>Persistent disks encrypted by default</li> <li>Customer-managed encryption keys (CMEK)</li> <li>Snapshots encrypted automatically</li> <li>Regional disks for data residency</li> </ul>"},{"location":"gcp/compute-engine/overview/#monitoring-and-operations","title":"Monitoring and Operations","text":""},{"location":"gcp/compute-engine/overview/#observability-strategy","title":"Observability Strategy","text":"<p>Metrics:</p> <ul> <li>CPU, memory, disk, network utilization</li> <li>Custom metrics via monitoring agent</li> <li>Application-level metrics</li> <li>Uptime checks for availability</li> </ul> <p>Logging:</p> <ul> <li>System logs, application logs</li> <li>Serial console output</li> <li>Audit logs for compliance</li> <li>Centralized logging in Cloud Logging</li> </ul> <p>Alerting:</p> <ul> <li>Proactive notification of issues</li> <li>Threshold-based alerts</li> <li>Log-based metrics</li> <li>Integration with incident management</li> </ul>"},{"location":"gcp/compute-engine/overview/#operational-patterns","title":"Operational Patterns","text":"<p>Managed Instance Groups (MIGs):</p> <ul> <li>Autoscaling based on CPU, LB utilization, custom metrics</li> <li>Autohealing with health checks</li> <li>Rolling updates with surge/unavailable controls</li> <li>Regional MIGs for zone distribution</li> </ul> <p>Instance Templates:</p> <ul> <li>Standardized VM configurations</li> <li>Version control for infrastructure</li> <li>Foundation for MIGs</li> <li>Consistent deployments</li> </ul> <p>Startup/Shutdown Scripts:</p> <ul> <li>Automated configuration</li> <li>State preservation</li> <li>Graceful shutdown handling</li> <li>Integration with external systems</li> </ul>"},{"location":"gcp/compute-engine/overview/#compliance-and-governance","title":"Compliance and Governance","text":""},{"location":"gcp/compute-engine/overview/#compliance-considerations","title":"Compliance Considerations","text":"<p>Data Residency:</p> <ul> <li>Regional resources stay in region</li> <li>Snapshots stored in specified locations</li> <li>Machine images location-controlled</li> <li>Logs can be restricted to regions</li> </ul> <p>Regulatory Requirements:</p> <ul> <li>HIPAA: Shielded VMs, encryption, audit logs</li> <li>PCI-DSS: Network isolation, encryption, logging</li> <li>SOC 2: Access controls, change management, monitoring</li> <li>GDPR: Data location, encryption, deletion capabilities</li> </ul> <p>Organizational Controls:</p> <ul> <li>Organization policies (constraints)</li> <li>Resource labels for tracking</li> <li>IAM hierarchy (org \u2192 folder \u2192 project)</li> <li>Separate projects for environments</li> </ul>"},{"location":"gcp/compute-engine/overview/#integration-patterns","title":"Integration Patterns","text":""},{"location":"gcp/compute-engine/overview/#with-other-gcp-services","title":"With Other GCP Services","text":"<p>Data Services:</p> <ul> <li>Cloud SQL (managed MySQL, PostgreSQL, SQL Server)</li> <li>Cloud Storage (object storage, backups, static content)</li> <li>Persistent Disk (block storage)</li> <li>Filestore (NFS file storage)</li> </ul> <p>Container Services:</p> <ul> <li>GKE for containerized workloads alongside VMs</li> <li>Hybrid architectures with VMs + containers</li> <li>Migration path: VMs \u2192 Containers \u2192 Serverless</li> </ul> <p>Analytics and ML:</p> <ul> <li>BigQuery for data warehousing</li> <li>Dataflow for stream/batch processing</li> <li>Vertex AI for ML workloads</li> <li>Data transfer from VMs to analytics services</li> </ul> <p>Management:</p> <ul> <li>Cloud Monitoring and Logging</li> <li>Cloud Deployment Manager / Terraform</li> <li>Cloud Build for CI/CD</li> <li>Artifact Registry for custom images</li> </ul>"},{"location":"gcp/compute-engine/overview/#best-practices-summary","title":"Best Practices Summary","text":""},{"location":"gcp/compute-engine/overview/#architecture-design","title":"Architecture Design","text":"<ol> <li>Use managed services first: Only use VMs when necessary</li> <li>Design for failure: Assume VMs can fail, use MIGs and health checks</li> <li>Implement monitoring: Proactive alerting before users notice issues</li> <li>Plan for scale: Use autoscaling, not manual intervention</li> <li>Optimize costs: Right-size, use Spot VMs, implement schedules</li> <li>Secure by default: Least privilege, private IPs, OS hardening</li> <li>Document decisions: Architecture decision records (ADRs)</li> </ol>"},{"location":"gcp/compute-engine/overview/#operational-excellence","title":"Operational Excellence","text":"<ol> <li>Infrastructure as Code: Terraform, Deployment Manager</li> <li>Immutable infrastructure: Replace, don\u2019t modify</li> <li>Blue-green deployments: Zero-downtime updates</li> <li>Disaster recovery plan: Test regularly, document procedures</li> <li>Capacity planning: Quota management, growth projections</li> <li>Cost monitoring: Budgets, alerts, regular reviews</li> </ol>"},{"location":"gcp/compute-engine/overview/#exam-focus-areas","title":"Exam Focus Areas","text":""},{"location":"gcp/compute-engine/overview/#key-topics-for-professional-architect","title":"Key Topics for Professional Architect","text":"<p>Design Decisions:</p> <ul> <li>When to use Compute Engine vs managed services</li> <li>Machine type selection criteria</li> <li>High availability architecture patterns</li> <li>Disaster recovery strategies</li> <li>Cost optimization approaches</li> </ul> <p>Scaling Patterns:</p> <ul> <li>Horizontal vs vertical scaling trade-offs</li> <li>Autoscaling configuration considerations</li> <li>Multi-region deployment strategies</li> <li>Load balancing options</li> </ul> <p>Security:</p> <ul> <li>Network isolation strategies</li> <li>IAM role design</li> <li>Encryption options</li> <li>Compliance requirements</li> </ul> <p>Operations:</p> <ul> <li>Monitoring and alerting strategy</li> <li>Update and patch management</li> <li>Backup and restore procedures</li> <li>Incident response planning</li> </ul> <p>Integration:</p> <ul> <li>Hybrid cloud connectivity</li> <li>Service integration patterns</li> <li>Data flow architecture</li> <li>Migration strategies</li> </ul>"},{"location":"gcp/compute-engine/snapshots/","title":"Disk Snapshots","text":""},{"location":"gcp/compute-engine/snapshots/#core-concepts","title":"Core Concepts","text":"<p>Snapshots provide incremental, point-in-time backups of persistent disks. Understanding snapshot architecture is essential for designing cost-effective backup and recovery strategies.</p> <p>Key Principle: Snapshots are incremental after the first full copy, making them ideal for frequent backups.</p>"},{"location":"gcp/compute-engine/snapshots/#incremental-architecture","title":"Incremental Architecture","text":""},{"location":"gcp/compute-engine/snapshots/#how-snapshots-work","title":"How Snapshots Work","text":"<p>First Snapshot:</p> <ul> <li>Full copy of all disk data</li> <li>Stored in Cloud Storage</li> <li>Time proportional to disk size</li> <li>Baseline for future snapshots</li> </ul> <p>Subsequent Snapshots:</p> <ul> <li>Only changed blocks since previous snapshot</li> <li>Significantly faster creation</li> <li>Space-efficient storage</li> <li>Automatic deduplication</li> </ul> <p>Chain Management:</p> <ul> <li>Google maintains snapshot chain automatically</li> <li>Deleting middle snapshot is safe</li> <li>Required blocks preserved</li> <li>No user management needed</li> </ul>"},{"location":"gcp/compute-engine/snapshots/#storage-efficiency-example","title":"Storage Efficiency Example","text":"<p>Scenario: 1 TB disk, 5% daily change, 30-day retention</p> <p>Traditional Full Backups:</p> <ul> <li>30 copies \u00d7 1000 GB = 30,000 GB</li> </ul> <p>Incremental Snapshots:</p> <ul> <li>Day 1: 1000 GB (full)</li> <li>Days 2-30: 50 GB each</li> <li>Total: 1000 + (29 \u00d7 50) = 2,450 GB</li> <li>92% storage savings</li> </ul>"},{"location":"gcp/compute-engine/snapshots/#snapshot-storage-locations","title":"Snapshot Storage Locations","text":""},{"location":"gcp/compute-engine/snapshots/#regional-storage","title":"Regional Storage","text":"<p>Characteristics:</p> <ul> <li>Stored in single region</li> <li>Same cost as multi-regional</li> <li>Faster restore (same region)</li> <li>Region-specific availability</li> </ul> <p>Use Cases:</p> <ul> <li>Fast restore requirements</li> <li>Region-bound compliance</li> <li>Cost optimization (same-region egress free)</li> <li>Non-critical data</li> </ul> <p>Limitation: Single region failure risk</p>"},{"location":"gcp/compute-engine/snapshots/#multi-regional-storage","title":"Multi-Regional Storage","text":"<p>Characteristics:</p> <ul> <li>Stored across multiple regions</li> <li>Geographic redundancy</li> <li>Same cost as regional</li> <li>Protected from regional failure</li> </ul> <p>Use Cases:</p> <ul> <li>Disaster recovery</li> <li>Cross-region restore capability</li> <li>Critical data protection</li> <li>Compliance requirements</li> </ul> <p>Trade-off: Slightly slower initial creation, cross-region restore incurs egress costs</p>"},{"location":"gcp/compute-engine/snapshots/#architecture-decision-matrix","title":"Architecture Decision Matrix","text":"Data Criticality Restore Frequency Storage Location Rationale Critical Daily Multi-regional DR protection Important Weekly Multi-regional Balance cost/protection Normal Daily Regional Cost optimization Development On-demand Regional Lowest cost"},{"location":"gcp/compute-engine/snapshots/#scheduled-snapshots","title":"Scheduled Snapshots","text":""},{"location":"gcp/compute-engine/snapshots/#snapshot-schedules","title":"Snapshot Schedules","text":"<p>Benefits:</p> <ul> <li>Automated backup lifecycle</li> <li>Consistent backup timing</li> <li>Retention policy enforcement</li> <li>Reduced operational overhead</li> <li>Guaranteed backup frequency</li> </ul> <p>Schedule Types:</p> <ul> <li>Hourly: Every N hours (critical data, 4-hour RPO)</li> <li>Daily: Specific time (standard backups, 24-hour RPO)</li> <li>Weekly: Specific day and time (less critical, 7-day RPO)</li> </ul> <p>Retention Policies:</p> <ul> <li>Automatic deletion of old snapshots</li> <li>Maximum retention: ~7.5 years</li> <li>Compliance-driven retention</li> <li>Cost control</li> </ul>"},{"location":"gcp/compute-engine/snapshots/#design-patterns","title":"Design Patterns","text":"<p>Tiered Backup Strategy:</p> <ul> <li>Hourly snapshots: Retain 24 hours (critical systems)</li> <li>Daily snapshots: Retain 7 days (standard)</li> <li>Weekly snapshots: Retain 4 weeks (long-term)</li> <li>Monthly snapshots: Retain 12 months (compliance)</li> </ul> <p>Pattern: Multiple schedules on same disk for different retention tiers</p>"},{"location":"gcp/compute-engine/snapshots/#snapshot-best-practices","title":"Snapshot Best Practices","text":""},{"location":"gcp/compute-engine/snapshots/#application-consistent-snapshots","title":"Application-Consistent Snapshots","text":"<p>Problem: Snapshot at disk level, may capture inconsistent state</p> <p>Solutions:</p> <ul> <li>Quiesce filesystem: Flush pending writes</li> <li>Database checkpoint: Ensure consistent state</li> <li>Application-level backup: Use native tools first</li> <li>\u2013guest-flush flag: Windows VSS integration</li> </ul> <p>Architecture Consideration: Snapshots are crash-consistent by default; application consistency requires coordination</p>"},{"location":"gcp/compute-engine/snapshots/#backup-windows","title":"Backup Windows","text":"<p>Considerations:</p> <ul> <li>Schedule during low-usage periods</li> <li>First snapshot has performance impact</li> <li>Subsequent snapshots minimal impact</li> <li>Background operation</li> </ul> <p>Pattern: 2-4 AM local time for daily snapshots (minimize business impact)</p>"},{"location":"gcp/compute-engine/snapshots/#recovery-strategies","title":"Recovery Strategies","text":""},{"location":"gcp/compute-engine/snapshots/#point-in-time-recovery","title":"Point-in-Time Recovery","text":"<p>Capabilities:</p> <ul> <li>Restore to any snapshot point</li> <li>Multiple recovery points (based on schedule)</li> <li>Granular recovery options</li> <li>Test restores without affecting production</li> </ul> <p>RTO Considerations:</p> <ul> <li>Disk creation from snapshot: 10-30 minutes</li> <li>VM recreation: 5-15 minutes</li> <li>Application startup: Varies</li> <li>Total RTO: 30-60 minutes typically</li> </ul>"},{"location":"gcp/compute-engine/snapshots/#partial-recovery","title":"Partial Recovery","text":"<p>Options:</p> <ul> <li>Restore single disk (not entire VM)</li> <li>Attach restored disk as secondary</li> <li>Copy needed files</li> <li>Detach and delete disk</li> </ul> <p>Use Case: Accidental deletion, specific file recovery</p>"},{"location":"gcp/compute-engine/snapshots/#cost-optimization","title":"Cost Optimization","text":""},{"location":"gcp/compute-engine/snapshots/#snapshot-cost-structure","title":"Snapshot Cost Structure","text":"<p>Storage Pricing:</p> <ul> <li>~$0.026/GB/month (regional and multi-regional)</li> <li>Incremental charges (only changed data)</li> <li>No retrieval fees</li> <li>No operation charges</li> </ul> <p>Cost Factors:</p> <ul> <li>Frequency of snapshots (more = more changed data)</li> <li>Change rate (higher = more storage)</li> <li>Retention period (longer = more total storage)</li> <li>Number of disks</li> </ul>"},{"location":"gcp/compute-engine/snapshots/#optimization-strategies","title":"Optimization Strategies","text":"<p>Retention Policies:</p> <ul> <li>Delete old snapshots automatically</li> <li>Balance recovery needs vs cost</li> <li>Different policies for different data tiers</li> <li>Regular review and adjustment</li> </ul> <p>Snapshot Consolidation:</p> <ul> <li>Fewer snapshots = lower cost</li> <li>Balance with RPO requirements</li> <li>Use snapshot schedules for consistency</li> </ul> <p>Right-Sizing Frequency:</p> <ul> <li>Critical data: Frequent snapshots justified</li> <li>Non-critical data: Less frequent acceptable</li> <li>Test environments: Manual/infrequent</li> </ul>"},{"location":"gcp/compute-engine/snapshots/#disaster-recovery-architecture","title":"Disaster Recovery Architecture","text":""},{"location":"gcp/compute-engine/snapshots/#cross-region-dr","title":"Cross-Region DR","text":"<p>Pattern:</p> <ol> <li>Snapshots in multi-regional storage</li> <li>DR plan documents restore procedure</li> <li>Network and firewall rules pre-configured in DR region</li> <li>Regular DR testing (quarterly)</li> <li>Runbooks and automation</li> </ol> <p>RTO Targets:</p> <ul> <li>Tier 1 (Critical): RTO 1 hour</li> <li>Hourly snapshots, multi-regional</li> <li>Automated restoration scripts</li> <li> <p>Pre-warmed network configuration</p> </li> <li> <p>Tier 2 (Important): RTO 4 hours</p> </li> <li>Daily snapshots, multi-regional</li> <li> <p>Documented procedures</p> </li> <li> <p>Tier 3 (Standard): RTO 24 hours</p> </li> <li>Daily snapshots, regional</li> <li>Manual restoration acceptable</li> </ul>"},{"location":"gcp/compute-engine/snapshots/#rpo-considerations","title":"RPO Considerations","text":"<p>Snapshot Frequency = RPO:</p> <ul> <li>Hourly snapshots: 1-hour RPO (max 1 hour data loss)</li> <li>Daily snapshots: 24-hour RPO</li> <li>Weekly snapshots: 7-day RPO</li> </ul> <p>Architecture Decision: Balance frequency cost vs acceptable data loss</p>"},{"location":"gcp/compute-engine/snapshots/#snapshot-limits","title":"Snapshot Limits","text":""},{"location":"gcp/compute-engine/snapshots/#operational-limits","title":"Operational Limits","text":"Limit Value Impact Snapshots per disk Unlimited No practical limit Snapshots per project 10,000 Request increase if needed Snapshot creation rate 1 per 10 min per disk Affects backup frequency Snapshot size 64 TB Matches max disk size <p>Architecture Implication: Large environments may need multiple projects for snapshot quotas</p>"},{"location":"gcp/compute-engine/snapshots/#comparison-snapshots-vs-machine-images","title":"Comparison: Snapshots vs Machine Images","text":""},{"location":"gcp/compute-engine/snapshots/#when-to-use-snapshots","title":"When to Use Snapshots","text":"<p>Advantages:</p> <ul> <li>Incremental (cost-effective)</li> <li>Frequent backups (daily/hourly)</li> <li>Per-disk granularity</li> <li>Fast creation (after first)</li> <li>Low storage cost</li> </ul> <p>Use Cases:</p> <ul> <li>Regular data protection</li> <li>Point-in-time recovery</li> <li>Database backups</li> <li>Frequent backup requirements</li> <li>Cost-sensitive environments</li> </ul>"},{"location":"gcp/compute-engine/snapshots/#when-to-use-machine-images","title":"When to Use Machine Images","text":"<p>Advantages:</p> <ul> <li>Complete VM configuration</li> <li>All disks in one operation</li> <li>Fast full restoration</li> <li>VM migration</li> </ul> <p>Use Cases:</p> <ul> <li>Weekly/monthly full backups</li> <li>VM cloning and migration</li> <li>Disaster recovery base</li> <li>Configuration preservation</li> </ul> <p>Architecture Pattern: Use both - snapshots for frequent backups, machine images for complete system backups</p>"},{"location":"gcp/compute-engine/snapshots/#security-considerations","title":"Security Considerations","text":""},{"location":"gcp/compute-engine/snapshots/#encryption","title":"Encryption","text":"<p>Default Encryption:</p> <ul> <li>All snapshots encrypted at rest</li> <li>Google-managed keys</li> <li>Transparent operation</li> <li>No configuration needed</li> </ul> <p>Customer-Managed Keys (CMEK):</p> <ul> <li>Control key lifecycle</li> <li>Regulatory compliance</li> <li>Key rotation policies</li> <li>Cloud KMS integration</li> </ul> <p>Trade-offs:</p> <ul> <li>CMEK: More control, more complexity</li> <li>Google-managed: Simpler, less control</li> <li>Performance impact minimal</li> </ul>"},{"location":"gcp/compute-engine/snapshots/#access-control","title":"Access Control","text":"<p>IAM Roles:</p> <ul> <li><code>roles/compute.storageAdmin</code>: Create/delete snapshots</li> <li><code>roles/compute.viewer</code>: View snapshots</li> <li>Principle of least privilege</li> <li>Separate snapshot admin from VM admin</li> </ul> <p>Snapshot Sharing:</p> <ul> <li>Cross-project via IAM</li> <li>Service account access</li> <li>Controlled sharing</li> <li>Audit access</li> </ul>"},{"location":"gcp/compute-engine/snapshots/#compliance-and-retention","title":"Compliance and Retention","text":""},{"location":"gcp/compute-engine/snapshots/#regulatory-requirements","title":"Regulatory Requirements","text":"<p>Common Requirements:</p> <ul> <li>HIPAA: 6-year retention, encryption, audit logs</li> <li>SOX: 7-year retention, access controls</li> <li>GDPR: Data location restrictions, deletion capability</li> <li>PCI-DSS: Encryption, access controls, retention</li> </ul> <p>Implementation:</p> <ul> <li>Snapshot schedules with appropriate retention</li> <li>Storage location selection (region/multi-region)</li> <li>CMEK for enhanced security</li> <li>Audit logging enabled</li> </ul>"},{"location":"gcp/compute-engine/snapshots/#retention-strategy","title":"Retention Strategy","text":"<p>Pattern:</p> <ul> <li>Short-term: Daily snapshots, 7-30 day retention (operational recovery)</li> <li>Long-term: Monthly snapshots, multi-year retention (compliance)</li> <li>Archive: Consider exporting to Cloud Storage for very long-term</li> </ul>"},{"location":"gcp/compute-engine/snapshots/#monitoring-and-alerting","title":"Monitoring and Alerting","text":""},{"location":"gcp/compute-engine/snapshots/#key-metrics","title":"Key Metrics","text":"<p>Monitor:</p> <ul> <li>Snapshot creation success/failure rate</li> <li>Time to create snapshots</li> <li>Storage costs trending</li> <li>Age of oldest snapshot</li> <li>Recovery testing results</li> </ul> <p>Alerts:</p> <ul> <li>Snapshot creation failures</li> <li>Storage quota approaching limits</li> <li>Unexpected cost increases</li> <li>Missing scheduled snapshots</li> <li>Compliance violations (retention)</li> </ul>"},{"location":"gcp/compute-engine/snapshots/#exam-focus-areas","title":"Exam Focus Areas","text":""},{"location":"gcp/compute-engine/snapshots/#design-decisions","title":"Design Decisions","text":"<ul> <li>Snapshot vs machine image trade-offs</li> <li>Frequency and retention policies</li> <li>Storage location selection (regional vs multi-regional)</li> <li>Cost optimization strategies</li> </ul>"},{"location":"gcp/compute-engine/snapshots/#backup-architecture","title":"Backup Architecture","text":"<ul> <li>Tiered backup strategies</li> <li>RPO/RTO planning</li> <li>Application-consistent backup methods</li> <li>Cross-region DR design</li> </ul>"},{"location":"gcp/compute-engine/snapshots/#cost-management","title":"Cost Management","text":"<ul> <li>Incremental snapshot cost model</li> <li>Retention policy optimization</li> <li>Storage location cost impact</li> <li>Snapshot cleanup strategies</li> </ul>"},{"location":"gcp/compute-engine/snapshots/#operations","title":"Operations","text":"<ul> <li>Scheduled snapshots configuration</li> <li>Automated lifecycle management</li> <li>Monitoring and alerting</li> <li>Testing and validation</li> </ul>"},{"location":"gcp/compute-engine/snapshots/#compliance","title":"Compliance","text":"<ul> <li>Retention requirements</li> <li>Encryption options (default vs CMEK)</li> <li>Access control design</li> <li>Audit logging</li> </ul>"},{"location":"gcp/compute-engine/vms/","title":"Virtual Machines (VMs)","text":""},{"location":"gcp/compute-engine/vms/#core-concepts","title":"Core Concepts","text":"<p>A Compute Engine VM instance is a virtualized server with customizable CPU, memory, storage, and networking resources. Understanding VM characteristics and lifecycle is essential for architectural decisions.</p> <p>Key Principle: VMs are ephemeral; design for failure and use persistent storage for stateful data.</p>"},{"location":"gcp/compute-engine/vms/#machine-types-selection","title":"Machine Types Selection","text":""},{"location":"gcp/compute-engine/vms/#predefined-vs-custom","title":"Predefined vs Custom","text":"<p>Predefined Machine Types:</p> <ul> <li>Standard configurations (e.g., n2-standard-4: 4 vCPUs, 16 GB RAM)</li> <li>Optimized for common workload patterns</li> <li>Simpler to manage and plan</li> <li>Use for: Most workloads with standard requirements</li> </ul> <p>Custom Machine Types:</p> <ul> <li>Specify exact vCPU and memory (0.9-8 GB RAM per vCPU)</li> <li>Cost optimization for specific ratios</li> <li>Extended memory available (up to 624 GB)</li> <li>Use for: Workloads with non-standard resource ratios</li> </ul>"},{"location":"gcp/compute-engine/vms/#machine-family-decision-matrix","title":"Machine Family Decision Matrix","text":"Workload Type Recommended Family Reasoning Web servers, APIs N2, N2D Balanced, cost-effective Databases (small-medium) N2, N2D Good memory/CPU ratio Large databases M-series Memory-optimized Batch processing E2, Spot VMs Cost-optimized HPC, gaming servers C2, C2D, H3 Highest CPU performance ML training A2, A3 GPU-accelerated Development/testing E2, Spot VMs Lowest cost"},{"location":"gcp/compute-engine/vms/#vm-lifecycle-and-states","title":"VM Lifecycle and States","text":""},{"location":"gcp/compute-engine/vms/#state-transitions","title":"State Transitions","text":"<ul> <li>PROVISIONING \u2192 STAGING \u2192 RUNNING: Normal startup</li> <li>RUNNING \u2192 STOPPING \u2192 TERMINATED: Clean shutdown</li> <li>TERMINATED \u2192 RUNNING: Start operation</li> <li>RUNNING \u2192 SUSPENDING \u2192 SUSPENDED (Beta): Save state to disk</li> </ul> <p>Architectural Impact:</p> <ul> <li>Billing stops in TERMINATED state (disk charges remain)</li> <li>No SLA for VM availability without MIG</li> <li>Live migration during maintenance (RUNNING \u2192 RUNNING transparently)</li> <li>Preemptible VMs: 30-second termination warning, 24-hour maximum</li> </ul>"},{"location":"gcp/compute-engine/vms/#spot-vms-preemptible","title":"Spot VMs (Preemptible)","text":""},{"location":"gcp/compute-engine/vms/#cost-vs-availability-trade-off","title":"Cost vs Availability Trade-off","text":"<p>Benefits:</p> <ul> <li>Up to 91% discount</li> <li>Same performance as regular VMs</li> <li>Significant cost savings at scale</li> </ul> <p>Limitations:</p> <ul> <li>Can be terminated at any time</li> <li>30-second warning via shutdown script</li> <li>No live migration</li> <li>No SLA</li> <li>24-hour maximum runtime</li> </ul>"},{"location":"gcp/compute-engine/vms/#architectural-use-cases","title":"Architectural Use Cases","text":"<p>Appropriate for:</p> <ul> <li>Batch processing jobs</li> <li>CI/CD pipelines</li> <li>Data processing (MapReduce-style)</li> <li>Rendering farms</li> <li>Fault-tolerant distributed systems</li> <li>Development/testing environments</li> </ul> <p>Not appropriate for:</p> <ul> <li>Databases (unless replicated)</li> <li>Stateful applications without external state</li> <li>Long-running single processes</li> <li>Applications requiring guaranteed uptime</li> <li>Services without checkpointing</li> </ul> <p>Design Pattern: Combine regular + Spot VMs in MIG for cost optimization with availability</p>"},{"location":"gcp/compute-engine/vms/#boot-disk-configuration","title":"Boot Disk Configuration","text":""},{"location":"gcp/compute-engine/vms/#disk-type-selection","title":"Disk Type Selection","text":"Disk Type IOPS Throughput Use Case Cost pd-standard Low Low Dev/test, backup target Lowest pd-balanced Medium Medium General purpose (recommended) Medium pd-ssd High High Databases, I/O intensive High pd-extreme Custom Very high Mission-critical databases Highest <p>Decision Criteria:</p> <ul> <li>Boot disk I/O rarely bottleneck (use pd-balanced)</li> <li>Data disks: Match I/O requirements</li> <li>Performance scales with disk size</li> <li>Consider Local SSD for highest performance (ephemeral)</li> </ul>"},{"location":"gcp/compute-engine/vms/#size-considerations","title":"Size Considerations","text":"<p>Minimum Recommendations:</p> <ul> <li>Linux: 20 GB (OS + updates + applications)</li> <li>Windows: 50 GB (OS + updates)</li> <li>Production: Add 30-50% buffer</li> </ul> <p>Performance Impact:</p> <ul> <li>Larger disks = higher IOPS/throughput</li> <li>100 GB pd-balanced: 600 IOPS</li> <li>1000 GB pd-balanced: 6000 IOPS</li> <li>Maximum depends on VM machine type</li> </ul>"},{"location":"gcp/compute-engine/vms/#security-options","title":"Security Options","text":""},{"location":"gcp/compute-engine/vms/#shielded-vms","title":"Shielded VMs","text":"<p>Components:</p> <ul> <li>Secure Boot: Verify bootloader and kernel integrity</li> <li>vTPM: Virtual Trusted Platform Module for key management</li> <li>Integrity Monitoring: Alert on boot sequence changes</li> </ul> <p>When to Enable:</p> <ul> <li>Production workloads (should be default)</li> <li>Compliance requirements (PCI-DSS, HIPAA)</li> <li>Protection against rootkits and bootkits</li> <li>Minimal performance impact</li> </ul>"},{"location":"gcp/compute-engine/vms/#confidential-vms","title":"Confidential VMs","text":"<p>Capabilities:</p> <ul> <li>Memory encryption using AMD SEV</li> <li>Isolates VM memory from Google and other VMs</li> <li>Hardware-based protection</li> </ul> <p>Trade-offs:</p> <ul> <li>Limited machine types (N2D, C2D)</li> <li>Slight performance overhead</li> <li>Higher cost</li> <li>Use for: Highly sensitive workloads, regulatory requirements</li> </ul>"},{"location":"gcp/compute-engine/vms/#sole-tenant-nodes","title":"Sole-Tenant Nodes","text":"<p>Use Cases:</p> <ul> <li>Licensing requirements (per-socket, per-core)</li> <li>Compliance requirements (physical isolation)</li> <li>Performance isolation</li> <li>Workload separation for security</li> </ul> <p>Considerations:</p> <ul> <li>Pay for entire node (all vCPUs), not per VM</li> <li>More expensive than shared tenancy</li> <li>Requires capacity planning</li> <li>Suitable for large, stable workloads</li> </ul>"},{"location":"gcp/compute-engine/vms/#network-configuration","title":"Network Configuration","text":""},{"location":"gcp/compute-engine/vms/#ip-addressing","title":"IP Addressing","text":"<p>Internal IP (Required):</p> <ul> <li>Private RFC 1918 address</li> <li>Assigned from subnet range</li> <li>Used for VPC communication</li> <li>Can be ephemeral or static</li> </ul> <p>External IP (Optional):</p> <ul> <li>Public internet-routable address</li> <li>Ephemeral (changes on stop/start) or static (reserved)</li> <li>Billed when reserved but not attached</li> <li>Alternative: Cloud NAT for outbound without public IP</li> </ul>"},{"location":"gcp/compute-engine/vms/#multiple-network-interfaces","title":"Multiple Network Interfaces","text":"<p>Capabilities:</p> <ul> <li>Up to 8 NICs per VM</li> <li>Each in different VPC network</li> <li>Each with own routing table</li> <li>Each can have internal and external IPs</li> </ul> <p>Use Cases:</p> <ul> <li>Network appliances (firewall, router)</li> <li>DMZ architectures</li> <li>Separate management network</li> <li>Traffic separation for security</li> </ul> <p>Limitations:</p> <ul> <li>Cannot be in same VPC</li> <li>No automatic traffic routing between interfaces</li> <li>Requires application-level routing</li> </ul>"},{"location":"gcp/compute-engine/vms/#service-accounts","title":"Service Accounts","text":""},{"location":"gcp/compute-engine/vms/#architecture-patterns","title":"Architecture Patterns","text":"<p>Default Compute Engine Service Account:</p> <ul> <li>Auto-created per project</li> <li>Has Editor role by default (too permissive)</li> <li>Used if no SA specified</li> <li>Don\u2019t use for production</li> </ul> <p>Custom Service Account (Recommended):</p> <ul> <li>Create per application/workload</li> <li>Grant minimal necessary permissions</li> <li>Principle of least privilege</li> <li>Easier to audit and rotate</li> </ul> <p>Scopes:</p> <ul> <li>Legacy mechanism (being replaced by IAM)</li> <li>Limit API access even with SA permissions</li> <li>Use <code>cloud-platform</code> scope + IAM roles (modern approach)</li> <li>Scopes cannot grant more access than IAM roles</li> </ul>"},{"location":"gcp/compute-engine/vms/#managed-instance-groups-migs","title":"Managed Instance Groups (MIGs)","text":""},{"location":"gcp/compute-engine/vms/#benefits","title":"Benefits","text":"<p>Availability:</p> <ul> <li>Autohealing with health checks</li> <li>Automatic replacement of unhealthy instances</li> <li>Distribution across zones (regional MIG)</li> <li>No single point of failure</li> </ul> <p>Scalability:</p> <ul> <li>Autoscaling based on metrics</li> <li>Horizontal scaling (add/remove instances)</li> <li>Integration with load balancing</li> <li>Handle traffic spikes automatically</li> </ul> <p>Management:</p> <ul> <li>Rolling updates with version control</li> <li>Canary deployments</li> <li>Consistent configuration via templates</li> <li>Automated operations</li> </ul>"},{"location":"gcp/compute-engine/vms/#autoscaling-configuration","title":"Autoscaling Configuration","text":"<p>Scaling Policies:</p> <ul> <li>CPU utilization (most common)</li> <li>HTTP load balancing utilization</li> <li>Cloud Pub/Sub queue size</li> <li>Custom metrics (Cloud Monitoring)</li> <li>Multiple policies (scales on any)</li> </ul> <p>Considerations:</p> <ul> <li>Cool-down period: Prevent thrashing</li> <li>Min/max instances: Set boundaries</li> <li>Scale-in controls: Prevent rapid scale-down</li> <li>Target utilization: 60-70% for headroom</li> </ul> <p>Architecture Pattern:</p> <ul> <li>Use MIG for all production workloads</li> <li>Even single-instance (autohealing benefit)</li> <li>Regional MIG for high availability</li> <li>Combine with load balancer for best results</li> </ul>"},{"location":"gcp/compute-engine/vms/#metadata-and-startup-scripts","title":"Metadata and Startup Scripts","text":""},{"location":"gcp/compute-engine/vms/#metadata-server","title":"Metadata Server","text":"<p>Purpose:</p> <ul> <li>Provide instance and project metadata</li> <li>Service account tokens</li> <li>Startup/shutdown scripts</li> <li>Custom key-value pairs</li> </ul> <p>Security Considerations:</p> <ul> <li>Accessible from within VM without authentication</li> <li>Service account tokens available</li> <li>Use Workload Identity for GKE instead</li> <li>Restrict metadata access if needed</li> </ul>"},{"location":"gcp/compute-engine/vms/#startup-scripts","title":"Startup Scripts","text":"<p>Use Cases:</p> <ul> <li>Software installation and configuration</li> <li>Join domain or cluster</li> <li>Register with external services</li> <li>Application deployment</li> </ul> <p>Best Practices:</p> <ul> <li>Idempotent execution</li> <li>Log to serial console</li> <li>Handle failures gracefully</li> <li>Keep minimal (use configuration management)</li> <li>Consider cloud-init or Ansible</li> </ul>"},{"location":"gcp/compute-engine/vms/#high-availability-patterns","title":"High Availability Patterns","text":""},{"location":"gcp/compute-engine/vms/#single-zone-architecture","title":"Single-Zone Architecture","text":"<p>Characteristics:</p> <ul> <li>All VMs in one zone</li> <li>Simplest design</li> <li>Lower cost (no cross-zone traffic)</li> <li>No zone failure protection</li> </ul> <p>Appropriate for:</p> <ul> <li>Development/testing</li> <li>Non-critical workloads</li> <li>Stateful services with external replication</li> <li>When zone failure is acceptable</li> </ul>"},{"location":"gcp/compute-engine/vms/#multi-zone-regional-mig","title":"Multi-Zone (Regional MIG)","text":"<p>Characteristics:</p> <ul> <li>VMs distributed across 3+ zones</li> <li>Zone failure protection</li> <li>Slightly higher cost (cross-zone traffic)</li> <li>99.99% availability with load balancer</li> </ul> <p>Design Considerations:</p> <ul> <li>Even distribution vs BALANCED (Google decides)</li> <li>Health check requirements</li> <li>Data replication strategy</li> <li>Shared storage (Filestore, Cloud SQL)</li> </ul>"},{"location":"gcp/compute-engine/vms/#multi-region","title":"Multi-Region","text":"<p>Characteristics:</p> <ul> <li>VMs in multiple regions</li> <li>Regional failure protection</li> <li>Global load balancing</li> <li>Higher cost (cross-region egress)</li> </ul> <p>When Required:</p> <ul> <li>Global user base (low latency)</li> <li>Disaster recovery (regional failure)</li> <li>Compliance (data residency)</li> <li>Highest availability requirements</li> </ul>"},{"location":"gcp/compute-engine/vms/#performance-considerations","title":"Performance Considerations","text":""},{"location":"gcp/compute-engine/vms/#cpu-platform","title":"CPU Platform","text":"<p>Options:</p> <ul> <li>Intel Cascade Lake</li> <li>Intel Ice Lake</li> <li>Intel Sapphire Rapids</li> <li>AMD Milan, Genoa</li> </ul> <p>Impact:</p> <ul> <li>Newer platforms: Better performance</li> <li>Varies by region/zone</li> <li>Can specify minimum platform</li> <li>Automatic upgrades over time</li> </ul>"},{"location":"gcp/compute-engine/vms/#cpu-overcommitment","title":"CPU Overcommitment","text":"<p>Shared-Core Machine Types:</p> <ul> <li>E2-micro, E2-small, E2-medium</li> <li>Burst capability above baseline</li> <li>Suitable for low-utilization workloads</li> <li>Cost-effective for development</li> </ul> <p>Standard and Higher:</p> <ul> <li>Dedicated vCPU allocation</li> <li>Consistent performance</li> <li>Production workloads</li> <li>Predictable behavior</li> </ul>"},{"location":"gcp/compute-engine/vms/#compliance-and-governance","title":"Compliance and Governance","text":""},{"location":"gcp/compute-engine/vms/#data-residency","title":"Data Residency","text":"<p>Control Mechanisms:</p> <ul> <li>Choose region for VM placement</li> <li>Boot disk in same region</li> <li>Snapshots can specify location</li> <li>Organization policy constraints</li> </ul>"},{"location":"gcp/compute-engine/vms/#labeling-strategy","title":"Labeling Strategy","text":"<p>Purpose:</p> <ul> <li>Cost allocation and tracking</li> <li>Resource organization</li> <li>Automation selection</li> <li>Compliance tagging</li> </ul> <p>Recommended Labels:</p> <ul> <li>environment (prod, staging, dev)</li> <li>team/cost-center</li> <li>application/service</li> <li>compliance-level</li> <li>data-classification</li> </ul>"},{"location":"gcp/compute-engine/vms/#exam-focus-areas","title":"Exam Focus Areas","text":""},{"location":"gcp/compute-engine/vms/#design-decisions","title":"Design Decisions","text":"<ul> <li>Machine type selection criteria</li> <li>When to use Spot VMs</li> <li>HA architecture patterns (single/multi-zone/region)</li> <li>Network design (internal vs external IPs)</li> </ul>"},{"location":"gcp/compute-engine/vms/#cost-optimization","title":"Cost Optimization","text":"<ul> <li>Right-sizing strategies</li> <li>Committed use vs on-demand</li> <li>Spot VM use cases and limitations</li> <li>Instance scheduling</li> </ul>"},{"location":"gcp/compute-engine/vms/#security","title":"Security","text":"<ul> <li>Service account best practices</li> <li>Shielded VMs vs Confidential VMs</li> <li>Network isolation patterns</li> <li>IAM role design</li> </ul>"},{"location":"gcp/compute-engine/vms/#scalability","title":"Scalability","text":"<ul> <li>MIG configuration</li> <li>Autoscaling policies</li> <li>Load balancing integration</li> <li>Regional distribution</li> </ul>"},{"location":"gcp/compute-engine/vms/#operations","title":"Operations","text":"<ul> <li>Update strategies (rolling, canary)</li> <li>Health check design</li> <li>Monitoring and alerting</li> <li>Startup/shutdown scripts</li> </ul>"},{"location":"gcp/databases/bigquery/","title":"BigQuery","text":""},{"location":"gcp/databases/bigquery/#core-concepts","title":"Core Concepts","text":"<p>BigQuery is a serverless, fully managed data warehouse for analytics at petabyte scale. SQL-based querying with automatic scaling and high performance.</p> <p>Key Principle: Designed for analytics (OLAP), not transactions (OLTP); pay for storage and queries, not infrastructure.</p>"},{"location":"gcp/databases/bigquery/#when-to-use-bigquery","title":"When to Use BigQuery","text":""},{"location":"gcp/databases/bigquery/#use-when","title":"\u2705 Use When","text":"<ul> <li>Analytics and reporting (data warehouse)</li> <li>Large-scale data analysis (terabytes to petabytes)</li> <li>Ad-hoc SQL queries</li> <li>Business intelligence and dashboards</li> <li>Log analysis at scale</li> <li>Machine learning on structured data</li> </ul>"},{"location":"gcp/databases/bigquery/#dont-use-when","title":"\u274c Don\u2019t Use When","text":"<ul> <li>Transactional workload (OLTP) \u2192 Cloud SQL, Spanner</li> <li>Low-latency reads/writes \u2192 Firestore, Bigtable</li> <li>Blob storage \u2192 Cloud Storage</li> <li>Small datasets (&lt;100 GB) \u2192 Cloud SQL may be cheaper</li> <li>Real-time streaming with millisecond latency \u2192 Bigtable</li> </ul>"},{"location":"gcp/databases/bigquery/#architecture","title":"Architecture","text":""},{"location":"gcp/databases/bigquery/#serverless","title":"Serverless","text":"<p>No infrastructure management:</p> <ul> <li>No servers to provision</li> <li>Automatic scaling</li> <li>No capacity planning</li> <li>Pay per query</li> </ul> <p>Separation of storage and compute:</p> <ul> <li>Storage: Pay for data stored</li> <li>Compute: Pay for queries executed</li> <li>Scale independently</li> </ul>"},{"location":"gcp/databases/bigquery/#data-organization","title":"Data Organization","text":"<p>Structure:</p> <pre><code>Project\n\u2514\u2500\u2500 Dataset (location, access control)\n    \u2514\u2500\u2500 Tables (schema, partitioning, clustering)\n        \u2514\u2500\u2500 Rows and columns\n</code></pre> <p>Dataset: Logical container, regional or multi-regional</p> <p>Table types:</p> <ul> <li>Native tables (managed by BigQuery)</li> <li>External tables (data in Cloud Storage/Drive/Bigtable)</li> <li>Views (virtual tables from queries)</li> <li>Materialized views (precomputed results)</li> </ul>"},{"location":"gcp/databases/bigquery/#performance-optimization","title":"Performance Optimization","text":""},{"location":"gcp/databases/bigquery/#partitioning","title":"Partitioning","text":"<p>Purpose: Divide large tables into smaller segments</p> <p>Types:</p> <ul> <li>Time-based (daily, hourly, monthly, yearly)</li> <li>Integer range</li> <li>Ingestion time</li> </ul> <p>Benefits:</p> <ul> <li>Faster queries (scan less data)</li> <li>Lower costs (process less data)</li> <li>Easier data management (delete old partitions)</li> </ul> <p>Example: Partition logs table by date, query only last 7 days</p>"},{"location":"gcp/databases/bigquery/#clustering","title":"Clustering","text":"<p>Purpose: Organize data within partitions by column values</p> <p>Benefits:</p> <ul> <li>Faster queries on clustered columns</li> <li>Lower costs (better pruning)</li> <li>Automatic (BigQuery manages)</li> </ul> <p>Use with: Up to 4 columns commonly queried together</p> <p>Pattern: Partition by date, cluster by user_id and region</p>"},{"location":"gcp/databases/bigquery/#query-optimization","title":"Query Optimization","text":"<p>Best practices:</p> <ul> <li>Select only needed columns (avoid <code>SELECT *</code>)</li> <li>Filter early (WHERE clause reduces data scanned)</li> <li>Use partitioned tables</li> <li>Avoid self-joins (use window functions)</li> <li>Use approximate aggregation (HyperLogLog for COUNT DISTINCT)</li> </ul> <p>Explanation tabs: Shows query execution plan, bytes processed</p>"},{"location":"gcp/databases/bigquery/#storage-options","title":"Storage Options","text":""},{"location":"gcp/databases/bigquery/#active-storage","title":"Active Storage","text":"<p>Default: Standard storage</p> <p>Price: ~$0.020/GB/month (first 10 GB free)</p> <p>Use for: Frequently queried data</p>"},{"location":"gcp/databases/bigquery/#long-term-storage","title":"Long-Term Storage","text":"<p>Automatic: Tables not edited for 90 days</p> <p>Price: ~$0.010/GB/month (50% discount)</p> <p>Benefit: Cost savings for archival data</p> <p>Important: Query pricing same regardless of storage type</p>"},{"location":"gcp/databases/bigquery/#data-loading","title":"Data Loading","text":""},{"location":"gcp/databases/bigquery/#batch-loading","title":"Batch Loading","text":"<p>Methods:</p> <ul> <li>Load jobs (Cloud Storage, local files)</li> <li>SQL INSERT statements</li> <li>Dataflow pipelines</li> <li>Third-party ETL tools</li> </ul> <p>Free: Batch loading is free</p> <p>Use for: Large bulk loads, scheduled ETL</p>"},{"location":"gcp/databases/bigquery/#streaming-inserts","title":"Streaming Inserts","text":"<p>Purpose: Real-time data ingestion</p> <p>Latency: Available for query immediately</p> <p>Cost: $0.010 per 200 MB (higher than batch)</p> <p>Limits: 100,000 rows/sec per table</p> <p>Use for: Real-time dashboards, log streaming</p> <p>Alternative: Storage Write API (cheaper, better performance)</p>"},{"location":"gcp/databases/bigquery/#data-export","title":"Data Export","text":"<p>Destinations:</p> <ul> <li>Cloud Storage (CSV, JSON, Avro, Parquet)</li> <li>Other BigQuery tables</li> <li>Google Sheets (small datasets)</li> </ul> <p>Limits: 1 GB per file (use wildcards for larger)</p> <p>Cost: Export is free, storage charges apply</p>"},{"location":"gcp/databases/bigquery/#access-control","title":"Access Control","text":""},{"location":"gcp/databases/bigquery/#dataset-level","title":"Dataset-Level","text":"<p>IAM Roles:</p> <ul> <li><code>bigquery.dataViewer</code>: Read data and metadata</li> <li><code>bigquery.dataEditor</code>: Create, update, delete tables</li> <li><code>bigquery.dataOwner</code>: Full control including permissions</li> </ul> <p>Scope: All tables in dataset</p>"},{"location":"gcp/databases/bigquery/#table-level","title":"Table-Level","text":"<p>Authorized views: Allow access to specific views, not underlying tables</p> <p>Use case: Row-level security, column masking</p>"},{"location":"gcp/databases/bigquery/#row-level-security","title":"Row-Level Security","text":"<p>Purpose: Filter rows based on user identity</p> <p>Implementation: Create row-level policies</p> <p>Example: Users only see their own organization\u2019s data</p>"},{"location":"gcp/databases/bigquery/#data-loss-prevention-dlp-api","title":"Data Loss Prevention (DLP) API","text":""},{"location":"gcp/databases/bigquery/#purpose","title":"Purpose","text":"<p>Discover, classify, and protect sensitive data in BigQuery (PII, PHI, financial data).</p>"},{"location":"gcp/databases/bigquery/#capabilities","title":"Capabilities","text":"<p>Discovery:</p> <ul> <li>Scan datasets for sensitive data</li> <li>Identify PII (SSN, credit cards, emails, phone numbers)</li> <li>Custom info types (regex patterns)</li> </ul> <p>Classification:</p> <ul> <li>Likelihood scores (very likely, likely, possible, unlikely)</li> <li>Info types (PERSON_NAME, CREDIT_CARD_NUMBER, etc.)</li> <li>Statistical analysis</li> </ul> <p>De-identification:</p> <ul> <li>Masking (replace with *****)</li> <li>Tokenization (consistent hash)</li> <li>Date shifting (preserve relative dates)</li> <li>Generalization (age \u2192 age range)</li> </ul>"},{"location":"gcp/databases/bigquery/#common-patterns","title":"Common Patterns","text":"<p>PII Discovery:</p> <pre><code>DLP API scans BigQuery table \u2192 Identifies sensitive columns \u2192 Generate report\n</code></pre> <p>De-identification Pipeline:</p> <pre><code>Source table \u2192 DLP API (de-identify) \u2192 Destination table (masked data)\n</code></pre> <p>Continuous Monitoring:</p> <pre><code>Cloud Functions + DLP API \u2192 Scan new data \u2192 Alert if PII found\n</code></pre>"},{"location":"gcp/databases/bigquery/#use-cases","title":"Use Cases","text":"<ul> <li>GDPR compliance (identify PII)</li> <li>HIPAA compliance (protect PHI)</li> <li>PCI-DSS (find credit card data)</li> <li>Data lake governance</li> <li>Safe analytics on sensitive data</li> </ul>"},{"location":"gcp/databases/bigquery/#integration","title":"Integration","text":"<p>Automatic: Scan tables directly via API</p> <p>Patterns:</p> <ul> <li>Scheduled scans (Cloud Scheduler + Cloud Functions)</li> <li>Pre-export scanning (ensure no PII exported)</li> <li>CI/CD validation (check for accidental PII)</li> </ul>"},{"location":"gcp/databases/bigquery/#cost","title":"Cost","text":"<p>DLP API pricing: Per GB scanned</p> <p>Optimization:</p> <ul> <li>Scan samples, not entire tables</li> <li>Use column-level scans</li> <li>Cache results, don\u2019t rescan unchanged data</li> </ul>"},{"location":"gcp/databases/bigquery/#bigquery-ml","title":"BigQuery ML","text":"<p>Purpose: Build and deploy ML models using SQL</p> <p>Supported models:</p> <ul> <li>Linear regression</li> <li>Logistic regression</li> <li>K-means clustering</li> <li>Time series forecasting</li> <li>Deep neural networks</li> <li>Import TensorFlow models</li> </ul> <p>Benefits: No data export, SQL-based, integrated</p> <p>Use case: Predictions on BigQuery data without separate ML platform</p>"},{"location":"gcp/databases/bigquery/#federated-queries","title":"Federated Queries","text":""},{"location":"gcp/databases/bigquery/#external-data-sources","title":"External Data Sources","text":"<p>Supported:</p> <ul> <li>Cloud Storage (CSV, JSON, Avro, Parquet, ORC)</li> <li>Cloud Bigtable</li> <li>Cloud SQL (read-only)</li> <li>Google Sheets</li> </ul> <p>Use cases:</p> <ul> <li>Query data without loading</li> <li>Join BigQuery with external data</li> <li>One-time analysis</li> <li>Data lake pattern (data in Storage, query in BigQuery)</li> </ul> <p>Limitations:</p> <ul> <li>Slower than native tables</li> <li>No caching</li> <li>Subject to external source performance</li> </ul>"},{"location":"gcp/databases/bigquery/#bi-engine","title":"BI Engine","text":"<p>Purpose: In-memory analysis for fast dashboards</p> <p>Benefits:</p> <ul> <li>Sub-second query response</li> <li>Interactive dashboards</li> <li>No query charges for cached data</li> <li>Automatic caching</li> </ul> <p>Capacity: Pay for memory reservation (per GB/hour)</p> <p>Use for: Frequently accessed dashboards, real-time analytics</p>"},{"location":"gcp/databases/bigquery/#high-availability","title":"High Availability","text":""},{"location":"gcp/databases/bigquery/#automatic","title":"Automatic","text":"<p>Built-in: Multi-zone replication within region</p> <p>No configuration needed: Highly available by default</p> <p>SLA: 99.99% monthly uptime</p>"},{"location":"gcp/databases/bigquery/#multi-region","title":"Multi-Region","text":"<p>Datasets: US, EU (multi-region) or specific regions</p> <p>Benefits:</p> <ul> <li>Geographic redundancy</li> <li>Lower latency for global users</li> <li>Higher availability</li> </ul> <p>Limitation: Cannot change location after creation</p>"},{"location":"gcp/databases/bigquery/#disaster-recovery","title":"Disaster Recovery","text":""},{"location":"gcp/databases/bigquery/#backup-strategy","title":"Backup Strategy","text":"<p>Table snapshots: Point-in-time copies</p> <p>Table clones: Lightweight copies (copy-on-write)</p> <p>Cross-region copy: Export and re-import in another region</p> <p>Time travel: Query data from up to 7 days ago</p> <p>Pattern: Periodic exports to Cloud Storage in multi-region</p>"},{"location":"gcp/databases/bigquery/#time-travel","title":"Time Travel","text":"<p>Purpose: Query historical data without explicit backups</p> <p>Duration: 7 days default (configurable 2-7 days)</p> <p>Use cases:</p> <ul> <li>Recover accidentally deleted/modified data</li> <li>Compare data over time</li> <li>Audit changes</li> </ul> <p>Query syntax: <code>FOR SYSTEM_TIME AS OF timestamp</code></p>"},{"location":"gcp/databases/bigquery/#deleted-table-recovery","title":"Deleted Table Recovery","text":"<p>Duration: Restore deleted tables within 7 days</p> <p>Limitation: Dataset deletion is permanent (cannot recover)</p>"},{"location":"gcp/databases/bigquery/#scalability","title":"Scalability","text":""},{"location":"gcp/databases/bigquery/#automatic-scaling","title":"Automatic Scaling","text":"<p>Query processing: Thousands of concurrent queries</p> <p>Storage: Exabyte scale</p> <p>No tuning: Automatic resource allocation</p> <p>Slots: Units of computational capacity (automatic or reserved)</p>"},{"location":"gcp/databases/bigquery/#reservation-and-slots","title":"Reservation and Slots","text":"<p>On-demand (default): Pay per query, automatic capacity</p> <p>Flat-rate pricing: Reserve slots, predictable cost</p> <p>Flex slots: Commit for 60 seconds minimum</p> <p>Use reserved when: Predictable workload, &gt;$10K/month queries</p>"},{"location":"gcp/databases/bigquery/#cost-optimization","title":"Cost Optimization","text":""},{"location":"gcp/databases/bigquery/#query-costs","title":"Query Costs","text":"<p>Pricing: $5 per TB processed (first 1 TB/month free)</p> <p>Optimization:</p> <ul> <li>Partition and cluster tables</li> <li>Select specific columns (not *)</li> <li>Use table preview (free)</li> <li>Cache query results (24 hours free)</li> <li>Approximate aggregation functions</li> <li>Streaming buffer charges</li> </ul>"},{"location":"gcp/databases/bigquery/#storage-costs","title":"Storage Costs","text":"<p>Active: $0.020/GB/month Long-term (90+ days): $0.010/GB/month (automatic)</p> <p>Optimization:</p> <ul> <li>Delete unused tables</li> <li>Use table expiration</li> <li>Export to Cloud Storage (cheaper long-term)</li> <li>Partition pruning</li> </ul>"},{"location":"gcp/databases/bigquery/#monitoring","title":"Monitoring","text":"<p>Metrics:</p> <ul> <li>Bytes scanned per query</li> <li>Slot utilization (reserved pricing)</li> <li>Storage trends</li> </ul> <p>Set budgets: Alert when costs exceed threshold</p>"},{"location":"gcp/databases/bigquery/#security","title":"Security","text":""},{"location":"gcp/databases/bigquery/#encryption","title":"Encryption","text":"<p>At rest: Automatic (Google-managed or CMEK)</p> <p>In transit: TLS by default</p> <p>Column-level encryption: Application-level before load</p>"},{"location":"gcp/databases/bigquery/#access-logging","title":"Access Logging","text":"<p>Audit logs:</p> <ul> <li>Admin Activity (dataset/table changes)</li> <li>Data Access (must enable, query logs)</li> </ul> <p>Use for: Compliance, security investigations</p>"},{"location":"gcp/databases/bigquery/#vpc-service-controls","title":"VPC Service Controls","text":"<p>Purpose: Prevent data exfiltration</p> <p>Pattern: BigQuery inside perimeter, cannot export outside</p>"},{"location":"gcp/databases/bigquery/#best-practices","title":"Best Practices","text":""},{"location":"gcp/databases/bigquery/#schema-design","title":"Schema Design","text":"<ul> <li>Denormalize for performance (avoid JOINs)</li> <li>Use nested/repeated fields (STRUCT, ARRAY)</li> <li>Partition by date/timestamp</li> <li>Cluster by high-cardinality columns</li> </ul>"},{"location":"gcp/databases/bigquery/#query-patterns","title":"Query Patterns","text":"<ul> <li>Filter early in WHERE clause</li> <li>Avoid SELECT * in production</li> <li>Use materialized views for common aggregations</li> <li>Sample large datasets for exploration (TABLESAMPLE)</li> </ul>"},{"location":"gcp/databases/bigquery/#data-lifecycle","title":"Data Lifecycle","text":"<ul> <li>Set table expiration for temporary data</li> <li>Archive to Cloud Storage for long-term</li> <li>Use partitioned tables for time-series</li> <li>Enable time travel for 7 days</li> </ul>"},{"location":"gcp/databases/bigquery/#cost-management","title":"Cost Management","text":"<ul> <li>Estimate before running (dry run)</li> <li>Use query validator</li> <li>Set maximum bytes billed</li> <li>Monitor spending with billing exports</li> </ul>"},{"location":"gcp/databases/bigquery/#exam-focus","title":"Exam Focus","text":""},{"location":"gcp/databases/bigquery/#core-concepts_1","title":"Core Concepts","text":"<ul> <li>Serverless data warehouse (no infrastructure)</li> <li>Separation of storage and compute</li> <li>Analytics (OLAP), not transactions (OLTP)</li> <li>Pay per query (on-demand) or slots (reserved)</li> </ul>"},{"location":"gcp/databases/bigquery/#use-cases_1","title":"Use Cases","text":"<ul> <li>Data warehouse and BI</li> <li>Log analysis at scale</li> <li>Ad-hoc analytics</li> <li>BigQuery ML for predictions</li> <li>NOT for: OLTP, low-latency K/V, small databases</li> </ul>"},{"location":"gcp/databases/bigquery/#performance","title":"Performance","text":"<ul> <li>Partitioning (reduce data scanned)</li> <li>Clustering (faster queries)</li> <li>Materialized views (precompute)</li> <li>Query optimization patterns</li> </ul>"},{"location":"gcp/databases/bigquery/#dlp-api","title":"DLP API","text":"<ul> <li>Discover sensitive data (PII, PHI)</li> <li>De-identification techniques</li> <li>Compliance (GDPR, HIPAA, PCI-DSS)</li> <li>Automated scanning patterns</li> </ul>"},{"location":"gcp/databases/bigquery/#cost-optimization_1","title":"Cost Optimization","text":"<ul> <li>Partition and cluster</li> <li>Avoid SELECT *</li> <li>Use caching</li> <li>Long-term storage (automatic after 90 days)</li> <li>Reserved slots for predictable workload</li> </ul>"},{"location":"gcp/databases/bigquery/#integration_1","title":"Integration","text":"<ul> <li>Federated queries (Cloud Storage, Bigtable, Cloud SQL)</li> <li>Streaming inserts vs batch loading</li> <li>Export to Cloud Storage</li> <li>BigQuery ML</li> </ul>"},{"location":"gcp/databases/bigquery/#disaster-recovery_1","title":"Disaster Recovery","text":"<ul> <li>Time travel (7 days)</li> <li>Table snapshots</li> <li>Cross-region exports</li> <li>Deleted table recovery (7 days)</li> </ul>"},{"location":"gcp/databases/cloud-sql/","title":"Cloud SQL","text":""},{"location":"gcp/databases/cloud-sql/#core-concepts","title":"Core Concepts","text":"<p>Cloud SQL is a fully managed relational database service for MySQL, PostgreSQL, and SQL Server. Managed backups, replication, failover, and scaling.</p> <p>Key Principle: Use for traditional relational workloads; Google manages infrastructure, you manage schema and queries.</p>"},{"location":"gcp/databases/cloud-sql/#cloud-sql-engines","title":"Cloud SQL Engines","text":"Engine Versions Use Case Max Size MySQL 5.6, 5.7, 8.0 Web apps, general purpose 64 TB PostgreSQL 9.6-16 Advanced SQL, compliance 64 TB SQL Server 2017, 2019, 2022 Windows apps, legacy 64 TB"},{"location":"gcp/databases/cloud-sql/#1st-gen-vs-2nd-gen","title":"1<sup>st</sup> Gen vs 2<sup>nd</sup> Gen","text":""},{"location":"gcp/databases/cloud-sql/#1st-generation-mysql-5556-only","title":"1<sup>st</sup> Generation (MySQL 5.5/5.6 only)","text":"<p>Status: Deprecated, legacy only</p> <p>Limitations:</p> <ul> <li>Max 500 GB storage</li> <li>No automatic storage increase</li> <li>Limited performance</li> <li>No private IP</li> </ul> <p>Migration required: Google recommends migrating to 2<sup>nd</sup> gen</p>"},{"location":"gcp/databases/cloud-sql/#2nd-generation-current","title":"2<sup>nd</sup> Generation (Current)","text":"<p>All new instances: MySQL 5.\u215e.0, PostgreSQL, SQL Server</p> <p>Features:</p> <ul> <li>Up to 64 TB storage</li> <li>Automatic storage increase</li> <li>Better performance (up to 7x faster)</li> <li>Private IP support</li> <li>Point-in-time recovery</li> <li>High availability configuration</li> </ul> <p>Recommendation: Always use 2<sup>nd</sup> gen for new deployments</p>"},{"location":"gcp/databases/cloud-sql/#when-to-use-cloud-sql","title":"When to Use Cloud SQL","text":""},{"location":"gcp/databases/cloud-sql/#use-when","title":"\u2705 Use When","text":"<ul> <li>Relational data with ACID requirements</li> <li>Existing MySQL/PostgreSQL/SQL Server app</li> <li>Structured data with relationships</li> <li>Transactional workloads (OLTP)</li> <li>Need SQL features (JOINs, transactions, constraints)</li> <li>Migrating from on-premises databases</li> </ul>"},{"location":"gcp/databases/cloud-sql/#dont-use-when","title":"\u274c Don\u2019t Use When","text":"<ul> <li>Global scale needed \u2192 Cloud Spanner</li> <li>Analytics workload \u2192 BigQuery</li> <li>NoSQL better fit \u2192 Firestore, Bigtable</li> <li>Serverless preferred \u2192 Firestore</li> <li>Extremely high throughput \u2192 Bigtable</li> </ul>"},{"location":"gcp/databases/cloud-sql/#machine-types","title":"Machine Types","text":""},{"location":"gcp/databases/cloud-sql/#shared-core","title":"Shared-Core","text":"<p>Types: db-f1-micro (0.6 GB), db-g1-small (1.7 GB)</p> <p>Use for: Development, testing, very light workloads</p> <p>Limitations: Burstable CPU, not for production</p>"},{"location":"gcp/databases/cloud-sql/#dedicated-core","title":"Dedicated-Core","text":"<p>Types: Standard (up to 96 vCPUs, 624 GB RAM)</p> <p>Use for: Production workloads</p> <p>High-memory: More RAM per vCPU (db-custom-*)</p> <p>Pattern: Start small, scale up based on monitoring</p>"},{"location":"gcp/databases/cloud-sql/#storage","title":"Storage","text":""},{"location":"gcp/databases/cloud-sql/#types","title":"Types","text":"<p>SSD (default, recommended):</p> <ul> <li>Better performance</li> <li>$0.17/GB/month</li> </ul> <p>HDD (legacy):</p> <ul> <li>Slower, cheaper</li> <li>$0.09/GB/month</li> <li>Not recommended</li> </ul>"},{"location":"gcp/databases/cloud-sql/#automatic-storage-increase","title":"Automatic Storage Increase","text":"<p>Enabled by default: Automatically grows when approaching capacity</p> <p>Thresholds: Increases when &lt;10% free or &lt;6 GB free</p> <p>Benefits: Prevents downtime from full disk</p> <p>Cost: Pay for actual usage</p> <p>Recommendation: Always enable for production</p>"},{"location":"gcp/databases/cloud-sql/#high-availability-ha","title":"High Availability (HA)","text":""},{"location":"gcp/databases/cloud-sql/#regional-ha-configuration","title":"Regional HA Configuration","text":"<p>Architecture:</p> <pre><code>Primary instance (zone A)\n  \u2193 Synchronous replication\nStandby instance (zone B, same region)\n</code></pre> <p>Automatic failover: Minutes (typically &lt;60 seconds)</p> <p>RPO: Zero (synchronous replication)</p> <p>RTO: &lt; 1 minute (automatic)</p> <p>Cost: 2x instance cost + network</p>"},{"location":"gcp/databases/cloud-sql/#how-it-works","title":"How It Works","text":"<p>Normal operation: Primary handles all traffic</p> <p>Failure detection: Heartbeat monitoring</p> <p>Failover: Standby promoted to primary automatically</p> <p>Reconnection: Applications reconnect to same IP (unchanged)</p>"},{"location":"gcp/databases/cloud-sql/#configuration","title":"Configuration","text":"<p>Enable HA: At creation or after (requires restart)</p> <p>Important: Must use private IP or public IP with authorized networks</p> <p>SLA: 99.95% uptime with HA enabled</p>"},{"location":"gcp/databases/cloud-sql/#when-to-use-ha","title":"When to Use HA","text":"<p>\u2705 Production databases requiring high availability</p> <p>\u274c Development/test (save cost, use snapshots for recovery)</p> <p>Trade-off: 2x cost for better availability (RPO zero, RTO &lt;1 min)</p>"},{"location":"gcp/databases/cloud-sql/#backups","title":"Backups","text":""},{"location":"gcp/databases/cloud-sql/#automated-backups","title":"Automated Backups","text":"<p>Frequency: Daily, during maintenance window</p> <p>Retention: 7-365 days (default 7)</p> <p>Type: Full backup + transaction logs</p> <p>Location: Same region or custom location</p> <p>Cost: $0.08/GB/month (cheaper than disk)</p> <p>Important: Enabled by default, always enable for production</p>"},{"location":"gcp/databases/cloud-sql/#on-demand-backups","title":"On-Demand Backups","text":"<p>Manual: Create anytime</p> <p>Retention: Until manually deleted (no automatic expiration)</p> <p>Use for: Before major changes, migrations</p> <p>Cost: Same as automated ($0.08/GB/month)</p>"},{"location":"gcp/databases/cloud-sql/#point-in-time-recovery-pitr","title":"Point-in-Time Recovery (PITR)","text":"<p>Requires: Automated backups + binary logging enabled</p> <p>Capability: Restore to any point in time (within retention period)</p> <p>Granularity: Down to the second</p> <p>Use case: Recover from accidental DELETE/UPDATE</p> <p>Example: Accidentally deleted data at 2 PM, restore to 1:59 PM</p>"},{"location":"gcp/databases/cloud-sql/#backup-best-practices","title":"Backup Best Practices","text":"<ul> <li>Enable automated backups (production)</li> <li>30-day retention minimum (compliance may require more)</li> <li>Test restoration quarterly</li> <li>Cross-region backups for DR</li> <li>On-demand backup before schema changes</li> </ul>"},{"location":"gcp/databases/cloud-sql/#disaster-recovery","title":"Disaster Recovery","text":""},{"location":"gcp/databases/cloud-sql/#cross-region-dr-strategies","title":"Cross-Region DR Strategies","text":"<p>Backup and Restore:</p> <ul> <li>Export backups to Cloud Storage (multi-regional)</li> <li>Restore in DR region on disaster</li> <li>RPO: 24 hours (daily backup)</li> <li>RTO: 1-2 hours (restore time)</li> <li>Cost: Lowest (backup storage only)</li> </ul> <p>Read Replica in DR Region:</p> <ul> <li>Async replication to DR region</li> <li>Promote replica to standalone on disaster</li> <li>RPO: Minutes (replication lag)</li> <li>RTO: &lt; 30 minutes (manual promotion)</li> <li>Cost: Medium (replica instance cost)</li> </ul> <p>Managed Cross-Region Replica:</p> <ul> <li>Automatic async replication</li> <li>Manual failover</li> <li>Easier management than manual replica</li> </ul>"},{"location":"gcp/databases/cloud-sql/#failover-process","title":"Failover Process","text":"<p>Regional HA (automatic):</p> <ol> <li>Primary fails</li> <li>Standby promoted (seconds)</li> <li>Applications reconnect automatically</li> </ol> <p>Cross-Region (manual):</p> <ol> <li>Declare disaster</li> <li>Promote read replica to standalone</li> <li>Update DNS/connection strings</li> <li>Point applications to new region</li> </ol>"},{"location":"gcp/databases/cloud-sql/#dr-testing","title":"DR Testing","text":"<p>Important: Test DR procedures regularly</p> <p>Process:</p> <ol> <li>Create read replica in DR region</li> <li>Simulate failover (promote replica)</li> <li>Verify application connectivity</li> <li>Measure actual RTO</li> <li>Document gaps and update procedures</li> </ol>"},{"location":"gcp/databases/cloud-sql/#read-replicas","title":"Read Replicas","text":""},{"location":"gcp/databases/cloud-sql/#purpose","title":"Purpose","text":"<p>Read scaling: Offload read queries from primary</p> <p>Analytics: Run heavy queries without impacting production</p> <p>DR: Promote to standalone on disaster</p>"},{"location":"gcp/databases/cloud-sql/#architecture","title":"Architecture","text":"<pre><code>Primary (read/write)\n  \u2193 Async replication\nRead Replica 1 (read-only)\nRead Replica 2 (read-only)\n</code></pre> <p>Replication: Asynchronous (eventual consistency)</p> <p>Lag: Typically seconds, monitor replication lag</p>"},{"location":"gcp/databases/cloud-sql/#configuration_1","title":"Configuration","text":"<p>Location: Same region, different region, or external (on-premises)</p> <p>Max replicas: 10 per primary</p> <p>Cascading: Replica of replica (reduce load on primary)</p> <p>Failover: Manual promotion to standalone instance</p>"},{"location":"gcp/databases/cloud-sql/#use-cases","title":"Use Cases","text":"<p>Read scaling: Distribute read traffic across replicas</p> <p>Reporting: Analytics on replica (not primary)</p> <p>DR: Cross-region replica for disaster recovery</p> <p>Migration: Replicate to new region, then migrate</p>"},{"location":"gcp/databases/cloud-sql/#scalability","title":"Scalability","text":""},{"location":"gcp/databases/cloud-sql/#vertical-scaling","title":"Vertical Scaling","text":"<p>Machine type: Upgrade vCPUs and memory</p> <p>Process: Requires restart (brief downtime)</p> <p>Pattern: Start small, scale up based on CPU/memory metrics</p> <p>Downtime: Seconds to minutes (failover to HA standby if configured)</p>"},{"location":"gcp/databases/cloud-sql/#storage-scaling","title":"Storage Scaling","text":"<p>Automatic: Storage increases automatically (no downtime)</p> <p>Manual: Can increase anytime (no downtime)</p> <p>Cannot decrease: Storage size cannot shrink</p> <p>Max: 64 TB per instance</p>"},{"location":"gcp/databases/cloud-sql/#connection-scaling","title":"Connection Scaling","text":"<p>Max connections: Based on machine type</p> <p>Formula: ~4 connections per GB of RAM</p> <p>Pooling: Use connection pooling in application (PgBouncer, ProxySQL)</p> <p>Cloud SQL Proxy: Recommended for secure connections</p>"},{"location":"gcp/databases/cloud-sql/#horizontal-scaling-read-replicas","title":"Horizontal Scaling (Read Replicas)","text":"<p>Read traffic: Distribute across replicas</p> <p>Write traffic: Only to primary (cannot scale writes this way)</p> <p>Sharding: Application-level for write scaling</p> <p>Alternative: Cloud Spanner for horizontal write scaling</p>"},{"location":"gcp/databases/cloud-sql/#maintenance","title":"Maintenance","text":""},{"location":"gcp/databases/cloud-sql/#maintenance-windows","title":"Maintenance Windows","text":"<p>Purpose: Apply patches, updates</p> <p>Frequency: Weekly, monthly, or as needed</p> <p>Duration: Minutes (with HA, seamless failover)</p> <p>Configuration: Choose day and hour</p> <p>Deny maintenance: Defer up to 365 days (not recommended)</p>"},{"location":"gcp/databases/cloud-sql/#updates","title":"Updates","text":"<p>Minor versions: Automatic (during maintenance window)</p> <p>Major versions: Manual (e.g., MySQL 5.7 \u2192 8.0)</p> <p>Downtime: With HA, minimal during maintenance</p>"},{"location":"gcp/databases/cloud-sql/#connectivity","title":"Connectivity","text":""},{"location":"gcp/databases/cloud-sql/#public-ip","title":"Public IP","text":"<p>Use for: Development, external access</p> <p>Security: Authorized networks (IP allowlisting)</p> <p>Cost: No additional charge</p> <p>Limitation: Exposed to internet</p>"},{"location":"gcp/databases/cloud-sql/#private-ip","title":"Private IP","text":"<p>Recommended for production:</p> <ul> <li>VPC peering-based connection</li> <li>No public internet</li> <li>Better security</li> <li>VPC resources access directly</li> </ul> <p>Requires: VPC with allocated IP range</p>"},{"location":"gcp/databases/cloud-sql/#cloud-sql-proxy","title":"Cloud SQL Proxy","text":"<p>Purpose: Secure connection without whitelisting IPs</p> <p>Benefits:</p> <ul> <li>Automatic encryption</li> <li>IAM-based authentication</li> <li>No IP management</li> </ul> <p>Use for: Applications in GCP, development from workstation</p>"},{"location":"gcp/databases/cloud-sql/#monitoring-and-performance","title":"Monitoring and Performance","text":""},{"location":"gcp/databases/cloud-sql/#key-metrics","title":"Key Metrics","text":"<p>Database:</p> <ul> <li>CPU utilization</li> <li>Memory utilization</li> <li>Disk I/O (IOPS, throughput)</li> <li>Connection count</li> <li>Replication lag (replicas)</li> </ul> <p>Application:</p> <ul> <li>Query performance</li> <li>Slow query log</li> <li>Transaction rates</li> </ul>"},{"location":"gcp/databases/cloud-sql/#query-insights","title":"Query Insights","text":"<p>Purpose: Identify slow queries</p> <p>Features:</p> <ul> <li>Top queries by execution time</li> <li>Query execution plan</li> <li>Historical performance</li> </ul> <p>Use for: Performance optimization, index tuning</p>"},{"location":"gcp/databases/cloud-sql/#recommendations","title":"Recommendations","text":"<p>Automatic: Google suggests improvements</p> <p>Types:</p> <ul> <li>Machine type sizing</li> <li>Index creation</li> <li>Storage optimization</li> </ul>"},{"location":"gcp/databases/cloud-sql/#cost-optimization","title":"Cost Optimization","text":""},{"location":"gcp/databases/cloud-sql/#instance-right-sizing","title":"Instance Right-Sizing","text":"<p>Start small: db-n1-standard-1 for most apps</p> <p>Scale up: Based on actual usage, not estimates</p> <p>Recommendations: Use Google\u2019s sizing recommendations</p>"},{"location":"gcp/databases/cloud-sql/#ha-trade-off","title":"HA Trade-off","text":"<p>Cost: 2x instance cost</p> <p>Decision: Production (enable), dev/test (disable)</p>"},{"location":"gcp/databases/cloud-sql/#storage_1","title":"Storage","text":"<p>Auto-increase: Prevents over-provisioning</p> <p>Backups: Cheaper than live storage ($0.08 vs $0.17/GB)</p> <p>Cleanup: Delete old manual backups</p>"},{"location":"gcp/databases/cloud-sql/#read-replicas_1","title":"Read Replicas","text":"<p>Cost: Full instance cost per replica</p> <p>Use sparingly: Only for actual read scaling needs</p> <p>Alternative: Cache layer (Memorystore) for reads</p>"},{"location":"gcp/databases/cloud-sql/#migration","title":"Migration","text":""},{"location":"gcp/databases/cloud-sql/#database-migration-service","title":"Database Migration Service","text":"<p>Purpose: Minimal-downtime migration to Cloud SQL</p> <p>Supported sources:</p> <ul> <li>On-premises MySQL, PostgreSQL</li> <li>AWS RDS</li> <li>Cloud SQL to Cloud SQL</li> </ul> <p>Process:</p> <ol> <li>Create migration job</li> <li>Full data copy</li> <li>Continuous replication</li> <li>Promote when ready (cutover)</li> </ol> <p>Downtime: Minutes (cutover only)</p>"},{"location":"gcp/databases/cloud-sql/#manual-migration","title":"Manual Migration","text":"<p>Export/Import:</p> <ul> <li>mysqldump, pg_dump</li> <li>Import to Cloud SQL</li> <li>Downtime = export + import time</li> </ul> <p>Replication:</p> <ul> <li>Set up replication from source</li> <li>Switch over when caught up</li> <li>Minimal downtime</li> </ul>"},{"location":"gcp/databases/cloud-sql/#security","title":"Security","text":""},{"location":"gcp/databases/cloud-sql/#encryption","title":"Encryption","text":"<p>At rest: Automatic (Google-managed or CMEK)</p> <p>In transit: TLS enforced (can require SSL)</p>"},{"location":"gcp/databases/cloud-sql/#iam-database-authentication","title":"IAM Database Authentication","text":"<p>MySQL, PostgreSQL: Use IAM users instead of passwords</p> <p>Benefits:</p> <ul> <li>Centralized identity</li> <li>No password management</li> <li>Automatic credential rotation</li> </ul>"},{"location":"gcp/databases/cloud-sql/#best-practices","title":"Best Practices","text":"<ul> <li>Use private IP (production)</li> <li>Cloud SQL Proxy for secure access</li> <li>Least privilege database users</li> <li>Enable SSL/TLS enforcement</li> <li>Audit logging (must enable)</li> <li>No default users in production</li> </ul>"},{"location":"gcp/databases/cloud-sql/#exam-focus","title":"Exam Focus","text":""},{"location":"gcp/databases/cloud-sql/#core-concepts_1","title":"Core Concepts","text":"<ul> <li>Fully managed MySQL, PostgreSQL, SQL Server</li> <li>2<sup>nd</sup> gen (always use, not 1<sup>st</sup> gen)</li> <li>Regional service (HA within region)</li> <li>Automatic backups and maintenance</li> </ul>"},{"location":"gcp/databases/cloud-sql/#high-availability","title":"High Availability","text":"<ul> <li>Regional HA (synchronous replication, automatic failover)</li> <li>RPO: Zero, RTO: &lt;1 minute</li> <li>99.95% SLA with HA</li> <li>Cost: 2x instance</li> </ul>"},{"location":"gcp/databases/cloud-sql/#disaster-recovery_1","title":"Disaster Recovery","text":"<ul> <li>Point-in-time recovery (PITR)</li> <li>Cross-region read replicas</li> <li>Backup export to Cloud Storage</li> <li>Manual failover to replica</li> </ul>"},{"location":"gcp/databases/cloud-sql/#scalability_1","title":"Scalability","text":"<ul> <li>Vertical: Upgrade machine type (brief downtime)</li> <li>Storage: Automatic increase (no downtime)</li> <li>Read scaling: Read replicas (async)</li> <li>Write scaling: Not supported (use Spanner)</li> </ul>"},{"location":"gcp/databases/cloud-sql/#backups_1","title":"Backups","text":"<ul> <li>Automated daily backups (7-365 day retention)</li> <li>On-demand backups (manual deletion)</li> <li>PITR requires automated backups + binary logging</li> <li>Test restoration regularly</li> </ul>"},{"location":"gcp/databases/cloud-sql/#use-cases_1","title":"Use Cases","text":"<ul> <li>Traditional relational apps</li> <li>OLTP workloads</li> <li>Migrating from on-premises</li> <li>NOT for: Global scale (Spanner), Analytics (BigQuery)</li> </ul>"},{"location":"gcp/databases/cloud-sql/#cost-optimization_1","title":"Cost Optimization","text":"<ul> <li>Right-size machine type</li> <li>HA only for production</li> <li>Use backups (cheaper than storage)</li> <li>Auto-increase storage</li> <li>Delete unused replicas/backups</li> </ul>"},{"location":"gcp/databases/gcp-databases/","title":"GCP Databases","text":""},{"location":"gcp/databases/gcp-databases/#core-concepts","title":"Core Concepts","text":"<p>GCP offers multiple database services, each optimized for specific use cases. Selecting the right database is critical for performance, cost, and scalability.</p> <p>Key Principle: No one-size-fits-all; match database to workload characteristics.</p>"},{"location":"gcp/databases/gcp-databases/#database-decision-tree","title":"Database Decision Tree","text":"<pre><code>Need SQL?\n\u251c\u2500 Yes \u2192 Transactions needed?\n\u2502   \u251c\u2500 Yes \u2192 Global scale?\n\u2502   \u2502   \u251c\u2500 Yes \u2192 Cloud Spanner\n\u2502   \u2502   \u2514\u2500 No \u2192 Cloud SQL\n\u2502   \u2514\u2500 No (Analytics) \u2192 BigQuery\n\u2502\n\u2514\u2500 No \u2192 Access pattern?\n    \u251c\u2500 Key-value, low latency \u2192 Memorystore (cache) or Firestore\n    \u251c\u2500 Wide-column, high throughput \u2192 Bigtable\n    \u2514\u2500 Document, serverless \u2192 Firestore\n</code></pre>"},{"location":"gcp/databases/gcp-databases/#database-comparison","title":"Database Comparison","text":"Database Type Use Case Max Scale Latency Cloud SQL Relational (SQL) OLTP, regional 64 TB Low Cloud Spanner Relational (SQL) OLTP, global Petabytes Low BigQuery Data warehouse Analytics (OLAP) Exabytes Seconds Firestore Document NoSQL Mobile, web apps Terabytes Low Bigtable Wide-column NoSQL Time-series, IoT Petabytes &lt;10ms Memorystore In-memory cache Cache layer 300 GB &lt;1ms"},{"location":"gcp/databases/gcp-databases/#cloud-spanner","title":"Cloud Spanner","text":""},{"location":"gcp/databases/gcp-databases/#overview","title":"Overview","text":"<p>Globally distributed, horizontally scalable, strongly consistent relational database with SQL support.</p> <p>Key features: ACID transactions, global replication, automatic sharding</p>"},{"location":"gcp/databases/gcp-databases/#when-to-use","title":"When to Use","text":"<p>\u2705 Appropriate for:</p> <ul> <li>Need global scale + SQL + strong consistency</li> <li>Multi-region application with low latency globally</li> <li>Outgrew Cloud SQL (need horizontal write scaling)</li> <li>Financial systems (consistency critical)</li> <li>Inventory systems (global consistency)</li> </ul> <p>\u274c Don\u2019t use when:</p> <ul> <li>Regional only (Cloud SQL cheaper)</li> <li>Analytics workload (BigQuery better)</li> <li>Don\u2019t need ACID transactions (Firestore or Bigtable cheaper)</li> </ul>"},{"location":"gcp/databases/gcp-databases/#architecture","title":"Architecture","text":"<p>Regional: Data in 3 zones within region</p> <p>Multi-region: Data across multiple regions (dual-region or multi-region)</p> <p>Replication: Synchronous across zones, async across regions</p>"},{"location":"gcp/databases/gcp-databases/#scalability","title":"Scalability","text":"<p>Horizontal: Add nodes for more throughput</p> <p>Writes: Scale linearly with nodes</p> <p>Reads: Can add read replicas</p> <p>Pattern: Start with 3 nodes (minimum for production), scale based on CPU</p>"},{"location":"gcp/databases/gcp-databases/#ha-and-dr","title":"HA and DR","text":"<p>HA: 99.99% (regional), 99.999% (multi-region)</p> <p>DR: Multi-region deployment, automatic failover</p> <p>RPO: Zero (synchronous within region)</p> <p>RTO: Automatic failover (seconds)</p>"},{"location":"gcp/databases/gcp-databases/#cost","title":"Cost","text":"<p>Expensive: $0.90/node/hour (regional), $3/node/hour (multi-region)</p> <p>Trade-off: Global scale + strong consistency = high cost</p> <p>Use when: Features justify cost (global scale needed)</p>"},{"location":"gcp/databases/gcp-databases/#firestore","title":"Firestore","text":""},{"location":"gcp/databases/gcp-databases/#overview_1","title":"Overview","text":"<p>Serverless NoSQL document database for mobile and web applications. Real-time synchronization and offline support.</p> <p>Key features: Real-time listeners, offline support, automatic scaling</p>"},{"location":"gcp/databases/gcp-databases/#when-to-use_1","title":"When to Use","text":"<p>\u2705 Appropriate for:</p> <ul> <li>Mobile/web applications</li> <li>Real-time data sync across clients</li> <li>Serverless backend</li> <li>Document-oriented data (JSON-like)</li> <li>Need offline support</li> </ul> <p>\u274c Don\u2019t use when:</p> <ul> <li>Complex queries/analytics (BigQuery)</li> <li>High-throughput writes (Bigtable)</li> <li>Strong ACID transactions (Spanner)</li> <li>Relational data (Cloud SQL)</li> </ul>"},{"location":"gcp/databases/gcp-databases/#modes","title":"Modes","text":"<p>Native mode (recommended):</p> <ul> <li>Real-time, offline, mobile SDKs</li> <li>Strong consistency</li> <li>Complex queries</li> <li>Auto-scaling</li> </ul> <p>Datastore mode (legacy):</p> <ul> <li>Server SDKs only</li> <li>Eventual consistency</li> <li>Limited query capabilities</li> <li>Use Native for new apps</li> </ul>"},{"location":"gcp/databases/gcp-databases/#scalability_1","title":"Scalability","text":"<p>Automatic: Scales automatically based on load</p> <p>Limits:</p> <ul> <li>1 write/sec per document</li> <li>10,000 writes/sec per database (soft limit)</li> </ul> <p>Pattern: Distribute writes across documents (avoid hotspots)</p>"},{"location":"gcp/databases/gcp-databases/#ha-and-dr_1","title":"HA and DR","text":"<p>Multi-region: Automatically replicated</p> <p>SLA: 99.999% (multi-region)</p> <p>Backups: Export to Cloud Storage (manual or scheduled)</p> <p>PITR: Not native (use exports)</p>"},{"location":"gcp/databases/gcp-databases/#cost_1","title":"Cost","text":"<p>Pay per: Reads, writes, deletes, storage</p> <p>Free tier: 1 GB storage, 50K reads, 20K writes/day</p> <p>Cheaper than: Cloud SQL for small, variable workloads</p>"},{"location":"gcp/databases/gcp-databases/#bigtable","title":"Bigtable","text":""},{"location":"gcp/databases/gcp-databases/#overview_2","title":"Overview","text":"<p>Wide-column NoSQL database for high-throughput, low-latency workloads at massive scale. HBase-compatible.</p> <p>Key features: Petabyte scale, &lt;10ms latency, high throughput</p>"},{"location":"gcp/databases/gcp-databases/#when-to-use_2","title":"When to Use","text":"<p>\u2705 Appropriate for:</p> <ul> <li>Time-series data (IoT, metrics, logs)</li> <li>High throughput (&gt;100K QPS)</li> <li>Large analytical workloads (MapReduce, Dataflow)</li> <li>IoT data ingestion</li> <li>Financial data (tick data)</li> <li>Ad tech (real-time bidding)</li> </ul> <p>\u274c Don\u2019t use when:</p> <ul> <li>Small dataset (&lt;1 TB) or low throughput</li> <li>Need transactions (Spanner)</li> <li>Need SQL (Cloud SQL, BigQuery)</li> <li>Document model better (Firestore)</li> </ul>"},{"location":"gcp/databases/gcp-databases/#architecture_1","title":"Architecture","text":"<p>Row key: Single key, determines data distribution</p> <p>Column families: Group related columns</p> <p>Sparse: Not all rows have all columns</p> <p>Important: Row key design critical for performance (avoid hotspots)</p>"},{"location":"gcp/databases/gcp-databases/#scalability_2","title":"Scalability","text":"<p>Horizontal: Add nodes for more throughput</p> <p>Linear scaling: Throughput increases with nodes</p> <p>Minimum: 3 nodes production (1 node dev)</p> <p>Pattern: ~10K QPS per node</p>"},{"location":"gcp/databases/gcp-databases/#ha-and-dr_2","title":"HA and DR","text":"<p>Replication: Optional (cross-region or multi-cluster)</p> <p>Backups: Scheduled or on-demand</p> <p>Consistency: Eventual consistency (replication)</p> <p>Use case: Replication for HA and low-latency global access</p>"},{"location":"gcp/databases/gcp-databases/#performance","title":"Performance","text":"<p>Row key design: Critical for performance</p> <p>Good: Distributed (field#timestamp)</p> <p>Bad: Sequential (timestamp#field) \u2192 hotspots</p> <p>Latency: Single-digit milliseconds (with good row key)</p>"},{"location":"gcp/databases/gcp-databases/#cost_2","title":"Cost","text":"<p>Expensive: $0.65/node/hour + storage</p> <p>Storage: SSD ($0.17/GB) or HDD ($0.026/GB)</p> <p>Minimum: 3 nodes production = ~$1,400/month minimum</p> <p>Use when: Volume justifies cost (&gt;1 TB, high throughput)</p>"},{"location":"gcp/databases/gcp-databases/#memorystore","title":"Memorystore","text":""},{"location":"gcp/databases/gcp-databases/#overview_3","title":"Overview","text":"<p>Fully managed in-memory data store (Redis or Memcached). Ultra-low latency caching.</p> <p>Key features: &lt;1ms latency, Redis/Memcached compatible</p>"},{"location":"gcp/databases/gcp-databases/#when-to-use_3","title":"When to Use","text":"<p>\u2705 Appropriate for:</p> <ul> <li>Cache layer (reduce database load)</li> <li>Session storage</li> <li>Real-time analytics</li> <li>Leaderboards, counters</li> <li>Pub/Sub messaging (Redis)</li> </ul> <p>\u274c Don\u2019t use when:</p> <ul> <li>Persistent storage needed (data loss on restart)</li> <li>Primary database (use actual database)</li> <li>Large datasets (max 300 GB)</li> </ul>"},{"location":"gcp/databases/gcp-databases/#redis-vs-memcached","title":"Redis vs Memcached","text":"Feature Redis Memcached Data structures Rich (lists, sets, sorted sets) Simple (key-value) Persistence Optional snapshots None Replication Yes (HA) No (single node) Pub/Sub Yes No Use case Complex caching, sessions Simple cache <p>Recommendation: Redis for most use cases (more features)</p>"},{"location":"gcp/databases/gcp-databases/#tiers","title":"Tiers","text":"<p>Basic (Redis only):</p> <ul> <li>Single node</li> <li>No replication</li> <li>Cheaper</li> <li>Dev/test</li> </ul> <p>Standard (Redis only):</p> <ul> <li>HA with replication</li> <li>Automatic failover</li> <li>Production</li> </ul>"},{"location":"gcp/databases/gcp-databases/#ha-and-dr_3","title":"HA and DR","text":"<p>Standard tier: Automatic failover (zone redundant)</p> <p>Backups: Export RDB snapshots (Redis)</p> <p>Persistence: Optional (reduces performance)</p> <p>Recommendation: Use as cache (can rebuild), not primary store</p>"},{"location":"gcp/databases/gcp-databases/#scalability_3","title":"Scalability","text":"<p>Vertical: Up to 300 GB per instance</p> <p>Horizontal: Application-level sharding</p> <p>Read replicas: Standard tier supports read replicas</p>"},{"location":"gcp/databases/gcp-databases/#cost_3","title":"Cost","text":"<p>Pricing: Per GB RAM per hour</p> <p>Basic: ~$0.036/GB/hour (~$26/GB/month)</p> <p>Standard: ~$0.054/GB/hour (~$39/GB/month)</p> <p>Expensive: For memory size, use for cache only (high value)</p>"},{"location":"gcp/databases/gcp-databases/#selection-criteria","title":"Selection Criteria","text":""},{"location":"gcp/databases/gcp-databases/#by-workload-type","title":"By Workload Type","text":"<p>OLTP (Transactional):</p> <ul> <li>Regional: Cloud SQL</li> <li>Global: Cloud Spanner</li> <li>Serverless: Firestore</li> </ul> <p>OLAP (Analytics):</p> <ul> <li>BigQuery (always)</li> </ul> <p>Key-Value:</p> <ul> <li>Cache: Memorystore</li> <li>Persistent: Firestore</li> </ul> <p>Time-Series:</p> <ul> <li>Bigtable (high-throughput)</li> <li>BigQuery (analytics)</li> </ul> <p>Mobile/Web:</p> <ul> <li>Firestore (first choice)</li> <li>Cloud SQL (if relational needed)</li> </ul>"},{"location":"gcp/databases/gcp-databases/#by-scale-requirements","title":"By Scale Requirements","text":"<p>&lt;100 GB: Cloud SQL, Firestore</p> <p>100 GB - 10 TB: Cloud SQL, Firestore, BigQuery</p> <p>&gt;10 TB: BigQuery, Bigtable, Spanner</p> <p>Petabyte: BigQuery, Bigtable</p>"},{"location":"gcp/databases/gcp-databases/#by-consistency-requirements","title":"By Consistency Requirements","text":"<p>Strong consistency:</p> <ul> <li>Cloud SQL</li> <li>Cloud Spanner</li> <li>Firestore (Native mode)</li> </ul> <p>Eventual consistency acceptable:</p> <ul> <li>Bigtable</li> <li>Memorystore</li> <li>Firestore (Datastore mode)</li> </ul>"},{"location":"gcp/databases/gcp-databases/#by-latency-requirements","title":"By Latency Requirements","text":"<p>&lt;1ms: Memorystore</p> <p>&lt;10ms: Bigtable, Firestore, Cloud SQL (local)</p> <p>&lt;100ms: Cloud SQL, Spanner</p> <p>Seconds: BigQuery</p>"},{"location":"gcp/databases/gcp-databases/#by-cost-sensitivity","title":"By Cost Sensitivity","text":"<p>Cheapest to most expensive:</p> <ol> <li>Cloud SQL (small workloads)</li> <li>Firestore (serverless, pay per use)</li> <li>BigQuery (analytics, pay per query)</li> <li>Memorystore (cache)</li> <li>Bigtable (high minimum)</li> <li>Cloud Spanner (most expensive)</li> </ol>"},{"location":"gcp/databases/gcp-databases/#multi-database-patterns","title":"Multi-Database Patterns","text":""},{"location":"gcp/databases/gcp-databases/#lambda-architecture","title":"Lambda Architecture","text":"<pre><code>Real-time: Bigtable (hot data, recent)\nBatch: BigQuery (cold data, historical)\nServing: Cached in Memorystore\n</code></pre>"},{"location":"gcp/databases/gcp-databases/#cqrs-command-query-responsibility-segregation","title":"CQRS (Command Query Responsibility Segregation)","text":"<pre><code>Writes: Cloud SQL or Spanner\nReads: Firestore or Memorystore (materialized views)\nAnalytics: BigQuery (separate data warehouse)\n</code></pre>"},{"location":"gcp/databases/gcp-databases/#cache-aside-pattern","title":"Cache-Aside Pattern","text":"<pre><code>Application \u2192 Check Memorystore\n  \u251c\u2500 Hit \u2192 Return cached data\n  \u2514\u2500 Miss \u2192 Query Cloud SQL \u2192 Cache result \u2192 Return\n</code></pre>"},{"location":"gcp/databases/gcp-databases/#polyglot-persistence","title":"Polyglot Persistence","text":"<p>Pattern: Use best database for each use case</p> <p>Example:</p> <ul> <li>User profiles: Firestore (flexible schema)</li> <li>Transactions: Cloud SQL (ACID)</li> <li>Analytics: BigQuery (data warehouse)</li> <li>Session data: Memorystore (cache)</li> <li>Time-series: Bigtable (IoT metrics)</li> </ul>"},{"location":"gcp/databases/gcp-databases/#migration-paths","title":"Migration Paths","text":""},{"location":"gcp/databases/gcp-databases/#on-premises-cloud-sql","title":"On-Premises \u2192 Cloud SQL","text":"<p>Tools: Database Migration Service</p> <p>Use when: Lift-and-shift, minimal changes</p>"},{"location":"gcp/databases/gcp-databases/#mysqlpostgresql-cloud-spanner","title":"MySQL/PostgreSQL \u2192 Cloud Spanner","text":"<p>Requires: Schema changes (avoid hotspots)</p> <p>Use when: Need global scale</p>"},{"location":"gcp/databases/gcp-databases/#mongodb-firestore","title":"MongoDB \u2192 Firestore","text":"<p>Use when: Document model, need serverless</p>"},{"location":"gcp/databases/gcp-databases/#hbase-bigtable","title":"HBase \u2192 Bigtable","text":"<p>Tools: HBase replication</p> <p>Use when: High-throughput time-series</p>"},{"location":"gcp/databases/gcp-databases/#traditional-dw-bigquery","title":"Traditional DW \u2192 BigQuery","text":"<p>Tools: Dataflow, Transfer Service</p> <p>Use when: Analytics, data warehouse consolidation</p>"},{"location":"gcp/databases/gcp-databases/#exam-focus","title":"Exam Focus","text":""},{"location":"gcp/databases/gcp-databases/#decision-criteria","title":"Decision Criteria","text":"<ul> <li>Workload type (OLTP vs OLAP)</li> <li>Scale requirements (GB to PB)</li> <li>Consistency needs (strong vs eventual)</li> <li>Latency requirements</li> <li>Cost sensitivity</li> </ul>"},{"location":"gcp/databases/gcp-databases/#database-characteristics","title":"Database Characteristics","text":"<p>Cloud SQL: Regional SQL, OLTP, &lt;64 TB Spanner: Global SQL, strong consistency, expensive BigQuery: Analytics, petabyte scale, serverless Firestore: Document, serverless, mobile/web Bigtable: Wide-column, high-throughput, time-series Memorystore: Cache, &lt;1ms latency, not persistent</p>"},{"location":"gcp/databases/gcp-databases/#use-cases","title":"Use Cases","text":"<ul> <li>Global application \u2192 Spanner</li> <li>Analytics \u2192 BigQuery</li> <li>Mobile app \u2192 Firestore</li> <li>IoT time-series \u2192 Bigtable</li> <li>Cache layer \u2192 Memorystore</li> <li>Regional OLTP \u2192 Cloud SQL</li> </ul>"},{"location":"gcp/databases/gcp-databases/#ha-and-dr_4","title":"HA and DR","text":"<ul> <li>Cloud SQL: Regional HA, cross-region replicas</li> <li>Spanner: Multi-region, automatic failover</li> <li>BigQuery: Multi-region, time travel</li> <li>Firestore: Multi-region, exports</li> <li>Bigtable: Replication, backups</li> <li>Memorystore: Standard tier HA</li> </ul>"},{"location":"gcp/databases/gcp-databases/#cost_4","title":"Cost","text":"<ul> <li>Most expensive: Spanner, Bigtable (high minimum)</li> <li>Cheapest: Cloud SQL (small), Firestore (serverless)</li> <li>BigQuery: Pay per query (can be cheap or expensive)</li> <li>Memorystore: Expensive per GB (use for cache)</li> </ul>"},{"location":"gcp/databases/gcp-databases/#common-patterns","title":"Common Patterns","text":"<ul> <li>Cache-aside (Memorystore + database)</li> <li>CQRS (write/read separation)</li> <li>Polyglot persistence (best tool per use case)</li> <li>Lambda architecture (real-time + batch)</li> </ul>"},{"location":"gcp/kubernetes/cluster-types/","title":"GKE Cluster Types","text":""},{"location":"gcp/kubernetes/cluster-types/#description","title":"Description","text":"<p>GKE offers two distinct cluster modes of operation: Standard and Autopilot. Each provides different levels of control, management responsibility, and pricing models. Understanding the differences is crucial for choosing the right cluster type for your workload.</p>"},{"location":"gcp/kubernetes/cluster-types/#standard-gke-clusters","title":"Standard GKE Clusters","text":""},{"location":"gcp/kubernetes/cluster-types/#description_1","title":"Description","text":"<p>Standard GKE clusters give you complete control over cluster configuration and node management. You\u2019re responsible for configuring node pools, managing scaling, security, and updates while Google manages the control plane.</p> <p>Model: You manage nodes, Google manages control plane.</p>"},{"location":"gcp/kubernetes/cluster-types/#key-features","title":"Key Features","text":""},{"location":"gcp/kubernetes/cluster-types/#full-node-control","title":"Full Node Control","text":"<ul> <li>Custom Machine Types: Choose any Compute Engine machine type</li> <li>Node Customization: Configure boot disk, local SSDs, GPUs, taints, labels</li> <li>SSH Access: Direct SSH access to nodes for debugging</li> <li>Custom Images: Use custom node images if needed</li> <li>DaemonSets: Run privileged pods on all nodes</li> </ul>"},{"location":"gcp/kubernetes/cluster-types/#flexible-node-pools","title":"Flexible Node Pools","text":"<ul> <li>Multiple Node Pools: Create pools with different machine types</li> <li>Node Taints and Labels: Control pod scheduling</li> <li>Spot VMs: Use preemptible/spot VMs for cost savings</li> <li>Node Pool Management: Manual or automated scaling per pool</li> </ul>"},{"location":"gcp/kubernetes/cluster-types/#networking-options","title":"Networking Options","text":"<ul> <li>Routes-Based: Traditional Kubernetes networking</li> <li>VPC-Native: Alias IP ranges (recommended)</li> <li>Network Policies: Calico or GKE Dataplane V2</li> <li>Private Clusters: No public IPs on nodes</li> </ul>"},{"location":"gcp/kubernetes/cluster-types/#advanced-features","title":"Advanced Features","text":"<ul> <li>Windows Node Pools: Run Windows containers</li> <li>Multi-Cluster Ingress: Share load balancers across clusters</li> <li>Config Connector: Manage GCP resources as Kubernetes objects</li> <li>Istio/ASM: Full service mesh capabilities</li> </ul>"},{"location":"gcp/kubernetes/cluster-types/#important-limits","title":"Important Limits","text":"Limit Value Notes Nodes per cluster 15,000 Across all node pools Node pools per cluster 1,000 Different configurations Pods per node 110 (default), 256 (max) Via \u2013max-pods-per-node Pods per cluster 200,000 Theoretical maximum Services per cluster 10,000 LoadBalancer type limited"},{"location":"gcp/kubernetes/cluster-types/#when-to-use-standard-gke","title":"When to Use Standard GKE","text":"<p>\u2705 Use Standard When:</p> <ol> <li> <p>Custom Infrastructure Requirements</p> </li> <li> <p>Need specific machine types or custom configurations</p> </li> <li>Require GPU or TPU nodes</li> <li>Need local SSDs for high-performance storage</li> <li> <p>Custom kernel modules or system-level modifications</p> </li> <li> <p>Full Control Over Nodes</p> </li> <li> <p>Need SSH access to nodes for debugging</p> </li> <li>Want to run privileged pods or DaemonSets</li> <li>Require custom node images</li> <li> <p>Need to configure node-level security</p> </li> <li> <p>Windows Workloads</p> </li> <li> <p>Running Windows containers</p> </li> <li>Mixed Linux/Windows workloads</li> <li> <p>.NET Framework applications</p> </li> <li> <p>Cost Optimization with Spot VMs</p> </li> <li> <p>Fault-tolerant batch workloads</p> </li> <li>CI/CD pipelines</li> <li>Development/testing environments</li> <li> <p>Up to 91% cost savings acceptable with interruptions</p> </li> <li> <p>Complex Networking Requirements</p> </li> <li> <p>Multiple network interfaces</p> </li> <li>Custom CNI plugins</li> <li>Advanced network policies</li> <li>Specific IP address management</li> </ol> <p>\u274c Don\u2019t Use Standard When:</p> <ol> <li> <p>Want Minimal Management</p> </li> <li> <p>Team lacks Kubernetes operations expertise</p> </li> <li>Prefer hands-off infrastructure management</li> <li> <p>Don\u2019t want to manage node scaling/updates</p> </li> <li> <p>Unpredictable Workloads</p> </li> <li> <p>Highly variable traffic patterns</p> </li> <li>Sporadic batch jobs</li> <li> <p>Cost efficiency more important than control</p> </li> <li> <p>Simplicity is Priority</p> </li> <li> <p>Small team without dedicated platform engineers</p> </li> <li>Rapid prototyping and development</li> <li>Quick time-to-market needed</li> </ol>"},{"location":"gcp/kubernetes/cluster-types/#pricing","title":"Pricing","text":"<p>Standard GKE Costs:</p> <ul> <li>Control Plane: </li> <li>Zonal clusters: Free</li> <li> <p>Regional clusters: $0.10/hour</p> </li> <li> <p>Nodes: Standard Compute Engine pricing</p> </li> <li>e2-medium: ~$0.03/hour</li> <li> <p>Spot VMs: ~$0.008/hour (up to 91% discount)</p> </li> <li> <p>Networking: Egress charges apply</p> </li> </ul> <p>Cost Optimization:</p> <pre><code># Use Spot VMs (preemptible) for cost savings\ngcloud container node-pools create spot-pool \\\n  --cluster=my-cluster \\\n  --machine-type=e2-medium \\\n  --spot \\\n  --num-nodes=3\n\n# Enable cluster autoscaling\ngcloud container clusters update my-cluster \\\n  --enable-autoscaling \\\n  --min-nodes=1 \\\n  --max-nodes=10\n</code></pre>"},{"location":"gcp/kubernetes/cluster-types/#autopilot-gke-clusters","title":"Autopilot GKE Clusters","text":""},{"location":"gcp/kubernetes/cluster-types/#description_2","title":"Description","text":"<p>Autopilot is a fully managed GKE mode where Google manages the entire cluster infrastructure including nodes, node pools, scaling, security, and networking. You only configure and deploy your pods.</p> <p>Model: Google manages everything, you manage workloads.</p>"},{"location":"gcp/kubernetes/cluster-types/#key-features_1","title":"Key Features","text":""},{"location":"gcp/kubernetes/cluster-types/#fully-managed-infrastructure","title":"Fully Managed Infrastructure","text":"<ul> <li>No Node Management: Google provisions and scales nodes automatically</li> <li>Automatic Scaling: Nodes scale based on pod resource requests</li> <li>Hardened Security: Security best practices enforced by default</li> <li>Automatic Updates: Both control plane and nodes updated automatically</li> <li>Optimized Configuration: Google-optimized settings for performance and cost</li> </ul>"},{"location":"gcp/kubernetes/cluster-types/#simplified-operations","title":"Simplified Operations","text":"<ul> <li>No Node Pools: Infrastructure abstracted away</li> <li>Per-Pod Billing: Pay only for CPU and memory requested by pods</li> <li>Resource-Based Scaling: Nodes added/removed based on pod requests</li> <li>Hands-Off Upgrades: No maintenance windows or disruption management</li> </ul>"},{"location":"gcp/kubernetes/cluster-types/#built-in-security","title":"Built-In Security","text":"<ul> <li>Workload Identity: Enabled by default</li> <li>Shielded Nodes: All nodes use shielded GKE nodes</li> <li>Secure by Default: Security best practices enforced</li> <li>No SSH Access: Nodes are not directly accessible (improved security)</li> </ul>"},{"location":"gcp/kubernetes/cluster-types/#restrictions-for-security-and-optimization","title":"Restrictions (for Security and Optimization)","text":"<ul> <li>No Privileged Pods: Cannot run privileged containers</li> <li>No Host Network: Pods can\u2019t use host networking</li> <li>No DaemonSets: With node selectors (some exceptions apply)</li> <li>Predefined Pod Resources: Must specify CPU/memory requests</li> <li>No Windows Nodes: Linux only</li> </ul>"},{"location":"gcp/kubernetes/cluster-types/#important-limits_1","title":"Important Limits","text":"Limit Value Notes Pods per cluster Auto-managed Scales based on demand Pod CPU request 0.25 to 32 vCPU In 0.25 vCPU increments Pod memory request 0.5 to 128 GB Specific ratios to CPU Ephemeral storage Up to 10 GB included Per pod Persistent volumes Unlimited (quota-based) Standard limits apply Services (LoadBalancer) Auto-managed Google handles capacity"},{"location":"gcp/kubernetes/cluster-types/#pod-resource-classes","title":"Pod Resource Classes","text":"<p>Autopilot uses predefined CPU-to-memory ratios:</p> Class CPU:Memory Ratio Example General Purpose 1:4 GB 1 vCPU : 4 GB RAM Scale-Out 1:1 GB 1 vCPU : 1 GB RAM Balanced 1:2 GB 1 vCPU : 2 GB RAM Memory-Optimized 1:6.5 GB 1 vCPU : 6.5 GB RAM"},{"location":"gcp/kubernetes/cluster-types/#when-to-use-autopilot-gke","title":"When to Use Autopilot GKE","text":"<p>\u2705 Use Autopilot When:</p> <ol> <li> <p>Minimal Operational Overhead</p> </li> <li> <p>Small team or no dedicated platform engineers</p> </li> <li>Want Google to handle all infrastructure decisions</li> <li>Prefer hands-off cluster management</li> <li> <p>Focus on application deployment, not infrastructure</p> </li> <li> <p>Unpredictable or Variable Workloads</p> </li> <li> <p>Traffic patterns vary significantly</p> </li> <li>Batch jobs with sporadic execution</li> <li>Development and testing environments</li> <li> <p>Cost efficiency through automatic scaling</p> </li> <li> <p>Security is Critical</p> </li> <li> <p>Want hardened defaults</p> </li> <li>Need compliance with security baselines</li> <li>Prefer least-privilege by default</li> <li> <p>Don\u2019t need privileged containers</p> </li> <li> <p>Optimal Cost Management</p> </li> <li> <p>Pay only for what you use (per-pod resources)</p> </li> <li>No over-provisioning nodes</li> <li>Automatic right-sizing</li> <li> <p>Scale-to-zero capability</p> </li> <li> <p>Standard Kubernetes Workloads</p> </li> <li> <p>Stateless applications</p> </li> <li>Microservices</li> <li>HTTP services and APIs</li> <li>Standard containerized applications</li> </ol> <p>\u274c Don\u2019t Use Autopilot When:</p> <ol> <li> <p>Need Privileged Access</p> </li> <li> <p>Running privileged containers</p> </li> <li>DaemonSets with node selectors</li> <li>Host networking required</li> <li> <p>SSH access to nodes needed</p> </li> <li> <p>Custom Node Configuration</p> </li> <li> <p>Specific machine types required</p> </li> <li>GPUs or TPUs needed</li> <li>Local SSDs for storage</li> <li> <p>Custom node images</p> </li> <li> <p>Windows Workloads</p> </li> <li> <p>Running Windows containers</p> </li> <li>.NET Framework applications</li> <li> <p>Windows-specific requirements</p> </li> <li> <p>Special Network Requirements</p> </li> <li> <p>Custom CNI plugins</p> </li> <li>Multiple network interfaces</li> <li> <p>Non-standard networking configurations</p> </li> <li> <p>Very Stable, Predictable Workloads</p> </li> <li> <p>24/7 steady-state traffic</p> </li> <li>Reserved capacity might be cheaper</li> <li>Committed use discounts apply (Standard with CUDs may be cheaper)</li> </ol>"},{"location":"gcp/kubernetes/cluster-types/#pricing_1","title":"Pricing","text":"<p>Autopilot GKE Costs:</p> <ul> <li>vCPU: $0.0445/hour per vCPU requested</li> <li>Memory: $0.00488/hour per GB requested</li> <li>Control Plane: Included in pod pricing</li> <li>Networking: Egress charges apply</li> </ul> <p>Example Cost Calculation:</p> <pre><code>Pod Request: 1 vCPU, 4 GB RAM\nHourly Cost: (1 \u00d7 $0.0445) + (4 \u00d7 $0.00488) = $0.064/hour\nMonthly Cost: $0.064 \u00d7 730 hours = ~$46.72/month per pod\n</code></pre> <p>Cost Optimization:</p> <pre><code># Right-size pod resources - you pay for requests\napiVersion: v1\nkind: Pod\nmetadata:\n  name: my-app\nspec:\n  containers:\n\n  - name: app\n    image: my-app:latest\n    resources:\n      requests:\n        cpu: \"250m\"      # 0.25 vCPU\n        memory: \"512Mi\"  # 0.5 GB\n      limits:\n        cpu: \"1000m\"\n        memory: \"2Gi\"\n</code></pre>"},{"location":"gcp/kubernetes/cluster-types/#comparison-matrix","title":"Comparison Matrix","text":"Feature Standard GKE Autopilot GKE Node Management Manual configuration Fully automated Pricing Per node-hour Per pod resource request Scaling Configure autoscaling Automatic Node Pools Manual creation/config Not applicable Machine Types Full choice Google-optimized SSH to Nodes Yes No Privileged Pods Yes No DaemonSets Yes (unrestricted) Limited GPU/TPU Yes Limited GPU support Windows Nodes Yes No Local SSDs Yes No Control Plane Cost $0.10/hr (regional) Included Security Baseline Manual config Hardened by default Best For Custom requirements Simplicity, cost efficiency"},{"location":"gcp/kubernetes/cluster-types/#choosing-between-standard-and-autopilot","title":"Choosing Between Standard and Autopilot","text":""},{"location":"gcp/kubernetes/cluster-types/#decision-framework","title":"Decision Framework","text":"<pre><code>Start Here: Do you need Windows nodes, GPUs, or privileged containers?\n    \u2502\n    \u251c\u2500 Yes \u2192 Standard GKE\n    \u2502\n    \u2514\u2500 No\n        \u2502\n        Do you have dedicated platform/ops team?\n        \u2502\n        \u251c\u2500 Yes \u2192 Do you need custom node configuration?\n        \u2502   \u2502\n        \u2502   \u251c\u2500 Yes \u2192 Standard GKE\n        \u2502   \u2514\u2500 No \u2192 Autopilot GKE (less operational overhead)\n        \u2502\n        \u2514\u2500 No \u2192 Autopilot GKE (hands-off management)\n</code></pre>"},{"location":"gcp/kubernetes/cluster-types/#when-to-choose-standard","title":"When to Choose Standard","text":"<ul> <li>Full control over infrastructure</li> <li>Custom hardware requirements (GPU, TPU, local SSD)</li> <li>Windows workloads</li> <li>Privileged containers or DaemonSets</li> <li>Spot VMs for cost optimization</li> <li>Experienced Kubernetes operations team</li> </ul>"},{"location":"gcp/kubernetes/cluster-types/#when-to-choose-autopilot","title":"When to Choose Autopilot","text":"<ul> <li>Minimal operational overhead</li> <li>Pay-per-pod cost model</li> <li>Unpredictable or variable workloads</li> <li>Security hardening by default</li> <li>Small team or no dedicated ops</li> <li>Standard containerized applications</li> </ul>"},{"location":"gcp/kubernetes/cluster-types/#migration-considerations","title":"Migration Considerations","text":""},{"location":"gcp/kubernetes/cluster-types/#standard-to-autopilot","title":"Standard to Autopilot","text":"<p>Potential Issues:</p> <ul> <li>Privileged pods will be rejected</li> <li>DaemonSets with node selectors may not work</li> <li>Need to define resource requests/limits</li> <li>Custom node configurations lost</li> </ul> <p>Migration Path:</p> <ol> <li>Audit existing workloads for incompatibilities</li> <li>Add resource requests/limits to all pods</li> <li>Remove privileged security contexts</li> <li>Test in new Autopilot cluster</li> <li>Migrate workloads gradually</li> </ol>"},{"location":"gcp/kubernetes/cluster-types/#autopilot-to-standard","title":"Autopilot to Standard","text":"<p>Reasons to Switch:</p> <ul> <li>Need GPU/TPU support</li> <li>Require Windows nodes</li> <li>Want Spot VM cost savings</li> <li>Need privileged containers</li> </ul> <p>Migration Path:</p> <ol> <li>Create Standard cluster with similar configuration</li> <li>Configure node pools and autoscaling</li> <li>Migrate workloads</li> <li>Optimize node pool configuration</li> </ol>"},{"location":"gcp/kubernetes/cluster-types/#regional-vs-zonal-clusters","title":"Regional vs Zonal Clusters","text":"<p>Both Standard and Autopilot support regional and zonal deployments:</p>"},{"location":"gcp/kubernetes/cluster-types/#zonal-clusters","title":"Zonal Clusters","text":"<p>Characteristics:</p> <ul> <li>Control plane in single zone</li> <li>Nodes in single zone (Standard) or multi-zone (Standard with manual pools)</li> <li>Lower cost (free control plane for Standard)</li> <li>Lower SLA (no SLA for zonal)</li> </ul> <p>Use Cases:</p> <ul> <li>Development and testing</li> <li>Non-critical workloads</li> <li>Cost-sensitive applications</li> </ul>"},{"location":"gcp/kubernetes/cluster-types/#regional-clusters","title":"Regional Clusters","text":"<p>Characteristics:</p> <ul> <li>Control plane replicated across 3 zones</li> <li>Nodes distributed across zones</li> <li>Higher availability (99.95% SLA)</li> <li>Higher cost ($0.10/hour for Standard, included for Autopilot)</li> </ul> <p>Use Cases:</p> <ul> <li>Production workloads</li> <li>High-availability requirements</li> <li>Mission-critical applications</li> </ul>"},{"location":"gcp/kubernetes/cluster-types/#best-practices","title":"Best Practices","text":""},{"location":"gcp/kubernetes/cluster-types/#for-standard-clusters","title":"For Standard Clusters","text":"<ol> <li> <p>Use Multiple Node Pools</p> </li> <li> <p>Separate pools for different workload types</p> </li> <li>Production vs. batch workloads</li> <li> <p>Different machine types for different needs</p> </li> <li> <p>Enable Autoscaling</p> </li> <li> <p>Configure cluster autoscaler</p> </li> <li>Set appropriate min/max nodes</li> <li> <p>Use node affinity for workload placement</p> </li> <li> <p>Use Spot VMs Wisely</p> </li> <li> <p>Only for fault-tolerant workloads</p> </li> <li>Not for critical services</li> <li> <p>Implement pod disruption budgets</p> </li> <li> <p>Right-Size Nodes</p> </li> <li> <p>Don\u2019t over-provision nodes</p> </li> <li>Use smaller nodes for better bin packing</li> <li>Monitor utilization and adjust</li> </ol>"},{"location":"gcp/kubernetes/cluster-types/#for-autopilot-clusters","title":"For Autopilot Clusters","text":"<ol> <li> <p>Define Resource Requests</p> </li> <li> <p>Required for all pods</p> </li> <li>Directly impacts cost</li> <li> <p>Use VPA for recommendations</p> </li> <li> <p>Optimize Pod Resources</p> </li> <li> <p>Right-size requests to actual usage</p> </li> <li>Avoid over-requesting resources</li> <li> <p>Use Vertical Pod Autoscaler</p> </li> <li> <p>Understand Restrictions</p> </li> <li> <p>No privileged pods</p> </li> <li>Limited DaemonSet capabilities</li> <li> <p>Plan accordingly</p> </li> <li> <p>Leverage Auto-Scaling</p> </li> <li> <p>HPA for pod replicas</p> </li> <li>Let Autopilot handle nodes</li> <li>No need to configure cluster autoscaler</li> </ol>"},{"location":"gcp/kubernetes/gke-overview/","title":"GKE Overview","text":""},{"location":"gcp/kubernetes/gke-overview/#description","title":"Description","text":"<p>Google Kubernetes Engine (GKE) is a managed Kubernetes service that provides a platform for deploying, managing, and scaling containerized applications using Google\u2019s infrastructure. GKE abstracts away the complexity of managing Kubernetes control planes and provides deep integration with Google Cloud services.</p> <p>Architecture: Managed Kubernetes clusters consisting of a control plane (managed by Google) and worker nodes (managed by Google or you, depending on cluster type).</p>"},{"location":"gcp/kubernetes/gke-overview/#key-features","title":"Key Features","text":""},{"location":"gcp/kubernetes/gke-overview/#managed-control-plane","title":"Managed Control Plane","text":"<ul> <li>Fully Managed: Google manages the Kubernetes API server, etcd, and other control plane components</li> <li>Automatic Updates: Control plane automatically updated with latest Kubernetes versions</li> <li>High Availability: Multi-zone control plane with 99.95% or 99.99% SLA (depending on cluster type)</li> <li>No Control Plane Costs: Standard clusters don\u2019t charge for control plane (Autopilot includes it in per-pod pricing)</li> </ul>"},{"location":"gcp/kubernetes/gke-overview/#cluster-management","title":"Cluster Management","text":"<ul> <li>Multiple Cluster Types: Standard (manual management) and Autopilot (fully managed)</li> <li>Auto-Scaling: Cluster autoscaling for nodes, Horizontal/Vertical Pod Autoscaling</li> <li>Auto-Repair: Automatically repairs unhealthy nodes</li> <li>Auto-Upgrade: Automatically upgrades nodes to match control plane version</li> <li>Node Pools: Logical grouping of nodes with same configuration</li> </ul>"},{"location":"gcp/kubernetes/gke-overview/#integration-with-google-cloud","title":"Integration with Google Cloud","text":"<ul> <li>Cloud Load Balancing: Automatic integration for Service type LoadBalancer</li> <li>Cloud Logging &amp; Monitoring: Native integration with Cloud Operations</li> <li>Workload Identity: Securely access Google Cloud APIs from pods</li> <li>Binary Authorization: Ensure only trusted container images are deployed</li> <li>VPC-Native Clusters: Pods get IP addresses from VPC subnet ranges</li> <li>Private Clusters: Control plane and nodes isolated from public internet</li> </ul>"},{"location":"gcp/kubernetes/gke-overview/#security","title":"Security","text":"<ul> <li>Workload Identity: Map Kubernetes service accounts to Google Cloud service accounts</li> <li>Shielded GKE Nodes: Verifiable node integrity</li> <li>Binary Authorization: Deploy-time security policy enforcement</li> <li>GKE Sandbox: Run untrusted workloads using gVisor</li> <li>Security Posture Dashboard: Centralized security recommendations</li> <li>Network Policies: Control pod-to-pod communication</li> </ul>"},{"location":"gcp/kubernetes/gke-overview/#developer-experience","title":"Developer Experience","text":"<ul> <li>Cloud Code: IDE integration for development and debugging</li> <li>Config Connector: Manage GCP resources through Kubernetes</li> <li>kubectl: Standard Kubernetes CLI</li> <li>Cloud Console UI: Web-based cluster management</li> <li>Cloud Shell: Browser-based CLI with pre-installed tools</li> </ul>"},{"location":"gcp/kubernetes/gke-overview/#important-limits","title":"Important Limits","text":"Limit Value Notes Max nodes per cluster 15,000 (Standard), 1,000 node pools Autopilot scales automatically Max pods per node 110 (default), up to 256 Configurable with <code>--max-pods-per-node</code> Max pods per cluster 200,000 (Standard) Autopilot manages this automatically Clusters per project 100 per location Soft limit, can be increased Node pools per cluster 1,000 Each pool can have different configurations Max PVs per cluster 256 PD per node, 128 local SSDs per node Persistent disk limits Services (LoadBalancer) 5 per node, 300 per cluster Network load balancer limits"},{"location":"gcp/kubernetes/gke-overview/#cluster-types-comparison","title":"Cluster Types Comparison","text":"Feature Standard GKE Autopilot GKE Node Management Manual Fully automated Pricing Model Per node-hour Per pod resource request Scaling Configure autoscaling Automatic Node Configuration Full control Google-managed SSH Access to Nodes Yes No Custom Machine Types Yes Predefined pod specs Node Pools Manual creation Automatically managed Security Baseline Configure manually Hardened by default Best For Custom requirements, full control Simplicity, hands-off operations"},{"location":"gcp/kubernetes/gke-overview/#when-to-use","title":"When to Use","text":""},{"location":"gcp/kubernetes/gke-overview/#use-gke-when","title":"\u2705 Use GKE When:","text":"<ol> <li> <p>Container Orchestration Needed</p> </li> <li> <p>Running microservices architectures</p> </li> <li>Need automatic scaling, self-healing, and rolling updates</li> <li> <p>Managing multiple containerized applications</p> </li> <li> <p>Kubernetes Expertise Available</p> </li> <li> <p>Team has Kubernetes knowledge</p> </li> <li>Want standard Kubernetes APIs and ecosystem</li> <li> <p>Need portability across clouds</p> </li> <li> <p>Google Cloud Integration Required</p> </li> <li> <p>Leveraging Google Cloud services (Cloud SQL, Pub/Sub, BigQuery)</p> </li> <li>Using Workload Identity for secure GCP access</li> <li> <p>Need integration with Cloud Load Balancing</p> </li> <li> <p>High Availability and Scalability</p> </li> <li> <p>Applications require 99.95%+ uptime</p> </li> <li>Need to scale from handful to thousands of pods</li> <li> <p>Multi-region or multi-zone deployments</p> </li> <li> <p>Managed Infrastructure Preferred</p> </li> <li> <p>Want Google to manage control plane</p> </li> <li>Prefer automatic updates and patches</li> <li>Need security hardening by default (Autopilot)</li> </ol>"},{"location":"gcp/kubernetes/gke-overview/#dont-use-gke-when","title":"\u274c Don\u2019t Use GKE When:","text":"<ol> <li> <p>Simple Applications</p> </li> <li> <p>Single container application better suited for Cloud Run</p> </li> <li>Serverless functions (use Cloud Functions)</li> <li> <p>Static websites (use Cloud Storage/Firebase Hosting)</p> </li> <li> <p>No Container Experience</p> </li> <li> <p>Team lacks container and Kubernetes knowledge</p> </li> <li>Learning curve not justified by requirements</li> <li> <p>Simpler solutions available (App Engine, Cloud Run)</p> </li> <li> <p>Windows-Heavy Workloads</p> </li> <li> <p>Primarily Windows containers (GKE supports Windows but consider GCE)</p> </li> <li> <p>Legacy Windows applications not containerized</p> </li> <li> <p>Extremely Cost-Sensitive Small Workloads</p> </li> <li> <p>Single small VM might be cheaper than minimum cluster</p> </li> <li>Very low traffic applications</li> <li> <p>Development environments (unless Autopilot)</p> </li> <li> <p>Complete Infrastructure Control Needed</p> </li> <li> <p>Need to modify control plane configuration</p> </li> <li>Require kernel-level modifications</li> <li>Custom network overlays incompatible with GKE</li> </ol>"},{"location":"gcp/kubernetes/gke-overview/#common-use-cases","title":"Common Use Cases","text":""},{"location":"gcp/kubernetes/gke-overview/#microservices-architecture","title":"Microservices Architecture","text":"<pre><code>GKE Cluster\n\u251c\u2500\u2500 Frontend Service (Deployment)\n\u2502   \u2514\u2500\u2500 Pods: 3 replicas\n\u251c\u2500\u2500 API Service (Deployment)\n\u2502   \u2514\u2500\u2500 Pods: 5 replicas\n\u251c\u2500\u2500 Auth Service (Deployment)\n\u2502   \u2514\u2500\u2500 Pods: 2 replicas\n\u2514\u2500\u2500 Database (StatefulSet)\n    \u2514\u2500\u2500 Pods: 3 replicas (with persistent volumes)\n\nIngress: HTTPS load balancer\nService Mesh: Istio for service-to-service communication\n</code></pre>"},{"location":"gcp/kubernetes/gke-overview/#cicd-pipeline","title":"CI/CD Pipeline","text":"<pre><code>GitHub \u2192 Cloud Build \u2192 Artifact Registry \u2192 GKE\n                                            \u251c\u2500\u2500 Dev Cluster\n                                            \u251c\u2500\u2500 Staging Cluster\n                                            \u2514\u2500\u2500 Production Cluster\n</code></pre>"},{"location":"gcp/kubernetes/gke-overview/#batch-processing","title":"Batch Processing","text":"<pre><code>GKE Autopilot\n\u2514\u2500\u2500 Jobs/CronJobs\n    \u251c\u2500\u2500 Data processing jobs (scales to zero when complete)\n    \u251c\u2500\u2500 ML training jobs\n    \u2514\u2500\u2500 ETL pipelines\n</code></pre>"},{"location":"gcp/kubernetes/gke-overview/#gke-vs-other-google-services","title":"GKE vs Other Google Services","text":"<p>GKE vs Cloud Run</p> <ul> <li>GKE: Full Kubernetes, more control, stateful workloads</li> <li>Cloud Run: Serverless containers, simpler, stateless HTTP services</li> </ul> <p>GKE vs Compute Engine</p> <ul> <li>GKE: Container orchestration, automatic scaling/healing</li> <li>Compute Engine: Full VM control, traditional applications</li> </ul> <p>GKE vs App Engine</p> <ul> <li>GKE: More flexibility, any language/runtime, complex apps</li> <li>App Engine: Simpler PaaS, limited languages, quick deployment</li> </ul>"},{"location":"gcp/kubernetes/gke-overview/#pricing-considerations","title":"Pricing Considerations","text":"<p>Standard GKE</p> <ul> <li>Control Plane: Free for zonal clusters, $0.10/hour for regional</li> <li>Nodes: Standard Compute Engine pricing for VMs</li> <li>Network: Egress charges apply</li> <li>Cost Optimization: Use Spot VMs, committed use discounts, right-size nodes</li> </ul> <p>Autopilot GKE</p> <ul> <li>No Node Charges: Pay only for pod resource requests</li> <li>Control Plane: Included in pod pricing</li> <li>vCPU: $0.0445/hour per vCPU requested</li> <li>Memory: $0.00488/hour per GB requested</li> <li>Cost Optimization: Right-size pod requests, use vertical pod autoscaler</li> </ul> <p>General Tips</p> <ul> <li>Use Autopilot for unpredictable workloads (pay only for used resources)</li> <li>Use Standard with Spot VMs for batch/fault-tolerant workloads (up to 91% discount)</li> <li>Enable cluster autoscaling to scale down during off-hours</li> <li>Use resource quotas to prevent cost overruns</li> </ul>"},{"location":"gcp/kubernetes/gke-overview/#getting-started","title":"Getting Started","text":""},{"location":"gcp/kubernetes/gke-overview/#create-a-gke-cluster-standard","title":"Create a GKE Cluster (Standard)","text":"<pre><code># Create zonal cluster\ngcloud container clusters create my-cluster \\\n  --zone=us-central1-a \\\n  --num-nodes=3 \\\n  --machine-type=e2-medium \\\n  --enable-autoscaling \\\n  --min-nodes=1 \\\n  --max-nodes=10\n\n# Get credentials\ngcloud container clusters get-credentials my-cluster --zone=us-central1-a\n\n# Verify connection\nkubectl get nodes\n</code></pre>"},{"location":"gcp/kubernetes/gke-overview/#create-a-gke-cluster-autopilot","title":"Create a GKE Cluster (Autopilot)","text":"<pre><code># Create Autopilot cluster (regional by default)\ngcloud container clusters create-auto my-autopilot-cluster \\\n  --region=us-central1\n\n# Get credentials\ngcloud container clusters get-credentials my-autopilot-cluster --region=us-central1\n\n# Deploy application - nodes provisioned automatically\nkubectl apply -f deployment.yaml\n</code></pre>"},{"location":"gcp/kubernetes/gke-overview/#best-practices","title":"Best Practices","text":""},{"location":"gcp/kubernetes/gke-overview/#1-cluster-configuration","title":"1. Cluster Configuration","text":"<ul> <li>Use regional clusters for production (99.95% SLA)</li> <li>Enable Workload Identity for secure GCP access</li> <li>Use VPC-native clusters (alias IP ranges)</li> <li>Enable Binary Authorization for production</li> <li>Configure maintenance windows for upgrades</li> </ul>"},{"location":"gcp/kubernetes/gke-overview/#2-security","title":"2. Security","text":"<ul> <li>Enable Workload Identity (not metadata server)</li> <li>Use least-privilege IAM roles</li> <li>Implement Network Policies</li> <li>Use Private clusters for sensitive workloads</li> <li>Enable GKE Dataplane V2 for improved networking</li> </ul>"},{"location":"gcp/kubernetes/gke-overview/#3-resource-management","title":"3. Resource Management","text":"<ul> <li>Set resource requests and limits on all pods</li> <li>Use resource quotas and limit ranges per namespace</li> <li>Enable Horizontal Pod Autoscaler for variable workloads</li> <li>Use Vertical Pod Autoscaler to right-size requests</li> </ul>"},{"location":"gcp/kubernetes/gke-overview/#4-monitoring-logging","title":"4. Monitoring &amp; Logging","text":"<ul> <li>Enable GKE monitoring and logging (now default)</li> <li>Use Workload metrics for application-level monitoring</li> <li>Set up alerts for cluster and pod health</li> <li>Use Cloud Trace for distributed tracing</li> </ul>"},{"location":"gcp/kubernetes/gke-overview/#5-cost-optimization","title":"5. Cost Optimization","text":"<ul> <li>Use Spot VMs for fault-tolerant workloads</li> <li>Enable cluster autoscaling</li> <li>Right-size node pools and pod resources</li> <li>Use Autopilot for variable or unpredictable workloads</li> <li>Clean up unused resources (PVs, LoadBalancers)</li> </ul>"},{"location":"gcp/kubernetes/scaling/","title":"GKE Scaling","text":""},{"location":"gcp/kubernetes/scaling/#description","title":"Description","text":"<p>Scaling in GKE encompasses multiple dimensions: horizontal pod scaling (adding more pod replicas), vertical pod scaling (increasing pod resources), and cluster scaling (adding more nodes). GKE provides automated scaling mechanisms to handle variable workloads efficiently while optimizing costs.</p> <p>Concept: Automatically adjust resources (pods and nodes) based on demand to maintain performance while optimizing costs.</p>"},{"location":"gcp/kubernetes/scaling/#types-of-scaling","title":"Types of Scaling","text":""},{"location":"gcp/kubernetes/scaling/#horizontal-pod-autoscaler-hpa","title":"Horizontal Pod Autoscaler (HPA)","text":"<p>Scales the number of pod replicas based on observed metrics.</p>"},{"location":"gcp/kubernetes/scaling/#vertical-pod-autoscaler-vpa","title":"Vertical Pod Autoscaler (VPA)","text":"<p>Adjusts CPU and memory requests/limits for containers.</p>"},{"location":"gcp/kubernetes/scaling/#cluster-autoscaler","title":"Cluster Autoscaler","text":"<p>Adds or removes nodes based on pod resource requirements.</p>"},{"location":"gcp/kubernetes/scaling/#multidimensional-pod-autoscaler-mpa","title":"Multidimensional Pod Autoscaler (MPA)","text":"<p>Scales both pod replicas and resources (GKE Autopilot feature).</p>"},{"location":"gcp/kubernetes/scaling/#horizontal-pod-autoscaler-hpa_1","title":"Horizontal Pod Autoscaler (HPA)","text":""},{"location":"gcp/kubernetes/scaling/#description_1","title":"Description","text":"<p>HPA automatically scales the number of pods in a deployment, replica set, or stateful set based on observed CPU utilization, memory usage, or custom metrics.</p>"},{"location":"gcp/kubernetes/scaling/#key-features","title":"Key Features","text":"<ul> <li>CPU-based Scaling: Scale based on CPU utilization (most common)</li> <li>Memory-based Scaling: Scale based on memory usage</li> <li>Custom Metrics: Scale on application-specific metrics (Pub/Sub queue length, HTTP requests/sec)</li> <li>External Metrics: Scale based on metrics from external systems</li> <li>Multiple Metrics: Combine different metrics for scaling decisions</li> <li>Configurable Behavior: Set min/max replicas, scaling velocity</li> </ul>"},{"location":"gcp/kubernetes/scaling/#when-to-use-hpa","title":"When to Use HPA","text":"<p>\u2705 Use HPA When:</p> <ul> <li>Traffic varies throughout the day</li> <li>Want automatic scaling based on load</li> <li>Cost optimization through dynamic scaling</li> <li>Stateless applications that can scale horizontally</li> </ul> <p>\u274c Don\u2019t Use HPA When:</p> <ul> <li>Stateful applications that can\u2019t easily add replicas</li> <li>Applications with long startup times (scale-up lag)</li> <li>Need vertical scaling (use VPA instead)</li> </ul>"},{"location":"gcp/kubernetes/scaling/#vertical-pod-autoscaler-vpa_1","title":"Vertical Pod Autoscaler (VPA)","text":""},{"location":"gcp/kubernetes/scaling/#description_2","title":"Description","text":"<p>VPA automatically adjusts CPU and memory requests and limits for containers based on historical usage patterns.</p>"},{"location":"gcp/kubernetes/scaling/#key-features_1","title":"Key Features","text":"<ul> <li>Right-Sizing: Automatically set appropriate resource requests</li> <li>Historical Analysis: Based on actual resource usage patterns</li> <li>Update Modes: Recommend, auto-update, or initial-only</li> <li>Container-Level: Can configure per-container in a pod</li> </ul>"},{"location":"gcp/kubernetes/scaling/#vpa-update-modes","title":"VPA Update Modes","text":"<p>Off (Recommendation Only):</p> <pre><code>updatePolicy:\n  updateMode: \"Off\"\n</code></pre> <ul> <li>VPA only generates recommendations</li> <li>No automatic updates</li> <li>Use for analysis before implementing</li> </ul> <p>Initial:</p> <pre><code>updatePolicy:\n  updateMode: \"Initial\"\n</code></pre> <ul> <li>Set resources only when pods are created</li> <li>No updates to running pods</li> <li>Good for stateful workloads</li> </ul> <p>Auto:</p> <pre><code>updatePolicy:\n  updateMode: \"Auto\"\n</code></pre> <ul> <li>Automatically update running pods</li> <li>Evicts and recreates pods to apply new resources</li> <li>Best for stateless workloads</li> </ul> <p>Recreate:</p> <pre><code>updatePolicy:\n  updateMode: \"Recreate\"\n</code></pre> <ul> <li>Similar to Auto but always recreates pods</li> <li>More disruptive</li> </ul>"},{"location":"gcp/kubernetes/scaling/#important-considerations","title":"Important Considerations","text":"<p>VPA Limitations:</p> <ul> <li>Cannot be used with HPA on same CPU/memory metrics</li> <li>Requires pod eviction to apply changes (except Initial mode)</li> <li>May cause brief downtime during updates</li> </ul>"},{"location":"gcp/kubernetes/scaling/#when-to-use-vpa","title":"When to Use VPA","text":"<p>\u2705 Use VPA When:</p> <ul> <li>Resource requests are incorrect or unknown</li> <li>Want automatic right-sizing based on usage</li> <li>Applications with varying resource needs over time</li> <li>Optimizing cost by eliminating over-provisioning</li> </ul> <p>\u274c Don\u2019t Use VPA When:</p> <ul> <li>Already using HPA on CPU/memory</li> <li>Cannot tolerate pod evictions</li> <li>Resource requirements are well-known and stable</li> <li>Application startup time is very long</li> </ul>"},{"location":"gcp/kubernetes/scaling/#cluster-autoscaler_1","title":"Cluster Autoscaler","text":""},{"location":"gcp/kubernetes/scaling/#description_3","title":"Description","text":"<p>Cluster Autoscaler automatically adjusts the number of nodes in a cluster based on pod resource requests that cannot be scheduled on existing nodes.</p>"},{"location":"gcp/kubernetes/scaling/#how-it-works","title":"How It Works","text":"<ol> <li>Pods are unschedulable due to insufficient resources</li> <li>Cluster Autoscaler detects pending pods</li> <li>New nodes added to accommodate pods</li> <li>When nodes are underutilized, they\u2019re removed</li> </ol>"},{"location":"gcp/kubernetes/scaling/#key-features_2","title":"Key Features","text":"<ul> <li>Automatic Scale-Up: Add nodes when pods can\u2019t be scheduled</li> <li>Automatic Scale-Down: Remove nodes when underutilized</li> <li>Node Pool Awareness: Scale specific node pools</li> <li>Cost Optimization: Reduce costs by removing unused nodes</li> <li>Configurable Behavior: Set min/max nodes, scale-down delays</li> </ul>"},{"location":"gcp/kubernetes/scaling/#important-limits","title":"Important Limits","text":"Limit Value Notes Min nodes per pool 0 Can scale to zero Max nodes per pool 1000 Configurable Max nodes per cluster 15,000 Total limit Scale-up time ~2-5 minutes Node provisioning time Scale-down time 10 minutes (default) Configurable delay"},{"location":"gcp/kubernetes/scaling/#when-to-use-cluster-autoscaler","title":"When to Use Cluster Autoscaler","text":"<p>\u2705 Use Cluster Autoscaler When:</p> <ul> <li>Workload varies significantly</li> <li>Want automatic infrastructure scaling</li> <li>Cost optimization important</li> <li>Using HPA (complement to pod autoscaling)</li> </ul> <p>\u274c Don\u2019t Use Cluster Autoscaler When:</p> <ul> <li>Workload is stable and predictable</li> <li>Cannot tolerate 2-5 minute scale-up delay</li> <li>Using Autopilot (handles this automatically)</li> </ul>"},{"location":"gcp/kubernetes/scaling/#autopilot-scaling-gke-autopilot","title":"Autopilot Scaling (GKE Autopilot)","text":""},{"location":"gcp/kubernetes/scaling/#description_4","title":"Description","text":"<p>In Autopilot mode, Google manages all scaling automatically. No cluster autoscaler configuration needed.</p>"},{"location":"gcp/kubernetes/scaling/#how-it-works_1","title":"How It Works","text":"<ul> <li>Nodes provisioned automatically based on pod requests</li> <li>Scales to zero when no workloads running</li> <li>Right-sized nodes for pod requirements</li> <li>No over-provisioning</li> </ul>"},{"location":"gcp/kubernetes/scaling/#autopilot-scaling-features","title":"Autopilot Scaling Features","text":"<p>Automatic:</p> <ul> <li>Node provisioning and removal</li> <li>Perfect bin-packing</li> <li>Cost optimization</li> <li>No configuration needed</li> </ul> <p>You Still Configure:</p> <ul> <li>HPA for pod replica scaling</li> <li>VPA for resource recommendations (optional)</li> <li>Pod resource requests (required)</li> </ul> <p>Set up Alerts:</p> <ul> <li>HPA at max replicas</li> <li>Cluster at max nodes</li> <li>High pod pending time</li> <li>Scaling failures</li> </ul>"},{"location":"gcp/networking/bgp/","title":"BGP (Border Gateway Protocol)","text":""},{"location":"gcp/networking/bgp/#description","title":"Description","text":"<p>BGP is the standard exterior gateway protocol used to exchange routing information between autonomous systems (AS) on the internet and within private networks. In Google Cloud, BGP is used with Cloud Router to enable dynamic routing between VPC networks and on-premises or other cloud networks via Cloud VPN and Cloud Interconnect.</p> <p>Purpose: Automatically exchange route information, enabling dynamic network topology changes without manual route updates.</p>"},{"location":"gcp/networking/bgp/#bgp-fundamentals","title":"BGP Fundamentals","text":""},{"location":"gcp/networking/bgp/#autonomous-system-as","title":"Autonomous System (AS)","text":"<ul> <li>Definition: Collection of IP networks under single administrative control</li> <li>ASN: Unique number identifying the AS</li> <li>Types:</li> <li>Public ASN: Registered with IANA (1-64511, 65536-4199999999)</li> <li>Private ASN: For internal use only (64512-65534, 4200000000-4294967294)</li> </ul>"},{"location":"gcp/networking/bgp/#bgp-session-types","title":"BGP Session Types","text":"<p>eBGP (External BGP):</p> <ul> <li>Between different autonomous systems</li> <li>Default: On-premises AS to Google Cloud AS</li> <li>TTL: 1 (directly connected peers)</li> <li>Use case: Cloud VPN, Cloud Interconnect</li> </ul> <p>iBGP (Internal BGP):</p> <ul> <li>Within same autonomous system</li> <li>TTL: Maximum (255)</li> <li>Use case: Less common in GCP, more for traditional networks</li> </ul>"},{"location":"gcp/networking/bgp/#bgp-attributes","title":"BGP Attributes","text":"<p>BGP uses various attributes to determine best path selection:</p> <ol> <li>AS-PATH: List of ASNs the route has traversed</li> <li>Next-Hop: IP address of next router in path</li> <li>MED (Multi-Exit Discriminator): Suggest preferred entry point</li> <li>Local Preference: Prefer specific exit point (iBGP only)</li> <li>Weight: Cisco proprietary (highest = best)</li> <li>Origin: How route was learned (IGP, EGP, Incomplete)</li> </ol>"},{"location":"gcp/networking/bgp/#bgp-path-selection","title":"BGP Path Selection","text":"<p>BGP selects the best path using these criteria in order:</p> <ol> <li>Highest Weight (Cisco-specific, local to router)</li> <li>Highest Local Preference (default: 100)</li> <li>Locally originated routes (network, aggregate, redistribute)</li> <li>Shortest AS-PATH</li> <li>Lowest Origin type (IGP &lt; EGP &lt; Incomplete)</li> <li>Lowest MED (if from same AS)</li> <li>eBGP over iBGP</li> <li>Lowest IGP metric to BGP next hop</li> <li>Oldest route (for eBGP paths)</li> <li>Lowest Router ID</li> </ol> <p>In GCP Context: You primarily control path selection via MED (from Cloud Router) and AS-PATH prepending (from on-premises).</p>"},{"location":"gcp/networking/bgp/#bgp-in-google-cloud","title":"BGP in Google Cloud","text":""},{"location":"gcp/networking/bgp/#cloud-router-asn-configuration","title":"Cloud Router ASN Configuration","text":"<p>When creating a Cloud Router, you specify an ASN:</p> <p>Private ASN Ranges:</p> <ul> <li>16-bit: 64512 - 65534 (for compatibility)</li> <li>32-bit: 4200000000 - 4294967294 (recommended)</li> </ul> <p>Best Practices:</p> <ul> <li>Use 32-bit private ASN (4-byte ASN) for better uniqueness</li> <li>Document ASN assignments across your organization</li> <li>Ensure Cloud Router ASN differs from peer ASN</li> <li>Use consistent ASN scheme (e.g., 4200000000 + project_number)</li> </ul>"},{"location":"gcp/networking/bgp/#bgp-session-configuration","title":"BGP Session Configuration","text":"<p>Each BGP session requires:</p> <pre><code>Cloud Router Configuration:\n\n  - Router ASN: 64512 (example)\n  - Interface: VPN tunnel or VLAN attachment\n  - Peer ASN: 65001 (on-premises ASN)\n  - Peer IP: Automatically assigned (VPN) or manually configured (Interconnect)\n  - Advertised Route Priority (MED): 100 (default)\n  - MD5 Authentication: Optional shared secret\n  - BFD: Enabled/Disabled\n</code></pre> <p>On-Premises Router Configuration:</p> <ul> <li>Must configure complementary BGP session</li> <li>Neighbor IP: Cloud Router\u2019s interface IP</li> <li>Neighbor ASN: Cloud Router\u2019s ASN</li> <li>Route advertisement: On-premises networks to advertise</li> </ul>"},{"location":"gcp/networking/bgp/#route-advertisement","title":"Route Advertisement","text":""},{"location":"gcp/networking/bgp/#from-cloud-router-to-on-premises","title":"From Cloud Router to On-Premises","text":"<p>Default Mode:</p> <ul> <li>Advertises all VPC subnet routes automatically</li> <li>Regional routing: Only subnets in the same region</li> <li>Global routing: All subnets in all regions</li> </ul> <p>Custom Mode:</p> <ul> <li>Manually specify IP ranges to advertise</li> <li>Can advertise:</li> <li>VPC subnet ranges</li> <li>Secondary subnet ranges</li> <li>Custom IP ranges</li> <li>Summary routes (supernets)</li> </ul> <p>Example: <pre><code>VPC Subnets:\n\n  - 10.1.0.0/24\n  - 10.2.0.0/24\n  - 10.3.0.0/24\n\nCustom Advertisement:\n\n  - 10.0.0.0/8 (summary route instead of individual /24s)\n</code></pre></p>"},{"location":"gcp/networking/bgp/#from-on-premises-to-cloud-router","title":"From On-Premises to Cloud Router","text":"<ul> <li>Configure on your on-premises router</li> <li>Advertise on-premises network ranges</li> <li>Cloud Router learns these routes via BGP</li> <li>Routes are programmed into VPC routing table</li> </ul> <p>Important: Learned routes count against quota (default 250, max 5000 per Cloud Router).</p>"},{"location":"gcp/networking/bgp/#route-priority-and-traffic-engineering","title":"Route Priority and Traffic Engineering","text":""},{"location":"gcp/networking/bgp/#using-med-multi-exit-discriminator","title":"Using MED (Multi-Exit Discriminator)","text":"<p>MED influences inbound traffic (from on-premises to GCP):</p> <p>Configuration on Cloud Router: <pre><code>Cloud Router 1 (Primary Path):\n\n  - Advertised Route Priority: 50 (lower = preferred)\n\nCloud Router 2 (Backup Path):\n\n  - Advertised Route Priority: 100 (higher = less preferred)\n</code></pre></p> <p>Result: On-premises router prefers routes via Cloud Router 1.</p> <p>Important:</p> <ul> <li>Lower MED = higher priority</li> <li>MED is compared only for routes from same neighboring AS</li> <li>Default MED: 100</li> </ul>"},{"location":"gcp/networking/bgp/#using-as-path-prepending","title":"Using AS-PATH Prepending","text":"<p>AS-PATH prepending influences outbound traffic (from GCP to on-premises):</p> <p>Configuration on On-Premises Router: <pre><code>Primary Path:\n\n  - Advertise: 192.168.0.0/16 (AS-PATH: 65001)\n\nBackup Path:\n\n  - Advertise: 192.168.0.0/16 (AS-PATH: 65001 65001 65001)\n  - Prepend AS 65001 twice to make path \"longer\"\n</code></pre></p> <p>Result: Cloud Router prefers routes via primary path (shorter AS-PATH).</p>"},{"location":"gcp/networking/bgp/#active-active-vs-active-passive","title":"Active-Active vs Active-Passive","text":"<p>Active-Active:</p> <ul> <li>Equal cost multi-path (ECMP)</li> <li>Same MED values on both paths</li> <li>Traffic load-balanced across both paths</li> <li>Use case: Maximize bandwidth, high availability</li> </ul> <p>Active-Passive:</p> <ul> <li>Primary path with lower MED</li> <li>Backup path with higher MED</li> <li>Traffic uses primary path unless it fails</li> <li>Use case: Controlled failover, predictable routing</li> </ul>"},{"location":"gcp/networking/bgp/#bfd-bidirectional-forwarding-detection","title":"BFD (Bidirectional Forwarding Detection)","text":""},{"location":"gcp/networking/bgp/#purpose","title":"Purpose","text":"<p>Fast failure detection for BGP sessions:</p> <ul> <li>BGP keepalive: 60 seconds (slow)</li> <li>BFD: 1-3 seconds (fast)</li> </ul>"},{"location":"gcp/networking/bgp/#how-it-works","title":"How It Works","text":"<ul> <li>Lightweight \u201chello\u201d packets exchanged between peers</li> <li>Independent of BGP protocol</li> <li>Detects link/path failures quickly</li> <li>Triggers BGP session teardown on failure</li> </ul>"},{"location":"gcp/networking/bgp/#configuration-in-gcp","title":"Configuration in GCP","text":"<pre><code>BGP Session:\n\n  - BFD: Enabled\n  - Min Transmit Interval: 1000 ms (default)\n  - Min Receive Interval: 1000 ms (default)\n  - Multiplier: 3 (default)\n</code></pre> <p>Calculation: 3 missed packets \u00d7 1000ms = ~3 second failure detection</p>"},{"location":"gcp/networking/bgp/#when-to-use-bfd","title":"When to Use BFD","text":"<ul> <li>\u2705 Production environments</li> <li>\u2705 Active-active configurations requiring fast failover</li> <li>\u2705 Critical workloads with HA requirements</li> <li>\u274c Non-production environments (adds complexity)</li> </ul>"},{"location":"gcp/networking/bgp/#common-scenarios","title":"Common Scenarios","text":""},{"location":"gcp/networking/bgp/#scenario-1-prefer-primary-vpn-tunnel","title":"Scenario 1: Prefer Primary VPN Tunnel","text":"<p>Requirement: Route traffic via Tunnel 1, use Tunnel 2 as backup.</p> <p>Solution: <pre><code>Cloud Router BGP Sessions:\n\n  - Tunnel 1: MED = 50\n  - Tunnel 2: MED = 100\n\nOn-premises routes traffic via Tunnel 1 (lower MED preferred).\n</code></pre></p>"},{"location":"gcp/networking/bgp/#scenario-2-regional-preference","title":"Scenario 2: Regional Preference","text":"<p>Requirement: US traffic uses US region, Europe traffic uses Europe region.</p> <p>Solution: <pre><code>Cloud Router US (us-central1):\n\n  - Advertises US subnets with MED = 50\n  - Advertises EU subnets with MED = 100\n\nCloud Router EU (europe-west1):\n\n  - Advertises EU subnets with MED = 50\n  - Advertises US subnets with MED = 100\n\nOn-premises router learns both, prefers regional routes (lower MED).\n</code></pre></p>"},{"location":"gcp/networking/bgp/#scenario-3-controlled-outbound-path","title":"Scenario 3: Controlled Outbound Path","text":"<p>Requirement: Force GCP to use specific on-premises path.</p> <p>Solution: <pre><code>On-Premises Router:\n\n  - Primary path: AS-PATH = 65001\n  - Backup path: AS-PATH = 65001 65001 65001 (prepended)\n\nCloud Router prefers primary (shorter AS-PATH).\n</code></pre></p>"},{"location":"gcp/networking/bgp/#scenario-4-load-balancing-across-tunnels","title":"Scenario 4: Load Balancing Across Tunnels","text":"<p>Requirement: Distribute traffic evenly across multiple VPN tunnels.</p> <p>Solution: <pre><code>Cloud Router:\n\n  - Tunnel 1: MED = 100\n  - Tunnel 2: MED = 100\n  - Tunnel 3: MED = 100\n  - Tunnel 4: MED = 100\n\nEqual cost paths enable ECMP load balancing.\n</code></pre></p>"},{"location":"gcp/networking/bgp/#md5-authentication","title":"MD5 Authentication","text":""},{"location":"gcp/networking/bgp/#purpose_1","title":"Purpose","text":"<p>Secure BGP sessions from spoofing and unauthorized peers.</p>"},{"location":"gcp/networking/bgp/#configuration","title":"Configuration","text":"<p>In GCP: <pre><code>gcloud compute routers add-bgp-peer ROUTER_NAME \\\n  --peer-name=PEER_NAME \\\n  --peer-asn=PEER_ASN \\\n  --interface=INTERFACE \\\n  --md5-authentication-key=\"SHARED_SECRET\"\n</code></pre></p> <p>On On-Premises Router (example): <pre><code>router bgp 65001\n  neighbor 169.254.1.1 remote-as 64512\n  neighbor 169.254.1.1 password SHARED_SECRET\n</code></pre></p> <p>Best Practices:</p> <ul> <li>Use strong, random keys (16+ characters)</li> <li>Store keys securely (secrets management)</li> <li>Rotate keys periodically</li> <li>Consistent configuration on both sides</li> </ul>"},{"location":"gcp/networking/bgp/#limits-and-quotas","title":"Limits and Quotas","text":"Item Limit Notes BGP sessions per Cloud Router 128 Default Learned routes per Cloud Router 250 (default), 5000 (max) Request increase if needed Advertised routes Unlimited Based on VPC subnets ASN range (private) 64512-65534, 4200000000-4294967294 RFC 6996 BGP keepalive 20 seconds Not configurable BGP hold timer 60 seconds Not configurable BFD intervals Min 1000ms Configurable per session"},{"location":"gcp/networking/bgp/#best-practices","title":"Best Practices","text":""},{"location":"gcp/networking/bgp/#1-asn-planning","title":"1. ASN Planning","text":"<ul> <li>Document all ASN assignments</li> <li>Use 32-bit ASNs to avoid conflicts</li> <li>Reserve ASN ranges for different purposes (prod, dev, regions)</li> <li>Ensure uniqueness across entire network</li> </ul>"},{"location":"gcp/networking/bgp/#2-route-management","title":"2. Route Management","text":"<ul> <li>Start with default advertisement, move to custom if needed</li> <li>Use route summarization to reduce route count</li> <li>Monitor learned routes against quotas</li> <li>Implement route filtering on both sides</li> </ul>"},{"location":"gcp/networking/bgp/#3-redundancy","title":"3. Redundancy","text":"<ul> <li>Configure multiple BGP sessions per Cloud Router</li> <li>Use diverse physical paths when possible</li> <li>Enable BFD for fast failover</li> <li>Test failover scenarios regularly</li> </ul>"},{"location":"gcp/networking/bgp/#4-traffic-engineering","title":"4. Traffic Engineering","text":"<ul> <li>Document MED and AS-PATH configurations</li> <li>Use consistent MED values for similar paths</li> <li>Avoid excessive AS-PATH prepending (3x max)</li> <li>Monitor traffic flow and adjust as needed</li> </ul>"},{"location":"gcp/networking/bgp/#5-security","title":"5. Security","text":"<ul> <li>Enable MD5 authentication on all BGP sessions</li> <li>Use firewall rules to restrict BGP peers</li> <li>Monitor BGP session status for anomalies</li> <li>Regular security audits of BGP configurations</li> </ul>"},{"location":"gcp/networking/bgp/#6-monitoring","title":"6. Monitoring","text":"<ul> <li>Set up alerts for BGP session down</li> <li>Monitor route count approaching limits</li> <li>Track route flapping (unstable routes)</li> <li>Use VPC Flow Logs to verify traffic paths</li> </ul>"},{"location":"gcp/networking/bgp/#troubleshooting","title":"Troubleshooting","text":""},{"location":"gcp/networking/bgp/#bgp-session-not-establishing","title":"BGP Session Not Establishing","text":"<p>Symptoms: BGP state stuck in \u201cIdle\u201d or \u201cActive\u201d</p> <p>Common Causes:</p> <ol> <li>ASN mismatch (peer ASN incorrect)</li> <li>MD5 authentication mismatch</li> <li>Incorrect peer IP address</li> <li>Firewall blocking TCP 179</li> <li>Interface down (VPN tunnel down)</li> </ol> <p>Debug Steps: <pre><code># Check BGP session status\ngcloud compute routers get-status ROUTER_NAME --region=REGION\n\n# Check interface status  \ngcloud compute vpn-tunnels describe TUNNEL_NAME --region=REGION\n\n# Verify configuration\ngcloud compute routers describe ROUTER_NAME --region=REGION\n</code></pre></p>"},{"location":"gcp/networking/bgp/#routes-not-being-advertisedlearned","title":"Routes Not Being Advertised/Learned","text":"<p>Symptoms: Expected routes missing from routing table</p> <p>Common Causes:</p> <ol> <li>VPC dynamic routing mode (regional vs global)</li> <li>Custom advertisement not configured</li> <li>Route quota exceeded</li> <li>AS-PATH loop detection</li> <li>Route filtering on peer side</li> </ol> <p>Debug Steps: <pre><code># Check advertised routes\ngcloud compute routers get-status ROUTER_NAME --region=REGION\n\n# Check VPC routing table\ngcloud compute routes list --filter=\"network:NETWORK_NAME\"\n\n# Verify learned routes count\ngcloud compute routers get-status ROUTER_NAME --region=REGION \\\n  --format=\"value(result.bgpPeerStatus[].numLearnedRoutes)\"\n</code></pre></p>"},{"location":"gcp/networking/bgp/#asymmetric-routing","title":"Asymmetric Routing","text":"<p>Symptoms: Inbound and outbound traffic taking different paths</p> <p>Diagnosis:</p> <ul> <li>Use VPC Flow Logs to trace traffic paths</li> <li>Check MED values on all BGP sessions</li> <li>Verify AS-PATH prepending configuration</li> <li>Review on-premises routing policy</li> </ul> <p>Solutions:</p> <ul> <li>Adjust MED to align inbound path with outbound</li> <li>Modify AS-PATH prepending on outbound path</li> <li>Ensure symmetric route advertisements</li> </ul>"},{"location":"gcp/networking/bgp/#related-concepts","title":"Related Concepts","text":"<ul> <li>Cloud Router: GCP service that implements BGP</li> <li>VPC Dynamic Routing: Determines scope of route advertisement</li> <li>Cloud VPN: Uses BGP for dynamic routing (HA VPN)</li> <li>Cloud Interconnect: Uses BGP for route exchange</li> <li>ECMP: Equal-Cost Multi-Path routing for load balancing</li> <li>VRF: Virtual Routing and Forwarding (used internally by Google)</li> </ul>"},{"location":"gcp/networking/bgp/#further-reading","title":"Further Reading","text":"<ul> <li>RFC 4271: BGP-4 Protocol Specification</li> <li>RFC 6996: Private Use AS Numbers</li> <li>RFC 5880: BFD Protocol</li> <li>RFC 4456: BGP Route Reflection</li> <li>Google Cloud Documentation: Cloud Router BGP Configuration</li> </ul>"},{"location":"gcp/networking/cloud-router/","title":"Cloud Router","text":""},{"location":"gcp/networking/cloud-router/#description","title":"Description","text":"<p>Cloud Router is a fully distributed and managed Google Cloud service that uses Border Gateway Protocol (BGP) to dynamically exchange routes between your Google Cloud VPC network and on-premises or other cloud networks. It eliminates the need for static routes and enables automatic failover, scalability, and simplified network management.</p> <p>Architecture: Software-defined router that runs in Google\u2019s network, enabling dynamic route propagation without physical router appliances.</p>"},{"location":"gcp/networking/cloud-router/#key-features","title":"Key Features","text":""},{"location":"gcp/networking/cloud-router/#dynamic-routing","title":"Dynamic Routing","text":"<ul> <li>BGP Protocol: Industry-standard protocol for route exchange</li> <li>Automatic Route Updates: Routes automatically updated when topology changes</li> <li>Multi-Path: Supports ECMP (Equal-Cost Multi-Path) routing</li> <li>Route Priorities: Control route selection with MED and AS-PATH</li> </ul>"},{"location":"gcp/networking/cloud-router/#high-availability","title":"High Availability","text":"<ul> <li>Fully Managed: No VMs to manage, patch, or scale</li> <li>No Single Point of Failure: Distributed service across Google infrastructure</li> <li>Redundant BGP Sessions: Multiple sessions for failover</li> <li>BFD Support: Bidirectional Forwarding Detection for fast failure detection</li> </ul>"},{"location":"gcp/networking/cloud-router/#integration","title":"Integration","text":"<ul> <li>Cloud VPN: Required for dynamic routing with HA VPN</li> <li>Cloud Interconnect: Enables BGP for Dedicated and Partner Interconnect</li> <li>Router Appliances: Integrates with third-party network virtual appliances</li> <li>Multiple Interfaces: Can peer with multiple VPN tunnels or VLAN attachments</li> </ul>"},{"location":"gcp/networking/cloud-router/#route-management","title":"Route Management","text":"<ul> <li>Route Advertisement: Automatically advertises VPC subnet routes</li> <li>Custom Route Advertisement: Selectively advertise specific IP ranges</li> <li>Learned Routes: Receives routes from on-premises via BGP</li> <li>Route Filtering: Control which routes to accept or advertise</li> <li>Route Priorities: Configure local preference and MED values</li> </ul>"},{"location":"gcp/networking/cloud-router/#important-limits","title":"Important Limits","text":"Limit Value Notes Cloud Routers per VPC 5 per region Can be increased BGP sessions per router 128 8 per tunnel/VLAN for redundancy Learned routes per router 250 (default), 5000 (max) Requires custom route advertisement mode Advertised routes Based on VPC subnets + custom ranges BGP route priority (MED) 0-4294967295 Lower is preferred ASN range (private) 64512-65534, 4200000000-4294967294 RFC 6996 BGP keepalive interval 20 seconds (default) Not configurable BGP hold time 60 seconds (default) Not configurable"},{"location":"gcp/networking/cloud-router/#route-advertisement-modes","title":"Route Advertisement Modes","text":""},{"location":"gcp/networking/cloud-router/#default-mode","title":"Default Mode","text":"<ul> <li>Automatically advertises all subnet routes in the VPC</li> <li>Simple configuration, minimal management</li> <li>Routes added/removed automatically as subnets change</li> <li>Cannot selectively control which subnets to advertise</li> </ul>"},{"location":"gcp/networking/cloud-router/#custom-mode","title":"Custom Mode","text":"<ul> <li>Manually specify which IP ranges to advertise</li> <li>Greater control over route advertisement</li> <li>Required for advertising &gt; 250 routes</li> <li>Can advertise routes outside VPC CIDR ranges</li> <li>Useful for summarizing routes</li> </ul> <p>Example Use Cases for Custom Mode:</p> <ul> <li>Advertising summary routes instead of individual subnets</li> <li>Advertising secondary IP ranges</li> <li>Advertising IP ranges from other VPCs (via VPC peering)</li> <li>Limiting route advertisements to specific subnets</li> </ul>"},{"location":"gcp/networking/cloud-router/#bgp-configuration","title":"BGP Configuration","text":""},{"location":"gcp/networking/cloud-router/#asn-autonomous-system-number","title":"ASN (Autonomous System Number)","text":"<ul> <li>Cloud Router ASN: Assigned to the Cloud Router</li> <li>Peer ASN: On-premises or partner router ASN</li> <li>Private ASNs: 64512-65534 (16-bit), 4200000000-4294967294 (32-bit)</li> <li>Public ASNs: Registered with IANA (if you own one)</li> </ul> <p>Important: ASN must be unique across your BGP topology. Cloud Router and peer must have different ASNs.</p>"},{"location":"gcp/networking/cloud-router/#bgp-peering","title":"BGP Peering","text":"<p>Each BGP session requires:</p> <ul> <li>Peer IP: On-premises router IP (for VPN, automatically assigned)</li> <li>Peer ASN: Autonomous System Number of peer router</li> <li>Interface: Cloud Router interface (VPN tunnel or VLAN attachment)</li> <li>Advertised Route Priority: MED value for this session</li> <li>MD5 Authentication: Optional but recommended for security</li> </ul>"},{"location":"gcp/networking/cloud-router/#route-priorities","title":"Route Priorities","text":"<p>MED (Multi-Exit Discriminator):</p> <ul> <li>Lower MED = higher priority</li> <li>Default: 100</li> <li>Range: 0-4294967295</li> <li>Use case: Prefer specific paths for inbound traffic from on-premises</li> </ul> <p>AS-PATH Prepending:</p> <ul> <li>Configured on peer side (on-premises router)</li> <li>Longer AS path = lower priority</li> <li>Use case: Influence route selection from Cloud Router perspective</li> </ul>"},{"location":"gcp/networking/cloud-router/#when-to-use","title":"When to Use","text":""},{"location":"gcp/networking/cloud-router/#use-cloud-router-when","title":"\u2705 Use Cloud Router When:","text":"<ol> <li>Hybrid Connectivity with Dynamic Routing</li> <li>Cloud VPN (HA VPN requires Cloud Router)</li> <li>Cloud Interconnect (Dedicated or Partner)</li> <li> <p>Need automatic route updates between GCP and on-premises</p> </li> <li> <p>High Availability Requirements</p> </li> <li>Automatic failover between redundant connections</li> <li>Multiple VPN tunnels or VLAN attachments</li> <li> <p>BFD for sub-second failure detection</p> </li> <li> <p>Complex Network Topologies</p> </li> <li>Multiple on-premises sites</li> <li>Multi-region GCP deployments</li> <li> <p>Hub-and-spoke architectures with dynamic routing</p> </li> <li> <p>Scalable Route Management</p> </li> <li>Large number of subnets that change frequently</li> <li>Want to avoid manual route updates</li> <li> <p>Need route summarization</p> </li> <li> <p>Network Virtual Appliances</p> </li> <li>Integrate third-party routers/firewalls</li> <li>Custom routing requirements</li> <li>Advanced traffic inspection/manipulation</li> </ol>"},{"location":"gcp/networking/cloud-router/#dont-use-cloud-router-when","title":"\u274c Don\u2019t Use Cloud Router When:","text":"<ol> <li>Simple Static Routing Sufficient</li> <li>Few static routes that rarely change</li> <li>Small-scale deployments</li> <li> <p>Classic VPN with static routes is adequate</p> </li> <li> <p>No Hybrid Connectivity</p> </li> <li>Pure cloud-only deployments with no on-premises integration</li> <li> <p>VPC-to-VPC routing handled by VPC peering</p> </li> <li> <p>Layer 2 Requirements</p> </li> <li>Need Layer 2 connectivity (Cloud Router is Layer 3)</li> <li>Use Dedicated Interconnect Layer 2 connections directly</li> </ol>"},{"location":"gcp/networking/cloud-router/#common-architectures","title":"Common Architectures","text":""},{"location":"gcp/networking/cloud-router/#ha-vpn-with-redundant-cloud-routers","title":"HA VPN with Redundant Cloud Routers","text":"<pre><code>On-Premises Router\n    \u251c\u2500\u2500 BGP Session 1 \u2500\u2500\u25b6 Cloud Router 1 (us-central1)\n    \u2502                         \u251c\u2500\u2500 VPN Tunnel 1\n    \u2502                         \u2514\u2500\u2500 VPN Tunnel 2\n    \u2514\u2500\u2500 BGP Session 2 \u2500\u2500\u25b6 Cloud Router 2 (us-central1)\n                              \u251c\u2500\u2500 VPN Tunnel 3\n                              \u2514\u2500\u2500 VPN Tunnel 4\n\nRoute Advertisement: All VPC subnets\nLearned Routes: On-premises CIDRs\nFailover: Automatic via BGP + BFD\n</code></pre>"},{"location":"gcp/networking/cloud-router/#multi-region-with-cloud-interconnect","title":"Multi-Region with Cloud Interconnect","text":"<pre><code>On-Premises\n    \u2514\u2500\u2500 Dedicated Interconnect\n            \u251c\u2500\u2500 VLAN Attachment 1 \u2500\u2500\u25b6 Cloud Router (us-central1)\n            \u2502                              \u2514\u2500\u2500 Advertises: us-central1 subnets\n            \u2514\u2500\u2500 VLAN Attachment 2 \u2500\u2500\u25b6 Cloud Router (europe-west1)\n                                           \u2514\u2500\u2500 Advertises: europe-west1 subnets\n\nVPC Network Mode: Regional dynamic routing\nRoute Preference: Regional (routes preferred in same region)\n</code></pre>"},{"location":"gcp/networking/cloud-router/#router-appliance-with-cloud-router","title":"Router Appliance with Cloud Router","text":"<pre><code>VPC Network\n    \u251c\u2500\u2500 Cloud Router (us-central1)\n    \u2502       \u251c\u2500\u2500 BGP Session \u2500\u2500\u25b6 Third-party Firewall/Router VM\n    \u2502       \u2514\u2500\u2500 Advertises: Custom route 0.0.0.0/0 (default route)\n    \u2502\n    \u2514\u2500\u2500 Subnets\n            \u2514\u2500\u2500 Routes: Default route via appliance VM\n</code></pre>"},{"location":"gcp/networking/cloud-router/#vpc-dynamic-routing-modes","title":"VPC Dynamic Routing Modes","text":"<p>Cloud Router behavior depends on VPC network\u2019s dynamic routing mode:</p>"},{"location":"gcp/networking/cloud-router/#regional-dynamic-routing-default","title":"Regional Dynamic Routing (Default)","text":"<ul> <li>Cloud Router only advertises regional subnet routes</li> <li>Cloud Router only programs learned routes in the same region</li> <li>Use case: Keep traffic regional, reduce inter-region costs</li> </ul> <p>Example: <pre><code>VPC with Regional Dynamic Routing\n\u251c\u2500\u2500 Cloud Router (us-central1)\n\u2502       \u2514\u2500\u2500 Advertises: Only us-central1 subnet routes\n\u2514\u2500\u2500 Cloud Router (europe-west1)\n        \u2514\u2500\u2500 Advertises: Only europe-west1 subnet routes\n</code></pre></p>"},{"location":"gcp/networking/cloud-router/#global-dynamic-routing","title":"Global Dynamic Routing","text":"<ul> <li>Cloud Router advertises all subnet routes across all regions</li> <li>Cloud Router programs learned routes globally in all regions</li> <li>Use case: Global access to on-premises, multi-region failover</li> </ul> <p>Example: <pre><code>VPC with Global Dynamic Routing\n\u251c\u2500\u2500 Cloud Router (us-central1)\n\u2502       \u2514\u2500\u2500 Advertises: ALL subnets (us-central1 + europe-west1 + asia-east1)\n\u2514\u2500\u2500 Cloud Router (europe-west1)\n        \u2514\u2500\u2500 Advertises: ALL subnets (us-central1 + europe-west1 + asia-east1)\n</code></pre></p> <p>Consideration: Global routing enables VMs in any region to reach on-premises via any Cloud Router, but may increase latency and inter-region egress costs.</p>"},{"location":"gcp/networking/cloud-router/#bfd-bidirectional-forwarding-detection","title":"BFD (Bidirectional Forwarding Detection)","text":""},{"location":"gcp/networking/cloud-router/#purpose","title":"Purpose","text":"<p>Provides fast failure detection (sub-second) for BGP sessions, enabling rapid failover.</p>"},{"location":"gcp/networking/cloud-router/#configuration","title":"Configuration","text":"<ul> <li>Enabled per BGP session</li> <li>Min transmit interval: 1000 ms (default)</li> <li>Min receive interval: 1000 ms (default)</li> <li>Multiplier: 3 (default) - 3 missed packets = failure</li> </ul>"},{"location":"gcp/networking/cloud-router/#benefits","title":"Benefits","text":"<ul> <li>Default BGP keepalive (60s) is slow; BFD detects failures in ~3 seconds</li> <li>Faster convergence on path failures</li> <li>Recommended for production deployments</li> </ul>"},{"location":"gcp/networking/cloud-router/#when-to-enable","title":"When to Enable","text":"<ul> <li>\u2705 Production environments requiring high availability</li> <li>\u2705 Active-active VPN tunnel configurations</li> <li>\u2705 Critical workloads needing fast failover</li> <li>\u274c Development/test environments (to reduce complexity)</li> </ul>"},{"location":"gcp/networking/cloud-router/#configuration-best-practices","title":"Configuration Best Practices","text":"<ol> <li>Redundancy</li> <li>Deploy at least two Cloud Routers per region for HA</li> <li>Use multiple BGP sessions per Cloud Router</li> <li> <p>Configure different route priorities for traffic steering</p> </li> <li> <p>Route Management</p> </li> <li>Start with default advertisement mode, switch to custom if needed</li> <li>Use route summarization to reduce advertised route count</li> <li>Document custom route advertisements clearly</li> <li> <p>Monitor learned routes vs quota limits</p> </li> <li> <p>BGP Tuning</p> </li> <li>Enable BFD for fast failure detection</li> <li>Set appropriate MED values for traffic steering</li> <li>Use MD5 authentication on BGP sessions</li> <li> <p>Coordinate AS-PATH prepending with network team</p> </li> <li> <p>Naming Conventions</p> </li> <li>Use descriptive names: <code>cloud-router-us-central1-vpn-primary</code></li> <li>Include region and purpose in the name</li> <li> <p>Document ASN assignments</p> </li> <li> <p>Monitoring</p> </li> <li>Monitor BGP session status</li> <li>Track learned routes count</li> <li>Alert on route quota approaching limits</li> <li>Monitor route propagation delays</li> </ol>"},{"location":"gcp/networking/cloud-router/#troubleshooting-common-issues","title":"Troubleshooting Common Issues","text":""},{"location":"gcp/networking/cloud-router/#bgp-session-not-establishing","title":"BGP Session Not Establishing","text":"<p>Check:</p> <ul> <li>Peer ASN configured correctly</li> <li>Interface (tunnel/VLAN) is UP</li> <li>On-premises router BGP configuration</li> <li>MD5 authentication matches (if used)</li> <li>Firewall allows BGP (TCP 179)</li> </ul>"},{"location":"gcp/networking/cloud-router/#routes-not-being-advertised","title":"Routes Not Being Advertised","text":"<p>Check:</p> <ul> <li>Cloud Router advertisement mode (default vs custom)</li> <li>VPC dynamic routing mode (regional vs global)</li> <li>Custom route ranges configured correctly</li> <li>Route quota not exceeded</li> </ul>"},{"location":"gcp/networking/cloud-router/#routes-not-being-learned","title":"Routes Not Being Learned","text":"<p>Check:</p> <ul> <li>BGP session established</li> <li>On-premises router advertising routes</li> <li>Learned route quota not exceeded</li> <li>Route priority/AS-PATH not causing rejection</li> </ul>"},{"location":"gcp/networking/cloud-router/#asymmetric-routing","title":"Asymmetric Routing","text":"<p>Cause: Different paths for inbound and outbound traffic</p> <p>Solutions:</p> <ul> <li>Adjust MED values to align inbound path</li> <li>Use AS-PATH prepending on outbound path</li> <li>Review VPC dynamic routing mode</li> <li>Check for multiple Cloud Routers with different priorities</li> </ul>"},{"location":"gcp/networking/cloud-router/#cost-considerations","title":"Cost Considerations","text":"<ul> <li>No direct charge for Cloud Router itself</li> <li>Charges apply for associated resources:</li> <li>VPN tunnels (per tunnel-hour)</li> <li>Interconnect VLAN attachments (per attachment-hour)</li> <li>Egress traffic via VPN/Interconnect</li> <li>Cost optimization:</li> <li>Use regional routing mode if global routing not needed (reduces inter-region egress)</li> <li>Consolidate BGP sessions where possible (within limits)</li> </ul>"},{"location":"gcp/networking/cloud-router/#related-services","title":"Related Services","text":"<ul> <li>Cloud VPN: Uses Cloud Router for dynamic routing</li> <li>Cloud Interconnect: Requires Cloud Router for BGP</li> <li>VPC Peering: Does not use Cloud Router (static routing)</li> <li>Network Connectivity Center: Centralized management of hybrid connectivity</li> <li>Router Appliances: Third-party NVAs integrated with Cloud Router</li> <li>Private Service Connect: Access Google services privately (doesn\u2019t use Cloud Router)</li> </ul>"},{"location":"gcp/networking/hybrid-connectivity/","title":"Hybrid Connectivity","text":""},{"location":"gcp/networking/hybrid-connectivity/#description","title":"Description","text":"<p>Hybrid connectivity solutions enable secure connections between on-premises networks and Google Cloud VPC networks. GCP offers two primary options: Cloud VPN (encrypted tunnels over the internet) and Cloud Interconnect (dedicated physical connections). Both work with Cloud Router to enable dynamic routing using BGP.</p>"},{"location":"gcp/networking/hybrid-connectivity/#cloud-vpn","title":"Cloud VPN","text":""},{"location":"gcp/networking/hybrid-connectivity/#description_1","title":"Description","text":"<p>IPsec VPN tunnels that encrypt traffic between your on-premises network and GCP VPC over the public internet.</p>"},{"location":"gcp/networking/hybrid-connectivity/#types","title":"Types","text":"<p>HA VPN (High Availability VPN)</p> <ul> <li>SLA: 99.99% availability</li> <li>Architecture: Two interfaces, two external IP addresses, redundant tunnels</li> <li>Routing: Dynamic (BGP) only</li> <li>Recommended: Standard option for production workloads</li> </ul> <p>Classic VPN</p> <ul> <li>SLA: 99.9% availability  </li> <li>Architecture: Single interface, single external IP</li> <li>Routing: Static or dynamic (BGP)</li> <li>Status: Legacy, HA VPN preferred</li> </ul>"},{"location":"gcp/networking/hybrid-connectivity/#key-features","title":"Key Features","text":"<ul> <li>Encryption: IPsec tunnel with IKEv1 or IKEv2</li> <li>Bandwidth: Up to 3 Gbps per tunnel, use multiple tunnels for more</li> <li>Routing: Dynamic routing with Cloud Router (BGP)</li> <li>Redundancy: Active-active configuration with HA VPN</li> <li>Cost-Effective: No dedicated hardware required</li> <li>Topology: Supports VPN to on-premises, VPN to another cloud, VPC-to-VPC</li> </ul>"},{"location":"gcp/networking/hybrid-connectivity/#limits","title":"Limits","text":"Limit Value Notes Throughput per tunnel 3 Gbps Use multiple tunnels for higher bandwidth Tunnels per gateway 8 (HA VPN) Can create multiple gateways Tunnels per Cloud Router 128 Default limit MTU 1460 bytes Due to IPsec overhead Encryption Overhead ~10-15% Impacts throughput"},{"location":"gcp/networking/hybrid-connectivity/#when-to-use-cloud-vpn","title":"When to Use Cloud VPN","text":"<p>\u2705 Use When:</p> <ul> <li>Moderate bandwidth requirements (&lt; 10 Gbps aggregate)</li> <li>Quick setup needed (hours, not weeks)</li> <li>Cost sensitivity (cheaper than Interconnect)</li> <li>Geographic flexibility (works anywhere with internet)</li> <li>Temporary connectivity needs</li> <li>Disaster recovery secondary path</li> </ul> <p>\u274c Don\u2019t Use When:</p> <ul> <li>Require &gt; 10 Gbps consistent throughput</li> <li>Need lowest possible latency (Interconnect is better)</li> <li>Compliance requires dedicated connection</li> <li>Require 99.99% SLA with deterministic routing (use Dedicated Interconnect)</li> </ul>"},{"location":"gcp/networking/hybrid-connectivity/#cloud-interconnect","title":"Cloud Interconnect","text":""},{"location":"gcp/networking/hybrid-connectivity/#description_2","title":"Description","text":"<p>Dedicated physical connections between on-premises infrastructure and Google Cloud, providing higher bandwidth and lower latency than VPN.</p>"},{"location":"gcp/networking/hybrid-connectivity/#types_1","title":"Types","text":"<p>Dedicated Interconnect</p> <ul> <li>Connection: Direct physical connection to Google network</li> <li>Bandwidth: 10 Gbps or 100 Gbps per circuit</li> <li>Location: Must be in a supported colocation facility</li> <li>SLA: 99.99% with proper redundancy configuration</li> <li>Use Case: High bandwidth, low latency, dedicated connectivity</li> </ul> <p>Partner Interconnect </p> <ul> <li>Connection: Through supported service provider</li> <li>Bandwidth: 50 Mbps to 50 Gbps per VLAN attachment</li> <li>Location: Service provider handles physical connectivity</li> <li>SLA: 99.99% with proper redundancy (depends on provider)</li> <li>Use Case: When not near Google edge location, or need managed service</li> </ul>"},{"location":"gcp/networking/hybrid-connectivity/#key-features_1","title":"Key Features","text":"<ul> <li>Private Connectivity: Traffic doesn\u2019t traverse public internet</li> <li>Low Latency: Direct connection to Google network</li> <li>High Bandwidth: 10-200 Gbps (Dedicated), 50 Mbps - 50 Gbps (Partner)</li> <li>Flexible Routing: BGP with Cloud Router</li> <li>Layer 2 or Layer 3: Dedicated supports both, Partner is Layer 3</li> <li>VLAN Attachments: Multiple VLANs over single physical connection</li> </ul>"},{"location":"gcp/networking/hybrid-connectivity/#limits_1","title":"Limits","text":"<p>Dedicated Interconnect</p> Limit Value Notes Bandwidth per connection 10 or 100 Gbps Fixed circuit sizes VLAN attachments per interconnect 16 Per cloud router Maximum total bandwidth 8 x 100 Gbps 800 Gbps per VPC MTU 1500 bytes (default), up to 1440 for Partner Standard Ethernet MTU <p>Partner Interconnect</p> Limit Value Notes Bandwidth per attachment 50 Mbps - 50 Gbps Varies by provider VLAN attachments per router 16 Can use multiple routers Provider availability Geographic dependent Check provider coverage"},{"location":"gcp/networking/hybrid-connectivity/#when-to-use-cloud-interconnect","title":"When to Use Cloud Interconnect","text":"<p>\u2705 Use Dedicated Interconnect When:</p> <ul> <li>Require &gt; 10 Gbps bandwidth</li> <li>Already have presence in Google colocation facility</li> <li>Need lowest latency to Google Cloud</li> <li>Large-scale enterprise workloads</li> <li>Compliance requires dedicated physical connection</li> <li>Predictable network performance required</li> </ul> <p>\u2705 Use Partner Interconnect When:</p> <ul> <li>Need &gt; 3 Gbps but not near Google colocation</li> <li>Want managed service from network provider</li> <li>Need flexible bandwidth (can start small, scale up)</li> <li>Faster deployment than Dedicated Interconnect</li> <li>Provider already serves your location</li> </ul> <p>\u274c Don\u2019t Use Interconnect When:</p> <ul> <li>Bandwidth requirements &lt; 1 Gbps (VPN more cost-effective)</li> <li>Quick POC or temporary project</li> <li>Budget constraints (VPN is cheaper)</li> <li>Geographic distance makes latency gains negligible</li> </ul>"},{"location":"gcp/networking/hybrid-connectivity/#architecture-patterns","title":"Architecture Patterns","text":""},{"location":"gcp/networking/hybrid-connectivity/#ha-vpn-with-redundancy","title":"HA VPN with Redundancy","text":"<p><pre><code>On-Premises Router 1 \u2550\u2550\u2550 Tunnel 1 \u2550\u2550\u2550\u2557\n                                      \u2560\u2550\u2550\u2550 HA VPN Gateway (Interface 0)\nOn-Premises Router 1 \u2550\u2550\u2550 Tunnel 2 \u2550\u2550\u2550\u255d\n\nOn-Premises Router 2 \u2550\u2550\u2550 Tunnel 3 \u2550\u2550\u2550\u2557\n                                      \u2560\u2550\u2550\u2550 HA VPN Gateway (Interface 1)\nOn-Premises Router 2 \u2550\u2550\u2550 Tunnel 4 \u2550\u2550\u2550\u255d\n</code></pre> Result: 99.99% SLA, survives any single router or tunnel failure</p>"},{"location":"gcp/networking/hybrid-connectivity/#dedicated-interconnect-with-redundancy","title":"Dedicated Interconnect with Redundancy","text":"<p><pre><code>On-Premises \u2550\u2550 Metro 1 Circuit 1 \u2550\u2550 Interconnect Location A \u2550\u2550\u2557\n                                                               \u2560\u2550\u2550 VPC Network\nOn-Premises \u2550\u2550 Metro 2 Circuit 2 \u2550\u2550 Interconnect Location B \u2550\u2550\u255d\n</code></pre> Result: 99.99% SLA, geographic and circuit redundancy</p>"},{"location":"gcp/networking/hybrid-connectivity/#hybrid-interconnect-vpn-backup","title":"Hybrid: Interconnect + VPN Backup","text":"<p><pre><code>Primary:   On-Premises \u2550\u2550 Interconnect \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550 VPC\nBackup:    On-Premises \u2550\u2550 HA VPN \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550= VPC\n</code></pre> Result: High availability with failover to VPN if Interconnect fails</p>"},{"location":"gcp/networking/hybrid-connectivity/#comparison-matrix","title":"Comparison Matrix","text":"Feature HA VPN Dedicated Interconnect Partner Interconnect Bandwidth Up to 3 Gbps/tunnel 10 or 100 Gbps 50 Mbps - 50 Gbps Latency Medium (internet) Low (direct) Low (direct) SLA 99.99% 99.99%* 99.99%* Setup Time Hours Weeks/months Days/weeks Cost Low High Medium Location Requirement None Colocation facility Provider coverage Encryption IPsec (built-in) Optional (MACsec) Optional Use Case Small-medium workloads Large enterprise Medium-large workloads <p>*With proper redundancy configuration</p>"},{"location":"gcp/networking/hybrid-connectivity/#configuration-requirements","title":"Configuration Requirements","text":""},{"location":"gcp/networking/hybrid-connectivity/#cloud-router","title":"Cloud Router","text":"<ul> <li>Required: For dynamic routing (BGP) with VPN and Interconnect</li> <li>ASN: Private ASN (64512-65534, 4200000000-4294967294) or public ASN</li> <li>BGP Sessions: One per tunnel/VLAN attachment</li> <li>Route Advertisement: Automatically advertises VPC subnet routes</li> </ul>"},{"location":"gcp/networking/hybrid-connectivity/#bgp-configuration","title":"BGP Configuration","text":"<ul> <li>BGP Peer: Configure on-premises router and Cloud Router</li> <li>ASN: Unique ASN for on-premises and Cloud Router</li> <li>MD5 Authentication: Optional but recommended</li> <li>Route Priorities: Set MED or local preference for failover</li> <li>BFD: Enable for faster failure detection (sub-second)</li> </ul>"},{"location":"gcp/networking/hybrid-connectivity/#firewall-considerations","title":"Firewall Considerations","text":"<ul> <li>Allow IKE (UDP 500, 4500) for VPN</li> <li>Allow BGP (TCP 179) if using dynamic routing</li> <li>Configure VPC firewall rules for on-premises IP ranges</li> <li>Consider hierarchical firewall policies for organization-wide rules</li> </ul>"},{"location":"gcp/networking/hybrid-connectivity/#best-practices","title":"Best Practices","text":"<ol> <li>Redundancy</li> <li>Always configure redundant tunnels/circuits</li> <li>Use separate physical paths when possible</li> <li> <p>Test failover scenarios regularly</p> </li> <li> <p>Route Management</p> </li> <li>Use BGP for dynamic routing</li> <li>Set appropriate route priorities (MED, AS-PATH prepend)</li> <li>Limit route advertisements (use route filters)</li> <li> <p>Enable BFD for fast failure detection</p> </li> <li> <p>Monitoring</p> </li> <li>Monitor tunnel/circuit status</li> <li>Set up alerts for connection failures</li> <li>Track bandwidth utilization</li> <li> <p>Use VPC Flow Logs for traffic analysis</p> </li> <li> <p>Security</p> </li> <li>Use strong encryption (AES-256-GCM for VPN)</li> <li>Enable MD5 authentication on BGP sessions</li> <li>Consider MACsec for Interconnect encryption</li> <li> <p>Implement principle of least privilege for route advertisements</p> </li> <li> <p>Capacity Planning</p> </li> <li>Size connections for peak + 30-50% headroom</li> <li>Plan for growth (easier to add VPN tunnels than Interconnect circuits)</li> <li>Consider burst traffic patterns</li> <li>Monitor usage trends</li> </ol>"},{"location":"gcp/networking/hybrid-connectivity/#cost-optimization","title":"Cost Optimization","text":"<p>Cloud VPN</p> <ul> <li>Charged per tunnel hour + egress bandwidth</li> <li>Multiple tunnels increase cost (but provide redundancy)</li> <li>Egress to on-premises charged at internet egress rates</li> </ul> <p>Cloud Interconnect</p> <ul> <li>Dedicated: Fixed port fee + egress bandwidth (discounted)</li> <li>Partner: Varies by provider + egress bandwidth</li> <li>Generally cheaper egress rates than VPN at scale</li> <li>Break-even typically around 1-3 Gbps sustained traffic</li> </ul>"},{"location":"gcp/networking/hybrid-connectivity/#related-services","title":"Related Services","text":"<ul> <li>Cloud Router: Dynamic routing for VPN and Interconnect</li> <li>Cloud DNS: Extend DNS resolution to on-premises</li> <li>Private Google Access: Access Google services without public IPs</li> <li>Network Connectivity Center: Central hub for managing hybrid connectivity</li> </ul>"},{"location":"gcp/networking/iam-roles/","title":"IAM Roles for Networking","text":""},{"location":"gcp/networking/iam-roles/#description","title":"Description","text":"<p>IAM (Identity and Access Management) controls who can perform what actions on which GCP networking resources. Proper IAM configuration is critical for security, compliance, and operational efficiency. This guide covers the key predefined roles for networking and when to use them.</p>"},{"location":"gcp/networking/iam-roles/#role-hierarchy","title":"Role Hierarchy","text":""},{"location":"gcp/networking/iam-roles/#primitive-roles-avoid-for-production","title":"Primitive Roles (Avoid for Production)","text":"<ul> <li>Owner: Full access (too permissive)</li> <li>Editor: Modify resources (too permissive)</li> <li>Viewer: Read-only access (acceptable for read operations)</li> </ul> <p>Recommendation: Use predefined or custom roles instead of primitive roles for networking.</p>"},{"location":"gcp/networking/iam-roles/#core-networking-roles","title":"Core Networking Roles","text":""},{"location":"gcp/networking/iam-roles/#compute-network-admin-rolescomputenetworkadmin","title":"Compute Network Admin (<code>roles/compute.networkAdmin</code>)","text":"<p>Permissions: Full control over networking resources</p> <p>Can:</p> <ul> <li>Create, modify, delete VPC networks, subnets, routes</li> <li>Configure firewall rules, firewall policies</li> <li>Manage VPN gateways, tunnels, Cloud Routers</li> <li>Configure load balancers and SSL certificates</li> <li>Create and delete Private Service Connect endpoints</li> <li>Manage network peering connections</li> <li>Configure Cloud NAT</li> <li>Administer Private Google Access settings</li> </ul> <p>Cannot:</p> <ul> <li>Create instances (requires <code>compute.instanceAdmin</code>)</li> <li>Modify organization policies</li> <li>Manage billing</li> <li>Create Shared VPC host projects (requires <code>compute.xpnAdmin</code>)</li> </ul> <p>Use Case:</p> <ul> <li>Network administrators managing VPC infrastructure</li> <li>Platform teams responsible for network design and implementation</li> <li>DevOps engineers managing cloud networking</li> </ul> <p>Scope: Project-level (can also grant at folder/org level)</p>"},{"location":"gcp/networking/iam-roles/#compute-network-user-rolescomputenetworkuser","title":"Compute Network User (<code>roles/compute.networkUser</code>)","text":"<p>Permissions: Use existing network resources but cannot create or modify them</p> <p>Can:</p> <ul> <li>Use VPC subnets to create instances</li> <li>Attach instances to existing subnets</li> <li>View network, subnet, route information (read-only)</li> <li>Use shared VPC subnets (critical for service projects)</li> </ul> <p>Cannot:</p> <ul> <li>Create or delete networks, subnets</li> <li>Modify firewall rules</li> <li>Create VPN or load balancers</li> <li>Change network configurations</li> </ul> <p>Use Case:</p> <ul> <li>Application teams creating VMs in service projects</li> <li>Developers deploying workloads to existing infrastructure</li> <li>Service accounts for GKE, Cloud Run needing network access</li> <li>Users in Shared VPC service projects</li> </ul> <p>Scope: Project-level or subnet-level (for granular access)</p> <p>Important: In Shared VPC, grant this role at subnet level in host project to service project service accounts.</p>"},{"location":"gcp/networking/iam-roles/#compute-network-viewer-rolescomputenetworkviewer","title":"Compute Network Viewer (<code>roles/compute.networkViewer</code>)","text":"<p>Permissions: Read-only access to networking resources</p> <p>Can:</p> <ul> <li>View VPC networks, subnets, routes</li> <li>View firewall rules and policies</li> <li>View VPN gateways, Cloud Routers, load balancers</li> <li>View network topology and configuration</li> <li>Access networking metrics and logs (if logging viewer role also granted)</li> </ul> <p>Cannot:</p> <ul> <li>Create, modify, or delete any resources</li> <li>Use network resources to create instances</li> </ul> <p>Use Case:</p> <ul> <li>Auditors reviewing network configurations</li> <li>Read-only access for troubleshooting</li> <li>Security teams reviewing firewall rules</li> <li>Junior team members learning infrastructure</li> </ul> <p>Scope: Project, folder, or organization level</p>"},{"location":"gcp/networking/iam-roles/#compute-security-admin-rolescomputesecurityadmin","title":"Compute Security Admin (<code>roles/compute.securityAdmin</code>)","text":"<p>Permissions: Manage security-related networking resources</p> <p>Can:</p> <ul> <li>Create, modify, delete firewall rules</li> <li>Manage SSL certificates and SSL policies</li> <li>Configure Cloud Armor security policies</li> <li>Manage organization firewall policies (if org-level)</li> <li>View network resources (read-only)</li> </ul> <p>Cannot:</p> <ul> <li>Create or modify VPC networks, subnets</li> <li>Create VPN or load balancers (only security configs)</li> <li>Modify routes</li> </ul> <p>Use Case:</p> <ul> <li>Security teams managing firewall rules</li> <li>Compliance teams enforcing security policies</li> <li>Separation of duties (security vs networking)</li> </ul> <p>Scope: Project, folder, or organization level</p>"},{"location":"gcp/networking/iam-roles/#shared-vpc-specific-roles","title":"Shared VPC Specific Roles","text":""},{"location":"gcp/networking/iam-roles/#compute-shared-vpc-admin-rolescomputexpnadmin","title":"Compute Shared VPC Admin (<code>roles/compute.xpnAdmin</code>)","text":"<p>Permissions: Enable and manage Shared VPC configuration</p> <p>Can:</p> <ul> <li>Enable/disable a project as Shared VPC host</li> <li>Attach/detach service projects to host projects</li> <li>View Shared VPC configuration</li> </ul> <p>Cannot:</p> <ul> <li>Create networks or subnets (requires <code>networkAdmin</code>)</li> <li>Grant networkUser role (requires IAM admin permissions)</li> </ul> <p>Use Case:</p> <ul> <li>Organization admins setting up Shared VPC</li> <li>Platform teams managing multi-project architectures</li> </ul> <p>Scope: Organization or folder level only (cannot grant at project level)</p> <p>Important: Typically paired with <code>compute.networkAdmin</code> for full Shared VPC management.</p>"},{"location":"gcp/networking/iam-roles/#service-project-admin-no-specific-role","title":"Service Project Admin (No specific role)","text":"<p>Pattern: Combine multiple roles for service project administrators</p> <p>Typical Combination:</p> <ul> <li><code>roles/compute.instanceAdmin</code>: Create VMs</li> <li><code>roles/compute.networkUser</code>: Use shared subnets (granted on host project subnets)</li> <li><code>roles/iam.serviceAccountUser</code>: Attach service accounts</li> </ul> <p>Use Case: Users who manage resources in service projects of Shared VPC</p>"},{"location":"gcp/networking/iam-roles/#specialized-networking-roles","title":"Specialized Networking Roles","text":""},{"location":"gcp/networking/iam-roles/#compute-load-balancer-admin-rolescomputeloadbalanceradmin","title":"Compute Load Balancer Admin (<code>roles/compute.loadBalancerAdmin</code>)","text":"<p>Permissions: Manage load balancing resources</p> <p>Can:</p> <ul> <li>Create, modify, delete load balancers</li> <li>Configure backend services, health checks</li> <li>Manage SSL certificates for load balancers</li> <li>Configure URL maps, target proxies</li> <li>Manage network endpoint groups (NEGs)</li> </ul> <p>Cannot:</p> <ul> <li>Modify VPC networks or subnets</li> <li>Create instances (backend resources)</li> </ul> <p>Use Case:</p> <ul> <li>Application teams managing their own load balancers</li> <li>Separation of duties for load balancer management</li> </ul>"},{"location":"gcp/networking/iam-roles/#compute-public-ip-admin-rolescomputepublicipadmin","title":"Compute Public IP Admin (<code>roles/compute.publicIpAdmin</code>)","text":"<p>Permissions: Manage external IP addresses</p> <p>Can:</p> <ul> <li>Reserve and release external IP addresses</li> <li>Promote ephemeral IPs to static</li> <li>View IP address usage</li> </ul> <p>Cannot:</p> <ul> <li>Attach IPs to instances (requires instance admin)</li> <li>Modify networks or subnets</li> </ul> <p>Use Case:</p> <ul> <li>IP address management teams</li> <li>Resource optimization (releasing unused IPs)</li> </ul>"},{"location":"gcp/networking/iam-roles/#service-networking-admin-rolesservicenetworkingnetworksadmin","title":"Service Networking Admin (<code>roles/servicenetworking.networksAdmin</code>)","text":"<p>Permissions: Manage private service connections</p> <p>Can:</p> <ul> <li>Create Private Service Connect endpoints</li> <li>Configure private IP allocations for Google services</li> <li>Manage peered VPC connections for managed services (e.g., Cloud SQL)</li> <li>Configure Private Service Connect forwarding rules</li> </ul> <p>Cannot:</p> <ul> <li>Modify VPC networks (requires <code>networkAdmin</code>)</li> </ul> <p>Use Case:</p> <ul> <li>Platform teams configuring private access to Google services</li> <li>Database administrators setting up Cloud SQL private IP</li> </ul>"},{"location":"gcp/networking/iam-roles/#dns-administrator-rolesdnsadmin","title":"DNS Administrator (<code>roles/dns.admin</code>)","text":"<p>Permissions: Manage Cloud DNS resources</p> <p>Can:</p> <ul> <li>Create, modify, delete DNS zones</li> <li>Manage DNS records</li> <li>Configure DNS peering</li> <li>Manage DNS policies (split horizon, inbound/outbound forwarding)</li> </ul> <p>Cannot:</p> <ul> <li>Modify VPC networks directly (but can peer DNS zones)</li> </ul> <p>Use Case:</p> <ul> <li>DNS administrators managing internal and external DNS</li> <li>Multi-cloud DNS integration</li> </ul>"},{"location":"gcp/networking/iam-roles/#custom-roles","title":"Custom Roles","text":""},{"location":"gcp/networking/iam-roles/#when-to-create-custom-roles","title":"When to Create Custom Roles","text":"<p>\u2705 Use Custom Roles When:</p> <ul> <li>Predefined roles are too permissive</li> <li>Need specific combination of permissions</li> <li>Compliance requires fine-grained access control</li> <li>Implementing least-privilege security model</li> </ul> <p>\u274c Avoid Custom Roles When:</p> <ul> <li>Predefined roles meet requirements</li> <li>Organization lacks resources to maintain custom roles</li> <li>Complexity outweighs security benefits</li> </ul>"},{"location":"gcp/networking/iam-roles/#example-custom-role-vpn-operator","title":"Example Custom Role: VPN Operator","text":"<p>Use Case: Allow VPN tunnel management without full network admin access</p> <pre><code>title: \"VPN Operator\"\ndescription: \"Manage VPN tunnels and Cloud Routers\"\nincludedPermissions:\n\n  - compute.vpnGateways.*\n  - compute.vpnTunnels.*\n  - compute.routers.*\n  - compute.routes.get\n  - compute.routes.list\n  - compute.networks.get\n  - compute.networks.list\n  - compute.regions.get\n  - compute.regions.list\n</code></pre>"},{"location":"gcp/networking/iam-roles/#example-custom-role-firewall-rule-manager","title":"Example Custom Role: Firewall Rule Manager","text":"<p>Use Case: Manage firewall rules without network creation permissions</p> <pre><code>title: \"Firewall Rule Manager\"\ndescription: \"Create and modify firewall rules only\"\nincludedPermissions:\n\n  - compute.firewalls.*\n  - compute.networks.get\n  - compute.networks.list\n  - compute.networks.updatePolicy\n</code></pre>"},{"location":"gcp/networking/iam-roles/#iam-best-practices-for-networking","title":"IAM Best Practices for Networking","text":""},{"location":"gcp/networking/iam-roles/#1-principle-of-least-privilege","title":"1. Principle of Least Privilege","text":"<ul> <li>Grant minimum permissions needed</li> <li>Use predefined roles when possible</li> <li>Create custom roles for specific needs</li> <li>Avoid primitive roles (Owner, Editor) in production</li> </ul>"},{"location":"gcp/networking/iam-roles/#2-use-groups-not-individual-users","title":"2. Use Groups, Not Individual Users","text":"<pre><code># Good: Grant role to group\ngcloud projects add-iam-policy-binding PROJECT_ID \\\n  --member=\"group:network-admins@example.com\" \\\n  --role=\"roles/compute.networkAdmin\"\n\n# Avoid: Grant role to individual users\ngcloud projects add-iam-policy-binding PROJECT_ID \\\n  --member=\"user:alice@example.com\" \\\n  --role=\"roles/compute.networkAdmin\"\n</code></pre>"},{"location":"gcp/networking/iam-roles/#3-scope-roles-appropriately","title":"3. Scope Roles Appropriately","text":"<p>Organization Level: Broad permissions across all projects <pre><code>gcloud organizations add-iam-policy-binding ORG_ID \\\n  --member=\"group:network-admins@example.com\" \\\n  --role=\"roles/compute.networkViewer\"\n</code></pre></p> <p>Folder Level: Specific business unit or environment <pre><code>gcloud resource-manager folders add-iam-policy-binding FOLDER_ID \\\n  --member=\"group:prod-network-admins@example.com\" \\\n  --role=\"roles/compute.networkAdmin\"\n</code></pre></p> <p>Project Level: Individual project permissions <pre><code>gcloud projects add-iam-policy-binding PROJECT_ID \\\n  --member=\"group:dev-team@example.com\" \\\n  --role=\"roles/compute.networkUser\"\n</code></pre></p> <p>Subnet Level (Shared VPC): Granular service project access <pre><code>gcloud compute networks subnets add-iam-policy-binding SUBNET_NAME \\\n  --member=\"serviceAccount:SERVICE_PROJECT_ID@cloudservices.gserviceaccount.com\" \\\n  --role=\"roles/compute.networkUser\" \\\n  --region=REGION\n</code></pre></p>"},{"location":"gcp/networking/iam-roles/#4-service-accounts-for-automation","title":"4. Service Accounts for Automation","text":"<p>Pattern: Use dedicated service accounts with minimal permissions</p> <pre><code># Create service account for Terraform\ngcloud iam service-accounts create terraform-network-sa \\\n  --display-name=\"Terraform Network Automation\"\n\n# Grant specific network admin permissions\ngcloud projects add-iam-policy-binding PROJECT_ID \\\n  --member=\"serviceAccount:terraform-network-sa@PROJECT_ID.iam.gserviceaccount.com\" \\\n  --role=\"roles/compute.networkAdmin\"\n</code></pre>"},{"location":"gcp/networking/iam-roles/#5-audit-and-review","title":"5. Audit and Review","text":"<ul> <li>Regularly review IAM bindings</li> <li>Use Cloud Asset Inventory for IAM audits</li> <li>Enable audit logging for IAM changes</li> <li>Implement periodic access reviews</li> <li>Remove unused or stale permissions</li> </ul>"},{"location":"gcp/networking/iam-roles/#6-separation-of-duties","title":"6. Separation of Duties","text":"<p>Example: Separate networking and security management</p> <pre><code>Network Admins Group:\n\n  - roles/compute.networkAdmin (VPC, subnets, routes, VPN)\n\nSecurity Admins Group:\n\n  - roles/compute.securityAdmin (firewall rules, SSL policies)\n\nShared VPC Admins Group:\n\n  - roles/compute.xpnAdmin (Shared VPC management)\n  - roles/compute.networkAdmin (network resources)\n</code></pre>"},{"location":"gcp/networking/iam-roles/#common-iam-patterns","title":"Common IAM Patterns","text":""},{"location":"gcp/networking/iam-roles/#pattern-1-shared-vpc-setup","title":"Pattern 1: Shared VPC Setup","text":"<p>Host Project: <pre><code># Enable Shared VPC (requires xpnAdmin at org/folder level)\ngcloud compute shared-vpc enable HOST_PROJECT_ID\n\n# Grant networkAdmin to network team\ngcloud projects add-iam-policy-binding HOST_PROJECT_ID \\\n  --member=\"group:network-admins@example.com\" \\\n  --role=\"roles/compute.networkAdmin\"\n</code></pre></p> <p>Service Project: <pre><code># Attach service project (requires xpnAdmin)\ngcloud compute shared-vpc associated-projects add SERVICE_PROJECT_ID \\\n  --host-project=HOST_PROJECT_ID\n\n# Grant networkUser on specific subnet to service project\ngcloud compute networks subnets add-iam-policy-binding SUBNET_NAME \\\n  --member=\"serviceAccount:SERVICE_PROJECT_NUMBER@cloudservices.gserviceaccount.com\" \\\n  --role=\"roles/compute.networkUser\" \\\n  --region=REGION\n\n# Grant instance admin to application team in service project\ngcloud projects add-iam-policy-binding SERVICE_PROJECT_ID \\\n  --member=\"group:app-team@example.com\" \\\n  --role=\"roles/compute.instanceAdmin\"\n</code></pre></p>"},{"location":"gcp/networking/iam-roles/#pattern-2-multi-environment-access-control","title":"Pattern 2: Multi-Environment Access Control","text":"<p>Production: <pre><code># Strict permissions, network changes require approval\ngcloud projects add-iam-policy-binding PROD_PROJECT_ID \\\n  --member=\"group:prod-network-admins@example.com\" \\\n  --role=\"roles/compute.networkAdmin\"\n\ngcloud projects add-iam-policy-binding PROD_PROJECT_ID \\\n  --member=\"group:developers@example.com\" \\\n  --role=\"roles/compute.networkViewer\"\n</code></pre></p> <p>Development: <pre><code># More permissive for faster iteration\ngcloud projects add-iam-policy-binding DEV_PROJECT_ID \\\n  --member=\"group:developers@example.com\" \\\n  --role=\"roles/compute.networkAdmin\"\n</code></pre></p>"},{"location":"gcp/networking/iam-roles/#pattern-3-read-only-network-auditor","title":"Pattern 3: Read-Only Network Auditor","text":"<pre><code># Grant read-only access for auditing\ngcloud organizations add-iam-policy-binding ORG_ID \\\n  --member=\"group:network-auditors@example.com\" \\\n  --role=\"roles/compute.networkViewer\"\n\n# Also grant logging viewer for complete audit trail\ngcloud organizations add-iam-policy-binding ORG_ID \\\n  --member=\"group:network-auditors@example.com\" \\\n  --role=\"roles/logging.viewer\"\n</code></pre>"},{"location":"gcp/networking/iam-roles/#troubleshooting-iam-issues","title":"Troubleshooting IAM Issues","text":""},{"location":"gcp/networking/iam-roles/#permission-denied-errors","title":"\u201cPermission Denied\u201d Errors","text":"<p>Symptoms: User cannot create VMs in Shared VPC subnet</p> <p>Common Cause: Missing <code>compute.networkUser</code> role on subnet</p> <p>Solution: <pre><code># Grant networkUser on specific subnet\ngcloud compute networks subnets add-iam-policy-binding SUBNET_NAME \\\n  --member=\"serviceAccount:SERVICE_ACCOUNT@example.com\" \\\n  --role=\"roles/compute.networkUser\" \\\n  --region=REGION\n</code></pre></p>"},{"location":"gcp/networking/iam-roles/#cannot-attach-service-project-to-shared-vpc","title":"Cannot Attach Service Project to Shared VPC","text":"<p>Common Cause: Missing <code>compute.xpnAdmin</code> role or granted at project level</p> <p>Solution: <pre><code># Grant xpnAdmin at organization or folder level\ngcloud organizations add-iam-policy-binding ORG_ID \\\n  --member=\"user:admin@example.com\" \\\n  --role=\"roles/compute.xpnAdmin\"\n</code></pre></p>"},{"location":"gcp/networking/iam-roles/#service-account-cannot-create-resources","title":"Service Account Cannot Create Resources","text":"<p>Common Cause: Impersonation or missing role on service account</p> <p>Solution: <pre><code># Grant service account user role\ngcloud iam service-accounts add-iam-policy-binding SERVICE_ACCOUNT@PROJECT.iam.gserviceaccount.com \\\n  --member=\"user:admin@example.com\" \\\n  --role=\"roles/iam.serviceAccountUser\"\n\n# OR grant directly to service account\ngcloud projects add-iam-policy-binding PROJECT_ID \\\n  --member=\"serviceAccount:SERVICE_ACCOUNT@PROJECT.iam.gserviceaccount.com\" \\\n  --role=\"roles/compute.networkAdmin\"\n</code></pre></p>"},{"location":"gcp/networking/iam-roles/#security-considerations","title":"Security Considerations","text":""},{"location":"gcp/networking/iam-roles/#1-avoid-over-permissive-roles","title":"1. Avoid Over-Permissive Roles","text":"<p>\u274c Bad: Granting <code>roles/owner</code> for network tasks \u2705 Good: Granting <code>roles/compute.networkAdmin</code></p>"},{"location":"gcp/networking/iam-roles/#2-protect-sensitive-resources","title":"2. Protect Sensitive Resources","text":"<ul> <li>Use organizational policies to enforce controls</li> <li>Require approval for firewall rule changes</li> <li>Implement four-eyes principle for production changes</li> </ul>"},{"location":"gcp/networking/iam-roles/#3-monitor-iam-changes","title":"3. Monitor IAM Changes","text":"<pre><code># Set up log-based alert for IAM policy changes\ngcloud logging sinks create iam-policy-changes \\\n  storage.googleapis.com/iam-audit-logs-bucket \\\n  --log-filter='protoPayload.methodName=\"SetIamPolicy\"'\n</code></pre>"},{"location":"gcp/networking/iam-roles/#4-rotate-service-account-keys","title":"4. Rotate Service Account Keys","text":"<ul> <li>Use short-lived credentials when possible</li> <li>Rotate long-lived keys regularly</li> <li>Use Workload Identity for GKE instead of service account keys</li> </ul>"},{"location":"gcp/networking/iam-roles/#5-conditional-iam-context-aware-access","title":"5. Conditional IAM (Context-Aware Access)","text":"<pre><code># Example: Grant networkAdmin only from corporate network\nbindings:\n\n  - members:\n      - group:network-admins@example.com\n    role: roles/compute.networkAdmin\n    condition:\n      expression: \"request.auth.claims.iss == 'https://accounts.google.com' &amp;&amp; \n                   origin.ip in ['203.0.113.0/24']\"\n      title: \"Allow only from corporate network\"\n</code></pre>"},{"location":"gcp/networking/iam-roles/#related-documentation","title":"Related Documentation","text":"<ul> <li>IAM Overview: https://cloud.google.com/iam/docs</li> <li>Predefined Roles: https://cloud.google.com/iam/docs/understanding-roles</li> <li>Custom Roles: https://cloud.google.com/iam/docs/creating-custom-roles</li> <li>Shared VPC IAM: https://cloud.google.com/vpc/docs/shared-vpc#iam_in_shared_vpc</li> <li>Best Practices: https://cloud.google.com/iam/docs/best-practices</li> </ul>"},{"location":"gcp/networking/load-balancing/","title":"Load Balancing","text":""},{"location":"gcp/networking/load-balancing/#description","title":"Description","text":"<p>Google Cloud Load Balancing is a fully distributed, software-defined managed service that distributes traffic across multiple backend instances. It\u2019s not instance-based or device-based, providing seamless auto-scaling and high availability. GCP offers multiple load balancer types optimized for different use cases.</p>"},{"location":"gcp/networking/load-balancing/#load-balancer-types-overview","title":"Load Balancer Types Overview","text":""},{"location":"gcp/networking/load-balancing/#global-vs-regional","title":"Global vs Regional","text":"<ul> <li>Global: Serve users globally from multiple regions, single anycast IP</li> <li>Regional: Serve users within a single region, useful for internal services</li> </ul>"},{"location":"gcp/networking/load-balancing/#external-vs-internal","title":"External vs Internal","text":"<ul> <li>External: Accept traffic from the internet</li> <li>Internal: Accept traffic only from within VPC or connected networks</li> </ul>"},{"location":"gcp/networking/load-balancing/#layer-4-vs-layer-7","title":"Layer 4 vs Layer 7","text":"<ul> <li>Layer 4 (Network/Transport): TCP/UDP, connection-based routing</li> <li>Layer 7 (Application): HTTP(S), content-based routing, URL mapping</li> </ul>"},{"location":"gcp/networking/load-balancing/#application-load-balancer-layer-7","title":"Application Load Balancer (Layer 7)","text":""},{"location":"gcp/networking/load-balancing/#external-application-load-balancer-global","title":"External Application Load Balancer (Global)","text":"<p>Description: Global Layer 7 load balancer for HTTP(S) traffic with content-based routing.</p> <p>Key Features:</p> <ul> <li>Global load balancing: Single anycast IP serves worldwide</li> <li>SSL/TLS termination: Managed certificates with automatic renewal</li> <li>Content-based routing: URL maps, host rules, path rules</li> <li>Cloud CDN integration: Caching at Google edge locations</li> <li>Cloud Armor: DDoS protection and WAF capabilities</li> <li>Backend types: Instance groups, NEGs (zonal, internet, serverless)</li> <li>HTTP/2 and HTTP/3: Modern protocol support</li> </ul> <p>Limits:</p> Limit Value Backend services per LB 50 URL maps 10 per LB Path matchers per URL map 250 Backends per backend service 50 Maximum request size 32 KB (headers + URL) <p>When to Use:</p> <ul> <li>\u2705 Global web applications with users worldwide</li> <li>\u2705 Need content-based routing (different paths to different backends)</li> <li>\u2705 Require SSL termination and certificate management</li> <li>\u2705 Want Cloud CDN for static content</li> <li>\u2705 Need DDoS protection and WAF</li> </ul> <p>When Not to Use:</p> <ul> <li>\u274c Non-HTTP traffic (use Network Load Balancer)</li> <li>\u274c Internal-only traffic (use Internal Application Load Balancer)</li> <li>\u274c UDP traffic (use Network Load Balancer)</li> </ul>"},{"location":"gcp/networking/load-balancing/#internal-application-load-balancer-regional","title":"Internal Application Load Balancer (Regional)","text":"<p>Description: Regional Layer 7 load balancer for internal HTTP(S) traffic within VPC.</p> <p>Key Features:</p> <ul> <li>Private load balancing: Internal IP addresses only</li> <li>Regional: Serves traffic within a region</li> <li>Cross-region support: Can access backends in other regions</li> <li>Service mesh integration: Works with Traffic Director</li> <li>Shared VPC support: Can be in host or service project</li> </ul> <p>When to Use:</p> <ul> <li>\u2705 Microservices architectures within VPC</li> <li>\u2705 Private APIs and internal applications</li> <li>\u2705 Service mesh deployments (with Traffic Director)</li> <li>\u2705 Multi-tier applications (frontend to backend)</li> </ul>"},{"location":"gcp/networking/load-balancing/#network-load-balancer-layer-4","title":"Network Load Balancer (Layer 4)","text":""},{"location":"gcp/networking/load-balancing/#external-network-load-balancer-regional","title":"External Network Load Balancer (Regional)","text":"<p>Description: Regional Layer 4 load balancer for TCP/UDP traffic, high performance and low latency.</p> <p>Types:</p> <ul> <li>Premium Tier: Anycast IP, global access</li> <li>Standard Tier: Regional IP, regional access</li> </ul> <p>Key Features:</p> <ul> <li>Pass-through: Preserves client IP addresses</li> <li>High throughput: Millions of requests per second</li> <li>UDP support: Gaming, streaming, DNS</li> <li>SSL/TLS: Not terminated (passes through to backends)</li> <li>Session affinity: Client IP, client IP and protocol, client IP-port-proto</li> </ul> <p>Limits:</p> Limit Value Forwarding rules per project 75 (regional) Backend services per LB 50 Backends per backend service 250 <p>When to Use:</p> <ul> <li>\u2705 Non-HTTP(S) protocols (TCP/UDP)</li> <li>\u2705 Need to preserve client IP</li> <li>\u2705 SSL passthrough required (SSL/TLS handled by backend)</li> <li>\u2705 High-performance gaming, streaming applications</li> <li>\u2705 Custom protocols on non-standard ports</li> </ul>"},{"location":"gcp/networking/load-balancing/#internal-network-load-balancer-regional","title":"Internal Network Load Balancer (Regional)","text":"<p>Description: Regional Layer 4 load balancer for internal TCP/UDP traffic within VPC.</p> <p>Types:</p> <ul> <li>Standard: Basic internal load balancing</li> <li>Advanced: Enhanced features, multi-subnet support</li> </ul> <p>Key Features:</p> <ul> <li>Private IPs: Only accessible within VPC</li> <li>High performance: Low latency, high throughput</li> <li>Failover: Automatic with health checks</li> <li>Multi-subnet: Backends can be in different subnets (Advanced)</li> </ul> <p>When to Use:</p> <ul> <li>\u2705 Internal databases, message queues</li> <li>\u2705 Private TCP/UDP services</li> <li>\u2705 High-performance internal applications</li> <li>\u2705 Load balancing for self-managed databases</li> </ul>"},{"location":"gcp/networking/load-balancing/#proxy-network-load-balancer-layer-4","title":"Proxy Network Load Balancer (Layer 4)","text":""},{"location":"gcp/networking/load-balancing/#external-proxy-network-load-balancer-globalregional","title":"External Proxy Network Load Balancer (Global/Regional)","text":"<p>Description: Global or Regional Layer 4 proxy for TCP traffic with SSL termination.</p> <p>Key Features:</p> <ul> <li>TCP proxy: Terminates TCP connections</li> <li>SSL offloading: Terminate SSL at load balancer</li> <li>Global: Single anycast IP, route to nearest backend</li> <li>Long-lived connections: Optimized for persistent connections</li> </ul> <p>When to Use:</p> <ul> <li>\u2705 Non-HTTP TCP traffic that needs SSL termination</li> <li>\u2705 Global applications on custom TCP ports</li> <li>\u2705 Need SSL offloading for TCP applications</li> <li>\u2705 WebSocket connections</li> </ul>"},{"location":"gcp/networking/load-balancing/#internal-proxy-network-load-balancer-regional","title":"Internal Proxy Network Load Balancer (Regional)","text":"<p>Description: Regional Layer 4 proxy for internal TCP traffic.</p> <p>When to Use:</p> <ul> <li>\u2705 Internal TCP applications needing SSL termination</li> <li>\u2705 Private WebSocket services</li> <li>\u2705 Cross-region backend support with internal IPs</li> </ul>"},{"location":"gcp/networking/load-balancing/#decision-matrix","title":"Decision Matrix","text":"<pre><code>                    Traffic Source\n                    \u2502\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502                       \u2502\n    External                Internal\n        \u2502                       \u2502\n        \u2502                       \u2502\n    HTTP(S)?                HTTP(S)?\n    \u2502       \u2502               \u2502       \u2502\n   Yes      No             Yes      No\n    \u2502       \u2502               \u2502       \u2502\n    \u2502       \u2502               \u2502       \u2502\n    \u2502   TCP/UDP?            \u2502   TCP/UDP?\n    \u2502   \u2502       \u2502           \u2502   \u2502       \u2502\n    \u2502  TCP     UDP          \u2502  TCP     UDP\n    \u2502   \u2502       \u2502           \u2502   \u2502       \u2502\n    \u25bc   \u25bc       \u25bc           \u25bc   \u25bc       \u25bc\n\nExternal    External  Ext.   Internal  Internal  Internal\nApplication Proxy N   Network  App LB   Proxy N   Network\nLB (Global) LB       LB        (Reg)    LB (Reg)  LB (Reg)\n</code></pre>"},{"location":"gcp/networking/load-balancing/#common-architectures","title":"Common Architectures","text":""},{"location":"gcp/networking/load-balancing/#global-multi-region-web-application","title":"Global Multi-Region Web Application","text":"<pre><code>Users (Global)\n    \u2502\n    \u25bc\nExternal Application Load Balancer (Anycast IP)\n    \u2502\n    \u251c\u2500\u2500\u2500 Backend: us-central1 (Instance Group)\n    \u251c\u2500\u2500\u2500 Backend: europe-west1 (Instance Group)\n    \u2514\u2500\u2500\u2500 Backend: asia-east1 (Instance Group)\n\nCloud CDN: Enabled\nCloud Armor: DDoS Protection\nSSL Certificate: Managed by Google\n</code></pre>"},{"location":"gcp/networking/load-balancing/#multi-tier-application-internal","title":"Multi-Tier Application (Internal)","text":"<pre><code>External Application LB \u2500\u2500\u25b6 Frontend Tier (Public)\n                                  \u2502\n                                  \u25bc\n                    Internal Application LB \u2500\u2500\u25b6 API Tier (Private)\n                                                     \u2502\n                                                     \u25bc\n                                       Internal Network LB \u2500\u2500\u25b6 Database Tier\n</code></pre>"},{"location":"gcp/networking/load-balancing/#hybrid-cloud-with-internal-lb","title":"Hybrid Cloud with Internal LB","text":"<pre><code>On-Premises Network\n    \u2502\n    \u25bc (Cloud VPN/Interconnect)\n    \u2502\nInternal Application Load Balancer\n    \u2502\n    \u251c\u2500\u2500\u2500 Backend: GKE Cluster (us-central1)\n    \u2514\u2500\u2500\u2500 Backend: Compute VMs (us-east1)\n</code></pre>"},{"location":"gcp/networking/load-balancing/#key-concepts","title":"Key Concepts","text":""},{"location":"gcp/networking/load-balancing/#backend-services","title":"Backend Services","text":"<ul> <li>Health checks: Determine backend availability</li> <li>Session affinity: Stick users to same backend</li> <li>Connection draining: Graceful shutdown, complete existing connections</li> <li>Timeout: How long to wait for backend response</li> <li>Load balancing mode: RATE, UTILIZATION, or CONNECTION based</li> </ul>"},{"location":"gcp/networking/load-balancing/#backend-types","title":"Backend Types","text":"<ul> <li>Instance groups: Managed or unmanaged VM groups</li> <li>Zonal NEGs: Endpoints in specific zones (GCE VMs, GKE pods)</li> <li>Internet NEGs: External endpoints (on-prem, other clouds)</li> <li>Serverless NEGs: Cloud Run, App Engine, Cloud Functions</li> <li>Hybrid NEGs: PSC-based endpoints</li> </ul>"},{"location":"gcp/networking/load-balancing/#health-checks","title":"Health Checks","text":"<ul> <li>Protocol: HTTP, HTTPS, TCP, SSL, HTTP/2</li> <li>Interval: Time between health checks</li> <li>Timeout: Time to wait for response  </li> <li>Healthy threshold: Consecutive successes to mark healthy</li> <li>Unhealthy threshold: Consecutive failures to mark unhealthy</li> </ul>"},{"location":"gcp/networking/load-balancing/#url-maps-https-load-balancers","title":"URL Maps (HTTP(S) Load Balancers)","text":"<ul> <li>Host rules: Route based on hostname</li> <li>Path matchers: Route based on URL path</li> <li>Path rules: Specific path patterns</li> <li>Default service: Fallback for unmatched requests</li> </ul>"},{"location":"gcp/networking/load-balancing/#features-comparison","title":"Features Comparison","text":"Feature External App LB Internal App LB External Net LB Internal Net LB Global \u2705 \u274c Premium tier \u274c SSL Termination \u2705 \u2705 \u274c \u274c Cloud CDN \u2705 \u274c \u274c \u274c Cloud Armor \u2705 \u274c \u274c \u274c URL Routing \u2705 \u2705 \u274c \u274c WebSocket \u2705 \u2705 Proxy LB Proxy LB UDP \u274c \u274c \u2705 \u2705 Preserve Client IP Custom header Custom header \u2705 \u2705 IPv6 \u2705 \u274c \u2705 \u274c"},{"location":"gcp/networking/load-balancing/#configuration-best-practices","title":"Configuration Best Practices","text":"<ol> <li>Health Checks</li> <li>Set appropriate intervals (not too aggressive)</li> <li>Use dedicated health check endpoints</li> <li>Return 200 OK only when truly healthy</li> <li> <p>Monitor health check failures</p> </li> <li> <p>Backend Configuration</p> </li> <li>Enable connection draining (30-60 seconds)</li> <li>Set reasonable session affinity when needed</li> <li>Configure appropriate timeouts</li> <li> <p>Use multiple backends per region for redundancy</p> </li> <li> <p>Security</p> </li> <li>Enable Cloud Armor for external HTTP(S) load balancers</li> <li>Use managed SSL certificates (auto-renewal)</li> <li>Implement SSL policies (minimum TLS version)</li> <li> <p>Restrict backend access with firewall rules</p> </li> <li> <p>Performance</p> </li> <li>Enable Cloud CDN for cacheable content</li> <li>Use HTTP/2 and HTTP/3</li> <li>Configure appropriate backend timeout values</li> <li> <p>Size backend capacity for peak + 30% headroom</p> </li> <li> <p>Monitoring</p> </li> <li>Set up monitoring for latency, error rates, traffic</li> <li>Alert on health check failures</li> <li>Monitor backend utilization</li> <li>Use Cloud Trace for request tracing</li> </ol>"},{"location":"gcp/networking/load-balancing/#pricing-considerations","title":"Pricing Considerations","text":"<p>Application Load Balancers:</p> <ul> <li>Forwarding rules (per hour)</li> <li>Data processing (per GB)</li> <li>Cloud CDN cache fill and egress (if enabled)</li> </ul> <p>Network Load Balancers:</p> <ul> <li>Forwarding rules (per hour)</li> <li>No data processing charges (passthrough)</li> </ul> <p>General Tips:</p> <ul> <li>Internal load balancers cheaper than external</li> <li>Data processing charges can be significant for HTTP(S) LBs</li> <li>Cloud CDN reduces origin traffic and data processing</li> <li>Consider regional vs global based on user distribution</li> </ul>"},{"location":"gcp/networking/load-balancing/#related-services","title":"Related Services","text":"<ul> <li>Cloud CDN: Content delivery network, integrates with HTTP(S) LB</li> <li>Cloud Armor: DDoS protection and WAF for HTTP(S) LB</li> <li>SSL Certificates: Managed or self-managed certificates</li> <li>Cloud DNS: DNS-based load balancing and failover</li> <li>Traffic Director: Service mesh traffic management</li> <li>Network Endpoint Groups: Advanced backend types</li> </ul>"},{"location":"gcp/networking/shared-vpc/","title":"Shared VPC","text":""},{"location":"gcp/networking/shared-vpc/#description","title":"Description","text":"<p>Shared VPC allows an organization to connect resources from multiple service projects to a common VPC network in a host project. This enables centralized network administration while maintaining project-level billing and resource organization. Network admins maintain control over the network resources while service project admins can create resources that use the shared network.</p> <p>Architecture: Centralized VPC network (host project) with attached service projects consuming the network resources.</p>"},{"location":"gcp/networking/shared-vpc/#key-features","title":"Key Features","text":""},{"location":"gcp/networking/shared-vpc/#organizational","title":"Organizational","text":"<ul> <li>Centralized Network Administration: Network team manages all network resources in one place</li> <li>Separation of Concerns: IAM separation between network admins and resource admins</li> <li>Project-Level Billing: Service projects maintain separate billing despite using shared network</li> <li>Cross-Project Resource Creation: VMs in service projects use subnets from host project</li> </ul>"},{"location":"gcp/networking/shared-vpc/#network-capabilities","title":"Network Capabilities","text":"<ul> <li>Shared Subnets: Service projects can use host project subnets</li> <li>Private IP Address Management: Centralized IP address space planning</li> <li>Firewall Rules: Centralized or delegated firewall rule management</li> <li>VPC Network Peering: Host project can peer with other VPCs, connectivity extends to service projects</li> <li>Hybrid Connectivity: Single point for VPN/Interconnect setup benefiting all service projects</li> </ul>"},{"location":"gcp/networking/shared-vpc/#security-access-control","title":"Security &amp; Access Control","text":"<ul> <li>Subnet-Level IAM: Granular permissions at subnet level</li> <li>Service Project Admins: Limited to resource creation, not network changes</li> <li>Organizational Policy: Can enforce Shared VPC usage across organization</li> <li>Audit Logging: Centralized network change tracking</li> </ul>"},{"location":"gcp/networking/shared-vpc/#important-limits","title":"Important Limits","text":"Limit Value Notes Service projects per host 100 Default, can be increased Host projects per organization No limit Can have multiple host projects Shared VPC Admin role Organization or folder level Cannot be granted at project level Subnet sharing Must explicitly share subnets Subnets not automatically shared VPC Peering Only host project can peer Service projects cannot create peering Cloud VPN/Interconnect Only in host project Service projects cannot create these Network tags Created in service projects But firewall rules in host project can use them"},{"location":"gcp/networking/shared-vpc/#iam-roles","title":"IAM Roles","text":""},{"location":"gcp/networking/shared-vpc/#host-project-roles","title":"Host Project Roles","text":"<ul> <li><code>roles/compute.xpnAdmin</code>: Shared VPC Admin (org/folder level) - enables/disables host projects</li> <li><code>roles/compute.networkAdmin</code>: Network Admin - manages network resources in host project</li> <li><code>roles/compute.securityAdmin</code>: Security Admin - manages firewall rules</li> </ul>"},{"location":"gcp/networking/shared-vpc/#service-project-roles","title":"Service Project Roles","text":"<ul> <li><code>roles/compute.networkUser</code>: Allows creating resources using shared subnets</li> <li><code>roles/compute.instanceAdmin</code>: Create instances but needs networkUser for Shared VPC subnets</li> </ul>"},{"location":"gcp/networking/shared-vpc/#when-to-use","title":"When to Use","text":""},{"location":"gcp/networking/shared-vpc/#use-shared-vpc-when","title":"\u2705 Use Shared VPC When:","text":"<ol> <li>Centralized Network Management</li> <li>Single network team managing infrastructure across multiple teams/projects</li> <li>Standardized network policies and security controls</li> <li> <p>Consistent IP address management across organization</p> </li> <li> <p>Hub and Spoke Architecture</p> </li> <li>Central hub (host project) with multiple spokes (service projects)</li> <li>Shared services (DNS, NTP, monitoring) accessed by all projects</li> <li> <p>Transitive connectivity requirements between spokes</p> </li> <li> <p>Enterprise Organizations</p> </li> <li>Multiple business units or teams with separate projects</li> <li>Compliance requirements for network oversight</li> <li> <p>Cost allocation per team while sharing network infrastructure</p> </li> <li> <p>Hybrid Cloud Deployments</p> </li> <li>Single VPN/Interconnect connection serving multiple projects</li> <li>On-premises integration for all cloud projects</li> <li> <p>Centralized egress/ingress control</p> </li> <li> <p>Multi-Tier Applications</p> </li> <li>Frontend, backend, and database in separate projects</li> <li>Private communication between tiers</li> <li>Different teams managing different tiers</li> </ol>"},{"location":"gcp/networking/shared-vpc/#dont-use-shared-vpc-when","title":"\u274c Don\u2019t Use Shared VPC When:","text":"<ol> <li>Complete Project Isolation Needed</li> <li>Projects must be fully network-isolated</li> <li>Regulatory requirements prevent network sharing</li> <li> <p>Use separate VPCs with VPC Peering if limited connectivity needed</p> </li> <li> <p>Small Organizations with Single Team</p> </li> <li>Overhead of Shared VPC not justified</li> <li>Single project VPC is simpler</li> <li> <p>Team has both network and resource admin responsibilities</p> </li> <li> <p>Cross-Organization Scenarios</p> </li> <li>Shared VPC only works within a single organization</li> <li> <p>Use VPC Peering for cross-organization connectivity</p> </li> <li> <p>Frequently Changing Network Topology</p> </li> <li>Service projects constantly added/removed</li> <li>Network architecture not yet stable</li> <li>Consider starting simple, migrate to Shared VPC later</li> </ol>"},{"location":"gcp/networking/shared-vpc/#common-architectures","title":"Common Architectures","text":""},{"location":"gcp/networking/shared-vpc/#hub-and-spoke-with-shared-vpc","title":"Hub and Spoke with Shared VPC","text":"<pre><code>Host Project (Hub)\n\u251c\u2500\u2500 Shared VPC Network\n\u2502   \u251c\u2500\u2500 Subnet A (us-central1)\n\u2502   \u251c\u2500\u2500 Subnet B (europe-west1)\n\u2502   \u2514\u2500\u2500 Subnet C (asia-east1)\n\u251c\u2500\u2500 Cloud VPN/Interconnect (on-premises)\n\u2514\u2500\u2500 VPC Peering (to partner networks)\n\nService Project 1 (Spoke) - Production\n\u251c\u2500\u2500 Uses Subnet A\n\u2514\u2500\u2500 Compute Engine VMs\n\nService Project 2 (Spoke) - Development  \n\u251c\u2500\u2500 Uses Subnet B\n\u2514\u2500\u2500 GKE Clusters\n\nService Project 3 (Spoke) - Data\n\u251c\u2500\u2500 Uses Subnet C\n\u2514\u2500\u2500 Cloud SQL, BigQuery\n</code></pre>"},{"location":"gcp/networking/shared-vpc/#multi-environment-setup","title":"Multi-Environment Setup","text":"<pre><code>Shared Services Host Project\n\u251c\u2500\u2500 Shared VPC Network\n\u2502   \u251c\u2500\u2500 Monitoring/Logging Subnet\n\u2502   \u251c\u2500\u2500 Artifact Registry Subnet\n\u2502   \u2514\u2500\u2500 DNS Subnet\n\nProduction Host Project\n\u251c\u2500\u2500 Shared VPC Network\n\u2502   \u2514\u2500\u2500 Production Subnets\n\u2514\u2500\u2500 Service Projects: App1, App2, App3\n\nNon-Production Host Project\n\u251c\u2500\u2500 Shared VPC Network\n\u2502   \u2514\u2500\u2500 Dev/Staging Subnets\n\u2514\u2500\u2500 Service Projects: Dev-App1, Dev-App2\n</code></pre>"},{"location":"gcp/networking/shared-vpc/#setup-process","title":"Setup Process","text":"<ol> <li> <p>Enable Host Project <pre><code>gcloud compute shared-vpc enable HOST_PROJECT_ID\n</code></pre></p> </li> <li> <p>Attach Service Project <pre><code>gcloud compute shared-vpc associated-projects add SERVICE_PROJECT_ID \\\n    --host-project=HOST_PROJECT_ID\n</code></pre></p> </li> <li> <p>Grant IAM Permissions <pre><code># Grant network user on specific subnet\ngcloud compute networks subnets add-iam-policy-binding SUBNET_NAME \\\n    --member=serviceAccount:SERVICE_PROJECT_NUMBER@cloudservices.gserviceaccount.com \\\n    --role=roles/compute.networkUser \\\n    --region=REGION\n</code></pre></p> </li> <li> <p>Create Resources in Service Project</p> </li> <li>Resources can now reference host project subnets</li> <li>Use <code>--subnet=projects/HOST_PROJECT/regions/REGION/subnetworks/SUBNET_NAME</code></li> </ol>"},{"location":"gcp/networking/shared-vpc/#configuration-best-practices","title":"Configuration Best Practices","text":"<ol> <li>IP Address Planning</li> <li>Plan comprehensive IP address scheme before implementation</li> <li>Leave room for growth in subnet ranges</li> <li> <p>Document subnet allocation per service project</p> </li> <li> <p>IAM Management</p> </li> <li>Use groups for role assignments, not individual users</li> <li>Grant minimal necessary permissions</li> <li> <p>Use subnet-level IAM for fine-grained control</p> </li> <li> <p>Firewall Strategy</p> </li> <li>Use hierarchical firewall policies for organization-wide rules</li> <li>Delegate specific firewall rules where appropriate</li> <li> <p>Leverage network tags for flexible rule application</p> </li> <li> <p>Monitoring &amp; Logging</p> </li> <li>Enable VPC Flow Logs on shared subnets</li> <li>Set up Cloud Monitoring for network metrics</li> <li> <p>Centralize logging in host project or separate logging project</p> </li> <li> <p>Service Project Management</p> </li> <li>Document which service projects use which subnets</li> <li>Regular audits of attached service projects</li> <li>Establish process for onboarding new service projects</li> </ol>"},{"location":"gcp/networking/shared-vpc/#troubleshooting-common-issues","title":"Troubleshooting Common Issues","text":"<p>Permission Denied Creating Resources</p> <ul> <li>Verify service project is attached to host project</li> <li>Check IAM permissions on specific subnet</li> <li>Ensure service account has <code>compute.networkUser</code> role</li> </ul> <p>Cannot See Shared Subnets</p> <ul> <li>Confirm subnet is in same region as resource</li> <li>Verify IAM permissions at subnet level</li> <li>Check if using correct subnet reference format</li> </ul> <p>Firewall Rules Not Working</p> <ul> <li>Verify target tags exist on resources in service projects</li> <li>Check if hierarchical firewall policies apply</li> <li>Confirm source/destination ranges include service project IPs</li> </ul>"},{"location":"gcp/networking/shared-vpc/#related-services","title":"Related Services","text":"<ul> <li>VPC Peering: Host project can peer with other VPCs</li> <li>Private Service Connect: Access managed services privately</li> <li>Cloud VPN/Interconnect: Hybrid connectivity through host project</li> <li>Hierarchical Firewall Policies: Organization-wide firewall management</li> <li>VPC Service Controls: Enhanced security perimeter around resources</li> </ul>"},{"location":"gcp/networking/vpc-peering/","title":"VPC Peering","text":""},{"location":"gcp/networking/vpc-peering/#description","title":"Description","text":"<p>VPC Network Peering enables private RFC 1918 connectivity across two VPC networks, allowing resources to communicate using internal IP addresses regardless of whether they belong to the same project or organization. Peering is a decentralized approach where each VPC network remains under its own administrative control.</p> <p>Architecture: Direct connection between two VPC networks creating a private connection path.</p>"},{"location":"gcp/networking/vpc-peering/#key-features","title":"Key Features","text":""},{"location":"gcp/networking/vpc-peering/#connectivity","title":"Connectivity","text":"<ul> <li>Private IP Communication: Resources communicate using internal IPs without traversing the public internet</li> <li>Cross-Project/Organization: Peering works across different projects and organizations</li> <li>Bidirectional: Traffic can flow in both directions once established</li> <li>No Single Point of Failure: Peering is not a physical connection but a logical configuration</li> <li>Transitive Peering: NOT supported - if VPC A peers with B, and B peers with C, A cannot reach C</li> </ul>"},{"location":"gcp/networking/vpc-peering/#performance-security","title":"Performance &amp; Security","text":"<ul> <li>Low Latency: Private Google network backbone</li> <li>No Bandwidth Bottleneck: No aggregated bandwidth limit</li> <li>Firewall Control: Each VPC maintains its own firewall rules</li> <li>No Encryption: Traffic is private but not encrypted (use application-level encryption if needed)</li> </ul>"},{"location":"gcp/networking/vpc-peering/#administrative","title":"Administrative","text":"<ul> <li>Independent Management: Each VPC admin controls their own network</li> <li>Subnet Expansion: Can expand subnet ranges after peering is established</li> <li>Import/Export Custom Routes: Optional custom route exchange</li> </ul>"},{"location":"gcp/networking/vpc-peering/#important-limits","title":"Important Limits","text":"Limit Value Notes Peering connections per VPC 25 Default limit Subnet IP ranges Cannot overlap Critical - overlapping IPs prevent peering Transitive peering Not supported Must create direct peering for each connection Network tags Not shared Tags don\u2019t cross peering boundaries Internal DNS Requires Cloud DNS peering VPC peering doesn\u2019t automatically enable DNS resolution Peering group limit 25 VPCs in a peering group All VPCs that peer to a common VPC"},{"location":"gcp/networking/vpc-peering/#when-to-use","title":"When to Use","text":""},{"location":"gcp/networking/vpc-peering/#use-vpc-peering-when","title":"\u2705 Use VPC Peering When:","text":"<ol> <li>Multi-Project Architecture</li> <li>Different teams manage separate projects but need private connectivity</li> <li>Hub and spoke architectures with limited spoke-to-spoke communication</li> <li> <p>Cost-effective private connectivity between projects</p> </li> <li> <p>Cross-Organization Collaboration</p> </li> <li>Partner organizations need to connect resources privately</li> <li>M&amp;A scenarios where networks need quick integration</li> <li> <p>Service provider connecting to customer VPCs</p> </li> <li> <p>Simple Network Topologies</p> </li> <li>Limited number of VPCs (under 25 per network)</li> <li>No need for transitive routing</li> <li> <p>Each VPC maintains administrative independence</p> </li> <li> <p>Performance-Critical Workloads</p> </li> <li>Low-latency requirements between VPCs</li> <li>High-throughput applications without bandwidth constraints</li> <li>Real-time data processing across projects</li> </ol>"},{"location":"gcp/networking/vpc-peering/#dont-use-vpc-peering-when","title":"\u274c Don\u2019t Use VPC Peering When:","text":"<ol> <li>Centralized Security/Routing Required</li> <li>Use Shared VPC instead for centralized network administration</li> <li> <p>Need unified security policies across all networks</p> </li> <li> <p>Transitive Routing Needed</p> </li> <li>Spoke-to-spoke communication in hub-and-spoke architecture</li> <li> <p>Use Cloud Router with VPN/Interconnect or redesign with Shared VPC</p> </li> <li> <p>Large-Scale Mesh Networks</p> </li> <li>More than 15-20 VPCs needing full mesh connectivity</li> <li>Quota exhaustion becomes challenging to manage</li> <li> <p>Consider Network Connectivity Center or Shared VPC</p> </li> <li> <p>Overlapping IP Ranges</p> </li> <li>Peering is impossible with overlapping subnets</li> <li>Would require NAT or network redesign</li> </ol>"},{"location":"gcp/networking/vpc-peering/#common-use-cases","title":"Common Use Cases","text":"<p>Hub and Spoke (Non-Transitive) <pre><code>Host Project (Hub) \u2190\u2192 Spoke Project 1\n                  \u2190\u2192 Spoke Project 2\n                  \u2190\u2192 Spoke Project 3\n</code></pre> Spokes cannot communicate with each other, only with the hub.</p> <p>Partner Integration <pre><code>Organization A VPC \u2190\u2192 Organization B VPC\n</code></pre> Private connectivity for data sharing, APIs, or service integration.</p> <p>Development/Production Isolation <pre><code>Production VPC \u2190\u2192 Shared Services VPC \u2190\u2192 Development VPC\n</code></pre> Controlled connectivity to shared services (monitoring, logging, artifact registry).</p>"},{"location":"gcp/networking/vpc-peering/#configuration-considerations","title":"Configuration Considerations","text":"<ol> <li>IP Planning: Ensure no subnet overlap before creating peering</li> <li>Route Export/Import: Configure custom route exchange if needed</li> <li>Firewall Rules: Update rules to allow traffic from peered CIDR ranges</li> <li>DNS Setup: Configure Cloud DNS peering for name resolution across VPCs</li> <li>Monitoring: Set up VPC Flow Logs to track inter-VPC traffic</li> <li>IAM Permissions: Requires <code>compute.networks.peer</code> permission on both networks</li> </ol>"},{"location":"gcp/networking/vpc-peering/#related-services","title":"Related Services","text":"<ul> <li>Shared VPC: Alternative for centralized network management</li> <li>Cloud DNS Peering: For DNS resolution across peered VPCs</li> <li>VPC Flow Logs: Monitor traffic between peered networks</li> <li>Cloud Router: For dynamic routing in hybrid connectivity (not directly used with VPC peering)</li> </ul>"},{"location":"gcp/operations/cloud-nat/","title":"Cloud NAT","text":""},{"location":"gcp/operations/cloud-nat/#core-concepts","title":"Core Concepts","text":"<p>Cloud NAT (Network Address Translation) provides outbound internet connectivity for private instances without external IP addresses. Managed service that scales automatically.</p> <p>Key Principle: Private instances access internet without being directly accessible from internet.</p>"},{"location":"gcp/operations/cloud-nat/#how-cloud-nat-works","title":"How Cloud NAT Works","text":"<pre><code>Private VM (no external IP) \u2192 Cloud NAT \u2192 Internet\nInternet responses \u2192 Cloud NAT \u2192 Private VM\n</code></pre> <p>Direction: Outbound only (initiated by VM)</p> <p>Inbound: NOT possible (VMs remain private, no unsolicited inbound)</p> <p>NAT Gateway: Managed by Google, no instances to maintain</p>"},{"location":"gcp/operations/cloud-nat/#when-to-use-cloud-nat","title":"When to Use Cloud NAT","text":""},{"location":"gcp/operations/cloud-nat/#use-when","title":"\u2705 Use When","text":"<ul> <li>VMs need internet access (updates, APIs, downloads)</li> <li>Don\u2019t want external IPs (security)</li> <li>Centralized egress control</li> <li>Need static external IPs for egress</li> <li>Outbound-only connectivity required</li> <li>Cost-effective outbound at scale</li> </ul>"},{"location":"gcp/operations/cloud-nat/#dont-use-when","title":"\u274c Don\u2019t Use When","text":"<ul> <li>Need inbound access \u2192 Use external IPs or Load Balancer</li> <li>Private Google API access only \u2192 Private Google Access sufficient</li> <li>VPC-to-VPC communication \u2192 VPC peering or VPN</li> </ul>"},{"location":"gcp/operations/cloud-nat/#cloud-nat-vs-alternatives","title":"Cloud NAT vs Alternatives","text":"Method Direction Cost Management Use Case Cloud NAT Outbound Per GB Managed Private VMs, internet access External IP Both Per IP Simple Public-facing services Proxy VM Both VM cost Self-managed Custom filtering Private Google Access GCP services only Free Simple Access GCP APIs only"},{"location":"gcp/operations/cloud-nat/#architecture","title":"Architecture","text":""},{"location":"gcp/operations/cloud-nat/#regional-service","title":"Regional Service","text":"<p>Scope: Cloud NAT is regional (serves VMs in specific region)</p> <p>Multi-region: Deploy separate Cloud NAT per region</p>"},{"location":"gcp/operations/cloud-nat/#attached-to-vpc","title":"Attached to VPC","text":"<p>Requirement: Cloud NAT attached to router in VPC</p> <p>Pattern:</p> <pre><code>VPC \u2192 Cloud Router \u2192 Cloud NAT \u2192 Internet\n  \u2514\u2500\u2500 Subnets with private VMs\n</code></pre>"},{"location":"gcp/operations/cloud-nat/#ip-address-allocation","title":"IP Address Allocation","text":"<p>Options:</p> <ul> <li>Automatic: Google assigns IPs</li> <li>Manual: Specify IP addresses</li> </ul> <p>Benefit of manual: Allowlist specific IPs with external services</p>"},{"location":"gcp/operations/cloud-nat/#port-allocation","title":"Port Allocation","text":"<p>Default: Dynamic port allocation</p> <p>Ports per VM: 64 minimum, 64K maximum</p> <p>Consideration: Each connection uses a port; more VMs = more ports needed</p>"},{"location":"gcp/operations/cloud-nat/#configuration-options","title":"Configuration Options","text":""},{"location":"gcp/operations/cloud-nat/#subnet-selection","title":"Subnet Selection","text":"<p>All subnets (default):</p> <ul> <li>All subnets in region use Cloud NAT</li> <li>Simplest configuration</li> </ul> <p>Selected subnets:</p> <ul> <li>Only specific subnets use Cloud NAT</li> <li>Granular control</li> <li>Mixed public/private architecture</li> </ul> <p>Primary and secondary ranges: Choose which IP ranges use NAT</p>"},{"location":"gcp/operations/cloud-nat/#nat-ip-addresses","title":"NAT IP Addresses","text":"<p>Automatic:</p> <ul> <li>Google allocates IPs</li> <li>Scalable</li> <li>Can\u2019t predict IPs</li> </ul> <p>Manual:</p> <ul> <li>Specify reserved external IPs</li> <li>Static IPs for allowlisting</li> <li>More control</li> </ul> <p>Number: Start with N IPs = VMs/32, increase if port exhaustion</p>"},{"location":"gcp/operations/cloud-nat/#logging","title":"Logging","text":"<p>Purpose: See which VM accessed what endpoint</p> <p>Log types:</p> <ul> <li>Translations (connections)</li> <li>Errors (dropped packets)</li> </ul> <p>Use for: Debugging, audit, security</p> <p>Cost: Log storage charges apply</p>"},{"location":"gcp/operations/cloud-nat/#common-patterns","title":"Common Patterns","text":""},{"location":"gcp/operations/cloud-nat/#standard-private-architecture","title":"Standard Private Architecture","text":"<pre><code>VMs (private, no external IP)\n  \u2192 Cloud NAT (outbound only)\n  \u2192 Internet (updates, API calls)\n</code></pre> <p>Benefit: Security (no inbound), cost (no external IPs)</p>"},{"location":"gcp/operations/cloud-nat/#hybrid-cloud","title":"Hybrid Cloud","text":"<pre><code>On-premises \u2192 Cloud VPN \u2192 VPC \u2192 Cloud NAT \u2192 Internet\n</code></pre> <p>Use case: On-premises uses cloud for internet egress</p>"},{"location":"gcp/operations/cloud-nat/#multi-environment","title":"Multi-Environment","text":"<pre><code>VPC-Dev \u2192 Cloud NAT (dev-nat) \u2192 Internet\nVPC-Staging \u2192 Cloud NAT (staging-nat) \u2192 Internet\nVPC-Prod \u2192 Cloud NAT (prod-nat) \u2192 Internet\n</code></pre> <p>Benefit: Separate egress IPs per environment</p>"},{"location":"gcp/operations/cloud-nat/#allowlist-with-external-services","title":"Allowlist with External Services","text":"<pre><code>VM \u2192 Cloud NAT (manual IPs: 1.2.3.4, 5.6.7.8) \u2192 External API\nExternal API allowlist: 1.2.3.4, 5.6.7.8\n</code></pre> <p>Use case: Third-party services require IP allowlisting</p>"},{"location":"gcp/operations/cloud-nat/#high-availability","title":"High Availability","text":"<p>Automatic: Cloud NAT is highly available by default</p> <p>No single point of failure: Managed service, Google handles redundancy</p> <p>Regional: Zone failure doesn\u2019t affect Cloud NAT</p> <p>Best practice: No special configuration needed, works automatically</p>"},{"location":"gcp/operations/cloud-nat/#port-exhaustion","title":"Port Exhaustion","text":""},{"location":"gcp/operations/cloud-nat/#understanding-ports","title":"Understanding Ports","text":"<p>Per IP: 64,511 usable ports (65,535 total - reserved)</p> <p>Per Connection: One port used</p> <p>Problem: Too many VMs or connections \u2192 run out of ports</p>"},{"location":"gcp/operations/cloud-nat/#symptoms","title":"Symptoms","text":"<ul> <li>Connection failures</li> <li>Timeouts to internet</li> <li>\u201cCannot assign requested address\u201d errors</li> </ul>"},{"location":"gcp/operations/cloud-nat/#solutions","title":"Solutions","text":"<p>Add more NAT IPs:</p> <ul> <li>More IPs = more ports</li> <li>2 IPs = ~130K ports</li> </ul> <p>Increase ports per VM:</p> <ul> <li>Default 64 minimum \u2192 Increase to 128, 256, etc.</li> <li>More ports = fewer VMs per IP</li> </ul> <p>Reduce connections:</p> <ul> <li>Connection pooling in applications</li> <li>Increase connection timeouts</li> <li>Close connections properly</li> </ul> <p>Formula: <code>Max VMs per IP = 64511 / min_ports_per_vm</code></p> <p>Example: 64 ports/VM = 1008 VMs per IP</p>"},{"location":"gcp/operations/cloud-nat/#monitoring-and-troubleshooting","title":"Monitoring and Troubleshooting","text":""},{"location":"gcp/operations/cloud-nat/#key-metrics","title":"Key Metrics","text":"<p>Port allocation:</p> <ul> <li>Allocated ports per VM</li> <li>Port usage percentage</li> <li>VMs approaching port limit</li> </ul> <p>Traffic:</p> <ul> <li>Sent/received bytes</li> <li>Dropped packets</li> <li>Connection count</li> </ul> <p>Errors:</p> <ul> <li>NAT allocation failures</li> <li>Port exhaustion</li> </ul>"},{"location":"gcp/operations/cloud-nat/#alerts","title":"Alerts","text":"<p>Critical:</p> <ul> <li>Port utilization &gt; 80%</li> <li>NAT allocation failures</li> <li>Dropped packets increasing</li> </ul> <p>Warning:</p> <ul> <li>Port utilization &gt; 60%</li> <li>High connection count per VM</li> </ul>"},{"location":"gcp/operations/cloud-nat/#common-issues","title":"Common Issues","text":"<p>Problem: VMs can\u2019t reach internet</p> <p>Check:</p> <ol> <li>Cloud NAT configured for subnet?</li> <li>Router exists in VPC?</li> <li>Firewall allows egress?</li> <li>Private Google Access interfering?</li> </ol> <p>Problem: Connections intermittently fail</p> <p>Cause: Port exhaustion</p> <p>Fix: Add NAT IPs or increase min ports per VM</p>"},{"location":"gcp/operations/cloud-nat/#security-considerations","title":"Security Considerations","text":""},{"location":"gcp/operations/cloud-nat/#egress-control","title":"Egress Control","text":"<p>Benefit: All outbound through known IPs</p> <p>Use cases:</p> <ul> <li>Security monitoring (log egress)</li> <li>Compliance (data loss prevention)</li> <li>Cost tracking (egress costs)</li> </ul>"},{"location":"gcp/operations/cloud-nat/#firewall-rules","title":"Firewall Rules","text":"<p>Still apply: VPC firewall rules control traffic</p> <p>Pattern:</p> <ul> <li>Firewall deny all egress (default)</li> <li>Firewall allow specific destinations (allowlist)</li> <li>Cloud NAT provides translation</li> </ul>"},{"location":"gcp/operations/cloud-nat/#no-inbound-risk","title":"No Inbound Risk","text":"<p>Key security feature: VMs unreachable from internet</p> <p>Even with Cloud NAT: No unsolicited inbound connections</p>"},{"location":"gcp/operations/cloud-nat/#cost-model","title":"Cost Model","text":"<p>Pricing:</p> <ul> <li>Per GB processed: ~$0.045/GB (us-central1)</li> <li>NAT gateway hours: ~$0.045/hour per gateway</li> <li>Data transfer charges apply (egress)</li> </ul> <p>Optimization:</p> <ul> <li>Use for all private VMs (better than external IPs)</li> <li>Not charged for internal traffic</li> <li>Regional data transfer cheaper</li> </ul> <p>Comparison with external IPs:</p> <ul> <li>External IP: ~$0.004/hour per IP (~$3/month)</li> <li>Cloud NAT: ~$33/month + $0.045/GB</li> </ul> <p>Decision: Cloud NAT cheaper at scale, better security</p>"},{"location":"gcp/operations/cloud-nat/#cloud-nat-vs-private-google-access","title":"Cloud NAT vs Private Google Access","text":"Feature Cloud NAT Private Google Access Destination Internet GCP APIs only IPs needed Yes (external) No Use case Internet access GCP service access Cost Per GB + gateway Free <p>Use together: Private Google Access for GCP services, Cloud NAT for internet</p>"},{"location":"gcp/operations/cloud-nat/#integration-with-other-services","title":"Integration with Other Services","text":""},{"location":"gcp/operations/cloud-nat/#with-load-balancers","title":"With Load Balancers","text":"<p>Inbound: Load Balancer handles inbound</p> <p>Outbound: Cloud NAT handles outbound from backends</p> <p>Pattern: Private backends behind LB, outbound via NAT</p>"},{"location":"gcp/operations/cloud-nat/#with-gke","title":"With GKE","text":"<p>Node pools: Private nodes use Cloud NAT for outbound</p> <p>Configuration: Automatic when enabling private cluster</p> <p>Image pulls: Container images downloaded via Cloud NAT</p>"},{"location":"gcp/operations/cloud-nat/#with-cloud-sql","title":"With Cloud SQL","text":"<p>Cloud SQL Proxy: Works with private IPs, no Cloud NAT needed</p> <p>Direct connection: Private IP connection (no NAT)</p>"},{"location":"gcp/operations/cloud-nat/#best-practices","title":"Best Practices","text":""},{"location":"gcp/operations/cloud-nat/#use-private-ips","title":"Use Private IPs","text":"<p>Default: VMs should NOT have external IPs</p> <p>Exception: Public-facing services only (load balancers)</p> <p>Benefit: Security, cost, centralized egress</p>"},{"location":"gcp/operations/cloud-nat/#monitor-port-usage","title":"Monitor Port Usage","text":"<p>Alert: Port utilization &gt; 60%</p> <p>Action: Add IPs or increase min ports before exhaustion</p>"},{"location":"gcp/operations/cloud-nat/#manual-ips-for-critical-services","title":"Manual IPs for Critical Services","text":"<p>Pattern: Manual NAT IPs for production</p> <p>Benefit: Predictable IPs for allowlisting, stability</p>"},{"location":"gcp/operations/cloud-nat/#separate-nat-per-environment","title":"Separate NAT per Environment","text":"<p>Pattern: Different NAT (and IPs) for dev/staging/prod</p> <p>Benefit: Isolation, separate monitoring, blast radius</p>"},{"location":"gcp/operations/cloud-nat/#enable-logging-selectively","title":"Enable Logging (Selectively)","text":"<p>Enable for: Security-sensitive, troubleshooting</p> <p>Disable for: High-traffic, cost-sensitive (logs expensive)</p>"},{"location":"gcp/operations/cloud-nat/#limitations","title":"Limitations","text":"<ul> <li>Regional service (not global)</li> <li>Outbound only (no inbound)</li> <li>Cannot use for inter-VPC traffic (use peering/VPN)</li> <li>Max 64K ports per VM (configurable)</li> <li>Cannot change router once configured</li> </ul>"},{"location":"gcp/operations/cloud-nat/#migration-patterns","title":"Migration Patterns","text":""},{"location":"gcp/operations/cloud-nat/#from-external-ips-to-cloud-nat","title":"From External IPs to Cloud NAT","text":"<p>Process:</p> <ol> <li>Deploy Cloud NAT</li> <li>Verify VM has private IP route to NAT</li> <li>Remove external IPs from VMs</li> <li>Test connectivity</li> <li>Update monitoring</li> </ol> <p>Benefit: Improved security, potentially lower cost</p>"},{"location":"gcp/operations/cloud-nat/#from-proxy-vms-to-cloud-nat","title":"From Proxy VMs to Cloud NAT","text":"<p>Benefit: Managed service, no maintenance, better availability</p> <p>Consideration: Lose custom filtering (move to firewall rules)</p>"},{"location":"gcp/operations/cloud-nat/#exam-focus","title":"Exam Focus","text":""},{"location":"gcp/operations/cloud-nat/#core-concepts_1","title":"Core Concepts","text":"<ul> <li>Outbound internet for private VMs</li> <li>No external IPs needed</li> <li>Managed service (highly available)</li> <li>Regional scope</li> </ul>"},{"location":"gcp/operations/cloud-nat/#use-cases","title":"Use Cases","text":"<ul> <li>Private VMs need internet access</li> <li>Security (no inbound)</li> <li>Centralized egress</li> <li>Static IP for allowlisting</li> </ul>"},{"location":"gcp/operations/cloud-nat/#architecture_1","title":"Architecture","text":"<ul> <li>Attached to Cloud Router in VPC</li> <li>Subnet selection (all or selected)</li> <li>IP allocation (automatic or manual)</li> <li>Per-region deployment</li> </ul>"},{"location":"gcp/operations/cloud-nat/#port-exhaustion_1","title":"Port Exhaustion","text":"<ul> <li>Understanding ports per IP (64K)</li> <li>Min ports per VM (64 default)</li> <li>Symptoms and solutions</li> <li>Monitoring and alerting</li> </ul>"},{"location":"gcp/operations/cloud-nat/#integration","title":"Integration","text":"<ul> <li>Works with Private Google Access</li> <li>GKE private clusters</li> <li>Load balancer backends</li> <li>Egress firewall rules still apply</li> </ul>"},{"location":"gcp/operations/cloud-nat/#security","title":"Security","text":"<ul> <li>Outbound only (no inbound)</li> <li>Egress control and monitoring</li> <li>No increased attack surface</li> <li>Combine with firewall rules</li> </ul>"},{"location":"gcp/operations/cloud-nat/#cost","title":"Cost","text":"<ul> <li>Per GB processed + gateway hours</li> <li>Cheaper than external IPs at scale</li> <li>Egress charges apply</li> <li>Not charged for internal traffic</li> </ul>"},{"location":"gcp/operations/cloud-operations/","title":"Cloud Operations","text":""},{"location":"gcp/operations/cloud-operations/#core-concepts","title":"Core Concepts","text":"<p>Cloud Operations (formerly Stackdriver) is a suite of tools for monitoring, logging, debugging, and managing applications and infrastructure. Provides unified observability across GCP and other clouds.</p> <p>Key Principle: Observability through metrics, logs, and traces; proactive monitoring, not reactive troubleshooting.</p>"},{"location":"gcp/operations/cloud-operations/#cloud-operations-suite","title":"Cloud Operations Suite","text":"Service Purpose Key Feature Cloud Monitoring Metrics, dashboards, alerts Time-series data, SLOs Cloud Logging Log collection and analysis Centralized logs, filters Cloud Trace Distributed tracing Request latency analysis Cloud Profiler CPU/memory profiling Production profiling Cloud Debugger Live debugging No code changes, snapshots Error Reporting Error aggregation Smart grouping, notifications"},{"location":"gcp/operations/cloud-operations/#cloud-monitoring","title":"Cloud Monitoring","text":""},{"location":"gcp/operations/cloud-operations/#purpose","title":"Purpose","text":"<p>Collect, visualize, and alert on metrics from GCP resources, applications, and external sources.</p>"},{"location":"gcp/operations/cloud-operations/#key-features","title":"Key Features","text":"<p>Metrics Collection:</p> <ul> <li>Automatic (GCP resources)</li> <li>Custom (application metrics)</li> <li>Agent-based (VM detailed metrics)</li> </ul> <p>Dashboards:</p> <ul> <li>Predefined (GCE, GKE, etc.)</li> <li>Custom (MQL, PromQL)</li> <li>Charts, tables, heatmaps</li> </ul> <p>Alerting:</p> <ul> <li>Metric-based (CPU &gt; 80%)</li> <li>Log-based (error rate spikes)</li> <li>Uptime checks (availability)</li> <li>Multi-condition policies</li> </ul> <p>SLIs and SLOs:</p> <ul> <li>Service-level indicators (latency, availability)</li> <li>Service-level objectives (99.9% uptime)</li> <li>Error budget tracking</li> </ul>"},{"location":"gcp/operations/cloud-operations/#common-patterns","title":"Common Patterns","text":"<p>Infrastructure Monitoring:</p> <pre><code>Alert: CPU &gt; 80% for 5 minutes \u2192 Page on-call\nAlert: Disk usage &gt; 90% \u2192 Auto-expand or alert\nUptime check: HTTP endpoint unavailable \u2192 Alert\n</code></pre> <p>Application Performance:</p> <pre><code>SLO: 99.9% requests &lt; 500ms\nError Budget: Calculate remaining tolerance\nAlert: Error budget burn rate too high\n</code></pre>"},{"location":"gcp/operations/cloud-operations/#monitoring-agent","title":"Monitoring Agent","text":"<p>Purpose: Collect system and application metrics from VMs</p> <p>Installation: Optional but recommended for detailed metrics</p> <p>Benefits: Memory, disk I/O, process metrics</p>"},{"location":"gcp/operations/cloud-operations/#cloud-logging","title":"Cloud Logging","text":""},{"location":"gcp/operations/cloud-operations/#purpose_1","title":"Purpose","text":"<p>Centralized log management: collection, storage, analysis, and alerting.</p>"},{"location":"gcp/operations/cloud-operations/#log-types","title":"Log Types","text":"<p>Platform Logs (automatic):</p> <ul> <li>Admin Activity (who did what)</li> <li>Data Access (who accessed data, must enable)</li> <li>System Events (GCP actions)</li> <li>Access Transparency (Google admin access)</li> </ul> <p>Application Logs:</p> <ul> <li>Stdout/stderr (automatic for many services)</li> <li>Structured logging (recommended)</li> <li>Custom log writes</li> </ul>"},{"location":"gcp/operations/cloud-operations/#log-routing","title":"Log Routing","text":"<p>Default behavior: Logs stored in Cloud Logging for 30 days</p> <p>Sinks: Export logs to destinations</p> <ul> <li>Cloud Storage (long-term archival)</li> <li>BigQuery (analysis, SQL queries)</li> <li>Pub/Sub (streaming to external systems)</li> <li>Other projects (centralized logging)</li> </ul> <p>Filters: Route specific logs (e.g., only errors)</p>"},{"location":"gcp/operations/cloud-operations/#log-based-metrics","title":"Log-Based Metrics","text":"<p>Purpose: Create metrics from log entries</p> <p>Use cases:</p> <ul> <li>Count error occurrences</li> <li>Track specific events</li> <li>Custom business metrics</li> <li>Alert on log patterns</li> </ul> <p>Example: Alert when 5XX errors &gt; 10/minute</p>"},{"location":"gcp/operations/cloud-operations/#best-practices","title":"Best Practices","text":"<p>Structured Logging:</p> <pre><code>{\n  \"severity\": \"ERROR\",\n  \"message\": \"Payment failed\",\n  \"userId\": \"123\",\n  \"amount\": 99.99\n}\n</code></pre> <p>Benefits: Searchable fields, better analysis</p> <p>Log Sampling: Reduce volume for high-traffic apps (sample 10%)</p> <p>Retention: Default 30 days, export to Storage for longer</p>"},{"location":"gcp/operations/cloud-operations/#cloud-trace","title":"Cloud Trace","text":""},{"location":"gcp/operations/cloud-operations/#purpose_2","title":"Purpose","text":"<p>Distributed tracing for understanding request latency across services.</p>"},{"location":"gcp/operations/cloud-operations/#how-it-works","title":"How It Works","text":"<pre><code>Request \u2192 Service A \u2192 Service B \u2192 Service C\n         \u2514\u2500\u2500\u2500 Trace spans collected \u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Trace: End-to-end request journey Span: Single operation within trace</p>"},{"location":"gcp/operations/cloud-operations/#use-cases","title":"Use Cases","text":"<ul> <li>Identify slow services in request path</li> <li>Understand service dependencies</li> <li>Optimize critical paths</li> <li>Troubleshoot latency issues</li> </ul>"},{"location":"gcp/operations/cloud-operations/#automatic-instrumentation","title":"Automatic Instrumentation","text":"<p>Supported:</p> <ul> <li>App Engine (automatic)</li> <li>Cloud Run (automatic)</li> <li>GKE (with service mesh)</li> </ul> <p>Manual:</p> <ul> <li>Compute Engine (use client libraries)</li> <li>Other services (OpenTelemetry)</li> </ul>"},{"location":"gcp/operations/cloud-operations/#analysis","title":"Analysis","text":"<p>Features:</p> <ul> <li>Latency distribution</li> <li>Request waterfall view</li> <li>Service dependency graph</li> <li>Comparison across traces</li> </ul> <p>Example: \u201c95% of requests to Service A take 200ms, but 5% take 5s due to Service B dependency\u201d</p>"},{"location":"gcp/operations/cloud-operations/#cloud-profiler","title":"Cloud Profiler","text":""},{"location":"gcp/operations/cloud-operations/#purpose_3","title":"Purpose","text":"<p>Continuous CPU and memory profiling in production with minimal overhead.</p>"},{"location":"gcp/operations/cloud-operations/#key-features_1","title":"Key Features","text":"<ul> <li>No performance impact (&lt;1% overhead)</li> <li>Always-on profiling</li> <li>Multiple languages (Java, Python, Go, Node.js)</li> <li>Compare time periods</li> </ul>"},{"location":"gcp/operations/cloud-operations/#use-cases_1","title":"Use Cases","text":"<ul> <li>Identify performance bottlenecks</li> <li>Optimize resource usage</li> <li>Reduce compute costs</li> <li>Memory leak detection</li> </ul>"},{"location":"gcp/operations/cloud-operations/#best-practice","title":"Best Practice","text":"<p>Enable in production: Designed for production use, safe</p>"},{"location":"gcp/operations/cloud-operations/#cloud-debugger","title":"Cloud Debugger","text":""},{"location":"gcp/operations/cloud-operations/#purpose_4","title":"Purpose","text":"<p>Debug production applications without stopping or restarting.</p>"},{"location":"gcp/operations/cloud-operations/#how-it-works_1","title":"How It Works","text":"<p>Snapshots: Capture variable state at specific line</p> <p>Logpoints: Inject log statement without code changes</p> <p>No downtime: Debug live production apps</p>"},{"location":"gcp/operations/cloud-operations/#limitations","title":"Limitations","text":"<ul> <li>Snapshots expire after capture</li> <li>Not all languages supported</li> <li>Cannot modify state (read-only)</li> </ul>"},{"location":"gcp/operations/cloud-operations/#use-case","title":"Use Case","text":"<p>Troubleshooting production issues without redeployment</p>"},{"location":"gcp/operations/cloud-operations/#error-reporting","title":"Error Reporting","text":""},{"location":"gcp/operations/cloud-operations/#purpose_5","title":"Purpose","text":"<p>Aggregate and display errors from applications, with smart grouping and notifications.</p>"},{"location":"gcp/operations/cloud-operations/#features","title":"Features","text":"<p>Smart Grouping: Similar errors grouped together</p> <p>Stack Trace: Full stack traces for debugging</p> <p>Notifications: Email, mobile alerts on new errors</p> <p>Integration: Works with Cloud Logging automatically</p>"},{"location":"gcp/operations/cloud-operations/#supported-services","title":"Supported Services","text":"<ul> <li>App Engine, Cloud Functions, Cloud Run (automatic)</li> <li>Compute Engine, GKE (via Logging agent)</li> <li>External applications (via API)</li> </ul>"},{"location":"gcp/operations/cloud-operations/#unified-observability","title":"Unified Observability","text":""},{"location":"gcp/operations/cloud-operations/#the-three-pillars","title":"The Three Pillars","text":"<p>Metrics (Cloud Monitoring):</p> <ul> <li>What is happening (CPU, memory, requests)</li> <li>When did it happen</li> <li>Historical trends</li> </ul> <p>Logs (Cloud Logging):</p> <ul> <li>Why it happened</li> <li>Detailed context</li> <li>Debugging information</li> </ul> <p>Traces (Cloud Trace):</p> <ul> <li>How requests flow through system</li> <li>Where latency occurs</li> <li>Service dependencies</li> </ul> <p>Together: Complete picture of system health</p>"},{"location":"gcp/operations/cloud-operations/#sli-slo-and-sla","title":"SLI, SLO, and SLA","text":""},{"location":"gcp/operations/cloud-operations/#definitions","title":"Definitions","text":"<p>SLI (Service Level Indicator):</p> <ul> <li>Quantitative measure of service level</li> <li>Examples: Latency, availability, error rate</li> </ul> <p>SLO (Service Level Objective):</p> <ul> <li>Target value for SLI</li> <li>Examples: 99.9% availability, 95<sup>th</sup> percentile latency &lt; 200ms</li> </ul> <p>SLA (Service Level Agreement):</p> <ul> <li>Contractual commitment</li> <li>Penalties if SLO not met</li> <li>Example: 99.95% uptime or refund</li> </ul>"},{"location":"gcp/operations/cloud-operations/#relationship","title":"Relationship","text":"<pre><code>SLI (measurement) \u2192 SLO (internal target) \u2192 SLA (customer contract)\n</code></pre>"},{"location":"gcp/operations/cloud-operations/#error-budget","title":"Error Budget","text":"<p>Concept: Allowed downtime based on SLO</p> <p>Example: 99.9% availability SLO = 43.2 minutes downtime/month allowed</p> <p>Use: Prioritize features vs reliability</p>"},{"location":"gcp/operations/cloud-operations/#monitoring-strategy","title":"Monitoring Strategy","text":""},{"location":"gcp/operations/cloud-operations/#what-to-monitor","title":"What to Monitor","text":"<p>Golden Signals (Google SRE):</p> <ul> <li>Latency: Request duration</li> <li>Traffic: Request rate</li> <li>Errors: Failed requests</li> <li>Saturation: Resource utilization</li> </ul> <p>Infrastructure:</p> <ul> <li>CPU, memory, disk, network</li> <li>Service health</li> <li>Resource quotas</li> </ul> <p>Application:</p> <ul> <li>Business metrics (orders, payments)</li> <li>User experience (page load time)</li> <li>Custom KPIs</li> </ul>"},{"location":"gcp/operations/cloud-operations/#alert-design","title":"Alert Design","text":"<p>Good Alerts:</p> <ul> <li>Actionable (can fix)</li> <li>Represent real problems</li> <li>Rarely false positives</li> </ul> <p>Bad Alerts:</p> <ul> <li>Noisy (too many)</li> <li>Not actionable</li> <li>Alert fatigue</li> </ul> <p>Best Practice: Alert on SLO burn rate, not arbitrary thresholds</p>"},{"location":"gcp/operations/cloud-operations/#cost-optimization","title":"Cost Optimization","text":"<p>Logging:</p> <ul> <li>Default 30-day retention (free)</li> <li>Export to Storage for cheaper long-term</li> <li>Use log exclusion filters (reduce volume)</li> <li>Sample high-volume logs</li> </ul> <p>Monitoring:</p> <ul> <li>Free tier: 150 MB logs ingestion/month</li> <li>Custom metrics charged beyond free tier</li> <li>Use sampling for high-cardinality metrics</li> </ul> <p>Trace/Profiler/Debugger: Free (no additional charge)</p>"},{"location":"gcp/operations/cloud-operations/#integration-patterns","title":"Integration Patterns","text":""},{"location":"gcp/operations/cloud-operations/#multi-cloud-monitoring","title":"Multi-Cloud Monitoring","text":"<p>Ops Agent: Monitor GCP, AWS, on-premises from single dashboard</p> <p>Use case: Unified monitoring across hybrid/multi-cloud</p>"},{"location":"gcp/operations/cloud-operations/#centralized-logging","title":"Centralized Logging","text":"<p>Pattern: All projects route logs to central project</p> <pre><code>Project A logs \u2192 Sink \u2192 Central Logging Project\nProject B logs \u2192 Sink \u2192 Central Logging Project\n</code></pre> <p>Benefits: Single pane of glass, better analysis</p>"},{"location":"gcp/operations/cloud-operations/#alert-routing","title":"Alert Routing","text":"<p>Integration:</p> <ul> <li>PagerDuty, Slack (notifications)</li> <li>Cloud Functions (automated remediation)</li> <li>Cloud Tasks (workflow orchestration)</li> </ul>"},{"location":"gcp/operations/cloud-operations/#compliance-and-audit","title":"Compliance and Audit","text":""},{"location":"gcp/operations/cloud-operations/#audit-logs","title":"Audit Logs","text":"<p>Types:</p> <ul> <li>Admin Activity: Always enabled, 400-day retention</li> <li>Data Access: Must enable, 30-day default retention</li> <li>System Events: Automatic</li> <li>Access Transparency: Google employee access</li> </ul> <p>Use for: Compliance evidence, security investigations</p>"},{"location":"gcp/operations/cloud-operations/#log-retention","title":"Log Retention","text":"<p>Compliance requirements:</p> <ul> <li>HIPAA: 6 years</li> <li>SOX: 7 years</li> <li>PCI-DSS: 1 year</li> </ul> <p>Implementation: Export logs to Cloud Storage, set retention policy</p>"},{"location":"gcp/operations/cloud-operations/#best-practices_1","title":"Best Practices","text":""},{"location":"gcp/operations/cloud-operations/#monitoring","title":"Monitoring","text":"<ul> <li>Define SLOs for critical services</li> <li>Alert on SLO burn rate</li> <li>Use dashboards for visibility</li> <li>Regular review of alert policies</li> </ul>"},{"location":"gcp/operations/cloud-operations/#logging","title":"Logging","text":"<ul> <li>Use structured logging</li> <li>Sample high-volume logs</li> <li>Export for long-term retention</li> <li>Enable data access logs for sensitive resources</li> </ul>"},{"location":"gcp/operations/cloud-operations/#tracing","title":"Tracing","text":"<ul> <li>Enable for all services</li> <li>Use for performance optimization</li> <li>Trace critical request paths</li> <li>Set sampling rate appropriately</li> </ul>"},{"location":"gcp/operations/cloud-operations/#observability","title":"Observability","text":"<ul> <li>Implement all three pillars (metrics, logs, traces)</li> <li>Correlate across pillars (same request ID)</li> <li>Monitor business metrics, not just infrastructure</li> <li>Proactive monitoring, not reactive</li> </ul>"},{"location":"gcp/operations/cloud-operations/#exam-focus","title":"Exam Focus","text":""},{"location":"gcp/operations/cloud-operations/#core-concepts_1","title":"Core Concepts","text":"<ul> <li>Observability pillars (metrics, logs, traces)</li> <li>SLI vs SLO vs SLA</li> <li>Golden signals (latency, traffic, errors, saturation)</li> <li>Error budgets</li> </ul>"},{"location":"gcp/operations/cloud-operations/#service-purpose","title":"Service Purpose","text":"<ul> <li>Cloud Monitoring: Metrics, alerts, dashboards</li> <li>Cloud Logging: Centralized logs, sinks</li> <li>Cloud Trace: Distributed tracing, latency</li> <li>Cloud Profiler: Production profiling</li> <li>Error Reporting: Error aggregation</li> </ul>"},{"location":"gcp/operations/cloud-operations/#architecture","title":"Architecture","text":"<ul> <li>Log routing (sinks to Storage, BigQuery, Pub/Sub)</li> <li>Centralized logging pattern</li> <li>Multi-cloud monitoring</li> <li>Alert routing and automation</li> </ul>"},{"location":"gcp/operations/cloud-operations/#best-practices_2","title":"Best Practices","text":"<ul> <li>SLO-based alerting</li> <li>Structured logging</li> <li>Log retention for compliance</li> <li>Enable data access logs for sensitive resources</li> <li>Sample high-volume logs</li> </ul>"},{"location":"gcp/operations/cloud-operations/#integration","title":"Integration","text":"<ul> <li>Automatic (App Engine, Cloud Run, GKE)</li> <li>Agent-based (Compute Engine)</li> <li>API/SDK (custom applications)</li> <li>Multi-cloud (Ops Agent)</li> </ul>"},{"location":"gcp/operations/compliance/","title":"Compliance","text":""},{"location":"gcp/operations/compliance/#core-concepts","title":"Core Concepts","text":"<p>Compliance ensures GCP usage meets regulatory, industry, and organizational requirements. GCP provides certifications, controls, and tools to support compliance efforts.</p> <p>Key Principle: Shared responsibility - GCP provides compliant infrastructure, customers ensure compliant usage.</p>"},{"location":"gcp/operations/compliance/#shared-responsibility-model","title":"Shared Responsibility Model","text":"Layer Google\u2019s Responsibility Customer\u2019s Responsibility Infrastructure Physical security, hardware N/A Platform Service certifications, controls Configuration, access control Data Encryption at rest/transit Data classification, handling Access IAM service availability User permissions, authentication Compliance Infrastructure certifications Workload compliance validation"},{"location":"gcp/operations/compliance/#common-compliance-frameworks","title":"Common Compliance Frameworks","text":""},{"location":"gcp/operations/compliance/#hipaa-healthcare","title":"HIPAA (Healthcare)","text":"<p>Requirements:</p> <ul> <li>Encryption at rest and in transit</li> <li>Access controls and audit logging</li> <li>Business Associate Agreement (BAA)</li> <li>Physical safeguards</li> </ul> <p>GCP Features:</p> <ul> <li>BAA available for covered services</li> <li>CMEK for additional control</li> <li>Access Transparency logs</li> <li>VPC Service Controls (data perimeter)</li> <li>Audit logs (Admin Activity, Data Access)</li> </ul> <p>Customer Actions:</p> <ul> <li>Sign BAA with Google</li> <li>Enable data access logs</li> <li>Implement access controls</li> <li>Regular access reviews</li> <li>Use covered services only</li> </ul>"},{"location":"gcp/operations/compliance/#pci-dss-payment-card-industry","title":"PCI-DSS (Payment Card Industry)","text":"<p>Requirements:</p> <ul> <li>Network segmentation</li> <li>Encryption of cardholder data</li> <li>Access controls</li> <li>Regular security testing</li> <li>Maintain security policies</li> </ul> <p>GCP Features:</p> <ul> <li>PCI-DSS Level 1 certification</li> <li>VPC for network isolation</li> <li>Cloud Armor for protection</li> <li>Web Security Scanner</li> <li>Cloud KMS for encryption</li> </ul> <p>Customer Actions:</p> <ul> <li>Scope PCI environment (minimize)</li> <li>Network segmentation (VPC, firewall rules)</li> <li>Quarterly vulnerability scans</li> <li>No cardholder data in logs</li> <li>Implement application controls</li> </ul>"},{"location":"gcp/operations/compliance/#gdpr-eu-data-protection","title":"GDPR (EU Data Protection)","text":"<p>Requirements:</p> <ul> <li>Data residency (EU data stays in EU)</li> <li>Right to erasure</li> <li>Data protection by design</li> <li>Breach notification</li> <li>Data Processing Agreement (DPA)</li> </ul> <p>GCP Features:</p> <ul> <li>EU regions available</li> <li>Organization Policies (location restrictions)</li> <li>DPA available</li> <li>Data deletion APIs</li> <li>Audit logs for compliance evidence</li> </ul> <p>Customer Actions:</p> <ul> <li>Data residency: Use EU regions only</li> <li>Organization Policy: Restrict resource locations</li> <li>Data inventory and classification</li> <li>Implement deletion processes</li> <li>Privacy impact assessments</li> </ul>"},{"location":"gcp/operations/compliance/#sox-sarbanes-oxley","title":"SOX (Sarbanes-Oxley)","text":"<p>Requirements:</p> <ul> <li>Financial data controls</li> <li>Audit trails</li> <li>Segregation of duties</li> <li>7-year data retention</li> </ul> <p>GCP Features:</p> <ul> <li>SOC \u00bd/3 reports available</li> <li>Audit logs (400-day retention for admin)</li> <li>IAM for segregation of duties</li> <li>Log export for long-term retention</li> </ul> <p>Customer Actions:</p> <ul> <li>Enable audit logs</li> <li>Export logs to Storage (7-year retention)</li> <li>Implement least privilege</li> <li>Separate production access</li> <li>Document controls</li> </ul>"},{"location":"gcp/operations/compliance/#iso-27001-information-security","title":"ISO 27001 (Information Security)","text":"<p>Requirements:</p> <ul> <li>Information security management system</li> <li>Risk assessment</li> <li>Security controls</li> <li>Continuous improvement</li> </ul> <p>GCP Features:</p> <ul> <li>ISO 27001 certified</li> <li>ISO 27017 (cloud security)</li> <li>ISO 27018 (privacy)</li> <li>Security Command Center</li> </ul> <p>Customer Actions:</p> <ul> <li>Implement ISMS</li> <li>Regular risk assessments</li> <li>Use GCP security features</li> <li>Document security controls</li> </ul>"},{"location":"gcp/operations/compliance/#gcp-compliance-tools","title":"GCP Compliance Tools","text":""},{"location":"gcp/operations/compliance/#compliance-reports-manager","title":"Compliance Reports Manager","text":"<p>Purpose: Access compliance reports and certifications</p> <p>Available:</p> <ul> <li>SOC \u00bd/3</li> <li>ISO 27001/27017/27018</li> <li>PCI-DSS AOC</li> <li>HIPAA attestation</li> </ul> <p>Access: Through GCP Console</p>"},{"location":"gcp/operations/compliance/#security-command-center","title":"Security Command Center","text":"<p>Purpose: Centralized security and compliance dashboard</p> <p>Features:</p> <ul> <li>Asset inventory and discovery</li> <li>Vulnerability scanning</li> <li>Compliance monitoring (CIS benchmarks)</li> <li>Security findings aggregation</li> <li>Policy violations</li> </ul> <p>Tiers: Standard (free), Premium (paid)</p>"},{"location":"gcp/operations/compliance/#policy-intelligence","title":"Policy Intelligence","text":"<p>Tools:</p> <ul> <li>IAM Recommender (over-permissioned accounts)</li> <li>Policy Analyzer (who has access)</li> <li>Policy Simulator (test policy changes)</li> <li>Policy Troubleshooter (debug access issues)</li> </ul> <p>Use for: Least privilege enforcement, access reviews</p>"},{"location":"gcp/operations/compliance/#access-transparency","title":"Access Transparency","text":"<p>Purpose: Logs of Google employee access to your data</p> <p>Use case: Compliance requirement for transparency</p> <p>Available: For Premium support customers</p> <p>Content: Who, what, when, why Google accessed</p>"},{"location":"gcp/operations/compliance/#data-residency-and-sovereignty","title":"Data Residency and Sovereignty","text":""},{"location":"gcp/operations/compliance/#resource-locations","title":"Resource Locations","text":"<p>Organization Policy: <code>constraints/gcp.resourceLocations</code></p> <p>Example: Restrict to EU regions only</p> <pre><code>Allowed values: in:eu-locations\nResult: Resources can only be created in EU\n</code></pre> <p>Enforcement: Blocks creation in non-compliant regions</p>"},{"location":"gcp/operations/compliance/#data-classification","title":"Data Classification","text":"<p>Levels:</p> <ul> <li>Public: No restrictions</li> <li>Internal: Access controlled</li> <li>Confidential: Encrypted, strict access</li> <li>Restricted: Maximum security (CMEK, VPC-SC)</li> </ul> <p>Implementation:</p> <ul> <li>Labels for classification</li> <li>Different projects for different levels</li> <li>Appropriate security controls per level</li> </ul>"},{"location":"gcp/operations/compliance/#assured-workloads","title":"Assured Workloads","text":"<p>Purpose: Enforces location and access restrictions for compliance</p> <p>Features:</p> <ul> <li>Guaranteed data location</li> <li>Personnel access controls (US only, no foreign nationals)</li> <li>Encryption requirements</li> <li>Regular compliance monitoring</li> </ul> <p>Use for: Government (FedRAMP, IL4), highly regulated industries</p>"},{"location":"gcp/operations/compliance/#encryption-and-key-management","title":"Encryption and Key Management","text":""},{"location":"gcp/operations/compliance/#encryption-options","title":"Encryption Options","text":"<p>Default (Google-managed):</p> <ul> <li>Automatic encryption at rest</li> <li>No configuration needed</li> <li>Google manages keys</li> </ul> <p>CMEK (Customer-managed):</p> <ul> <li>Customer controls keys in Cloud KMS</li> <li>Can revoke access</li> <li>Key rotation policies</li> <li>Audit key usage</li> </ul> <p>CSEK (Customer-supplied):</p> <ul> <li>Customer provides keys per operation</li> <li>Google doesn\u2019t store keys</li> <li>Maximum control, maximum complexity</li> </ul> <p>Decision:</p> <ul> <li>Default: Most workloads</li> <li>CMEK: Compliance requires customer control</li> <li>CSEK: Maximum security, rare</li> </ul>"},{"location":"gcp/operations/compliance/#key-management-best-practices","title":"Key Management Best Practices","text":"<ul> <li>Separate keys per environment (dev, prod)</li> <li>Regular key rotation (automatic in KMS)</li> <li>Least privilege for key access</li> <li>Audit key usage</li> <li>Destroy keys when no longer needed</li> </ul>"},{"location":"gcp/operations/compliance/#audit-logging","title":"Audit Logging","text":""},{"location":"gcp/operations/compliance/#log-types-for-compliance","title":"Log Types for Compliance","text":"<p>Admin Activity (always on):</p> <ul> <li>Who did what (API calls)</li> <li>400-day retention</li> <li>Free</li> </ul> <p>Data Access (must enable):</p> <ul> <li>Who accessed what data</li> <li>30-day default retention</li> <li>Chargeable</li> </ul> <p>System Events:</p> <ul> <li>GCP-initiated actions</li> <li>Automatic</li> </ul> <p>Access Transparency:</p> <ul> <li>Google employee access</li> <li>Premium support only</li> </ul>"},{"location":"gcp/operations/compliance/#log-retention-requirements","title":"Log Retention Requirements","text":"Framework Retention Implementation HIPAA 6 years Export to Cloud Storage SOX 7 years Export to Cloud Storage PCI-DSS 1 year Default + export GDPR Per policy Configurable <p>Pattern: Export logs to Cloud Storage with retention policy</p>"},{"location":"gcp/operations/compliance/#access-controls-for-compliance","title":"Access Controls for Compliance","text":""},{"location":"gcp/operations/compliance/#principle-of-least-privilege","title":"Principle of Least Privilege","text":"<p>Implementation:</p> <ul> <li>Predefined roles (not basic roles)</li> <li>Custom roles for specific needs</li> <li>Regular access reviews (quarterly)</li> <li>Remove unused permissions</li> </ul> <p>Tools: IAM Recommender for over-permissioned accounts</p>"},{"location":"gcp/operations/compliance/#segregation-of-duties","title":"Segregation of Duties","text":"<p>Pattern: Separate roles for different functions</p> <p>Examples:</p> <ul> <li>Compute Admin \u2260 Network Admin</li> <li>Developer (create resources) \u2260 Security Admin (set policies)</li> <li>No single person has complete access</li> </ul> <p>Implementation: Multiple administrators, separate roles</p>"},{"location":"gcp/operations/compliance/#mfa-multi-factor-authentication","title":"MFA (Multi-Factor Authentication)","text":"<p>Requirement: Most compliance frameworks require MFA</p> <p>Enforcement:</p> <ul> <li>Workspace/Cloud Identity policy</li> <li>Mandatory for admin accounts</li> <li>Context-aware access (IAP)</li> </ul>"},{"location":"gcp/operations/compliance/#network-security-for-compliance","title":"Network Security for Compliance","text":""},{"location":"gcp/operations/compliance/#network-segmentation","title":"Network Segmentation","text":"<p>Methods:</p> <ul> <li>Separate VPCs per environment</li> <li>VPC firewall rules (default deny)</li> <li>Private Google Access (no internet)</li> <li>Cloud NAT (controlled egress)</li> <li>VPC Service Controls (data perimeter)</li> </ul> <p>PCI-DSS: Requires network segmentation for cardholder data environment</p>"},{"location":"gcp/operations/compliance/#vpc-service-controls","title":"VPC Service Controls","text":"<p>Purpose: Create security perimeter around resources</p> <p>Benefits:</p> <ul> <li>Prevent data exfiltration</li> <li>Restrict API access to perimeter</li> <li>Complements IAM</li> </ul> <p>Use case: Protect sensitive data (PII, PHI, financial)</p>"},{"location":"gcp/operations/compliance/#private-connectivity","title":"Private Connectivity","text":"<p>Options:</p> <ul> <li>Cloud VPN (encrypted tunnel)</li> <li>Cloud Interconnect (dedicated connection)</li> <li>Private Service Connect (private access to services)</li> </ul> <p>Use for: Hybrid compliance, data cannot traverse internet</p>"},{"location":"gcp/operations/compliance/#compliance-monitoring","title":"Compliance Monitoring","text":""},{"location":"gcp/operations/compliance/#continuous-compliance","title":"Continuous Compliance","text":"<p>Approach:</p> <ul> <li>Organization Policies (preventive)</li> <li>Security Command Center (detective)</li> <li>Audit logs (evidence)</li> <li>Regular reviews (corrective)</li> </ul> <p>Automation: Policy violations trigger alerts, automated remediation</p>"},{"location":"gcp/operations/compliance/#compliance-reporting","title":"Compliance Reporting","text":"<p>Evidence Collection:</p> <ul> <li>Audit logs exported and retained</li> <li>Security Command Center reports</li> <li>Access reviews documented</li> <li>Policy enforcement documented</li> </ul> <p>Audits: Provide evidence to auditors via Compliance Reports Manager</p>"},{"location":"gcp/operations/compliance/#incident-response","title":"Incident Response","text":""},{"location":"gcp/operations/compliance/#data-breach-notification","title":"Data Breach Notification","text":"<p>GDPR: 72-hour notification requirement</p> <p>Preparation:</p> <ul> <li>Incident response plan</li> <li>Contact lists (DPO, legal)</li> <li>Detection mechanisms (Cloud Monitoring alerts)</li> <li>Evidence collection (audit logs)</li> </ul> <p>GCP Support: Premium support for incident assistance</p>"},{"location":"gcp/operations/compliance/#forensics","title":"Forensics","text":"<p>Tools:</p> <ul> <li>Cloud Logging (what happened)</li> <li>VPC Flow Logs (network traffic)</li> <li>Access Transparency (Google access)</li> <li>Disk snapshots (preserve evidence)</li> </ul> <p>Best Practice: Enable comprehensive logging before incident</p>"},{"location":"gcp/operations/compliance/#third-party-risk-management","title":"Third-Party Risk Management","text":""},{"location":"gcp/operations/compliance/#vendor-assessment","title":"Vendor Assessment","text":"<p>Questions:</p> <ul> <li>What certifications do they have?</li> <li>Where is data stored/processed?</li> <li>What access do they have?</li> <li>How is data encrypted?</li> <li>What are their security practices?</li> </ul> <p>GCP Certifications: ISO, SOC, PCI, HIPAA, FedRAMP</p>"},{"location":"gcp/operations/compliance/#subprocessors","title":"Subprocessors","text":"<p>GDPR requirement: Know where data is processed</p> <p>GCP: Transparent list of subprocessors</p> <p>Action: Review and approve subprocessor list</p>"},{"location":"gcp/operations/compliance/#compliance-by-service","title":"Compliance by Service","text":""},{"location":"gcp/operations/compliance/#hipaa-covered-services","title":"HIPAA-Covered Services","text":"<p>Covered: Compute Engine, Cloud Storage, BigQuery, Cloud SQL, GKE</p> <p>NOT Covered: Firebase, Cloud Datastore (some exceptions), App Engine Standard</p> <p>Check: Compliance documentation for current list</p>"},{"location":"gcp/operations/compliance/#pci-dss-scope","title":"PCI-DSS Scope","text":"<p>In-scope: Infrastructure services (GCE, GCS, VPC)</p> <p>Customer responsibility: Application-level controls</p> <p>Best practice: Minimize PCI scope (tokenization, separate environment)</p>"},{"location":"gcp/operations/compliance/#best-practices","title":"Best Practices","text":""},{"location":"gcp/operations/compliance/#design-for-compliance","title":"Design for Compliance","text":"<ul> <li>Data classification from start</li> <li>Encryption by default</li> <li>Principle of least privilege</li> <li>Comprehensive logging</li> <li>Regular security reviews</li> </ul>"},{"location":"gcp/operations/compliance/#documentation","title":"Documentation","text":"<ul> <li>Architecture diagrams</li> <li>Data flow diagrams</li> <li>Security controls documentation</li> <li>Policy documentation</li> <li>Incident response plan</li> </ul>"},{"location":"gcp/operations/compliance/#regular-reviews","title":"Regular Reviews","text":"<ul> <li>Quarterly access reviews</li> <li>Annual security assessments</li> <li>Policy effectiveness reviews</li> <li>Penetration testing (if required)</li> <li>Audit log reviews</li> </ul>"},{"location":"gcp/operations/compliance/#automation","title":"Automation","text":"<ul> <li>Organization Policies (enforce)</li> <li>Security Command Center (detect)</li> <li>Cloud Functions (remediate)</li> <li>Infrastructure as Code (consistency)</li> </ul>"},{"location":"gcp/operations/compliance/#exam-focus","title":"Exam Focus","text":""},{"location":"gcp/operations/compliance/#shared-responsibility","title":"Shared Responsibility","text":"<ul> <li>What Google provides vs customer responsibilities</li> <li>Infrastructure security vs application security</li> <li>Compliance certifications vs compliance validation</li> </ul>"},{"location":"gcp/operations/compliance/#framework-requirements","title":"Framework Requirements","text":"<ul> <li>HIPAA: BAA, encryption, access logs</li> <li>PCI-DSS: Network segmentation, scans, no CHD in logs</li> <li>GDPR: Data residency, right to erasure, DPA</li> <li>SOX: 7-year retention, segregation of duties</li> </ul>"},{"location":"gcp/operations/compliance/#gcp-features","title":"GCP Features","text":"<ul> <li>Organization Policies for governance</li> <li>VPC Service Controls for data perimeter</li> <li>CMEK for key control</li> <li>Audit logs for evidence</li> <li>Security Command Center for monitoring</li> </ul>"},{"location":"gcp/operations/compliance/#architecture","title":"Architecture","text":"<ul> <li>Data residency (location restrictions)</li> <li>Network segmentation (VPC, firewall rules)</li> <li>Encryption options (default, CMEK, CSEK)</li> <li>Log retention and export</li> <li>Access control patterns</li> </ul>"},{"location":"gcp/operations/compliance/#common-patterns","title":"Common Patterns","text":"<ul> <li>Separate projects per environment</li> <li>Export logs for long-term retention</li> <li>Organization Policy for location restriction</li> <li>VPC Service Controls for sensitive data</li> <li>Least privilege with regular reviews</li> </ul>"},{"location":"gcp/operations/disaster-recovery/","title":"Disaster Recovery","text":""},{"location":"gcp/operations/disaster-recovery/#core-concepts","title":"Core Concepts","text":"<p>Disaster Recovery (DR) is the ability to recover IT systems and data after a disaster. Understanding RPO/RTO requirements drives DR strategy selection.</p> <p>Key Principle: Plan for failure; the goal is not to prevent disasters, but to minimize impact.</p>"},{"location":"gcp/operations/disaster-recovery/#rpo-and-rto","title":"RPO and RTO","text":""},{"location":"gcp/operations/disaster-recovery/#recovery-point-objective-rpo","title":"Recovery Point Objective (RPO)","text":"<p>Definition: Maximum acceptable data loss (time between last backup and disaster)</p> <p>Examples:</p> <ul> <li>RPO 24 hours: Lose up to 24 hours of data</li> <li>RPO 1 hour: Lose up to 1 hour of data</li> <li>RPO near-zero: Minimal to no data loss</li> </ul> <p>Determines: Backup frequency</p>"},{"location":"gcp/operations/disaster-recovery/#recovery-time-objective-rto","title":"Recovery Time Objective (RTO)","text":"<p>Definition: Maximum acceptable downtime (time to restore operations)</p> <p>Examples:</p> <ul> <li>RTO 4 hours: System down max 4 hours</li> <li>RTO 1 hour: System down max 1 hour</li> <li>RTO minutes: Near-instant recovery</li> </ul> <p>Determines: DR strategy and architecture</p>"},{"location":"gcp/operations/disaster-recovery/#rto-vs-rpo-matrix","title":"RTO vs RPO Matrix","text":"Tier RPO RTO Strategy Cost Tier 0 (Critical) Near-zero &lt; 1 hour Active-active Very High Tier 1 (Important) &lt; 1 hour &lt; 4 hours Hot standby High Tier 2 (Standard) &lt; 24 hours &lt; 24 hours Warm standby Medium Tier 3 (Low priority) &lt; 7 days &lt; 48 hours Backup/restore Low"},{"location":"gcp/operations/disaster-recovery/#disaster-recovery-strategies","title":"Disaster Recovery Strategies","text":""},{"location":"gcp/operations/disaster-recovery/#backup-and-restore-cheapest-slowest","title":"Backup and Restore (Cheapest, Slowest)","text":"<p>Architecture: Backup data, restore on disaster</p> <p>RPO: Hours to days (backup frequency) RTO: Hours (restoration time) Cost: Lowest (storage only)</p> <p>Implementation:</p> <ul> <li>Compute Engine: Snapshots, machine images</li> <li>Databases: Cloud SQL backups, manual dumps</li> <li>Storage: Cloud Storage versioning, replication</li> </ul> <p>Use when: Cost-critical, acceptable downtime hours/days</p>"},{"location":"gcp/operations/disaster-recovery/#pilot-light-low-cost-faster","title":"Pilot Light (Low Cost, Faster)","text":"<p>Architecture: Minimal infrastructure always running in DR region</p> <p>Core components:</p> <ul> <li>Database replicas (standby)</li> <li>Pre-configured but minimal compute</li> <li>Network infrastructure ready</li> </ul> <p>RPO: 1-4 hours (replication lag) RTO: 1-2 hours (scale up DR environment) Cost: Low (minimal running resources)</p> <p>Implementation:</p> <ul> <li>Cloud SQL read replicas in DR region</li> <li>Load balancer pre-configured</li> <li>Startup scripts ready</li> <li>Scale compute on failover</li> </ul> <p>Use when: Balance cost and recovery time</p>"},{"location":"gcp/operations/disaster-recovery/#warm-standby-medium-cost-fast","title":"Warm Standby (Medium Cost, Fast)","text":"<p>Architecture: Scaled-down version running in DR region</p> <p>Running:</p> <ul> <li>Database active (replication)</li> <li>Reduced compute capacity</li> <li>Can handle reduced load immediately</li> <li>Scale up for full capacity</li> </ul> <p>RPO: Minutes to 1 hour (near real-time replication) RTO: 30-60 minutes (scale up) Cost: Medium (running infrastructure at reduced capacity)</p> <p>Implementation:</p> <ul> <li>Cloud SQL with HA and replicas</li> <li>GKE cluster with fewer nodes</li> <li>Cloud Run with min instances</li> <li>Regional persistent disks</li> </ul> <p>Use when: Important applications, can tolerate brief reduced capacity</p>"},{"location":"gcp/operations/disaster-recovery/#hot-standby-active-passive-high-cost-very-fast","title":"Hot Standby / Active-Passive (High Cost, Very Fast)","text":"<p>Architecture: Full capacity in DR region, ready but not serving traffic</p> <p>Characteristics:</p> <ul> <li>Everything running at full capacity</li> <li>Immediate failover</li> <li>No scale-up needed</li> <li>Single-digit minute RTO</li> </ul> <p>RPO: Near-zero (synchronous or near-synchronous replication) RTO: &lt; 5 minutes (DNS/load balancer switch) Cost: High (2x infrastructure, mostly idle in DR)</p> <p>Implementation:</p> <ul> <li>Global load balancer with health checks</li> <li>Full compute capacity in both regions</li> <li>Database with synchronous replication</li> <li>Automatic failover configured</li> </ul> <p>Use when: Mission-critical, minimal downtime acceptable</p>"},{"location":"gcp/operations/disaster-recovery/#active-active-multi-region-highest-cost-zero-rto","title":"Active-Active / Multi-Region (Highest Cost, Zero RTO)","text":"<p>Architecture: Full capacity in multiple regions, all serving traffic</p> <p>Characteristics:</p> <ul> <li>No failover needed (both active)</li> <li>Zero RTO (automatic)</li> <li>Highest availability</li> <li>Most complex (data consistency)</li> </ul> <p>RPO: Near-zero (multi-region replication) RTO: No planned downtime (automatic) Cost: Highest (2x+ infrastructure, all active)</p> <p>Implementation:</p> <ul> <li>Global HTTP(S) Load Balancer</li> <li>Cloud Spanner (global database) or eventual consistency</li> <li>Multi-region Cloud Storage</li> <li>Cloud CDN for static content</li> </ul> <p>Use when: Zero-tolerance for downtime, global applications</p>"},{"location":"gcp/operations/disaster-recovery/#disaster-scenarios","title":"Disaster Scenarios","text":""},{"location":"gcp/operations/disaster-recovery/#scenario-1-zone-failure","title":"Scenario 1: Zone Failure","text":"<p>Impact: Resources in single zone unavailable</p> <p>Protection:</p> <ul> <li>Regional persistent disks (automatic failover)</li> <li>Regional MIGs (distribute across zones)</li> <li>GKE regional clusters</li> <li>Multi-zone database (Cloud SQL HA)</li> </ul> <p>RTO: Minutes (automatic) RPO: Zero (synchronous replication within region)</p>"},{"location":"gcp/operations/disaster-recovery/#scenario-2-regional-disaster","title":"Scenario 2: Regional Disaster","text":"<p>Impact: Entire region unavailable</p> <p>Protection:</p> <ul> <li>Multi-region deployment</li> <li>Cross-region snapshots/backups</li> <li>Global load balancer</li> <li>Multi-region database (Spanner) or cross-region replication</li> </ul> <p>RTO: Depends on strategy (minutes to hours) RPO: Depends on replication method</p>"},{"location":"gcp/operations/disaster-recovery/#scenario-3-data-corruptiondeletion","title":"Scenario 3: Data Corruption/Deletion","text":"<p>Impact: Data corrupted or accidentally deleted</p> <p>Protection:</p> <ul> <li>Cloud Storage versioning</li> <li>Persistent disk snapshots</li> <li>Database backups and PITR (point-in-time recovery)</li> <li>Retention policies (prevent deletion)</li> </ul> <p>RTO: Minutes to hours (restore time) RPO: Backup frequency</p>"},{"location":"gcp/operations/disaster-recovery/#scenario-4-application-bugbad-deployment","title":"Scenario 4: Application Bug/Bad Deployment","text":"<p>Impact: Application malfunction from bad release</p> <p>Protection:</p> <ul> <li>Blue-green deployments</li> <li>Canary deployments</li> <li>Rollback capability</li> <li>Immutable infrastructure</li> </ul> <p>RTO: Minutes (rollback) RPO: Zero (no data loss)</p>"},{"location":"gcp/operations/disaster-recovery/#dr-for-specific-services","title":"DR for Specific Services","text":""},{"location":"gcp/operations/disaster-recovery/#compute-engine","title":"Compute Engine","text":"<p>Backup:</p> <ul> <li>Machine images (full VM)</li> <li>Persistent disk snapshots (incremental)</li> <li>Custom images for OS</li> </ul> <p>RPO: Snapshot frequency (hourly, daily) RTO: 15-60 minutes (restore and boot)</p> <p>Cross-region DR: Snapshots in multi-regional storage, restore in DR region</p>"},{"location":"gcp/operations/disaster-recovery/#cloud-sql","title":"Cloud SQL","text":"<p>Backup:</p> <ul> <li>Automated backups (daily)</li> <li>Point-in-time recovery (7-35 days)</li> <li>On-demand backups</li> </ul> <p>HA Configuration:</p> <ul> <li>Regional HA (automatic failover within region)</li> <li>Cross-region read replicas</li> <li>External replica (on-premises)</li> </ul> <p>RPO: </p> <ul> <li>HA: Zero (synchronous)</li> <li>Read replicas: Seconds (near real-time)</li> <li>Backups: 24 hours</li> </ul> <p>RTO:</p> <ul> <li>HA: &lt; 1 minute (automatic)</li> <li>Cross-region failover: Minutes (manual promotion)</li> <li>Backup restore: 30-60 minutes</li> </ul>"},{"location":"gcp/operations/disaster-recovery/#gke","title":"GKE","text":"<p>Backup:</p> <ul> <li>Persistent volume snapshots</li> <li>Etcd backups (cluster state)</li> <li>Workload configuration (GitOps)</li> </ul> <p>HA Configuration:</p> <ul> <li>Regional cluster (multi-zone control plane)</li> <li>Multi-cluster setup (cross-region)</li> <li>Config Sync (disaster recovery clusters)</li> </ul> <p>RPO: Depends on replication strategy RTO: Minutes (regional cluster), hours (multi-region)</p>"},{"location":"gcp/operations/disaster-recovery/#cloud-storage","title":"Cloud Storage","text":"<p>Backup:</p> <ul> <li>Object versioning</li> <li>Cross-region replication (Turbo Replication)</li> <li>Dual-region or multi-region storage</li> </ul> <p>RPO:</p> <ul> <li>Standard: Eventual consistency (versioning)</li> <li>Turbo Replication: &lt; 15 minutes</li> </ul> <p>RTO: Immediate (automatic failover for multi/dual-region)</p>"},{"location":"gcp/operations/disaster-recovery/#bigquery","title":"BigQuery","text":"<p>Backup:</p> <ul> <li>Table snapshots (point-in-time)</li> <li>Export to Cloud Storage</li> <li>Cross-region dataset copy</li> </ul> <p>RPO: Snapshot frequency RTO: Minutes to hours (restore table)</p> <p>Multi-region: BigQuery datasets in US/EU automatically multi-region</p>"},{"location":"gcp/operations/disaster-recovery/#testing-dr-plans","title":"Testing DR Plans","text":""},{"location":"gcp/operations/disaster-recovery/#importance","title":"Importance","text":"<p>Untested DR plan = No DR plan</p> <p>Common failures:</p> <ul> <li>Backup restores never tested</li> <li>Failover process not documented</li> <li>Insufficient permissions in DR region</li> <li>Network connectivity issues</li> <li>Dependencies not identified</li> </ul>"},{"location":"gcp/operations/disaster-recovery/#testing-types","title":"Testing Types","text":"<p>Tabletop Exercise (Cheapest):</p> <ul> <li>Walk through DR procedure</li> <li>Identify gaps in documentation</li> <li>No actual failover</li> </ul> <p>Simulated Disaster (Best):</p> <ul> <li>Actual failover to DR environment</li> <li>Test restoration procedures</li> <li>Measure RTO/RPO</li> <li>Identify issues before real disaster</li> </ul> <p>Chaos Engineering:</p> <ul> <li>Intentional failures (zone, service)</li> <li>Verify automatic recovery</li> <li>Test resilience continuously</li> </ul>"},{"location":"gcp/operations/disaster-recovery/#testing-frequency","title":"Testing Frequency","text":"<ul> <li>Critical systems: Quarterly</li> <li>Important systems: Semi-annually</li> <li>Standard systems: Annually</li> </ul>"},{"location":"gcp/operations/disaster-recovery/#dr-drill-checklist","title":"DR Drill Checklist","text":"<ol> <li>Document current state</li> <li>Initiate DR procedure</li> <li>Time each step (measure RTO)</li> <li>Verify data integrity (validate RPO)</li> <li>Test application functionality</li> <li>Document issues and gaps</li> <li>Update DR plan</li> <li>Conduct post-mortem</li> </ol>"},{"location":"gcp/operations/disaster-recovery/#dr-plan-documentation","title":"DR Plan Documentation","text":""},{"location":"gcp/operations/disaster-recovery/#essential-components","title":"Essential Components","text":"<p>Contact Information:</p> <ul> <li>On-call rotation</li> <li>Escalation path</li> <li>Vendor contacts (Google support)</li> </ul> <p>System Inventory:</p> <ul> <li>Critical services and dependencies</li> <li>Data classification</li> <li>RPO/RTO requirements per system</li> </ul> <p>Procedures:</p> <ul> <li>Step-by-step recovery instructions</li> <li>Failover triggers (when to activate)</li> <li>Rollback procedures</li> <li>Communication plan</li> </ul> <p>Architecture Diagrams:</p> <ul> <li>Normal operations</li> <li>DR configuration</li> <li>Network topology</li> <li>Data flow</li> </ul>"},{"location":"gcp/operations/disaster-recovery/#runbooks","title":"Runbooks","text":"<p>Format: Step-by-step instructions</p> <p>Include:</p> <ul> <li>Prerequisites and permissions</li> <li>Commands to execute</li> <li>Expected output</li> <li>Verification steps</li> <li>Rollback procedure</li> </ul> <p>Accessibility: Available offline (printed or local)</p>"},{"location":"gcp/operations/disaster-recovery/#cost-optimization","title":"Cost Optimization","text":""},{"location":"gcp/operations/disaster-recovery/#right-size-dr-strategy","title":"Right-Size DR Strategy","text":"<p>Anti-pattern: Same strategy for all systems</p> <p>Best practice: Tier systems by criticality</p> <p>Example:</p> <ul> <li>Tier 0 (Payment): Active-active (expensive)</li> <li>Tier 1 (Order mgmt): Hot standby</li> <li>Tier 2 (Reporting): Pilot light</li> <li>Tier 3 (Archives): Backup/restore</li> </ul>"},{"location":"gcp/operations/disaster-recovery/#use-appropriate-storage-classes","title":"Use Appropriate Storage Classes","text":"<p>Backups:</p> <ul> <li>Recent (&lt; 30 days): Standard or Nearline</li> <li>Historical (30-90 days): Coldline</li> <li>Compliance (&gt; 365 days): Archive</li> </ul> <p>Savings: 50-80% on long-term backup storage</p>"},{"location":"gcp/operations/disaster-recovery/#minimize-idle-resources","title":"Minimize Idle Resources","text":"<p>Pilot light: Minimal resources, scale on disaster</p> <p>Scheduled scaling: Non-24/7 workloads scale down off-hours</p>"},{"location":"gcp/operations/disaster-recovery/#leverage-managed-services","title":"Leverage Managed Services","text":"<p>Example: Cloud SQL HA vs self-managed replication</p> <p>Benefit: Lower operational cost, better reliability</p>"},{"location":"gcp/operations/disaster-recovery/#compliance-considerations","title":"Compliance Considerations","text":""},{"location":"gcp/operations/disaster-recovery/#retention-requirements","title":"Retention Requirements","text":"<p>Regulatory:</p> <ul> <li>HIPAA: 6 years</li> <li>SOX: 7 years</li> <li>PCI-DSS: 1 year</li> </ul> <p>Implementation: Backup retention policies, object lifecycle</p>"},{"location":"gcp/operations/disaster-recovery/#geographic-requirements","title":"Geographic Requirements","text":"<p>GDPR: EU data must stay in EU</p> <p>Implementation:</p> <ul> <li>Organization Policy (location restrictions)</li> <li>Snapshots in appropriate regions</li> <li>Cross-region DR within compliant regions</li> </ul>"},{"location":"gcp/operations/disaster-recovery/#encryption","title":"Encryption","text":"<p>At rest: Automatic (CMEK for compliance)</p> <p>In transit: Automatic for cross-region replication</p>"},{"location":"gcp/operations/disaster-recovery/#monitoring-and-alerting","title":"Monitoring and Alerting","text":""},{"location":"gcp/operations/disaster-recovery/#health-checks","title":"Health Checks","text":"<p>Purpose: Detect failures automatically</p> <p>Implementation:</p> <ul> <li>Load balancer health checks</li> <li>Uptime checks (Cloud Monitoring)</li> <li>Application health endpoints</li> </ul>"},{"location":"gcp/operations/disaster-recovery/#failover-automation","title":"Failover Automation","text":"<p>Trigger: Health check failures</p> <p>Action:</p> <ul> <li>Automatic (load balancer, Cloud SQL HA)</li> <li>Semi-automatic (alert + manual approval)</li> <li>Manual (runbook execution)</li> </ul>"},{"location":"gcp/operations/disaster-recovery/#backup-monitoring","title":"Backup Monitoring","text":"<p>Metrics:</p> <ul> <li>Backup success/failure rate</li> <li>Time since last successful backup</li> <li>Backup size trends</li> </ul> <p>Alerts:</p> <ul> <li>Backup failure</li> <li>Backup age &gt; RPO threshold</li> <li>Restoration test failure</li> </ul>"},{"location":"gcp/operations/disaster-recovery/#best-practices","title":"Best Practices","text":""},{"location":"gcp/operations/disaster-recovery/#3-2-1-backup-rule","title":"3-2-1 Backup Rule","text":"<p>3 copies: Original + 2 backups 2 media types: Disk snapshots + object storage 1 offsite: Different region</p> <p>GCP Implementation:</p> <ul> <li>Original: Persistent disks</li> <li>Backup 1: Regional snapshots</li> <li>Backup 2: Multi-regional Cloud Storage</li> </ul>"},{"location":"gcp/operations/disaster-recovery/#immutable-backups","title":"Immutable Backups","text":"<p>Purpose: Prevent ransomware/malicious deletion</p> <p>Implementation:</p> <ul> <li>Cloud Storage retention locks</li> <li>Separate backup project (no delete permissions)</li> <li>Object holds</li> </ul>"},{"location":"gcp/operations/disaster-recovery/#automate-everything","title":"Automate Everything","text":"<p>Automate:</p> <ul> <li>Backup creation (scheduled)</li> <li>Retention enforcement (lifecycle)</li> <li>Testing (scheduled DR drills)</li> <li>Monitoring (automatic alerts)</li> </ul> <p>Why: Human error is common failure point</p>"},{"location":"gcp/operations/disaster-recovery/#document-and-communicate","title":"Document and Communicate","text":"<p>Documentation: Updated with each change</p> <p>Communication: Team aware of DR procedures</p> <p>Training: Regular DR training for on-call</p>"},{"location":"gcp/operations/disaster-recovery/#exam-focus","title":"Exam Focus","text":""},{"location":"gcp/operations/disaster-recovery/#rpo-vs-rto","title":"RPO vs RTO","text":"<ul> <li>Definition and difference</li> <li>How they drive strategy selection</li> <li>Cost implications</li> </ul>"},{"location":"gcp/operations/disaster-recovery/#dr-strategies","title":"DR Strategies","text":"<ul> <li>Backup/restore, pilot light, warm standby, hot standby, active-active</li> <li>Cost vs RTO/RPO trade-offs</li> <li>When to use each</li> </ul>"},{"location":"gcp/operations/disaster-recovery/#service-specific-dr","title":"Service-Specific DR","text":"<ul> <li>Compute Engine (snapshots, machine images)</li> <li>Cloud SQL (HA, read replicas, backups)</li> <li>Cloud Storage (versioning, replication)</li> <li>GKE (regional clusters, multi-cluster)</li> </ul>"},{"location":"gcp/operations/disaster-recovery/#testing","title":"Testing","text":"<ul> <li>Importance of testing</li> <li>Testing frequency</li> <li>DR drill procedures</li> </ul>"},{"location":"gcp/operations/disaster-recovery/#architecture-patterns","title":"Architecture Patterns","text":"<ul> <li>Multi-zone (zone failure)</li> <li>Multi-region (regional disaster)</li> <li>Global (active-active)</li> <li>Tiered approach (different strategies per tier)</li> </ul>"},{"location":"gcp/operations/disaster-recovery/#cost-optimization_1","title":"Cost Optimization","text":"<ul> <li>Tier systems by criticality</li> <li>Appropriate storage classes</li> <li>Minimize idle resources in DR</li> <li>Leverage managed services</li> </ul>"},{"location":"gcp/paas-serverless/anthos/","title":"Anthos","text":""},{"location":"gcp/paas-serverless/anthos/#core-concepts","title":"Core Concepts","text":"<p>Anthos is a hybrid and multi-cloud application platform built on Kubernetes. Enables consistent development and operations across on-premises, GCP, AWS, and Azure.</p> <p>Key Principle: Write once, run anywhere; consistent K8s experience across environments.</p>"},{"location":"gcp/paas-serverless/anthos/#anthos-components","title":"Anthos Components","text":"Component Purpose Key Feature GKE/Anthos clusters Kubernetes clusters Anywhere (on-prem, clouds) Anthos Config Management Policy and config GitOps, multi-cluster sync Anthos Service Mesh Service-to-service communication Traffic management, security Cloud Run for Anthos Serverless on K8s Knative-based Anthos on VMware/Bare Metal On-premises K8s Run on existing infrastructure"},{"location":"gcp/paas-serverless/anthos/#when-to-use-anthos","title":"When to Use Anthos","text":""},{"location":"gcp/paas-serverless/anthos/#use-anthos-when","title":"\u2705 Use Anthos When","text":"<ul> <li>Hybrid cloud strategy (on-prem + cloud)</li> <li>Multi-cloud deployment needed</li> <li>Existing on-premises infrastructure</li> <li>Kubernetes standardization across environments</li> <li>Modernizing VMs to containers gradually</li> <li>Need consistent policy enforcement</li> <li>Service mesh requirements</li> </ul>"},{"location":"gcp/paas-serverless/anthos/#dont-use-anthos-when","title":"\u274c Don\u2019t Use Anthos When","text":"<ul> <li>GCP-only deployment \u2192 GKE sufficient</li> <li>Simple applications \u2192 Cloud Run</li> <li>No Kubernetes needed \u2192 App Engine, Cloud Functions</li> <li>Not ready for containerization \u2192 Compute Engine</li> <li>Budget-constrained (Anthos has licensing cost)</li> </ul>"},{"location":"gcp/paas-serverless/anthos/#deployment-options","title":"Deployment Options","text":""},{"location":"gcp/paas-serverless/anthos/#gke-google-cloud","title":"GKE (Google Cloud)","text":"<p>Benefits: Fully managed, latest features, easiest</p> <p>Use case: Cloud-native applications on GCP</p>"},{"location":"gcp/paas-serverless/anthos/#anthos-on-vmware","title":"Anthos on VMware","text":"<p>Architecture: Run GKE on VMware vSphere</p> <p>Benefits:</p> <ul> <li>Use existing VMware investment</li> <li>Keep data on-premises (compliance)</li> <li>Hybrid cloud connectivity</li> </ul> <p>Requirements: vSphere 6.7+, sufficient resources</p>"},{"location":"gcp/paas-serverless/anthos/#anthos-on-bare-metal","title":"Anthos on Bare Metal","text":"<p>Architecture: Run GKE directly on physical servers</p> <p>Benefits:</p> <ul> <li>No hypervisor overhead</li> <li>Edge computing scenarios</li> <li>Cost savings (no VMware licenses)</li> </ul> <p>Requirements: Qualified hardware, network configuration</p>"},{"location":"gcp/paas-serverless/anthos/#anthos-on-awsazure","title":"Anthos on AWS/Azure","text":"<p>Purpose: Consistent K8s across clouds</p> <p>Benefits: Multi-cloud strategy, avoid lock-in</p> <p>Limitations: Additional complexity, cost</p>"},{"location":"gcp/paas-serverless/anthos/#anthos-config-management-acm","title":"Anthos Config Management (ACM)","text":""},{"location":"gcp/paas-serverless/anthos/#purpose","title":"Purpose","text":"<p>Centralized configuration and policy management for multiple clusters using GitOps.</p>"},{"location":"gcp/paas-serverless/anthos/#key-features","title":"Key Features","text":"<p>Policy Controller:</p> <ul> <li>Enforce policies across clusters</li> <li>Based on Open Policy Agent (OPA)</li> <li>Examples: Require labels, restrict registries, enforce resource limits</li> </ul> <p>Config Sync:</p> <ul> <li>Sync configs from Git to clusters</li> <li>Single source of truth</li> <li>Automatic reconciliation</li> <li>Namespace and cluster-scoped configs</li> </ul>"},{"location":"gcp/paas-serverless/anthos/#gitops-workflow","title":"GitOps Workflow","text":"<pre><code>Git Repository (configs) \u2192 Config Sync \u2192 Multiple clusters apply configs\n</code></pre> <p>Benefits: Version control, audit trail, declarative, automated</p>"},{"location":"gcp/paas-serverless/anthos/#common-policies","title":"Common Policies","text":"<ul> <li>Require specific labels</li> <li>Enforce namespace quotas</li> <li>Restrict container registries</li> <li>Require pod security policies</li> <li>Enforce naming conventions</li> </ul>"},{"location":"gcp/paas-serverless/anthos/#anthos-service-mesh-asm","title":"Anthos Service Mesh (ASM)","text":""},{"location":"gcp/paas-serverless/anthos/#purpose_1","title":"Purpose","text":"<p>Managed service mesh for observability, security, and traffic management.</p>"},{"location":"gcp/paas-serverless/anthos/#architecture","title":"Architecture","text":"<p>Based on Istio, fully managed by Google</p> <p>Components:</p> <ul> <li>Control Plane: Managed by Google</li> <li>Data Plane: Envoy sidecars in pods</li> </ul>"},{"location":"gcp/paas-serverless/anthos/#features","title":"Features","text":"<p>Traffic Management:</p> <ul> <li>Load balancing</li> <li>Circuit breaking</li> <li>Retries and timeouts</li> <li>Canary deployments</li> <li>Traffic splitting</li> </ul> <p>Security:</p> <ul> <li>mTLS between services (automatic)</li> <li>Authorization policies</li> <li>Service-to-service auth</li> <li>Certificate management</li> </ul> <p>Observability:</p> <ul> <li>Service topology visualization</li> <li>Distributed tracing</li> <li>Metrics and logs</li> <li>Service-level objectives (SLOs)</li> </ul>"},{"location":"gcp/paas-serverless/anthos/#use-cases","title":"Use Cases","text":"<ul> <li>Microservices communication</li> <li>Zero-trust security</li> <li>Canary deployments</li> <li>Service observability</li> <li>Multi-cluster service mesh</li> </ul>"},{"location":"gcp/paas-serverless/anthos/#cloud-run-for-anthos","title":"Cloud Run for Anthos","text":"<p>Purpose: Serverless containers on your GKE clusters</p> <p>Benefits:</p> <ul> <li>Knative-based</li> <li>Auto-scaling (including to zero)</li> <li>Simplified deployment</li> <li>Use existing GKE clusters</li> </ul> <p>vs Cloud Run: Same developer experience, runs on your infrastructure</p> <p>Use case: Serverless on-premises or in specific clusters</p>"},{"location":"gcp/paas-serverless/anthos/#architecture-patterns","title":"Architecture Patterns","text":""},{"location":"gcp/paas-serverless/anthos/#hybrid-application","title":"Hybrid Application","text":"<pre><code>On-prem (Anthos on VMware): Legacy systems + databases\nGCP (GKE): Modern microservices\nConnected via: VPN/Interconnect + Anthos Service Mesh\n</code></pre> <p>Benefits: Gradual migration, keep sensitive data on-prem</p>"},{"location":"gcp/paas-serverless/anthos/#multi-region-ha","title":"Multi-Region HA","text":"<pre><code>GKE Cluster (us-central1)\nGKE Cluster (europe-west1)\nGKE Cluster (asia-east1)\nManaged by: Anthos Config Management\nService Mesh: Cross-cluster communication\n</code></pre>"},{"location":"gcp/paas-serverless/anthos/#edge-computing","title":"Edge Computing","text":"<pre><code>Anthos on Bare Metal (retail stores/factories)\nCentral GKE (cloud)\nSync: Anthos Config Management\n</code></pre> <p>Use case: Low latency, local data processing, offline capability</p>"},{"location":"gcp/paas-serverless/anthos/#multi-cloud","title":"Multi-Cloud","text":"<pre><code>GKE (GCP) + EKS via Anthos (AWS) + AKS via Anthos (Azure)\nUnified: Config Management + Service Mesh\n</code></pre> <p>Benefits: Avoid vendor lock-in, geographic coverage, redundancy</p>"},{"location":"gcp/paas-serverless/anthos/#migrate-to-containers-m4c","title":"Migrate to Containers (M4C)","text":"<p>Purpose: Migrate VMs to containers running on GKE/Anthos</p> <p>Process:</p> <ol> <li>Discover and assess VMs</li> <li>Generate migration plan</li> <li>Convert VM to container</li> <li>Deploy to GKE/Anthos</li> <li>Optimize containerized workload</li> </ol> <p>Use case: Modernize legacy applications, VM to container migration</p>"},{"location":"gcp/paas-serverless/anthos/#security-features","title":"Security Features","text":""},{"location":"gcp/paas-serverless/anthos/#binary-authorization","title":"Binary Authorization","text":"<p>Purpose: Enforce only trusted container images deployed</p> <p>How: Cryptographic signatures on images, attestation checks</p> <p>Use case: Compliance, supply chain security</p>"},{"location":"gcp/paas-serverless/anthos/#policy-controller","title":"Policy Controller","text":"<p>Purpose: Enforce organizational policies on clusters</p> <p>Examples:</p> <ul> <li>Only approved container registries</li> <li>Required labels on resources</li> <li>Prohibited capabilities</li> <li>Resource quotas</li> </ul>"},{"location":"gcp/paas-serverless/anthos/#service-mesh-security","title":"Service Mesh Security","text":"<p>mTLS: Automatic encryption between services Authorization: Fine-grained access control Certificate Management: Automated, no manual cert handling</p>"},{"location":"gcp/paas-serverless/anthos/#workload-identity","title":"Workload Identity","text":"<p>Purpose: Kubernetes service accounts \u2192 Google service accounts</p> <p>Benefits: Secure access to GCP services, no keys in pods</p>"},{"location":"gcp/paas-serverless/anthos/#cost-considerations","title":"Cost Considerations","text":"<p>Anthos Licensing:</p> <ul> <li>Per-vCPU pricing for on-premises</li> <li>Included with GKE on GCP</li> <li>Additional cost for multi-cloud</li> </ul> <p>Infrastructure Costs:</p> <ul> <li>GKE: Standard GCP compute pricing</li> <li>On-premises: Your hardware + VMware licenses (if applicable)</li> <li>Network: Interconnect, VPN, egress</li> </ul> <p>Optimization:</p> <ul> <li>Right-size clusters</li> <li>Use Autopilot GKE (managed)</li> <li>Optimize workload placement</li> <li>Monitor with recommendations</li> </ul>"},{"location":"gcp/paas-serverless/anthos/#monitoring-and-operations","title":"Monitoring and Operations","text":""},{"location":"gcp/paas-serverless/anthos/#cloud-monitoring","title":"Cloud Monitoring","text":"<ul> <li>Cluster metrics</li> <li>Application metrics</li> <li>Service mesh metrics</li> <li>Custom metrics</li> </ul>"},{"location":"gcp/paas-serverless/anthos/#cloud-logging","title":"Cloud Logging","text":"<ul> <li>Cluster logs</li> <li>Application logs</li> <li>Audit logs</li> <li>Centralized logging</li> </ul>"},{"location":"gcp/paas-serverless/anthos/#service-mesh-observability","title":"Service Mesh Observability","text":"<ul> <li>Service topology</li> <li>Request rates</li> <li>Latencies</li> <li>Error rates</li> <li>SLI/SLO tracking</li> </ul>"},{"location":"gcp/paas-serverless/anthos/#exam-focus","title":"Exam Focus","text":""},{"location":"gcp/paas-serverless/anthos/#core-concepts_1","title":"Core Concepts","text":"<ul> <li>What is Anthos (hybrid/multi-cloud K8s platform)</li> <li>Components (GKE, Config Mgmt, Service Mesh)</li> <li>When to use vs GKE alone</li> <li>Deployment options (VMware, bare metal, clouds)</li> </ul>"},{"location":"gcp/paas-serverless/anthos/#use-cases_1","title":"Use Cases","text":"<ul> <li>Hybrid cloud (on-prem + cloud)</li> <li>Multi-cloud strategy</li> <li>Edge computing</li> <li>Gradual VM-to-container migration</li> </ul>"},{"location":"gcp/paas-serverless/anthos/#architecture_1","title":"Architecture","text":"<ul> <li>Hybrid application design</li> <li>Multi-cluster management</li> <li>Service mesh benefits</li> <li>GitOps with Config Management</li> </ul>"},{"location":"gcp/paas-serverless/anthos/#config-management","title":"Config Management","text":"<ul> <li>Policy enforcement</li> <li>Config sync</li> <li>GitOps workflow</li> <li>Multi-cluster consistency</li> </ul>"},{"location":"gcp/paas-serverless/anthos/#service-mesh","title":"Service Mesh","text":"<ul> <li>mTLS between services</li> <li>Traffic management</li> <li>Observability</li> <li>Security policies</li> </ul>"},{"location":"gcp/paas-serverless/anthos/#security","title":"Security","text":"<ul> <li>Binary Authorization</li> <li>Policy Controller</li> <li>Workload Identity</li> <li>Zero-trust networking</li> </ul>"},{"location":"gcp/paas-serverless/anthos/#migration","title":"Migration","text":"<ul> <li>Migrate to Containers (M4C)</li> <li>VM to container modernization</li> <li>Gradual migration strategy</li> </ul>"},{"location":"gcp/paas-serverless/app-engine/","title":"App Engine","text":""},{"location":"gcp/paas-serverless/app-engine/#core-concepts","title":"Core Concepts","text":"<p>App Engine is a fully managed PaaS for building web applications and APIs without infrastructure management. Two environments: Standard (sandboxed, instant scaling) and Flexible (containers, more control).</p> <p>Key Decision: Use Standard for simple web apps with variable traffic; Flexible for custom runtimes or background processing.</p>"},{"location":"gcp/paas-serverless/app-engine/#standard-vs-flexible","title":"Standard vs Flexible","text":"Feature Standard Flexible Scaling 0 to N instances (instant) Min 1 instance (gradual) Runtime Specific language versions Any Docker container Startup Milliseconds Minutes Pricing Instance hours ($0.05/hr) vCPU/memory hours Free Tier 28 instance hours/day None SSH No Yes Timeout 60s (10m tasks) 60m"},{"location":"gcp/paas-serverless/app-engine/#when-to-use","title":"When to Use","text":""},{"location":"gcp/paas-serverless/app-engine/#standard-environment","title":"Standard Environment","text":"<p>\u2705 Use when:</p> <ul> <li>Simple web apps, REST APIs</li> <li>Unpredictable traffic (scale to zero)</li> <li>Budget-conscious (free tier)</li> <li>Supported runtime sufficient</li> <li>Fast startup critical</li> </ul> <p>\u274c Don\u2019t use when:</p> <ul> <li>Need custom runtime/libraries</li> <li>Background processing required</li> <li>Write to local disk needed</li> <li>Need SSH access</li> </ul>"},{"location":"gcp/paas-serverless/app-engine/#flexible-environment","title":"Flexible Environment","text":"<p>\u2705 Use when:</p> <ul> <li>Custom runtime/dependencies</li> <li>Native code libraries</li> <li>Background workers</li> <li>Existing Docker containers</li> </ul> <p>\u274c Don\u2019t use when:</p> <ul> <li>Can use Standard (cheaper)</li> <li>Need multi-region \u2192 Cloud Run</li> <li>Need K8s control \u2192 GKE</li> </ul>"},{"location":"gcp/paas-serverless/app-engine/#architecture-patterns","title":"Architecture Patterns","text":""},{"location":"gcp/paas-serverless/app-engine/#services-microservices","title":"Services (Microservices)","text":"<pre><code>app.yaml           \u2192 default service (frontend)\napi.yaml           \u2192 api service (backend)\nworker.yaml        \u2192 worker service (async tasks)\n</code></pre> <p>Benefits: Independent scaling, separate deploys, logical separation</p>"},{"location":"gcp/paas-serverless/app-engine/#traffic-splitting","title":"Traffic Splitting","text":"<p>Use cases: Canary (5% new version), A/B testing (50/50), blue-green</p> <p>Methods: IP, cookie, random</p>"},{"location":"gcp/paas-serverless/app-engine/#scaling-strategies","title":"Scaling Strategies","text":"<p>Automatic (most common): Scale based on load Basic (Standard only): Fixed instances, idle shutdown Manual: Fixed instance count</p>"},{"location":"gcp/paas-serverless/app-engine/#key-limitations","title":"Key Limitations","text":"<p>Standard:</p> <ul> <li>60s request timeout</li> <li>Read-only filesystem (except /tmp)</li> <li>Outbound HTTP/S only</li> <li>No background threads</li> </ul> <p>Flexible:</p> <ul> <li>Always \u22651 instance (cost)</li> <li>Slower cold start</li> <li>No free tier</li> </ul> <p>Both:</p> <ul> <li>Single region deployment</li> <li>No global load balancing</li> </ul>"},{"location":"gcp/paas-serverless/app-engine/#integration","title":"Integration","text":"<p>Native:</p> <ul> <li>Cloud Datastore/Firestore (automatic indexing)</li> <li>Cloud Tasks (async work)</li> <li>Cloud Storage (static files)</li> <li>Cloud SQL (via proxy)</li> </ul> <p>Identity-Aware Proxy: Zero-trust authentication without code</p>"},{"location":"gcp/paas-serverless/app-engine/#cost-optimization","title":"Cost Optimization","text":"<p>Standard: Scale to zero, free tier, instance hours Flexible: Always running, higher base cost</p> <p>Strategy: Use Standard unless Flexible required</p>"},{"location":"gcp/paas-serverless/app-engine/#exam-focus","title":"Exam Focus","text":"<ul> <li>Standard vs Flexible decision criteria</li> <li>When App Engine vs Cloud Run vs GKE</li> <li>Scaling configurations</li> <li>Service architecture</li> <li>Request timeout constraints</li> <li>Regional limitation</li> </ul>"},{"location":"gcp/paas-serverless/cloud-endpoints/","title":"Cloud Endpoints","text":""},{"location":"gcp/paas-serverless/cloud-endpoints/#core-concepts","title":"Core Concepts","text":"<p>Cloud Endpoints is an API management system for developing, deploying, and managing APIs. Provides authentication, monitoring, and API key management for your services.</p> <p>Key Principle: API gateway and management layer; use for API security, monitoring, and developer experience.</p>"},{"location":"gcp/paas-serverless/cloud-endpoints/#endpoints-vs-alternatives","title":"Endpoints vs Alternatives","text":"Feature Cloud Endpoints Apigee API Gateway Cloud Armor Purpose API management Full API platform Managed gateway DDoS protection Backends GCP services Any GCP/on-prem Load balanced Cost Low (free tier) High Medium DDoS protection Complexity Low-Medium High Low Low Use case GCP APIs Enterprise APIs Simple routing Security only"},{"location":"gcp/paas-serverless/cloud-endpoints/#supported-backends","title":"Supported Backends","text":""},{"location":"gcp/paas-serverless/cloud-endpoints/#cloud-endpoints-for-openapi","title":"Cloud Endpoints for OpenAPI","text":"<p>Backends:</p> <ul> <li>App Engine Standard</li> <li>Cloud Functions</li> <li>Cloud Run</li> <li>Compute Engine</li> <li>GKE</li> </ul> <p>Specification: OpenAPI (Swagger) 2.0</p> <p>Use case: RESTful HTTP APIs</p>"},{"location":"gcp/paas-serverless/cloud-endpoints/#cloud-endpoints-for-grpc","title":"Cloud Endpoints for gRPC","text":"<p>Backends:</p> <ul> <li>GKE</li> <li>Compute Engine</li> <li>Any gRPC service</li> </ul> <p>Specification: gRPC with protocol buffers</p> <p>Use case: High-performance microservices</p>"},{"location":"gcp/paas-serverless/cloud-endpoints/#when-to-use","title":"When to Use","text":""},{"location":"gcp/paas-serverless/cloud-endpoints/#use-cloud-endpoints-when","title":"\u2705 Use Cloud Endpoints When","text":"<ul> <li>Need API key management</li> <li>Require API monitoring/logging</li> <li>Multiple API versions</li> <li>Rate limiting needed</li> <li>Developer portal desired</li> <li>JWT validation required</li> </ul>"},{"location":"gcp/paas-serverless/cloud-endpoints/#use-apigee-when","title":"\u2705 Use Apigee When","text":"<ul> <li>Enterprise API management</li> <li>Monetization required</li> <li>Complex transformations</li> <li>Legacy system integration</li> <li>Multi-cloud/hybrid APIs</li> <li>Advanced analytics</li> </ul>"},{"location":"gcp/paas-serverless/cloud-endpoints/#dont-use-when","title":"\u274c Don\u2019t Use When","text":"<ul> <li>Simple internal APIs (IAM sufficient)</li> <li>No API key management needed</li> <li>Just need DDoS protection \u2192 Cloud Armor</li> <li>Just need authentication \u2192 IAP</li> </ul>"},{"location":"gcp/paas-serverless/cloud-endpoints/#key-features","title":"Key Features","text":""},{"location":"gcp/paas-serverless/cloud-endpoints/#authentication","title":"Authentication","text":"<p>API Keys:</p> <ul> <li>Simple credential</li> <li>Per-project or per-application</li> <li>Rate limiting per key</li> <li>Revocable</li> </ul> <p>JWT (JSON Web Tokens):</p> <ul> <li>Service account authentication</li> <li>OAuth 2.0 integration</li> <li>Firebase Auth integration</li> <li>Custom issuers</li> </ul> <p>Decision: API keys for simple, JWT for enterprise</p>"},{"location":"gcp/paas-serverless/cloud-endpoints/#rate-limiting","title":"Rate Limiting","text":"<p>Quota Configuration:</p> <pre><code>quota:\n  limits:\n\n    - name: \"read-limit\"\n      metric: \"read-requests\"\n      values:\n        STANDARD: 10000  # per day\n\n    - name: \"write-limit\"\n      metric: \"write-requests\"\n      values:\n        STANDARD: 1000\n</code></pre> <p>Per-User Quotas: Prevent abuse, fair usage</p>"},{"location":"gcp/paas-serverless/cloud-endpoints/#monitoring","title":"Monitoring","text":"<p>Automatic Metrics:</p> <ul> <li>Request count</li> <li>Latency</li> <li>Error rate</li> <li>API key usage</li> </ul> <p>Integration: Cloud Monitoring dashboards</p>"},{"location":"gcp/paas-serverless/cloud-endpoints/#developer-portal","title":"Developer Portal","text":"<p>Features:</p> <ul> <li>API documentation (from OpenAPI spec)</li> <li>Try the API interface</li> <li>API key management</li> <li>Usage statistics</li> </ul> <p>Generation: Automatic from spec</p>"},{"location":"gcp/paas-serverless/cloud-endpoints/#architecture-patterns","title":"Architecture Patterns","text":""},{"location":"gcp/paas-serverless/cloud-endpoints/#api-gateway-pattern","title":"API Gateway Pattern","text":"<pre><code>Client \u2192 Cloud Endpoints \u2192 Backend (App Engine/Cloud Run/GKE)\n</code></pre> <p>Benefits: Centralized auth, monitoring, rate limiting</p>"},{"location":"gcp/paas-serverless/cloud-endpoints/#multi-version-api","title":"Multi-Version API","text":"<pre><code>Endpoints:\n  /v1 \u2192 Backend v1\n  /v2 \u2192 Backend v2\n</code></pre> <p>Use case: Backward compatibility, gradual migration</p>"},{"location":"gcp/paas-serverless/cloud-endpoints/#microservices-gateway","title":"Microservices Gateway","text":"<pre><code>Endpoints \u2192 Service A (Cloud Run)\n         \u2192 Service B (Cloud Functions)\n         \u2192 Service C (GKE)\n</code></pre> <p>Benefits: Unified API surface, per-service auth</p>"},{"location":"gcp/paas-serverless/cloud-endpoints/#deployment-models","title":"Deployment Models","text":""},{"location":"gcp/paas-serverless/cloud-endpoints/#extensible-service-proxy-esp","title":"Extensible Service Proxy (ESP)","text":"<p>Architecture: Nginx-based proxy in front of backend</p> <p>Deployment:</p> <ul> <li>Sidecar container (GKE)</li> <li>Separate proxy (Compute Engine)</li> </ul> <p>Limitations: Legacy, being replaced by ESPv2</p>"},{"location":"gcp/paas-serverless/cloud-endpoints/#extensible-service-proxy-v2-espv2","title":"Extensible Service Proxy v2 (ESPv2)","text":"<p>Architecture: Envoy-based proxy (modern)</p> <p>Benefits:</p> <ul> <li>Better performance</li> <li>More features</li> <li>Active development</li> </ul> <p>Recommendation: Use ESPv2 for new deployments</p>"},{"location":"gcp/paas-serverless/cloud-endpoints/#cloud-endpoints-on-cloud-run","title":"Cloud Endpoints on Cloud Run","text":"<p>Pattern: Deploy ESP as container alongside backend</p> <pre><code>Cloud Run Service = ESP container + Backend logic\n</code></pre> <p>Benefits: Serverless, auto-scaling, simple deployment</p>"},{"location":"gcp/paas-serverless/cloud-endpoints/#configuration","title":"Configuration","text":""},{"location":"gcp/paas-serverless/cloud-endpoints/#openapi-specification","title":"OpenAPI Specification","text":"<p>Required fields:</p> <pre><code>swagger: \"2.0\"\nhost: \"api.example.com\"\nx-google-endpoints:\n\n  - name: \"api.example.com\"\n    target: \"SERVICE_NAME\"\nsecurityDefinitions:\n  api_key:\n    type: \"apiKey\"\n    name: \"key\"\n    in: \"query\"\n</code></pre>"},{"location":"gcp/paas-serverless/cloud-endpoints/#security-schemes","title":"Security Schemes","text":"<p>API Key:</p> <pre><code>securityDefinitions:\n  api_key:\n    type: apiKey\n    name: key\n    in: query\n</code></pre> <p>JWT:</p> <pre><code>securityDefinitions:\n  jwt_auth:\n    authorizationUrl: \"\"\n    flow: \"implicit\"\n    type: \"oauth2\"\n    x-google-issuer: \"issuer-url\"\n    x-google-jwks_uri: \"jwks-url\"\n</code></pre>"},{"location":"gcp/paas-serverless/cloud-endpoints/#cost-model","title":"Cost Model","text":"<p>Free tier: 2M API calls/month</p> <p>Pricing (beyond free tier):</p> <ul> <li>$3 per million calls</li> <li>No infrastructure costs (serverless)</li> </ul> <p>Backend costs: Separate (App Engine, Cloud Run, etc.)</p>"},{"location":"gcp/paas-serverless/cloud-endpoints/#limitations","title":"Limitations","text":"<ul> <li>HTTP/REST and gRPC only</li> <li>OpenAPI 2.0 (not 3.0 yet)</li> <li>Regional deployment (backend determines)</li> <li>Limited transformation capabilities vs Apigee</li> </ul>"},{"location":"gcp/paas-serverless/cloud-endpoints/#apigee-comparison","title":"Apigee Comparison","text":""},{"location":"gcp/paas-serverless/cloud-endpoints/#use-cloud-endpoints-when_1","title":"Use Cloud Endpoints When","text":"<ul> <li>Simple API management</li> <li>GCP-native backends</li> <li>Cost-sensitive</li> <li>Fast setup needed</li> <li>Internal/partner APIs</li> </ul>"},{"location":"gcp/paas-serverless/cloud-endpoints/#use-apigee-when_1","title":"Use Apigee When","text":"<ul> <li>Enterprise-grade API platform</li> <li>API monetization</li> <li>Complex mediation/transformation</li> <li>Hybrid/multi-cloud</li> <li>Advanced analytics/ML</li> <li>API product management</li> </ul> <p>Cost: Apigee 10-100x more expensive, more features</p>"},{"location":"gcp/paas-serverless/cloud-endpoints/#security-best-practices","title":"Security Best Practices","text":""},{"location":"gcp/paas-serverless/cloud-endpoints/#api-key-management","title":"API Key Management","text":"<ul> <li>Restrict keys to specific APIs</li> <li>Rotate keys regularly</li> <li>Use per-app keys</li> <li>Monitor key usage</li> </ul>"},{"location":"gcp/paas-serverless/cloud-endpoints/#jwt-validation","title":"JWT Validation","text":"<ul> <li>Verify issuer and audience</li> <li>Check expiration</li> <li>Use HTTPS only</li> <li>Validate signatures</li> </ul>"},{"location":"gcp/paas-serverless/cloud-endpoints/#network-security","title":"Network Security","text":"<ul> <li>VPC Service Controls</li> <li>Cloud Armor for DDoS</li> <li>HTTPS enforcement</li> <li>Rate limiting</li> </ul>"},{"location":"gcp/paas-serverless/cloud-endpoints/#integration","title":"Integration","text":"<p>With Cloud Services:</p> <ul> <li>Identity Platform: User authentication</li> <li>Cloud Monitoring: Metrics and alerting</li> <li>Cloud Logging: Request logging</li> <li>Cloud Trace: Distributed tracing</li> </ul>"},{"location":"gcp/paas-serverless/cloud-endpoints/#common-patterns","title":"Common Patterns","text":""},{"location":"gcp/paas-serverless/cloud-endpoints/#mobile-backend","title":"Mobile Backend","text":"<pre><code>Mobile App \u2192 Endpoints (API key) \u2192 Cloud Functions/Run\n</code></pre>"},{"location":"gcp/paas-serverless/cloud-endpoints/#partner-api","title":"Partner API","text":"<pre><code>Partner \u2192 Endpoints (JWT) \u2192 Backend services\n</code></pre>"},{"location":"gcp/paas-serverless/cloud-endpoints/#public-api","title":"Public API","text":"<pre><code>Developer \u2192 Endpoints (API key + rate limit) \u2192 Your API\n</code></pre>"},{"location":"gcp/paas-serverless/cloud-endpoints/#exam-focus","title":"Exam Focus","text":""},{"location":"gcp/paas-serverless/cloud-endpoints/#design-decisions","title":"Design Decisions","text":"<ul> <li>Endpoints vs Apigee</li> <li>When to use API gateway</li> <li>Authentication method selection</li> <li>Backend service choice</li> </ul>"},{"location":"gcp/paas-serverless/cloud-endpoints/#architecture","title":"Architecture","text":"<ul> <li>API gateway pattern</li> <li>Multi-version APIs</li> <li>Microservices gateway</li> <li>Security layers</li> </ul>"},{"location":"gcp/paas-serverless/cloud-endpoints/#features","title":"Features","text":"<ul> <li>API key management</li> <li>Rate limiting</li> <li>JWT authentication</li> <li>Monitoring capabilities</li> </ul>"},{"location":"gcp/paas-serverless/cloud-endpoints/#deployment","title":"Deployment","text":"<ul> <li>ESP vs ESPv2</li> <li>Integration with backends</li> <li>Cloud Run deployment</li> <li>GKE sidecar pattern</li> </ul>"},{"location":"gcp/paas-serverless/cloud-functions/","title":"Cloud Functions","text":""},{"location":"gcp/paas-serverless/cloud-functions/#core-concepts","title":"Core Concepts","text":"<p>Cloud Functions is a serverless execution environment for single-purpose functions triggered by events. Deploy code without infrastructure management.</p> <p>Key Principle: Event-driven, single-purpose functions; use for glue code and event processing, not full applications.</p>"},{"location":"gcp/paas-serverless/cloud-functions/#generations-comparison","title":"Generations Comparison","text":"Feature 1<sup>st</sup> Gen 2<sup>nd</sup> Gen (Recommended) Runtime Cloud Functions runtime Cloud Run infrastructure Timeout 9 minutes 60 minutes Instances Max 3000 Max 1000 per region Concurrency 1 request/instance Up to 1000/instance Min instances 0 only 0-1000 Traffic splitting No Yes Use case Simple events Production workloads <p>Recommendation: Use 2<sup>nd</sup> gen for new projects</p>"},{"location":"gcp/paas-serverless/cloud-functions/#when-to-use-cloud-functions","title":"When to Use Cloud Functions","text":""},{"location":"gcp/paas-serverless/cloud-functions/#use-when","title":"\u2705 Use When","text":"<ul> <li>Event-driven automation (Pub/Sub, Storage, Firestore)</li> <li>Lightweight API endpoints</li> <li>Webhooks and callbacks</li> <li>Data transformation pipelines</li> <li>Scheduled tasks (simple cron)</li> <li>Glue code between services</li> </ul>"},{"location":"gcp/paas-serverless/cloud-functions/#dont-use-when","title":"\u274c Don\u2019t Use When","text":"<ul> <li>Complex applications \u2192 Cloud Run</li> <li>Long-running tasks (&gt;60m) \u2192 Compute Engine</li> <li>Stateful processing \u2192 Cloud Run + storage</li> <li>High concurrency needed \u2192 Cloud Run (better concurrency)</li> <li>Need HTTP routing \u2192 Cloud Run</li> </ul>"},{"location":"gcp/paas-serverless/cloud-functions/#event-sources","title":"Event Sources","text":""},{"location":"gcp/paas-serverless/cloud-functions/#http-functions","title":"HTTP Functions","text":"<p>Use cases: Webhooks, API endpoints, HTTP callbacks</p> <p>Invocation: Direct HTTP request</p>"},{"location":"gcp/paas-serverless/cloud-functions/#background-functions","title":"Background Functions","text":"<p>Pub/Sub:</p> <ul> <li>Message-driven processing</li> <li>Async workflows</li> <li>Fan-out patterns</li> </ul> <p>Cloud Storage:</p> <ul> <li>File upload processing</li> <li>Image transformation</li> <li>Data validation</li> </ul> <p>Firestore:</p> <ul> <li>Document create/update/delete triggers</li> <li>Data validation</li> <li>Derived data updates</li> </ul> <p>Firebase:</p> <ul> <li>Authentication events</li> <li>Realtime Database triggers</li> <li>Analytics events</li> </ul> <p>Cloud Logging:</p> <ul> <li>Log-based triggers</li> <li>Custom alerting</li> </ul>"},{"location":"gcp/paas-serverless/cloud-functions/#architecture-patterns","title":"Architecture Patterns","text":""},{"location":"gcp/paas-serverless/cloud-functions/#etl-pipeline","title":"ETL Pipeline","text":"<pre><code>Cloud Storage upload \u2192 Cloud Function \u2192 Transform \u2192 BigQuery\n</code></pre>"},{"location":"gcp/paas-serverless/cloud-functions/#image-processing","title":"Image Processing","text":"<pre><code>Storage (upload) \u2192 Function (resize) \u2192 Storage (thumbnails)\n</code></pre>"},{"location":"gcp/paas-serverless/cloud-functions/#pubsub-fan-out","title":"Pub/Sub Fan-Out","text":"<pre><code>Pub/Sub message \u2192 Multiple Cloud Functions (parallel processing)\n</code></pre>"},{"location":"gcp/paas-serverless/cloud-functions/#api-gateway","title":"API Gateway","text":"<pre><code>Client \u2192 Cloud Function \u2192 Backend services\n</code></pre>"},{"location":"gcp/paas-serverless/cloud-functions/#serverless-cron","title":"Serverless Cron","text":"<pre><code>Cloud Scheduler \u2192 Pub/Sub \u2192 Cloud Function\n</code></pre>"},{"location":"gcp/paas-serverless/cloud-functions/#configuration","title":"Configuration","text":""},{"location":"gcp/paas-serverless/cloud-functions/#timeout","title":"Timeout","text":"<ul> <li>1<sup>st</sup> gen: Max 9 minutes</li> <li>2<sup>nd</sup> gen: Max 60 minutes</li> <li>Default: 60 seconds</li> </ul> <p>Decision: Set based on expected execution time</p>"},{"location":"gcp/paas-serverless/cloud-functions/#memory","title":"Memory","text":"<ul> <li>Range: 128 MB to 32 GB</li> <li>CPU scales with memory</li> <li>More memory = higher cost</li> </ul> <p>Decision: Start low, increase if needed</p>"},{"location":"gcp/paas-serverless/cloud-functions/#concurrency-2nd-gen-only","title":"Concurrency (2<sup>nd</sup> gen only)","text":"<ul> <li>Max 1000 concurrent requests per instance</li> <li>More concurrency = fewer cold starts</li> <li>Less concurrency = more isolation</li> </ul>"},{"location":"gcp/paas-serverless/cloud-functions/#min-instances","title":"Min Instances","text":"<ul> <li>0: Scale to zero (cost-effective)</li> <li> <p>0: Reduce cold starts (always warm)</p> </li> </ul> <p>Cost: Charged for min instances even when idle</p>"},{"location":"gcp/paas-serverless/cloud-functions/#cold-starts","title":"Cold Starts","text":"<p>Typical Duration: 1-2 seconds (varies by runtime and dependencies)</p> <p>Mitigation:</p> <ul> <li>Min instances &gt; 0</li> <li>Lighter dependencies</li> <li>Keep functions warm (scheduled ping)</li> <li>Use 2<sup>nd</sup> gen (better concurrency)</li> </ul> <p>Architecture Decision: Accept cold starts or pay for min instances</p>"},{"location":"gcp/paas-serverless/cloud-functions/#networking","title":"Networking","text":"<p>Default: Public internet egress</p> <p>VPC Connector: Access private resources (Cloud SQL, Memorystore)</p> <p>Serverless VPC Access: Connect to VPC without public IPs</p>"},{"location":"gcp/paas-serverless/cloud-functions/#security","title":"Security","text":""},{"location":"gcp/paas-serverless/cloud-functions/#iam","title":"IAM","text":"<ul> <li>Function-level permissions</li> <li>Invoker role for who can call</li> <li>Service account for what function can access</li> </ul>"},{"location":"gcp/paas-serverless/cloud-functions/#authentication","title":"Authentication","text":"<p>HTTP Functions:</p> <ul> <li>Require authentication (default)</li> <li>Allow unauthenticated (<code>allUsers</code> invoker)</li> </ul> <p>Background Functions: Triggered by events (auth via service)</p>"},{"location":"gcp/paas-serverless/cloud-functions/#secret-manager","title":"Secret Manager","text":"<p>Integration: Environment variables from secrets</p> <p>Pattern: Store API keys, credentials securely</p>"},{"location":"gcp/paas-serverless/cloud-functions/#cost-model","title":"Cost Model","text":"<p>Pricing (approximate):</p> <ul> <li>Invocations: $0.40 per million</li> <li>Compute time: Based on memory allocation</li> <li>Networking: Egress charged</li> </ul> <p>Free tier:</p> <ul> <li>2M invocations/month</li> <li>400k GB-seconds/month</li> <li>200k GHz-seconds/month</li> </ul> <p>Optimization:</p> <ul> <li>Right-size memory</li> <li>Minimize dependencies (faster startup)</li> <li>Scale to zero when idle</li> <li>Use 2<sup>nd</sup> gen (better concurrency)</li> </ul>"},{"location":"gcp/paas-serverless/cloud-functions/#limitations","title":"Limitations","text":""},{"location":"gcp/paas-serverless/cloud-functions/#1st-gen","title":"1<sup>st</sup> Gen","text":"<ul> <li>1 concurrent request per instance</li> <li>9-minute timeout</li> <li>No traffic splitting</li> <li>Slower cold starts</li> </ul>"},{"location":"gcp/paas-serverless/cloud-functions/#2nd-gen","title":"2<sup>nd</sup> Gen","text":"<ul> <li>60-minute timeout</li> <li>1000 instances per region max</li> <li>32 GB memory max</li> <li>4 vCPU max</li> </ul> <p>Both:</p> <ul> <li>Stateless (ephemeral disk)</li> <li>No direct inbound connections</li> <li>Limited execution environment</li> </ul>"},{"location":"gcp/paas-serverless/cloud-functions/#common-patterns","title":"Common Patterns","text":""},{"location":"gcp/paas-serverless/cloud-functions/#async-processing","title":"Async Processing","text":"<pre><code>App Engine/Cloud Run \u2192 Pub/Sub \u2192 Cloud Function (background work)\n</code></pre>"},{"location":"gcp/paas-serverless/cloud-functions/#file-processing","title":"File Processing","text":"<pre><code>Upload to GCS \u2192 Cloud Function \u2192 Process \u2192 Store result\n</code></pre>"},{"location":"gcp/paas-serverless/cloud-functions/#database-triggers","title":"Database Triggers","text":"<pre><code>Firestore write \u2192 Cloud Function \u2192 Update derived data\n</code></pre>"},{"location":"gcp/paas-serverless/cloud-functions/#scheduled-jobs","title":"Scheduled Jobs","text":"<pre><code>Cloud Scheduler \u2192 Pub/Sub \u2192 Cloud Function \u2192 Task execution\n</code></pre>"},{"location":"gcp/paas-serverless/cloud-functions/#webhook-handler","title":"Webhook Handler","text":"<pre><code>External service \u2192 Cloud Function (HTTP) \u2192 Process event\n</code></pre>"},{"location":"gcp/paas-serverless/cloud-functions/#migration-to-cloud-run","title":"Migration to Cloud Run","text":"<p>When to migrate:</p> <ul> <li>Need longer timeout (&gt;60m)</li> <li>Complex routing required</li> <li>Better concurrency needed</li> <li>Multi-region deployment</li> <li>Traffic splitting</li> </ul> <p>How: Package function in container, deploy to Cloud Run</p>"},{"location":"gcp/paas-serverless/cloud-functions/#best-practices","title":"Best Practices","text":""},{"location":"gcp/paas-serverless/cloud-functions/#function-design","title":"Function Design","text":"<ul> <li>Single purpose per function</li> <li>Idempotent (can retry safely)</li> <li>Stateless</li> <li>Fast execution</li> <li>Minimal dependencies</li> </ul>"},{"location":"gcp/paas-serverless/cloud-functions/#error-handling","title":"Error Handling","text":"<ul> <li>Background functions: Throw exception to retry</li> <li>HTTP functions: Return appropriate status codes</li> <li>Implement retry logic with backoff</li> <li>Use dead letter queues (Pub/Sub)</li> </ul>"},{"location":"gcp/paas-serverless/cloud-functions/#monitoring","title":"Monitoring","text":"<ul> <li>Cloud Logging for logs</li> <li>Cloud Monitoring for metrics</li> <li>Set up alerts for errors</li> <li>Track cold start frequency</li> </ul>"},{"location":"gcp/paas-serverless/cloud-functions/#exam-focus","title":"Exam Focus","text":""},{"location":"gcp/paas-serverless/cloud-functions/#design-decisions","title":"Design Decisions","text":"<ul> <li>Cloud Functions vs Cloud Run</li> <li>1<sup>st</sup> gen vs 2<sup>nd</sup> gen</li> <li>Event source selection</li> <li>When to use functions vs full app</li> </ul>"},{"location":"gcp/paas-serverless/cloud-functions/#architecture","title":"Architecture","text":"<ul> <li>Event-driven patterns</li> <li>Function chaining</li> <li>Pub/Sub fan-out</li> <li>VPC integration</li> </ul>"},{"location":"gcp/paas-serverless/cloud-functions/#configuration_1","title":"Configuration","text":"<ul> <li>Timeout selection</li> <li>Memory allocation</li> <li>Concurrency settings</li> <li>Min instances trade-off</li> </ul>"},{"location":"gcp/paas-serverless/cloud-functions/#limitations_1","title":"Limitations","text":"<ul> <li>Timeout constraints</li> <li>Concurrency limits</li> <li>Stateless requirement</li> <li>Cold start impact</li> </ul>"},{"location":"gcp/paas-serverless/cloud-functions/#cost-optimization","title":"Cost Optimization","text":"<ul> <li>Right-sizing</li> <li>Scale to zero</li> <li>2<sup>nd</sup> gen efficiency</li> <li>Minimize dependencies</li> </ul>"},{"location":"gcp/paas-serverless/cloud-iam/","title":"Cloud IAM","text":""},{"location":"gcp/paas-serverless/cloud-iam/#core-concepts","title":"Core Concepts","text":"<p>IAM (Identity and Access Management) controls who can do what on which resources. Foundation of GCP security through policy-based access control.</p> <p>Key Principle: Grant least privilege; use predefined roles when possible; organize with resource hierarchy.</p>"},{"location":"gcp/paas-serverless/cloud-iam/#iam-model","title":"IAM Model","text":"<p>Who (Identity) + Can do what (Role) + On which resource (Resource)</p> <pre><code>Principal + Role + Resource = IAM Policy Binding\n</code></pre>"},{"location":"gcp/paas-serverless/cloud-iam/#identities-who","title":"Identities (Who)","text":""},{"location":"gcp/paas-serverless/cloud-iam/#types","title":"Types","text":"<p>Google Account: Individual user (alice@gmail.com) Service Account: Application/service identity (sa@project.iam.gserviceaccount.com) Google Group: Collection of users (team@example.com) Google Workspace domain: All users in domain (example.com) Cloud Identity domain: Domain without Google Workspace</p> <p>allUsers: Anyone on internet (public) allAuthenticatedUsers: Any authenticated Google account</p>"},{"location":"gcp/paas-serverless/cloud-iam/#service-accounts","title":"Service Accounts","text":"<p>Types:</p> <ul> <li>User-managed: Created by you, fully controlled</li> <li>Default: Auto-created (Compute, App Engine)</li> <li>Google-managed: Used by Google services</li> </ul> <p>Keys:</p> <ul> <li>Google-managed (recommended): Automatic rotation</li> <li>User-managed: Manual creation, rotation responsibility</li> </ul> <p>Best Practice: Use Workload Identity (GKE), avoid keys when possible</p>"},{"location":"gcp/paas-serverless/cloud-iam/#roles-what","title":"Roles (What)","text":""},{"location":"gcp/paas-serverless/cloud-iam/#role-types","title":"Role Types","text":"Type Granularity Use Case Example Basic Project-level Legacy, avoid Owner, Editor, Viewer Predefined Service-level Standard use Storage Admin, Compute Admin Custom Fine-grained Specific needs Custom limited permissions"},{"location":"gcp/paas-serverless/cloud-iam/#basic-roles-legacy-avoid","title":"Basic Roles (Legacy - Avoid)","text":"<ul> <li>Owner: Full control + billing</li> <li>Editor: Modify resources</li> <li>Viewer: Read-only</li> </ul> <p>Problem: Too broad, violates least privilege</p> <p>Recommendation: Use predefined or custom roles instead</p>"},{"location":"gcp/paas-serverless/cloud-iam/#predefined-roles","title":"Predefined Roles","text":"<p>Examples:</p> <ul> <li><code>roles/storage.objectViewer</code>: Read objects in Cloud Storage</li> <li><code>roles/compute.instanceAdmin.v1</code>: Manage Compute Engine instances</li> <li><code>roles/bigquery.dataEditor</code>: Edit BigQuery data</li> </ul> <p>Format: <code>roles/SERVICE.ROLE</code></p> <p>Best Practice: Use predefined when available, narrowest scope needed</p>"},{"location":"gcp/paas-serverless/cloud-iam/#custom-roles","title":"Custom Roles","text":"<p>When to use:</p> <ul> <li>Predefined too broad</li> <li>Specific permission combination needed</li> <li>Compliance requirements</li> </ul> <p>Limitations:</p> <ul> <li>Project or organization level only (not folder)</li> <li>More maintenance overhead</li> <li>Can become outdated with API changes</li> </ul>"},{"location":"gcp/paas-serverless/cloud-iam/#resource-hierarchy","title":"Resource Hierarchy","text":"<pre><code>Organization\n\u251c\u2500\u2500 Folder (Department)\n\u2502   \u251c\u2500\u2500 Folder (Team)\n\u2502   \u2502   \u2514\u2500\u2500 Project\n\u2502   \u2514\u2500\u2500 Project\n\u2514\u2500\u2500 Project\n</code></pre> <p>Policy Inheritance: Child inherits parent\u2019s policies (additive, cannot restrict)</p> <p>Best Practice: Grant at highest appropriate level</p>"},{"location":"gcp/paas-serverless/cloud-iam/#iam-policy","title":"IAM Policy","text":""},{"location":"gcp/paas-serverless/cloud-iam/#policy-structure","title":"Policy Structure","text":"<pre><code>{\n  \"bindings\": [\n    {\n      \"role\": \"roles/storage.objectViewer\",\n      \"members\": [\n        \"user:alice@example.com\",\n        \"serviceAccount:sa@project.iam.gserviceaccount.com\"\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"gcp/paas-serverless/cloud-iam/#policy-evaluation","title":"Policy Evaluation","text":"<ol> <li>Deny policies (if exists) - takes precedence</li> <li>Organization policy constraints</li> <li>IAM allow policies (union of all)</li> <li>Result: Allow if any allow, deny if no allow or explicit deny</li> </ol> <p>Key: IAM is additive (cannot remove inherited permissions at lower level)</p>"},{"location":"gcp/paas-serverless/cloud-iam/#conditions","title":"Conditions","text":"<p>Purpose: Time-based or attribute-based access</p> <p>Examples:</p> <pre><code>condition:\n  title: \"Expires in 2024\"\n  expression: \"request.time &lt; timestamp('2024-12-31T23:59:59Z')\"\n</code></pre> <pre><code>condition:\n  title: \"Only from corporate network\"\n  expression: \"origin.ip in ['203.0.113.0/24']\"\n</code></pre> <p>Use cases: Temporary access, resource tagging, IP restrictions</p>"},{"location":"gcp/paas-serverless/cloud-iam/#organization-policies","title":"Organization Policies","text":"<p>Purpose: Set guardrails across organization (restrictions)</p> <p>Examples:</p> <ul> <li>Disable service account key creation</li> <li>Require Shielded VMs</li> <li>Restrict resource locations</li> <li>Disable public IP on VMs</li> </ul> <p>Difference from IAM: Constraints (what cannot be done) vs permissions (what can be done)</p>"},{"location":"gcp/paas-serverless/cloud-iam/#best-practices","title":"Best Practices","text":""},{"location":"gcp/paas-serverless/cloud-iam/#principle-of-least-privilege","title":"Principle of Least Privilege","text":"<ul> <li>Grant minimum permissions needed</li> <li>Use predefined roles over basic</li> <li>Regular permission audits</li> <li>Remove unused permissions</li> </ul>"},{"location":"gcp/paas-serverless/cloud-iam/#resource-hierarchy_1","title":"Resource Hierarchy","text":"<ul> <li>Use folders for teams/environments</li> <li>Grant at highest appropriate level</li> <li>Separate projects for isolation</li> <li>Consistent naming conventions</li> </ul>"},{"location":"gcp/paas-serverless/cloud-iam/#service-accounts_1","title":"Service Accounts","text":"<ul> <li>One service account per application/function</li> <li>Use Workload Identity (GKE)</li> <li>Avoid user-managed keys</li> <li>Never commit service account keys</li> <li>Rotate keys regularly if must use</li> </ul>"},{"location":"gcp/paas-serverless/cloud-iam/#groups-for-users","title":"Groups for Users","text":"<ul> <li>Manage users via Google Groups</li> <li>Grant roles to groups, not individuals</li> <li>Easier permission management</li> <li>Better auditability</li> </ul>"},{"location":"gcp/paas-serverless/cloud-iam/#separation-of-duties","title":"Separation of Duties","text":"<ul> <li>Split admin roles (compute admin \u2260 network admin)</li> <li>No single person has complete access</li> <li>Require approval for sensitive operations</li> <li>Multiple administrators</li> </ul>"},{"location":"gcp/paas-serverless/cloud-iam/#common-patterns","title":"Common Patterns","text":""},{"location":"gcp/paas-serverless/cloud-iam/#multi-project-setup","title":"Multi-Project Setup","text":"<pre><code>Shared VPC Project: Network admins\nDev Project: Developers (editor)\nStaging Project: Developers (viewer), CI/CD (editor)\nProd Project: Ops (admin), CI/CD (deployer)\n</code></pre>"},{"location":"gcp/paas-serverless/cloud-iam/#service-to-service-auth","title":"Service-to-Service Auth","text":"<pre><code>Service A (SA-A) \u2192 Service B\nService B: IAM policy grants SA-A required role\n</code></pre> <p>No API keys: Use service account authentication</p>"},{"location":"gcp/paas-serverless/cloud-iam/#workload-identity-gke","title":"Workload Identity (GKE)","text":"<pre><code>Kubernetes SA \u2192 Google SA \u2192 GCP services\n</code></pre> <p>Benefit: No service account keys in pods</p>"},{"location":"gcp/paas-serverless/cloud-iam/#iam-for-specific-services","title":"IAM for Specific Services","text":""},{"location":"gcp/paas-serverless/cloud-iam/#compute-engine","title":"Compute Engine","text":"<ul> <li><code>compute.instanceAdmin</code>: Manage instances</li> <li><code>compute.networkAdmin</code>: Manage networks</li> <li><code>compute.securityAdmin</code>: Manage firewall rules</li> </ul>"},{"location":"gcp/paas-serverless/cloud-iam/#cloud-storage","title":"Cloud Storage","text":"<ul> <li><code>storage.objectViewer</code>: Read objects</li> <li><code>storage.objectCreator</code>: Write objects</li> <li><code>storage.objectAdmin</code>: Full object control</li> <li><code>storage.admin</code>: Bucket + object control</li> </ul>"},{"location":"gcp/paas-serverless/cloud-iam/#bigquery","title":"BigQuery","text":"<ul> <li><code>bigquery.dataViewer</code>: Read data</li> <li><code>bigquery.dataEditor</code>: Edit data</li> <li><code>bigquery.admin</code>: Full control</li> </ul> <p>Dataset-level: More granular than project</p>"},{"location":"gcp/paas-serverless/cloud-iam/#gke","title":"GKE","text":"<ul> <li><code>container.admin</code>: Full cluster control</li> <li><code>container.developer</code>: Deploy workloads</li> <li><code>container.viewer</code>: Read-only</li> </ul>"},{"location":"gcp/paas-serverless/cloud-iam/#troubleshooting-access","title":"Troubleshooting Access","text":""},{"location":"gcp/paas-serverless/cloud-iam/#policy-analyzer","title":"Policy Analyzer","text":"<p>Tool: IAM Policy Analyzer in Console</p> <p>Capabilities:</p> <ul> <li>Why does user have access?</li> <li>What can user access?</li> <li>Policy troubleshooting</li> </ul>"},{"location":"gcp/paas-serverless/cloud-iam/#policy-simulator","title":"Policy Simulator","text":"<p>Purpose: Test policy changes before applying</p> <p>Use case: Verify least privilege, prevent breaking changes</p>"},{"location":"gcp/paas-serverless/cloud-iam/#audit-logs","title":"Audit Logs","text":"<p>Admin Activity: Who changed what Data Access: Who accessed what data (must enable) System Events: Automated actions</p> <p>Best Practice: Enable data access logs for sensitive resources</p>"},{"location":"gcp/paas-serverless/cloud-iam/#security-considerations","title":"Security Considerations","text":""},{"location":"gcp/paas-serverless/cloud-iam/#avoid-basic-roles","title":"Avoid Basic Roles","text":"<p>Problem: Too permissive</p> <p>Solution: Use predefined or custom roles</p>"},{"location":"gcp/paas-serverless/cloud-iam/#service-account-keys","title":"Service Account Keys","text":"<p>Risk: Can be stolen, no expiration</p> <p>Mitigation:</p> <ul> <li>Use Workload Identity</li> <li>Use Google-managed keys</li> <li>Rotate user-managed keys</li> <li>Disable key creation (org policy)</li> </ul>"},{"location":"gcp/paas-serverless/cloud-iam/#public-access","title":"Public Access","text":"<p>Risk: Data exposure</p> <p>Mitigation:</p> <ul> <li>Avoid <code>allUsers</code>, <code>allAuthenticatedUsers</code></li> <li>Use signed URLs for temporary access</li> <li>Public access prevention org policy</li> <li>Regular access reviews</li> </ul>"},{"location":"gcp/paas-serverless/cloud-iam/#over-privileged-service-accounts","title":"Over-Privileged Service Accounts","text":"<p>Risk: Compromised service = broad access</p> <p>Mitigation:</p> <ul> <li>One SA per application</li> <li>Minimum permissions</li> <li>Regular permission audits</li> <li>Service account impersonation for debugging</li> </ul>"},{"location":"gcp/paas-serverless/cloud-iam/#iam-vs-acls","title":"IAM vs ACLs","text":"Feature IAM ACLs Granularity Bucket or project Object-level Recommended Yes (uniform bucket-level) No (legacy) Complexity Lower Higher Audit Easier Harder <p>Recommendation: Use uniform bucket-level access (IAM only)</p>"},{"location":"gcp/paas-serverless/cloud-iam/#exam-focus","title":"Exam Focus","text":""},{"location":"gcp/paas-serverless/cloud-iam/#fundamentals","title":"Fundamentals","text":"<ul> <li>Who (principal) + What (role) + Where (resource)</li> <li>Policy inheritance (additive)</li> <li>Basic vs predefined vs custom roles</li> <li>Service account types and uses</li> </ul>"},{"location":"gcp/paas-serverless/cloud-iam/#best-practices_1","title":"Best Practices","text":"<ul> <li>Least privilege principle</li> <li>Resource hierarchy design</li> <li>Service account management</li> <li>Separation of duties</li> </ul>"},{"location":"gcp/paas-serverless/cloud-iam/#common-patterns_1","title":"Common Patterns","text":"<ul> <li>Multi-project organization</li> <li>Service-to-service auth</li> <li>Workload Identity</li> <li>Temporary access (conditions)</li> </ul>"},{"location":"gcp/paas-serverless/cloud-iam/#security","title":"Security","text":"<ul> <li>Avoid basic roles</li> <li>Key management</li> <li>Public access risks</li> <li>Organization policies</li> </ul>"},{"location":"gcp/paas-serverless/cloud-iam/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Policy Analyzer usage</li> <li>Audit logs</li> <li>Policy simulation</li> <li>Access debugging</li> </ul>"},{"location":"gcp/paas-serverless/cloud-run/","title":"Cloud Run","text":""},{"location":"gcp/paas-serverless/cloud-run/#core-concepts","title":"Core Concepts","text":"<p>Cloud Run is a fully managed serverless platform for running stateless containers. Deploy any containerized application that responds to HTTP requests without managing infrastructure.</p> <p>Key Principle: Containers that scale to zero, pay per request, any language/library.</p>"},{"location":"gcp/paas-serverless/cloud-run/#cloud-run-vs-alternatives","title":"Cloud Run vs Alternatives","text":"Feature Cloud Run App Engine Standard Cloud Functions GKE Autopilot Unit Container Runtime sandbox Function Pod Languages Any (container) Specific versions Specific versions Any Scale to zero Yes Yes Yes No (min 1) Cold start ~1s ~100ms ~1s N/A Multi-region Yes (managed) No No Manual Control Container-level Runtime-level Function-level Full K8s"},{"location":"gcp/paas-serverless/cloud-run/#when-to-use-cloud-run","title":"When to Use Cloud Run","text":""},{"location":"gcp/paas-serverless/cloud-run/#use-when","title":"\u2705 Use When","text":"<ul> <li>Stateless HTTP workloads (APIs, web apps)</li> <li>Any language/framework (containerized)</li> <li>Multi-region deployment needed</li> <li>Scale to zero desired</li> <li>Pay-per-request model preferred</li> <li>Existing containers to deploy</li> </ul>"},{"location":"gcp/paas-serverless/cloud-run/#dont-use-when","title":"\u274c Don\u2019t Use When","text":"<ul> <li>Long-running background jobs \u2192 Cloud Tasks + Cloud Run or Compute Engine</li> <li>WebSocket/streaming needed \u2192 GKE</li> <li>Stateful applications \u2192 GKE + StatefulSets</li> <li>Sub-100ms latency critical \u2192 Keep warm instances</li> </ul>"},{"location":"gcp/paas-serverless/cloud-run/#architecture-patterns","title":"Architecture Patterns","text":""},{"location":"gcp/paas-serverless/cloud-run/#multi-region-deployment","title":"Multi-Region Deployment","text":"<p>Global Load Balancer:</p> <pre><code>Global LB \u2192 Cloud Run (us-central1)\n         \u2192 Cloud Run (europe-west1)\n         \u2192 Cloud Run (asia-east1)\n</code></pre> <p>Benefits: Low latency globally, high availability, automatic failover</p>"},{"location":"gcp/paas-serverless/cloud-run/#service-to-service-communication","title":"Service-to-Service Communication","text":"<p>Pattern: Cloud Run services calling each other</p> <p>Authentication: Service accounts + IAM, no public access</p>"},{"location":"gcp/paas-serverless/cloud-run/#event-driven-architecture","title":"Event-Driven Architecture","text":"<p>Triggers:</p> <ul> <li>Pub/Sub push subscriptions</li> <li>Cloud Storage events (via Eventarc)</li> <li>Cloud Scheduler (cron)</li> <li>Direct HTTP invocation</li> </ul>"},{"location":"gcp/paas-serverless/cloud-run/#key-features","title":"Key Features","text":""},{"location":"gcp/paas-serverless/cloud-run/#autoscaling","title":"Autoscaling","text":"<p>Configuration:</p> <ul> <li>Min instances: 0-1000 (0 for scale to zero)</li> <li>Max instances: 1-1000</li> <li>Concurrency: 1-1000 requests per instance</li> </ul> <p>Strategy: Min 0 for cost, min &gt;0 for latency</p>"},{"location":"gcp/paas-serverless/cloud-run/#request-timeout","title":"Request Timeout","text":"<ul> <li>Default: 5 minutes</li> <li>Max: 60 minutes</li> <li>Configurable per service</li> </ul>"},{"location":"gcp/paas-serverless/cloud-run/#cpu-allocation","title":"CPU Allocation","text":"<p>Always allocated (default): CPU always available, faster response Allocated during request: CPU only during request, cheaper</p> <p>Decision: Always allocated for low latency, request-only for cost</p>"},{"location":"gcp/paas-serverless/cloud-run/#networking","title":"Networking","text":"<p>Ingress:</p> <ul> <li>All: Public internet</li> <li>Internal: VPC only</li> <li>Internal + Cloud Load Balancing: VPC + global LB</li> </ul> <p>Egress: Via VPC connector for private access</p>"},{"location":"gcp/paas-serverless/cloud-run/#concurrency-model","title":"Concurrency Model","text":"<p>Default: 80 concurrent requests per instance</p> <p>High concurrency (80-1000): Fewer instances, lower cost, shared memory Low concurrency (1-10): More instances, isolation, predictable performance</p> <p>Decision: High for stateless, low for resource-intensive</p>"},{"location":"gcp/paas-serverless/cloud-run/#security","title":"Security","text":"<p>Default:</p> <ul> <li>HTTPS only, managed certificates</li> <li>Require authentication by default</li> <li>Per-service IAM permissions</li> </ul> <p>Allow unauthenticated: <code>allUsers</code> invoker role (public APIs)</p> <p>VPC Service Controls: Perimeter-based security</p>"},{"location":"gcp/paas-serverless/cloud-run/#cost-model","title":"Cost Model","text":"<p>Pricing:</p> <ul> <li>Request: $0.40 per million</li> <li>CPU time: $0.00002400 per vCPU-second</li> <li>Memory: $0.00000250 per GiB-second</li> <li>Free tier: 2M requests, 360k GiB-seconds/month</li> </ul> <p>Optimization:</p> <ul> <li>Scale to zero when idle</li> <li>Right-size CPU/memory</li> <li>CPU allocation during request only</li> <li>High concurrency</li> </ul>"},{"location":"gcp/paas-serverless/cloud-run/#cloud-run-jobs","title":"Cloud Run Jobs","text":"<p>Purpose: Run containers to completion (batch, data processing)</p> <p>Differences from Services:</p> <ul> <li>No HTTP endpoint</li> <li>Run to completion, then stop</li> <li>Parallel execution support</li> <li>Scheduled via Cloud Scheduler</li> </ul> <p>Use cases: ETL, batch processing, scheduled tasks</p>"},{"location":"gcp/paas-serverless/cloud-run/#integration-patterns","title":"Integration Patterns","text":"<p>With Cloud Services:</p> <ul> <li>Cloud SQL: Direct connection via Unix socket</li> <li>Firestore: Native client libraries</li> <li>Pub/Sub: Push subscriptions trigger Cloud Run</li> <li>Cloud Storage: Eventarc for bucket events</li> <li>Secret Manager: Environment variables from secrets</li> </ul> <p>Service Mesh: Cloud Run integrates with Anthos Service Mesh (advanced)</p>"},{"location":"gcp/paas-serverless/cloud-run/#common-patterns","title":"Common Patterns","text":""},{"location":"gcp/paas-serverless/cloud-run/#api-gateway-pattern","title":"API Gateway Pattern","text":"<pre><code>API Gateway (Cloud Endpoints) \u2192 Cloud Run (backend)\n</code></pre>"},{"location":"gcp/paas-serverless/cloud-run/#fan-out-pattern","title":"Fan-Out Pattern","text":"<pre><code>Cloud Run (coordinator) \u2192 Pub/Sub \u2192 Multiple Cloud Run services\n</code></pre>"},{"location":"gcp/paas-serverless/cloud-run/#async-processing","title":"Async Processing","text":"<pre><code>Cloud Run (web) \u2192 Pub/Sub \u2192 Cloud Run (worker)\n</code></pre>"},{"location":"gcp/paas-serverless/cloud-run/#limitations","title":"Limitations","text":"<ul> <li>60m max request timeout</li> <li>32 GiB max memory per instance</li> <li>4 vCPU max per instance</li> <li>Stateless (local disk ephemeral)</li> <li>Cold starts (~1s typical)</li> </ul>"},{"location":"gcp/paas-serverless/cloud-run/#migration-paths","title":"Migration Paths","text":"<p>From App Engine: Containerize app, deploy to Cloud Run From GKE: Extract stateless services to Cloud Run From Compute Engine: Containerize, remove state</p>"},{"location":"gcp/paas-serverless/cloud-run/#exam-focus","title":"Exam Focus","text":""},{"location":"gcp/paas-serverless/cloud-run/#design-decisions","title":"Design Decisions","text":"<ul> <li>Cloud Run vs App Engine vs Cloud Functions</li> <li>When to use multi-region</li> <li>Concurrency configuration</li> <li>CPU allocation strategy</li> </ul>"},{"location":"gcp/paas-serverless/cloud-run/#architecture","title":"Architecture","text":"<ul> <li>Service-to-service auth</li> <li>Event-driven patterns</li> <li>Global deployment</li> <li>VPC integration</li> </ul>"},{"location":"gcp/paas-serverless/cloud-run/#cost-optimization","title":"Cost Optimization","text":"<ul> <li>Scale to zero</li> <li>CPU allocation modes</li> <li>Right-sizing</li> <li>Concurrency tuning</li> </ul>"},{"location":"gcp/paas-serverless/cloud-run/#security_1","title":"Security","text":"<ul> <li>IAM authentication</li> <li>Public vs private services</li> <li>VPC Service Controls</li> <li>Secret management</li> </ul>"},{"location":"gcp/paas-serverless/cloud-run/#limitations_1","title":"Limitations","text":"<ul> <li>Request timeout</li> <li>Stateless requirement</li> <li>Cold start consideration</li> <li>Resource limits</li> </ul>"},{"location":"gcp/security/cloud-armor/","title":"Cloud Armor","text":""},{"location":"gcp/security/cloud-armor/#core-concepts","title":"Core Concepts","text":"<p>Cloud Armor is a DDoS protection and Web Application Firewall (WAF) service that defends applications from attacks at the edge of Google\u2019s network.</p> <p>Key Principle: Layer 7 (application) and Layer \u00be (network) defense at scale.</p>"},{"location":"gcp/security/cloud-armor/#protection-types","title":"Protection Types","text":"Type Protection Use Case Standard L\u00be DDoS (network/transport) Volumetric attacks Advanced L7 DDoS (application) Application-layer attacks WAF Rules OWASP Top 10, custom rules SQLi, XSS, etc. Rate Limiting Request throttling API protection, abuse prevention"},{"location":"gcp/security/cloud-armor/#when-to-use-cloud-armor","title":"When to Use Cloud Armor","text":""},{"location":"gcp/security/cloud-armor/#use-when","title":"\u2705 Use When","text":"<ul> <li>Public-facing applications need DDoS protection</li> <li>WAF rules required (OWASP protection)</li> <li>Geographic restrictions needed (geo-blocking)</li> <li>Rate limiting per client IP</li> <li>Bot management required</li> <li>Adaptive protection desired</li> </ul>"},{"location":"gcp/security/cloud-armor/#dont-use-when","title":"\u274c Don\u2019t Use When","text":"<ul> <li>Internal applications only \u2192 VPC firewall rules</li> <li>No load balancer \u2192 Cloud Armor requires LB</li> <li>Simple firewall rules \u2192 VPC firewall sufficient</li> <li>Cost-sensitive small apps \u2192 Basic DDoS included free</li> </ul>"},{"location":"gcp/security/cloud-armor/#architecture","title":"Architecture","text":"<p>Deployment:</p> <pre><code>Internet \u2192 Cloud Armor (at edge) \u2192 Load Balancer \u2192 Backend\n</code></pre> <p>Attachment: Cloud Armor policies attach to backend services</p> <p>Enforcement: At Google\u2019s edge network (blocks before reaching LB)</p>"},{"location":"gcp/security/cloud-armor/#security-policies","title":"Security Policies","text":""},{"location":"gcp/security/cloud-armor/#allowdeny-rules","title":"Allow/Deny Rules","text":"<p>Priority-based evaluation: Lower number = higher priority</p> <p>Actions:</p> <ul> <li>Allow: Permit request</li> <li>Deny (403): Block with HTTP 403</li> <li>Deny (404): Block with HTTP 404 (hide resource)</li> <li>Deny (502): Block with HTTP 502</li> <li>Rate-based ban: Temporarily ban IP</li> </ul> <p>Conditions:</p> <ul> <li>Source IP/CIDR</li> <li>Geographic location (country/region)</li> <li>Request headers</li> <li>Request method</li> <li>Custom expressions (CEL)</li> </ul>"},{"location":"gcp/security/cloud-armor/#example-rules","title":"Example Rules","text":"<p>Geo-blocking:</p> <pre><code>Priority 1000: Deny traffic from specific countries\nPriority 2000: Allow all other traffic\n</code></pre> <p>Rate limiting:</p> <pre><code>Priority 500: Deny if &gt;100 requests/minute from single IP\n</code></pre> <p>OWASP protection:</p> <pre><code>Priority 100: Deny SQLi patterns\nPriority 200: Deny XSS patterns\n</code></pre>"},{"location":"gcp/security/cloud-armor/#preconfigured-waf-rules","title":"Preconfigured WAF Rules","text":"<p>ModSecurity Core Rule Set:</p> <ul> <li>SQL injection (SQLi)</li> <li>Cross-site scripting (XSS)</li> <li>Local file inclusion (LFI)</li> <li>Remote file inclusion (RFI)</li> <li>Remote code execution (RCE)</li> <li>Protocol attacks</li> <li>Session fixation</li> </ul> <p>Sensitivity Levels: 0-4 (0 = most sensitive, more false positives)</p> <p>Tuning: Adjust sensitivity or create exceptions</p>"},{"location":"gcp/security/cloud-armor/#adaptive-protection","title":"Adaptive Protection","text":"<p>Purpose: Automatic detection and mitigation of L7 DDoS</p> <p>How it works:</p> <ol> <li>Machine learning baseline normal traffic</li> <li>Detect anomalies (traffic spikes, patterns)</li> <li>Auto-generate security rules</li> <li>Apply rules to mitigate attack</li> </ol> <p>Use case: Unknown attack patterns, zero-day protection</p> <p>Recommendation: Enable for production applications</p>"},{"location":"gcp/security/cloud-armor/#rate-limiting","title":"Rate Limiting","text":"<p>Types:</p> <ul> <li>Rate-based ban: Ban IP after threshold exceeded</li> <li>Rate limiting: Throttle requests (return 429)</li> </ul> <p>Granularity:</p> <ul> <li>Per client IP</li> <li>Per user (if authenticated)</li> <li>Global across all IPs</li> </ul> <p>Use cases:</p> <ul> <li>API protection (prevent abuse)</li> <li>Login protection (brute force)</li> <li>Resource exhaustion prevention</li> </ul> <p>Example: 100 requests/minute per IP</p>"},{"location":"gcp/security/cloud-armor/#geographic-restrictions","title":"Geographic Restrictions","text":"<p>Allow/Deny by:</p> <ul> <li>Country</li> <li>Region (within country)</li> </ul> <p>Use cases:</p> <ul> <li>Compliance (GDPR, data residency)</li> <li>Reduce attack surface</li> <li>Licensing restrictions</li> </ul> <p>Considerations: VPN/proxy may circumvent</p>"},{"location":"gcp/security/cloud-armor/#named-ip-lists","title":"Named IP Lists","text":"<p>Purpose: Reusable IP allowlists/denylists</p> <p>Use cases:</p> <ul> <li>Corporate office IPs (allowlist)</li> <li>Known malicious IPs (denylist)</li> <li>Partner/vendor IPs</li> <li>Third-party threat intel</li> </ul> <p>Management: Centrally managed, referenced in policies</p>"},{"location":"gcp/security/cloud-armor/#integration-with-cloud-monitoring","title":"Integration with Cloud Monitoring","text":"<p>Metrics:</p> <ul> <li>Requests allowed/denied</li> <li>Requests by country</li> <li>Rate limiting events</li> <li>WAF rule matches</li> <li>Adaptive protection alerts</li> </ul> <p>Logging: Request logs include Cloud Armor decision</p> <p>Alerting: Configure alerts on attack patterns</p>"},{"location":"gcp/security/cloud-armor/#security-layers","title":"Security Layers","text":"<p>Defense in Depth:</p> <pre><code>Layer 1: Cloud Armor (DDoS, WAF, geo-blocking)\nLayer 2: Load Balancer (SSL termination, health checks)\nLayer 3: IAP (authentication) or API keys\nLayer 4: Application (authorization, validation)\n</code></pre>"},{"location":"gcp/security/cloud-armor/#cost-model","title":"Cost Model","text":"<p>Standard tier: $0.75/policy/month + $0.0075/1M requests</p> <p>Advanced DDoS protection: Additional cost for L7 adaptive protection</p> <p>Optimization: Consolidate rules, use one policy per backend</p>"},{"location":"gcp/security/cloud-armor/#supported-services","title":"Supported Services","text":"<p>Works with:</p> <ul> <li>Global HTTP(S) Load Balancer</li> <li>Global SSL Proxy Load Balancer</li> <li>Global TCP Proxy Load Balancer</li> </ul> <p>Does NOT work with:</p> <ul> <li>Regional load balancers</li> <li>Internal load balancers</li> <li>Network load balancers</li> <li>Services without load balancers</li> </ul>"},{"location":"gcp/security/cloud-armor/#architecture-patterns","title":"Architecture Patterns","text":""},{"location":"gcp/security/cloud-armor/#public-web-application","title":"Public Web Application","text":"<pre><code>Users \u2192 Cloud Armor (WAF + DDoS) \u2192 Global LB \u2192 App Engine/Cloud Run\n</code></pre> <p>Protection: SQLi, XSS, DDoS, geo-blocking</p>"},{"location":"gcp/security/cloud-armor/#api-gateway","title":"API Gateway","text":"<pre><code>Clients \u2192 Cloud Armor (rate limiting) \u2192 Global LB \u2192 Cloud Endpoints \u2192 Backend\n</code></pre> <p>Protection: Rate limiting, geographic restrictions</p>"},{"location":"gcp/security/cloud-armor/#multi-region-ha","title":"Multi-Region HA","text":"<pre><code>Global LB with Cloud Armor \u2192 Backend (us-central1)\n                           \u2192 Backend (europe-west1)\n</code></pre> <p>Benefits: DDoS at edge, backend protected</p>"},{"location":"gcp/security/cloud-armor/#cloud-armor-vs-alternatives","title":"Cloud Armor vs Alternatives","text":"Need Solution L7 DDoS + WAF Cloud Armor L\u00be only Standard DDoS (free) Internal traffic VPC firewall rules VM-specific rules VPC firewall Network-level blocking VPC firewall"},{"location":"gcp/security/cloud-armor/#common-patterns","title":"Common Patterns","text":""},{"location":"gcp/security/cloud-armor/#owasp-protection","title":"OWASP Protection","text":"<p>Enable: Preconfigured WAF rules Tune: Adjust sensitivity (start at 2-3) Monitor: Review false positives Exceptions: Create allow rules for known safe patterns</p>"},{"location":"gcp/security/cloud-armor/#rate-limiting_1","title":"Rate Limiting","text":"<p>Configure: Threshold per use case (API: 1000/min, Login: 10/min) Action: Rate-based ban or throttle (429) Monitoring: Track banned IPs</p>"},{"location":"gcp/security/cloud-armor/#geo-fencing","title":"Geo-Fencing","text":"<p>Allow: Specific countries only Deny: High-risk countries Compliance: Data residency requirements</p>"},{"location":"gcp/security/cloud-armor/#limitations","title":"Limitations","text":"<ul> <li>Global load balancers only (not regional)</li> <li>HTTP/HTTPS traffic only</li> <li>Max 200 rules per policy</li> <li>CEL expressions have complexity limits</li> <li>Real-time monitoring, not prevention (some delay)</li> </ul>"},{"location":"gcp/security/cloud-armor/#best-practices","title":"Best Practices","text":""},{"location":"gcp/security/cloud-armor/#layered-security","title":"Layered Security","text":"<ul> <li>Cloud Armor (edge protection)</li> <li>IAP (authentication)</li> <li>Application validation (input sanitization)</li> <li>Database parameterization (SQLi prevention)</li> </ul>"},{"location":"gcp/security/cloud-armor/#monitoring","title":"Monitoring","text":"<ul> <li>Enable request logging</li> <li>Set up alerts for attacks</li> <li>Review denied requests regularly</li> <li>Tune rules based on false positives</li> </ul>"},{"location":"gcp/security/cloud-armor/#testing","title":"Testing","text":"<ul> <li>Test rules in staging first</li> <li>Start with logging mode (no blocking)</li> <li>Monitor before enforcing</li> <li>Have rollback plan</li> </ul>"},{"location":"gcp/security/cloud-armor/#exam-focus","title":"Exam Focus","text":""},{"location":"gcp/security/cloud-armor/#core-concepts_1","title":"Core Concepts","text":"<ul> <li>DDoS protection (L\u00be standard, L7 advanced)</li> <li>WAF rules (OWASP, custom)</li> <li>Edge protection (blocks at Google edge)</li> <li>Requires load balancer</li> </ul>"},{"location":"gcp/security/cloud-armor/#use-cases","title":"Use Cases","text":"<ul> <li>Public application protection</li> <li>OWASP Top 10 defense</li> <li>Geographic restrictions</li> <li>Rate limiting (API, brute force)</li> </ul>"},{"location":"gcp/security/cloud-armor/#architecture_1","title":"Architecture","text":"<ul> <li>Attachment to backend services</li> <li>Works with global LB only</li> <li>Defense in depth layers</li> <li>Multi-region protection</li> </ul>"},{"location":"gcp/security/cloud-armor/#features","title":"Features","text":"<ul> <li>Preconfigured WAF rules</li> <li>Adaptive protection (ML-based)</li> <li>Rate limiting per IP</li> <li>Named IP lists</li> <li>CEL expressions</li> </ul>"},{"location":"gcp/security/cloud-armor/#limitations_1","title":"Limitations","text":"<ul> <li>Global LB only (not regional/internal)</li> <li>HTTP/HTTPS only</li> <li>Max rules per policy</li> <li>Cannot protect services without LB</li> </ul>"},{"location":"gcp/security/cloud-sql-proxy/","title":"Cloud SQL Proxy","text":""},{"location":"gcp/security/cloud-sql-proxy/#core-concepts","title":"Core Concepts","text":"<p>Cloud SQL Proxy provides secure access to Cloud SQL instances without whitelisting IPs or managing SSL certificates. Encrypted connection with automatic authentication.</p> <p>Key Principle: Secure database access via authenticated proxy, not public IPs.</p>"},{"location":"gcp/security/cloud-sql-proxy/#how-it-works","title":"How It Works","text":"<pre><code>Application \u2192 Cloud SQL Proxy (local) \u2192 Encrypted tunnel \u2192 Cloud SQL\n</code></pre> <p>Authentication: Uses service account or user credentials Encryption: Automatic TLS encryption Connection: Via Cloud SQL Admin API, not direct TCP</p>"},{"location":"gcp/security/cloud-sql-proxy/#when-to-use","title":"When to Use","text":""},{"location":"gcp/security/cloud-sql-proxy/#use-when","title":"\u2705 Use When","text":"<ul> <li>Connecting from Compute Engine, GKE, Cloud Run</li> <li>Development from local machine</li> <li>Secure connection without public IP</li> <li>Don\u2019t want to manage SSL certificates</li> <li>Need automatic credential rotation</li> <li>Connecting from App Engine Standard (required)</li> </ul>"},{"location":"gcp/security/cloud-sql-proxy/#dont-use-when","title":"\u274c Don\u2019t Use When","text":"<ul> <li>Cloud SQL Auth Proxy alternative exists (Cloud Run, Cloud Functions native support)</li> <li>Performance critical (slight latency overhead) \u2192 Private IP</li> <li>Serverless VPC Access better fit (Cloud Functions to private Cloud SQL)</li> </ul>"},{"location":"gcp/security/cloud-sql-proxy/#connection-methods","title":"Connection Methods","text":""},{"location":"gcp/security/cloud-sql-proxy/#private-ip-recommended-for-gcp","title":"Private IP (Recommended for GCP)","text":"<p>Architecture:</p> <pre><code>VM/GKE (same VPC) \u2192 Private IP \u2192 Cloud SQL\n</code></pre> <p>Benefits:</p> <ul> <li>No internet egress</li> <li>Lower latency</li> <li>No proxy needed (direct connection)</li> <li>Better performance</li> </ul> <p>Requirements: VPC peering, same region</p>"},{"location":"gcp/security/cloud-sql-proxy/#public-ip-cloud-sql-proxy","title":"Public IP + Cloud SQL Proxy","text":"<p>Architecture:</p> <pre><code>Application \u2192 Cloud SQL Proxy \u2192 Public IP \u2192 Cloud SQL\n</code></pre> <p>Benefits:</p> <ul> <li>Works from anywhere</li> <li>No VPC peering needed</li> <li>Automatic encryption</li> </ul> <p>Use when: External connections, multi-region, development</p>"},{"location":"gcp/security/cloud-sql-proxy/#public-ip-authorized-networks","title":"Public IP + Authorized Networks","text":"<p>Architecture:</p> <pre><code>Application (whitelisted IP) \u2192 Public IP \u2192 Cloud SQL\n</code></pre> <p>Not recommended: Manual IP management, less secure</p>"},{"location":"gcp/security/cloud-sql-proxy/#deployment-patterns","title":"Deployment Patterns","text":""},{"location":"gcp/security/cloud-sql-proxy/#compute-engine-vm","title":"Compute Engine / VM","text":"<p>Sidecar pattern:</p> <pre><code># Proxy runs on VM\ncloud-sql-proxy &lt;INSTANCE_CONNECTION_NAME&gt;\n# App connects to localhost:3306\n</code></pre>"},{"location":"gcp/security/cloud-sql-proxy/#gke-sidecar","title":"GKE Sidecar","text":"<p>Pattern: Proxy container in same pod as application</p> <pre><code>containers:\n\n- name: app\n  image: myapp\n\n- name: cloud-sql-proxy\n  image: gcr.io/cloud-sql-connectors/cloud-sql-proxy\n  command: [\"/cloud-sql-proxy\", \"PROJECT:REGION:INSTANCE\"]\n</code></pre> <p>Benefits: One proxy per pod, scales with app</p>"},{"location":"gcp/security/cloud-sql-proxy/#gke-workload-identity","title":"GKE Workload Identity","text":"<p>Best practice: Use Workload Identity instead of service account keys</p> <pre><code>K8s Service Account \u2192 Google Service Account \u2192 Cloud SQL\n</code></pre>"},{"location":"gcp/security/cloud-sql-proxy/#app-engine-standard","title":"App Engine Standard","text":"<p>Built-in: No proxy needed, automatic connection</p> <p>Configuration: Specify instance in <code>app.yaml</code></p>"},{"location":"gcp/security/cloud-sql-proxy/#cloud-run","title":"Cloud Run","text":"<p>Native support: Use Unix socket connection</p> <p>Configuration: Environment variable with connection name</p>"},{"location":"gcp/security/cloud-sql-proxy/#cloud-functions","title":"Cloud Functions","text":"<p>Serverless VPC Access: Connect via VPC to private IP</p> <p>Or: Cloud SQL Proxy in application code (2<sup>nd</sup> gen only)</p>"},{"location":"gcp/security/cloud-sql-proxy/#authentication","title":"Authentication","text":""},{"location":"gcp/security/cloud-sql-proxy/#service-account-production","title":"Service Account (Production)","text":"<p>Requirements:</p> <ul> <li>Service account with Cloud SQL Client role</li> <li>Credentials available to proxy</li> </ul> <p>Best practice: Workload Identity (GKE), not keys</p>"},{"location":"gcp/security/cloud-sql-proxy/#user-credentials-development","title":"User Credentials (Development)","text":"<p>Method: <code>gcloud auth application-default login</code></p> <p>Use for: Local development only, not production</p>"},{"location":"gcp/security/cloud-sql-proxy/#security-benefits","title":"Security Benefits","text":""},{"location":"gcp/security/cloud-sql-proxy/#no-ip-whitelisting","title":"No IP Whitelisting","text":"<p>Problem with authorized networks: IP management overhead, security risk</p> <p>Solution: Proxy uses IAM authentication, no IP restrictions</p>"},{"location":"gcp/security/cloud-sql-proxy/#automatic-encryption","title":"Automatic Encryption","text":"<p>All connections encrypted: No manual SSL certificate management</p> <p>TLS version: Automatically upgraded</p>"},{"location":"gcp/security/cloud-sql-proxy/#credential-management","title":"Credential Management","text":"<p>Automatic rotation: Service accounts rotate automatically</p> <p>No hardcoded passwords: Uses Google credentials</p>"},{"location":"gcp/security/cloud-sql-proxy/#performance-considerations","title":"Performance Considerations","text":""},{"location":"gcp/security/cloud-sql-proxy/#latency","title":"Latency","text":"<p>Overhead: ~5-10ms additional latency (proxy hop)</p> <p>Mitigation: Use private IP for latency-sensitive workloads</p>"},{"location":"gcp/security/cloud-sql-proxy/#connection-pooling","title":"Connection Pooling","text":"<p>Proxy handles: Multiple connections to database</p> <p>Application: Still needs connection pooling</p> <p>Best practice: Configure app connection pool appropriately</p>"},{"location":"gcp/security/cloud-sql-proxy/#resource-usage","title":"Resource Usage","text":"<p>Lightweight: Minimal CPU/memory overhead</p> <p>Scaling: One proxy can handle many connections</p>"},{"location":"gcp/security/cloud-sql-proxy/#cost","title":"Cost","text":"<p>Proxy: Free (no additional charge)</p> <p>Data transfer:</p> <ul> <li>Private IP: No egress charges (same region)</li> <li>Public IP: Standard egress charges apply</li> </ul> <p>Cloud SQL: Normal instance charges</p>"},{"location":"gcp/security/cloud-sql-proxy/#cloud-sql-proxy-vs-alternatives","title":"Cloud SQL Proxy vs Alternatives","text":"Method Security Performance Complexity Cost Private IP High Best Medium Low Cloud SQL Proxy High Good Low Medium Authorized Networks Medium Good Low Medium Direct (no security) Low Best Lowest Low <p>Recommendation: Private IP for production, Proxy for development/external</p>"},{"location":"gcp/security/cloud-sql-proxy/#common-patterns","title":"Common Patterns","text":""},{"location":"gcp/security/cloud-sql-proxy/#development-setup","title":"Development Setup","text":"<pre><code>Developer laptop \u2192 Cloud SQL Proxy \u2192 Cloud SQL (dev instance)\n</code></pre> <p>Benefit: Secure access without VPN</p>"},{"location":"gcp/security/cloud-sql-proxy/#multi-environment","title":"Multi-Environment","text":"<pre><code>Dev: Cloud SQL Proxy (public IP)\nStaging: Private IP (VPC peering)\nProduction: Private IP (VPC peering)\n</code></pre>"},{"location":"gcp/security/cloud-sql-proxy/#hybrid-cloud","title":"Hybrid Cloud","text":"<pre><code>On-premises app \u2192 Cloud SQL Proxy \u2192 Cloud Interconnect \u2192 Cloud SQL\n</code></pre>"},{"location":"gcp/security/cloud-sql-proxy/#migration","title":"Migration","text":"<pre><code>Legacy app (on-prem) \u2192 Cloud SQL Proxy \u2192 Cloud SQL\n</code></pre> <p>Use case: Gradual migration, hybrid connectivity</p>"},{"location":"gcp/security/cloud-sql-proxy/#troubleshooting","title":"Troubleshooting","text":""},{"location":"gcp/security/cloud-sql-proxy/#connection-failures","title":"Connection Failures","text":"<p>Check:</p> <ul> <li>Service account has Cloud SQL Client role</li> <li>Instance connection name correct</li> <li>Network connectivity (firewall rules)</li> <li>Cloud SQL Admin API enabled</li> </ul>"},{"location":"gcp/security/cloud-sql-proxy/#performance-issues","title":"Performance Issues","text":"<p>Diagnose:</p> <ul> <li>Proxy latency vs network latency</li> <li>Connection pool configuration</li> <li>Database query performance</li> <li>Consider private IP for better performance</li> </ul>"},{"location":"gcp/security/cloud-sql-proxy/#authentication-errors","title":"Authentication Errors","text":"<p>Common causes:</p> <ul> <li>Missing IAM permissions</li> <li>Expired credentials</li> <li>Service account key not found</li> <li>Wrong instance connection name</li> </ul>"},{"location":"gcp/security/cloud-sql-proxy/#best-practices","title":"Best Practices","text":""},{"location":"gcp/security/cloud-sql-proxy/#use-private-ip-when-possible","title":"Use Private IP When Possible","text":"<p>Production: Private IP for best performance/security</p> <p>Proxy: Development, external access, migration</p>"},{"location":"gcp/security/cloud-sql-proxy/#workload-identity-for-gke","title":"Workload Identity for GKE","text":"<p>Avoid: Service account key files in pods</p> <p>Use: Workload Identity for automatic credential management</p>"},{"location":"gcp/security/cloud-sql-proxy/#connection-pooling_1","title":"Connection Pooling","text":"<p>Application-level: Configure connection pool</p> <p>Proxy: Not a connection pool replacement</p>"},{"location":"gcp/security/cloud-sql-proxy/#monitoring","title":"Monitoring","text":"<p>Metrics: Connection count, latency, errors</p> <p>Logging: Enable Cloud SQL logs</p>"},{"location":"gcp/security/cloud-sql-proxy/#limitations","title":"Limitations","text":"<ul> <li>Slight latency overhead (vs direct connection)</li> <li>Requires Cloud SQL Admin API enabled</li> <li>One proxy instance per application (typically)</li> <li>Not for extremely high-performance scenarios (use private IP)</li> </ul>"},{"location":"gcp/security/cloud-sql-proxy/#exam-focus","title":"Exam Focus","text":""},{"location":"gcp/security/cloud-sql-proxy/#core-concepts_1","title":"Core Concepts","text":"<ul> <li>Secure access without IP whitelisting</li> <li>Automatic encryption (TLS)</li> <li>IAM-based authentication</li> <li>Proxy architecture (local proxy to Cloud SQL)</li> </ul>"},{"location":"gcp/security/cloud-sql-proxy/#use-cases","title":"Use Cases","text":"<ul> <li>Secure database access</li> <li>Development from local machine</li> <li>No public IP on Cloud SQL</li> <li>Avoid SSL certificate management</li> <li>App Engine Standard (required)</li> </ul>"},{"location":"gcp/security/cloud-sql-proxy/#architecture","title":"Architecture","text":"<ul> <li>Sidecar pattern (GKE)</li> <li>Private IP vs Proxy decision</li> <li>Workload Identity integration</li> <li>Multi-environment strategy</li> </ul>"},{"location":"gcp/security/cloud-sql-proxy/#security","title":"Security","text":"<ul> <li>No hardcoded credentials</li> <li>Automatic encryption</li> <li>IAM authentication</li> <li>Better than authorized networks</li> </ul>"},{"location":"gcp/security/cloud-sql-proxy/#alternatives","title":"Alternatives","text":"<ul> <li>Private IP (best for production in GCP)</li> <li>Serverless VPC Access (Cloud Functions)</li> <li>Native support (Cloud Run, App Engine)</li> </ul>"},{"location":"gcp/security/identity-aware-proxy/","title":"Identity-Aware Proxy (IAP)","text":""},{"location":"gcp/security/identity-aware-proxy/#core-concepts","title":"Core Concepts","text":"<p>IAP provides zero-trust access control for applications without VPN. Authenticate users via Google identity before allowing access to applications.</p> <p>Key Principle: Context-aware access based on user identity and device, not network location.</p>"},{"location":"gcp/security/identity-aware-proxy/#how-iap-works","title":"How IAP Works","text":"<pre><code>User \u2192 IAP (authenticate + authorize) \u2192 Application\n</code></pre> <p>Authentication: Google Account, Workspace, Cloud Identity Authorization: IAM permissions check Result: Signed headers to application (user identity)</p>"},{"location":"gcp/security/identity-aware-proxy/#when-to-use-iap","title":"When to Use IAP","text":""},{"location":"gcp/security/identity-aware-proxy/#use-when","title":"\u2705 Use When","text":"<ul> <li>Protect internal apps without VPN</li> <li>Google-based authentication needed</li> <li>Context-aware access control</li> <li>Zero-trust security model</li> <li>Centralized access management</li> <li>Need user identity in application</li> </ul>"},{"location":"gcp/security/identity-aware-proxy/#dont-use-when","title":"\u274c Don\u2019t Use When","text":"<ul> <li>Public applications (no auth needed)</li> <li>Non-HTTP protocols \u2192 Cloud VPN</li> <li>Need OAuth/SAML flexibility \u2192 Custom auth</li> <li>API authentication \u2192 API keys, service accounts</li> <li>Machine-to-machine \u2192 Service accounts</li> </ul>"},{"location":"gcp/security/identity-aware-proxy/#supported-resources","title":"Supported Resources","text":"<p>App Engine: Native integration Compute Engine: Via load balancer GKE: Via Ingress with BackendConfig Cloud Run: Not supported (use Cloud Run auth instead) Cloud Functions: Not supported (use function auth)</p>"},{"location":"gcp/security/identity-aware-proxy/#architecture-patterns","title":"Architecture Patterns","text":""},{"location":"gcp/security/identity-aware-proxy/#internal-web-app","title":"Internal Web App","text":"<pre><code>Employees \u2192 IAP \u2192 GKE (internal dashboard)\n</code></pre> <p>Benefits: No VPN, Google identity, simple</p>"},{"location":"gcp/security/identity-aware-proxy/#multi-tier-application","title":"Multi-Tier Application","text":"<pre><code>Users \u2192 IAP \u2192 App Engine (frontend) \u2192 Backend (internal, no IAP)\n</code></pre> <p>Security: Only frontend exposed via IAP</p>"},{"location":"gcp/security/identity-aware-proxy/#admin-access","title":"Admin Access","text":"<pre><code>Admins \u2192 IAP (conditional access) \u2192 VM instances (SSH/RDP)\n</code></pre> <p>Features: TCP forwarding for SSH/RDP without public IPs</p>"},{"location":"gcp/security/identity-aware-proxy/#access-levels-context-aware","title":"Access Levels (Context-Aware)","text":"<p>Conditions:</p> <ul> <li>IP address range</li> <li>Device policy (managed, encrypted)</li> <li>Geographic location</li> <li>Time of day</li> </ul> <p>Use case: Only allow from corporate network + managed devices</p> <p>Example: Access allowed if:</p> <pre><code>IP in corporate range AND device managed AND location = US\n</code></pre>"},{"location":"gcp/security/identity-aware-proxy/#iam-integration","title":"IAM Integration","text":"<p>Roles:</p> <ul> <li><code>roles/iap.httpsResourceAccessor</code>: Access web apps</li> <li><code>roles/iap.tunnelResourceAccessor</code>: SSH/RDP via IAP</li> </ul> <p>Granularity: Project, service, or URL path</p> <p>Best Practice: Use Google Groups for user management</p>"},{"location":"gcp/security/identity-aware-proxy/#tcp-forwarding","title":"TCP Forwarding","text":"<p>Purpose: SSH/RDP to VMs without public IPs or bastion hosts</p> <p>Architecture:</p> <pre><code>User \u2192 IAP tunnel \u2192 Private VM (no external IP)\n</code></pre> <p>Command: <code>gcloud compute ssh</code> or <code>gcloud compute start-iap-tunnel</code></p> <p>Benefits: No bastion host, no firewall rules for SSH, audit logging</p>"},{"location":"gcp/security/identity-aware-proxy/#signed-headers","title":"Signed Headers","text":"<p>Purpose: Application receives authenticated user identity</p> <p>Headers Provided:</p> <ul> <li><code>X-Goog-Authenticated-User-Email</code>: User email</li> <li><code>X-Goog-Authenticated-User-ID</code>: User ID</li> </ul> <p>Use case: Application-level authorization, logging</p> <p>Security: Verify header signature (prevent spoofing)</p>"},{"location":"gcp/security/identity-aware-proxy/#security-considerations","title":"Security Considerations","text":""},{"location":"gcp/security/identity-aware-proxy/#header-verification","title":"Header Verification","text":"<p>Risk: Malicious users could spoof headers</p> <p>Mitigation: Verify JWT signature, only trust IAP-verified headers</p>"},{"location":"gcp/security/identity-aware-proxy/#defense-in-depth","title":"Defense in Depth","text":"<p>Pattern: IAP + Application-level auth</p> <p>Reason: IAP handles authentication, app handles fine-grained authorization</p>"},{"location":"gcp/security/identity-aware-proxy/#disable-direct-access","title":"Disable Direct Access","text":"<p>Problem: Users could bypass IAP via VM external IP</p> <p>Solution:</p> <ul> <li>Remove external IPs (use IAP tunnel)</li> <li>Firewall rules block direct access</li> <li>VPC Service Controls</li> </ul>"},{"location":"gcp/security/identity-aware-proxy/#cost","title":"Cost","text":"<p>Pricing: Free (no additional charge for IAP itself)</p> <p>Related costs: Load balancer, compute resources</p>"},{"location":"gcp/security/identity-aware-proxy/#limitations","title":"Limitations","text":"<ul> <li>HTTP/HTTPS only (TCP forwarding for SSH/RDP)</li> <li>Requires Google identity</li> <li>Limited to GCP and on-prem (not other clouds)</li> <li>Some latency overhead (auth check)</li> </ul>"},{"location":"gcp/security/identity-aware-proxy/#iap-vs-alternatives","title":"IAP vs Alternatives","text":"Need Solution Web app auth IAP VPN access Cloud VPN API auth API keys, OAuth Service-to-service Service accounts Custom auth Cloud Endpoints, custom code"},{"location":"gcp/security/identity-aware-proxy/#exam-focus","title":"Exam Focus","text":""},{"location":"gcp/security/identity-aware-proxy/#core-concepts_1","title":"Core Concepts","text":"<ul> <li>Zero-trust access control</li> <li>Google identity integration</li> <li>No VPN needed</li> <li>Context-aware access</li> </ul>"},{"location":"gcp/security/identity-aware-proxy/#use-cases","title":"Use Cases","text":"<ul> <li>Internal web applications</li> <li>SSH/RDP without bastion</li> <li>Admin access control</li> <li>Replace VPN for web apps</li> </ul>"},{"location":"gcp/security/identity-aware-proxy/#architecture","title":"Architecture","text":"<ul> <li>Supported services (App Engine, GCE, GKE)</li> <li>TCP forwarding for SSH/RDP</li> <li>Signed headers for user identity</li> <li>Access levels (conditional)</li> </ul>"},{"location":"gcp/security/identity-aware-proxy/#security","title":"Security","text":"<ul> <li>Defense in depth (IAP + app auth)</li> <li>Header verification</li> <li>Disable direct access</li> <li>IAM role management</li> </ul>"},{"location":"gcp/security/identity-aware-proxy/#limitations_1","title":"Limitations","text":"<ul> <li>HTTP/HTTPS only</li> <li>Google identity required</li> <li>Not for Cloud Run/Functions</li> <li>No support for non-Google clouds</li> </ul>"},{"location":"gcp/security/organization-policies/","title":"Organization Policies","text":""},{"location":"gcp/security/organization-policies/#core-concepts","title":"Core Concepts","text":"<p>Organization Policies set constraints on resources across the entire organization. Guardrails that prevent misconfigurations and enforce compliance.</p> <p>Key Principle: Centralized governance; restrictions that override individual permissions.</p>"},{"location":"gcp/security/organization-policies/#organization-policies-vs-iam","title":"Organization Policies vs IAM","text":"Feature Organization Policies IAM Purpose What can/cannot be done Who can do what Type Constraints/restrictions Permissions/allowances Example \u201cVMs must be in us-central1\u201d \u201cUser can create VMs\u201d Precedence Overrides IAM Evaluated after org policies <p>Relationship: Org policies restrict, IAM permits within those restrictions</p>"},{"location":"gcp/security/organization-policies/#policy-types","title":"Policy Types","text":""},{"location":"gcp/security/organization-policies/#list-constraints","title":"List Constraints","text":"<p>Allow or deny specific values</p> <p>Examples:</p> <ul> <li>Allowed VM instance types: <code>constraints/compute.vmExternalIpAccess</code></li> <li>Allowed resource locations: <code>constraints/gcp.resourceLocations</code></li> <li>Allowed service accounts: <code>constraints/iam.allowedPolicyMemberDomains</code></li> </ul> <p>Modes:</p> <ul> <li>Allow list: Only specified values permitted</li> <li>Deny list: Specified values prohibited</li> <li>Allow all: No restrictions</li> <li>Deny all: Completely prohibited</li> </ul>"},{"location":"gcp/security/organization-policies/#boolean-constraints","title":"Boolean Constraints","text":"<p>Enable or disable features</p> <p>Examples:</p> <ul> <li><code>constraints/compute.disableSerialPortAccess</code>: Disable serial port</li> <li><code>constraints/sql.restrictPublicIp</code>: Prevent Cloud SQL public IPs</li> <li><code>constraints/iam.disableServiceAccountKeyCreation</code>: No service account keys</li> </ul> <p>Values: True (enforced) or False (not enforced)</p>"},{"location":"gcp/security/organization-policies/#common-organization-policies","title":"Common Organization Policies","text":""},{"location":"gcp/security/organization-policies/#resource-locations","title":"Resource Locations","text":"<p>Constraint: <code>constraints/gcp.resourceLocations</code></p> <p>Purpose: Restrict where resources can be created</p> <p>Use case: GDPR compliance (EU only), data residency</p> <p>Example: Only allow <code>in:us-locations</code></p>"},{"location":"gcp/security/organization-policies/#disable-vm-external-ips","title":"Disable VM External IPs","text":"<p>Constraint: <code>constraints/compute.vmExternalIpAccess</code></p> <p>Purpose: Force VMs to use Cloud NAT or private IPs</p> <p>Use case: Security (no public VMs), centralized egress</p> <p>Example: Deny all external IPs</p>"},{"location":"gcp/security/organization-policies/#restrict-service-account-key-creation","title":"Restrict Service Account Key Creation","text":"<p>Constraint: <code>constraints/iam.disableServiceAccountKeyCreation</code></p> <p>Purpose: Prevent user-managed service account keys</p> <p>Use case: Security (keys can be stolen), force Workload Identity</p> <p>Example: Enforce true</p>"},{"location":"gcp/security/organization-policies/#allowed-policy-member-domains","title":"Allowed Policy Member Domains","text":"<p>Constraint: <code>constraints/iam.allowedPolicyMemberDomains</code></p> <p>Purpose: Only allow IAM members from specific domains</p> <p>Use case: Prevent external sharing</p> <p>Example: Only allow <code>@company.com</code> and <code>@company.gserviceaccount.com</code></p>"},{"location":"gcp/security/organization-policies/#require-os-login","title":"Require OS Login","text":"<p>Constraint: <code>constraints/compute.requireOsLogin</code></p> <p>Purpose: Enforce OS Login for SSH (IAM-based)</p> <p>Use case: Centralized access control, no SSH keys</p>"},{"location":"gcp/security/organization-policies/#shielded-vms","title":"Shielded VMs","text":"<p>Constraint: <code>constraints/compute.requireShieldedVm</code></p> <p>Purpose: Require Shielded VMs (Secure Boot, vTPM)</p> <p>Use case: Security baseline, compliance</p>"},{"location":"gcp/security/organization-policies/#uniform-bucket-level-access","title":"Uniform Bucket-Level Access","text":"<p>Constraint: <code>constraints/storage.uniformBucketLevelAccess</code></p> <p>Purpose: Require uniform bucket-level access (IAM only, no ACLs)</p> <p>Use case: Simplified permissions, better security</p>"},{"location":"gcp/security/organization-policies/#disable-service-account-key-upload","title":"Disable Service Account Key Upload","text":"<p>Constraint: <code>constraints/iam.disableServiceAccountKeyUpload</code></p> <p>Purpose: Prevent external service account key upload</p> <p>Use case: Security (prevent stolen keys from other orgs)</p>"},{"location":"gcp/security/organization-policies/#policy-hierarchy","title":"Policy Hierarchy","text":"<pre><code>Organization (broadest)\n\u251c\u2500\u2500 Folder\n\u2502   \u2514\u2500\u2500 Project (narrowest)\n</code></pre> <p>Inheritance: Child inherits parent policies</p> <p>Merging:</p> <ul> <li>List constraints: Union of allowed values (most restrictive wins)</li> <li>Boolean constraints: Cannot override parent (parent true = child must be true)</li> </ul> <p>Example:</p> <pre><code>Org: Allow locations = [us-*, eu-*]\nFolder: Allow locations = [us-central1, us-east1]\nResult: Only us-central1, us-east1 (intersection)\n</code></pre>"},{"location":"gcp/security/organization-policies/#policy-evaluation","title":"Policy Evaluation","text":"<p>Order:</p> <ol> <li>Check organization policy constraints</li> <li>If allowed by org policy, check IAM permissions</li> <li>If both pass, action permitted</li> </ol> <p>Example:</p> <pre><code>User has IAM permission to create VM\nOrg policy restricts locations to us-central1\nUser tries to create VM in europe-west1\nResult: DENIED (org policy blocks)\n</code></pre>"},{"location":"gcp/security/organization-policies/#when-to-use","title":"When to Use","text":""},{"location":"gcp/security/organization-policies/#use-organization-policies-when","title":"\u2705 Use Organization Policies When","text":"<ul> <li>Need to enforce governance across org</li> <li>Compliance requirements (data residency, security)</li> <li>Prevent costly mistakes (expensive resources)</li> <li>Security baselines (Shielded VMs, no public IPs)</li> <li>Consistent standards across projects</li> <li>Prevent shadow IT</li> </ul>"},{"location":"gcp/security/organization-policies/#dont-use-when","title":"\u274c Don\u2019t Use When","text":"<ul> <li>Need per-resource control \u2192 IAM</li> <li>Need to grant permissions \u2192 IAM</li> <li>Temporary exceptions common \u2192 Difficult with org policies</li> </ul>"},{"location":"gcp/security/organization-policies/#exceptions","title":"Exceptions","text":""},{"location":"gcp/security/organization-policies/#tags","title":"Tags","text":"<p>Purpose: Allow exceptions via tags</p> <p>Pattern:</p> <ol> <li>Define org policy with tag condition</li> <li>Tag resources that need exception</li> <li>Policy doesn\u2019t apply to tagged resources</li> </ol> <p>Use case: \u201cNo external IPs\u201d except for bastion hosts (tagged)</p>"},{"location":"gcp/security/organization-policies/#policy-override","title":"Policy Override","text":"<p>Carefully consider: Exceptions reduce effectiveness</p> <p>Better: Rethink policy or architecture</p>"},{"location":"gcp/security/organization-policies/#implementation-strategy","title":"Implementation Strategy","text":""},{"location":"gcp/security/organization-policies/#gradual-rollout","title":"Gradual Rollout","text":"<p>Phase 1: Audit mode (log violations, don\u2019t block) Phase 2: Enforce on new resources Phase 3: Remediate existing violations Phase 4: Full enforcement</p>"},{"location":"gcp/security/organization-policies/#testing","title":"Testing","text":"<p>Dry run: Test policy on test folder/project first</p> <p>Monitor: Check for blocked legitimate actions</p>"},{"location":"gcp/security/organization-policies/#communication","title":"Communication","text":"<p>Announce: Warn teams before enforcement</p> <p>Document: Explain rationale and exceptions</p>"},{"location":"gcp/security/organization-policies/#common-patterns","title":"Common Patterns","text":""},{"location":"gcp/security/organization-policies/#data-residency-gdpr","title":"Data Residency (GDPR)","text":"<pre><code>Org policy: Only EU locations\nResult: All resources must be in EU\nCompliance: GDPR data residency\n</code></pre>"},{"location":"gcp/security/organization-policies/#security-baseline","title":"Security Baseline","text":"<pre><code>Policies:\n\n- Require Shielded VMs\n- Disable service account keys\n- Require OS Login\n- No public IPs on VMs\nResult: Enforced security minimum\n</code></pre>"},{"location":"gcp/security/organization-policies/#cost-control","title":"Cost Control","text":"<pre><code>Policies:\n\n- Allowed VM types (no expensive ones)\n- Allowed GKE node types\n- Disable APIs (prevent usage)\nResult: Control cloud spend\n</code></pre>"},{"location":"gcp/security/organization-policies/#shadow-it-prevention","title":"Shadow IT Prevention","text":"<pre><code>Policy: Only allow @company.com domain members\nResult: Can't share with external users\n</code></pre>"},{"location":"gcp/security/organization-policies/#monitoring-and-compliance","title":"Monitoring and Compliance","text":""},{"location":"gcp/security/organization-policies/#policy-analyzer","title":"Policy Analyzer","text":"<p>Purpose: See which policies apply to resources</p> <p>Use case: Troubleshooting, compliance verification</p>"},{"location":"gcp/security/organization-policies/#compliance-reports","title":"Compliance Reports","text":"<p>Integration: Security Command Center</p> <p>Shows: Violations, compliance status</p>"},{"location":"gcp/security/organization-policies/#audit-logs","title":"Audit Logs","text":"<p>Logs: Policy changes, constraint evaluations</p> <p>Use for: Security audits, compliance evidence</p>"},{"location":"gcp/security/organization-policies/#best-practices","title":"Best Practices","text":""},{"location":"gcp/security/organization-policies/#start-broad-get-specific","title":"Start Broad, Get Specific","text":"<p>Phase 1: Organization-level policies (baseline) Phase 2: Folder-level policies (teams/divisions) Phase 3: Project-level only if necessary</p>"},{"location":"gcp/security/organization-policies/#document-rationale","title":"Document Rationale","text":"<p>Each policy: Why it exists, what it prevents</p> <p>Exceptions: Document and regularly review</p>"},{"location":"gcp/security/organization-policies/#regular-reviews","title":"Regular Reviews","text":"<p>Quarterly: Review policies, remove obsolete</p> <p>Annual: Comprehensive security review</p>"},{"location":"gcp/security/organization-policies/#least-restrictive","title":"Least Restrictive","text":"<p>Balance: Security vs developer productivity</p> <p>Avoid: Overly restrictive policies that block legitimate work</p>"},{"location":"gcp/security/organization-policies/#troubleshooting","title":"Troubleshooting","text":""},{"location":"gcp/security/organization-policies/#permission-denied-despite-iam","title":"\u201cPermission Denied\u201d Despite IAM","text":"<p>Check: Organization policies may be blocking</p> <p>Tool: Policy Analyzer to see constraints</p>"},{"location":"gcp/security/organization-policies/#policy-not-applying","title":"Policy Not Applying","text":"<p>Check: Inheritance, policy propagation delay</p> <p>Verify: Resource hierarchy</p>"},{"location":"gcp/security/organization-policies/#conflicts","title":"Conflicts","text":"<p>Understand: Most restrictive wins</p> <p>Resolution: Review parent policies</p>"},{"location":"gcp/security/organization-policies/#limitations","title":"Limitations","text":"<ul> <li>Max 20 boolean constraints per resource</li> <li>Max 20 list constraints per resource</li> <li>Propagation delay (up to 15 minutes)</li> <li>Cannot disable for single resource (use tags)</li> <li>Some services don\u2019t support all constraints</li> </ul>"},{"location":"gcp/security/organization-policies/#organization-policies-vs-vpc-service-controls","title":"Organization Policies vs VPC Service Controls","text":"Feature Organization Policies VPC Service Controls Scope Resource constraints API perimeter security Purpose Prevent misconfig Prevent data exfil Example \u201cNo public IPs\u201d \u201cData can\u2019t leave VPC\u201d Granularity Resource types API calls <p>Use both: Complementary security layers</p>"},{"location":"gcp/security/organization-policies/#exam-focus","title":"Exam Focus","text":""},{"location":"gcp/security/organization-policies/#core-concepts_1","title":"Core Concepts","text":"<ul> <li>Constraints vs permissions</li> <li>List vs boolean constraints</li> <li>Policy hierarchy and inheritance</li> <li>Org policies override IAM</li> </ul>"},{"location":"gcp/security/organization-policies/#common-policies","title":"Common Policies","text":"<ul> <li>Resource location restrictions</li> <li>VM external IP disable</li> <li>Service account key restrictions</li> <li>Domain restrictions</li> <li>Shielded VM requirements</li> </ul>"},{"location":"gcp/security/organization-policies/#use-cases","title":"Use Cases","text":"<ul> <li>Data residency (GDPR)</li> <li>Security baselines</li> <li>Cost control</li> <li>Shadow IT prevention</li> <li>Compliance enforcement</li> </ul>"},{"location":"gcp/security/organization-policies/#architecture","title":"Architecture","text":"<ul> <li>Hierarchy (org \u2192 folder \u2192 project)</li> <li>Policy inheritance</li> <li>Exception handling (tags)</li> <li>Gradual rollout strategy</li> </ul>"},{"location":"gcp/security/organization-policies/#integration","title":"Integration","text":"<ul> <li>IAM relationship (restrictive, not permissive)</li> <li>VPC Service Controls (complementary)</li> <li>Security Command Center</li> <li>Policy Analyzer for troubleshooting</li> </ul>"},{"location":"gcp/security/organization-policies/#best-practices_1","title":"Best Practices","text":"<ul> <li>Start at org level</li> <li>Document rationale</li> <li>Test before full enforcement</li> <li>Regular reviews</li> <li>Balance security and productivity</li> </ul>"},{"location":"gcp/security/secret-manager/","title":"Secret Manager","text":""},{"location":"gcp/security/secret-manager/#core-concepts","title":"Core Concepts","text":"<p>Secret Manager stores and manages sensitive data like API keys, passwords, and certificates. Centralized secret storage with access control, versioning, and audit logging.</p> <p>Key Principle: Never hardcode secrets in code or config files; store centrally with access control.</p>"},{"location":"gcp/security/secret-manager/#when-to-use-secret-manager","title":"When to Use Secret Manager","text":""},{"location":"gcp/security/secret-manager/#use-when","title":"\u2705 Use When","text":"<ul> <li>Need to store API keys, passwords, tokens</li> <li>Application needs secrets at runtime</li> <li>Want centralized secret management</li> <li>Need secret versioning and rotation</li> <li>Audit logging required</li> <li>Multiple services share secrets</li> <li>CI/CD pipelines need secrets</li> </ul>"},{"location":"gcp/security/secret-manager/#dont-use-when","title":"\u274c Don\u2019t Use When","text":"<ul> <li>Public configuration values \u2192 Environment variables</li> <li>Large files (&gt;64 KB) \u2192 Cloud Storage</li> <li>Database passwords with Cloud SQL \u2192 Cloud SQL auth</li> <li>Service account authentication \u2192 Workload Identity</li> </ul>"},{"location":"gcp/security/secret-manager/#secret-manager-vs-alternatives","title":"Secret Manager vs Alternatives","text":"Need Solution API keys, passwords Secret Manager Service account auth Workload Identity (no keys) Large files/certs Cloud Storage with CMEK Config values (non-secret) Environment variables Database passwords Secret Manager or Cloud SQL auth"},{"location":"gcp/security/secret-manager/#architecture","title":"Architecture","text":""},{"location":"gcp/security/secret-manager/#secret-structure","title":"Secret Structure","text":"<pre><code>Secret (API key name)\n\u251c\u2500\u2500 Version 1 (enabled)\n\u251c\u2500\u2500 Version 2 (enabled) \u2190 Latest\n\u2514\u2500\u2500 Version 3 (disabled)\n</code></pre> <p>Secret: Container (metadata, IAM policies) Version: Actual secret value (immutable)</p>"},{"location":"gcp/security/secret-manager/#versioning","title":"Versioning","text":"<p>Immutable versions: Cannot modify, only create new</p> <p>States:</p> <ul> <li>Enabled: Can be accessed</li> <li>Disabled: Cannot be accessed (soft delete)</li> <li>Destroyed: Permanently deleted (after disabled)</li> </ul> <p>Use case: Rotate secrets without downtime</p>"},{"location":"gcp/security/secret-manager/#access-control","title":"Access Control","text":""},{"location":"gcp/security/secret-manager/#iam-roles","title":"IAM Roles","text":"<p>Secret Manager Admin (<code>roles/secretmanager.admin</code>):</p> <ul> <li>Full control (create, delete, manage IAM)</li> </ul> <p>Secret Manager Secret Accessor (<code>roles/secretmanager.secretAccessor</code>):</p> <ul> <li>Read secret values (most common for apps)</li> </ul> <p>Secret Manager Secret Version Manager (<code>roles/secretmanager.secretVersionManager</code>):</p> <ul> <li>Create/enable/disable versions</li> </ul> <p>Secret Manager Viewer (<code>roles/secretmanager.viewer</code>):</p> <ul> <li>View metadata only (not values)</li> </ul>"},{"location":"gcp/security/secret-manager/#best-practices","title":"Best Practices","text":"<ul> <li>One secret per service/app: Granular access control</li> <li>Grant accessor role: Only to services that need it</li> <li>Use service accounts: For applications</li> <li>Separate secrets: Dev, staging, prod</li> </ul>"},{"location":"gcp/security/secret-manager/#integration-patterns","title":"Integration Patterns","text":""},{"location":"gcp/security/secret-manager/#cloud-run-cloud-functions","title":"Cloud Run / Cloud Functions","text":"<p>Environment variable:</p> <pre><code>Declare secret in config \u2192 Mounted as env var or file\n</code></pre> <p>No code changes: Secret available as environment variable</p> <p>Auto-refresh: Updates when secret rotated (Cloud Run 2<sup>nd</sup> gen)</p>"},{"location":"gcp/security/secret-manager/#gke","title":"GKE","text":"<p>Kubernetes Secret:</p> <pre><code>External Secrets Operator \u2192 Secret Manager \u2192 K8s Secret\n</code></pre> <p>Or CSI Driver: Mount secrets as volumes</p>"},{"location":"gcp/security/secret-manager/#compute-engine","title":"Compute Engine","text":"<p>Access via API:</p> <pre><code>from google.cloud import secretmanager\nclient = secretmanager.SecretManagerServiceClient()\nname = \"projects/PROJECT/secrets/SECRET/versions/latest\"\nresponse = client.access_secret_version(request={\"name\": name})\nsecret = response.payload.data.decode(\"UTF-8\")\n</code></pre> <p>Service account: VM needs accessor role</p>"},{"location":"gcp/security/secret-manager/#cicd","title":"CI/CD","text":"<p>Cloud Build:</p> <pre><code>availableSecrets:\n  secretManager:\n\n  - versionName: projects/PROJECT/secrets/SECRET/versions/latest\n    env: 'API_KEY'\n</code></pre> <p>Use for: Deploy keys, API tokens</p>"},{"location":"gcp/security/secret-manager/#secret-rotation","title":"Secret Rotation","text":""},{"location":"gcp/security/secret-manager/#manual-rotation","title":"Manual Rotation","text":"<p>Process:</p> <ol> <li>Create new secret version</li> <li>Update applications to use latest</li> <li>Disable old version</li> <li>After grace period, destroy old version</li> </ol>"},{"location":"gcp/security/secret-manager/#automatic-rotation","title":"Automatic Rotation","text":"<p>Pattern: Cloud Function/Cloud Scheduler triggers rotation</p> <p>Steps:</p> <ol> <li>Scheduler triggers function</li> <li>Function generates new secret</li> <li>Function creates new version</li> <li>Function updates dependent services</li> <li>Function disables old version</li> </ol> <p>Use for: Regularly rotated passwords, API keys</p>"},{"location":"gcp/security/secret-manager/#rotation-strategy","title":"Rotation Strategy","text":"<p>API Keys: 90-day rotation Passwords: 60-day rotation Certificates: Before expiration Service accounts: Prefer Workload Identity (no keys to rotate)</p>"},{"location":"gcp/security/secret-manager/#replication","title":"Replication","text":""},{"location":"gcp/security/secret-manager/#automatic-replication","title":"Automatic Replication","text":"<p>Default: Secret replicated across multiple regions within geography</p> <p>Benefit: High availability, disaster recovery</p>"},{"location":"gcp/security/secret-manager/#user-managed-replication","title":"User-Managed Replication","text":"<p>Specify regions: Control exact locations</p> <p>Use case: Data residency requirements, compliance</p> <p>Example: Only replicate in EU regions (GDPR)</p>"},{"location":"gcp/security/secret-manager/#encryption","title":"Encryption","text":""},{"location":"gcp/security/secret-manager/#google-managed-keys-default","title":"Google-Managed Keys (Default)","text":"<p>Automatic: Secrets encrypted at rest</p> <p>No config: Works out of the box</p>"},{"location":"gcp/security/secret-manager/#customer-managed-encryption-keys-cmek","title":"Customer-Managed Encryption Keys (CMEK)","text":"<p>Cloud KMS integration: Your keys, your control</p> <p>Use case: Compliance requires customer-managed keys</p> <p>Benefit: Can revoke access by disabling key</p>"},{"location":"gcp/security/secret-manager/#monitoring-and-audit","title":"Monitoring and Audit","text":""},{"location":"gcp/security/secret-manager/#audit-logs","title":"Audit Logs","text":"<p>Logged actions:</p> <ul> <li>Secret access (who accessed what when)</li> <li>Secret creation/deletion</li> <li>Version changes</li> <li>IAM policy changes</li> </ul> <p>Use for: Compliance, security investigations</p>"},{"location":"gcp/security/secret-manager/#monitoring","title":"Monitoring","text":"<p>Metrics:</p> <ul> <li>Access count</li> <li>Access latency</li> <li>Version count</li> </ul> <p>Alerts: Unusual access patterns, access failures</p>"},{"location":"gcp/security/secret-manager/#cost","title":"Cost","text":"<p>Pricing:</p> <ul> <li>Active secret versions: $0.06/version/month</li> <li>Access operations: $0.03 per 10,000 accesses</li> <li>Replication: Included (no extra charge)</li> </ul> <p>Optimization:</p> <ul> <li>Delete unused versions</li> <li>Consolidate secrets (don\u2019t over-segment)</li> <li>Rotate only when necessary</li> </ul>"},{"location":"gcp/security/secret-manager/#common-patterns","title":"Common Patterns","text":""},{"location":"gcp/security/secret-manager/#database-password","title":"Database Password","text":"<p>Store: Database password in Secret Manager Access: Application reads on startup Rotate: Quarterly, update via function</p>"},{"location":"gcp/security/secret-manager/#api-keys","title":"API Keys","text":"<p>Store: Third-party API keys Access: Applications read latest version Benefit: Centralized management, easy rotation</p>"},{"location":"gcp/security/secret-manager/#tls-certificates","title":"TLS Certificates","text":"<p>Store: Private keys, certificates (if &lt;64 KB) Access: Load balancers, applications Rotate: Before expiration</p>"},{"location":"gcp/security/secret-manager/#multi-environment","title":"Multi-Environment","text":"<p>Pattern: Separate secrets for each environment</p> <pre><code>db-password-dev\ndb-password-staging\ndb-password-prod\n</code></pre> <p>Benefit: Clear separation, different access controls</p>"},{"location":"gcp/security/secret-manager/#security-best-practices","title":"Security Best Practices","text":""},{"location":"gcp/security/secret-manager/#never-hardcode-secrets","title":"Never Hardcode Secrets","text":"<p>Bad: <code>API_KEY = \"abc123\"</code> in code</p> <p>Good: <code>API_KEY = secret_manager.get(\"api-key\")</code></p>"},{"location":"gcp/security/secret-manager/#minimal-access","title":"Minimal Access","text":"<p>Principle: Grant accessor role only to services that need it</p> <p>Avoid: Overly broad permissions</p>"},{"location":"gcp/security/secret-manager/#separate-environments","title":"Separate Environments","text":"<p>Pattern: Different secrets for dev/staging/prod</p> <p>Benefit: Compromise in dev doesn\u2019t affect prod</p>"},{"location":"gcp/security/secret-manager/#regular-rotation","title":"Regular Rotation","text":"<p>Schedule: Rotate secrets regularly (90 days typical)</p> <p>Automation: Use Cloud Functions for automatic rotation</p>"},{"location":"gcp/security/secret-manager/#audit-regularly","title":"Audit Regularly","text":"<p>Review: Access logs monthly</p> <p>Alert: Unusual access patterns</p>"},{"location":"gcp/security/secret-manager/#use-latest-version","title":"Use Latest Version","text":"<p>Pattern: Applications reference \u201clatest\u201d version</p> <p>Benefit: Automatic pickup of rotated secrets</p>"},{"location":"gcp/security/secret-manager/#secret-manager-vs-environment-variables","title":"Secret Manager vs Environment Variables","text":"<p>Environment Variables:</p> <ul> <li>\u2705 Fast access</li> <li>\u2705 Simple</li> <li>\u274c No central management</li> <li>\u274c No versioning</li> <li>\u274c No audit logs</li> <li>\u274c Visible in config</li> </ul> <p>Secret Manager:</p> <ul> <li>\u2705 Centralized</li> <li>\u2705 Versioning</li> <li>\u2705 Audit logs</li> <li>\u2705 Access control</li> <li>\u274c Slight latency</li> <li>\u274c More complex</li> </ul> <p>Decision: Secret Manager for sensitive data, env vars for non-secrets</p>"},{"location":"gcp/security/secret-manager/#integration-with-other-services","title":"Integration with Other Services","text":""},{"location":"gcp/security/secret-manager/#cloud-build","title":"Cloud Build","text":"<p>Mount secrets: Available as environment variables in build steps</p>"},{"location":"gcp/security/secret-manager/#cloud-functions","title":"Cloud Functions","text":"<p>Automatic: Declare in config, available as env var</p>"},{"location":"gcp/security/secret-manager/#cloud-run","title":"Cloud Run","text":"<p>Automatic: Mount as env var or file volume</p>"},{"location":"gcp/security/secret-manager/#gke_1","title":"GKE","text":"<p>External Secrets Operator: Sync to K8s Secrets</p>"},{"location":"gcp/security/secret-manager/#app-engine","title":"App Engine","text":"<p>Access via client library: Read in application code</p>"},{"location":"gcp/security/secret-manager/#limitations","title":"Limitations","text":"<ul> <li>Max 64 KB per secret version</li> <li>Max 100 versions per secret</li> <li>Secret names: 255 characters max</li> <li>No automatic rotation (must implement)</li> <li>Cannot modify version (create new)</li> </ul>"},{"location":"gcp/security/secret-manager/#troubleshooting","title":"Troubleshooting","text":""},{"location":"gcp/security/secret-manager/#access-denied","title":"Access Denied","text":"<p>Check:</p> <ul> <li>Service account has accessor role</li> <li>Secret exists and is enabled</li> <li>Secret Manager API enabled</li> <li>Correct project/secret name</li> </ul>"},{"location":"gcp/security/secret-manager/#secret-not-updated","title":"Secret Not Updated","text":"<p>Issue: Application using cached value</p> <p>Solution: Restart application, use latest version alias</p>"},{"location":"gcp/security/secret-manager/#high-costs","title":"High Costs","text":"<p>Cause: Too many versions</p> <p>Solution: Delete old disabled versions</p>"},{"location":"gcp/security/secret-manager/#exam-focus","title":"Exam Focus","text":""},{"location":"gcp/security/secret-manager/#core-concepts_1","title":"Core Concepts","text":"<ul> <li>Centralized secret storage</li> <li>Versioning (immutable versions)</li> <li>IAM-based access control</li> <li>Audit logging</li> </ul>"},{"location":"gcp/security/secret-manager/#use-cases","title":"Use Cases","text":"<ul> <li>API keys, passwords, tokens</li> <li>Database credentials</li> <li>TLS certificates (&lt;64 KB)</li> <li>CI/CD secrets</li> <li>Multi-environment secrets</li> </ul>"},{"location":"gcp/security/secret-manager/#architecture_1","title":"Architecture","text":"<ul> <li>Secret versioning strategy</li> <li>Rotation patterns</li> <li>Integration with Cloud Run/Functions/GKE</li> <li>Replication (automatic vs user-managed)</li> </ul>"},{"location":"gcp/security/secret-manager/#security","title":"Security","text":"<ul> <li>Never hardcode secrets</li> <li>Minimal access (accessor role)</li> <li>Separate environments</li> <li>Regular rotation</li> <li>Audit logging</li> </ul>"},{"location":"gcp/security/secret-manager/#integration","title":"Integration","text":"<ul> <li>Cloud Run/Functions (env vars)</li> <li>GKE (External Secrets Operator)</li> <li>Cloud Build (secret mounting)</li> <li>Compute Engine (client library)</li> </ul>"},{"location":"gcp/security/secret-manager/#best-practices_1","title":"Best Practices","text":"<ul> <li>One secret per service</li> <li>Use \u201clatest\u201d version alias</li> <li>Regular rotation (automated)</li> <li>Delete old versions</li> <li>Monitor access patterns</li> </ul>"},{"location":"gcp/security/web-security-scanner/","title":"Web Security Scanner","text":""},{"location":"gcp/security/web-security-scanner/#core-concepts","title":"Core Concepts","text":"<p>Web Security Scanner automatically scans web applications for common vulnerabilities. Identifies security issues before attackers exploit them.</p> <p>Key Principle: Automated vulnerability scanning for web apps; complements manual security reviews.</p>"},{"location":"gcp/security/web-security-scanner/#what-it-scans-for","title":"What It Scans For","text":"<p>OWASP Top 10:</p> <ul> <li>Cross-site scripting (XSS)</li> <li>Flash injection</li> <li>Mixed content (HTTP in HTTPS)</li> <li>Outdated/insecure libraries</li> <li>Clear text password submission</li> </ul> <p>Other Vulnerabilities:</p> <ul> <li>Invalid content types</li> <li>Invalid headers</li> <li>XSS via AngularJS templates</li> <li>Insecure JavaScript libraries</li> </ul> <p>Does NOT scan for:</p> <ul> <li>SQL injection (complex, risk of data corruption)</li> <li>Authentication/authorization flaws</li> <li>Business logic vulnerabilities</li> <li>Zero-day vulnerabilities</li> <li>Server misconfigurations (use Cloud Asset Inventory)</li> </ul>"},{"location":"gcp/security/web-security-scanner/#when-to-use","title":"When to Use","text":""},{"location":"gcp/security/web-security-scanner/#use-when","title":"\u2705 Use When","text":"<ul> <li>Need automated security testing</li> <li>Continuous vulnerability scanning</li> <li>Pre-production testing</li> <li>Compliance requirements (demonstrate scanning)</li> <li>Part of CI/CD pipeline</li> <li>Public-facing web applications</li> </ul>"},{"location":"gcp/security/web-security-scanner/#dont-use-when","title":"\u274c Don\u2019t Use When","text":"<ul> <li>Production databases (no SQLi scanning)</li> <li>APIs without web UI \u2192 Use manual testing</li> <li>Need comprehensive penetration testing \u2192 Hire experts</li> <li>Real-time protection needed \u2192 Cloud Armor</li> </ul>"},{"location":"gcp/security/web-security-scanner/#scan-types","title":"Scan Types","text":""},{"location":"gcp/security/web-security-scanner/#managed-scans-recommended","title":"Managed Scans (Recommended)","text":"<p>Automatic:</p> <ul> <li>App Engine, Cloud Run, GKE with Ingress</li> <li>Automatic discovery</li> <li>Scheduled scans</li> </ul> <p>Benefits: No configuration, continuous monitoring</p>"},{"location":"gcp/security/web-security-scanner/#custom-scans","title":"Custom Scans","text":"<p>Manual setup:</p> <ul> <li>Specify target URL</li> <li>Configure authentication</li> <li>Set scan schedule</li> </ul> <p>Use for: Compute Engine, external hosting, custom setups</p>"},{"location":"gcp/security/web-security-scanner/#authentication","title":"Authentication","text":"<p>Supported:</p> <ul> <li>Google Account: For protected App Engine apps</li> <li>Custom login: Record login sequence</li> </ul> <p>Process:</p> <ol> <li>Configure authentication in scanner</li> <li>Scanner logs in before scanning</li> <li>Scans authenticated pages</li> </ol> <p>Limitation: Basic auth flows only (no complex 2FA/MFA)</p>"},{"location":"gcp/security/web-security-scanner/#scan-frequency","title":"Scan Frequency","text":"<p>Options:</p> <ul> <li>On-demand (manual trigger)</li> <li>Daily</li> <li>Weekly</li> <li>Monthly</li> </ul> <p>Recommendation: Weekly for production, daily for high-risk apps</p> <p>Considerations: Scanner generates traffic (test on staging first)</p>"},{"location":"gcp/security/web-security-scanner/#results-and-reporting","title":"Results and Reporting","text":""},{"location":"gcp/security/web-security-scanner/#finding-severity","title":"Finding Severity","text":"<ul> <li>High: Critical vulnerabilities, immediate action</li> <li>Medium: Important issues, plan remediation</li> <li>Low: Minor issues, good practice</li> </ul> <p>False Positives: Possible, verify findings</p>"},{"location":"gcp/security/web-security-scanner/#integration","title":"Integration","text":"<p>Security Command Center: Centralized findings Cloud Monitoring: Alerts on new vulnerabilities Cloud Logging: Scan execution logs</p>"},{"location":"gcp/security/web-security-scanner/#architecture-patterns","title":"Architecture Patterns","text":""},{"location":"gcp/security/web-security-scanner/#cicd-integration","title":"CI/CD Integration","text":"<pre><code>Code commit \u2192 Build \u2192 Deploy to staging \u2192 Web Security Scanner \u2192 \nIf pass \u2192 Deploy to production\nIf fail \u2192 Alert developers\n</code></pre>"},{"location":"gcp/security/web-security-scanner/#multi-environment-scanning","title":"Multi-Environment Scanning","text":"<pre><code>Dev environment: Weekly scans\nStaging: Daily scans (pre-production validation)\nProduction: Monthly scans (compliance)\n</code></pre>"},{"location":"gcp/security/web-security-scanner/#defense-in-depth","title":"Defense in Depth","text":"<pre><code>Layer 1: Secure coding practices\nLayer 2: Web Security Scanner (find issues)\nLayer 3: Cloud Armor (runtime protection)\nLayer 4: Security reviews (manual)\n</code></pre>"},{"location":"gcp/security/web-security-scanner/#limitations","title":"Limitations","text":""},{"location":"gcp/security/web-security-scanner/#coverage-limitations","title":"Coverage Limitations","text":"<ul> <li>No SQLi testing (safety concerns)</li> <li>No authenticated session testing (limited)</li> <li>No API testing (web UI only)</li> <li>No file upload testing</li> <li>No business logic testing</li> </ul>"},{"location":"gcp/security/web-security-scanner/#technical-limitations","title":"Technical Limitations","text":"<ul> <li>Max 100 starting URLs</li> <li>Max scan duration: 24 hours</li> <li>Rate limited (won\u2019t overwhelm site)</li> <li>Crawls only linked pages (no fuzzing)</li> </ul>"},{"location":"gcp/security/web-security-scanner/#false-negatives","title":"False Negatives","text":"<ul> <li>May miss vulnerabilities</li> <li>Not replacement for penetration testing</li> <li>No zero-day detection</li> <li>Limited coverage of complex apps</li> </ul>"},{"location":"gcp/security/web-security-scanner/#cost","title":"Cost","text":"<p>Pricing: Free (no charge for Web Security Scanner)</p> <p>Related costs: Compute resources being scanned</p>"},{"location":"gcp/security/web-security-scanner/#web-security-scanner-vs-alternatives","title":"Web Security Scanner vs Alternatives","text":"Need Solution Automated scanning Web Security Scanner Comprehensive pentest Third-party security firm Runtime protection Cloud Armor Code analysis Static analysis tools API security testing OWASP ZAP, Burp Suite Container scanning Binary Authorization, Artifact Analysis"},{"location":"gcp/security/web-security-scanner/#best-practices","title":"Best Practices","text":""},{"location":"gcp/security/web-security-scanner/#scan-staging-first","title":"Scan Staging First","text":"<p>Pattern: Always test scanner on non-production</p> <p>Reason: Verify behavior, avoid production impact</p>"},{"location":"gcp/security/web-security-scanner/#review-findings-promptly","title":"Review Findings Promptly","text":"<p>Process:</p> <ol> <li>Scan completes</li> <li>Review findings same day</li> <li>Prioritize by severity</li> <li>Fix high/medium issues</li> <li>Re-scan to verify</li> </ol>"},{"location":"gcp/security/web-security-scanner/#combine-with-manual-testing","title":"Combine with Manual Testing","text":"<p>Strategy: Scanner + manual reviews + penetration testing</p> <p>Frequency: Scanner (continuous), manual (quarterly), pentest (annually)</p>"},{"location":"gcp/security/web-security-scanner/#integrate-into-cicd","title":"Integrate into CI/CD","text":"<p>Automation: Scan on every staging deployment</p> <p>Gates: Block production deployment if high-severity findings</p>"},{"location":"gcp/security/web-security-scanner/#remediation-guidance","title":"Remediation Guidance","text":"<p>Provided:</p> <ul> <li>Vulnerability description</li> <li>Affected URL</li> <li>Evidence (sample request/response)</li> <li>Remediation steps</li> </ul> <p>Example - XSS:</p> <ul> <li>Finding: Reflected XSS on search page</li> <li>Evidence: <code>&lt;script&gt;alert(1)&lt;/script&gt;</code> reflected</li> <li>Remediation: Sanitize user input, encode output</li> </ul>"},{"location":"gcp/security/web-security-scanner/#common-findings","title":"Common Findings","text":""},{"location":"gcp/security/web-security-scanner/#mixed-content","title":"Mixed Content","text":"<p>Issue: HTTPS page loading HTTP resources</p> <p>Fix: Use HTTPS for all resources or protocol-relative URLs</p>"},{"location":"gcp/security/web-security-scanner/#xss","title":"XSS","text":"<p>Issue: User input reflected without sanitization</p> <p>Fix: Input validation, output encoding, CSP headers</p>"},{"location":"gcp/security/web-security-scanner/#outdated-libraries","title":"Outdated Libraries","text":"<p>Issue: Using vulnerable JavaScript libraries</p> <p>Fix: Update to latest secure versions</p>"},{"location":"gcp/security/web-security-scanner/#clear-text-passwords","title":"Clear Text Passwords","text":"<p>Issue: Login form submits over HTTP</p> <p>Fix: Use HTTPS for authentication</p>"},{"location":"gcp/security/web-security-scanner/#compliance","title":"Compliance","text":"<p>Use for:</p> <ul> <li>PCI-DSS: Quarterly vulnerability scanning requirement</li> <li>SOC 2: Demonstrate security testing</li> <li>HIPAA: Security controls validation</li> <li>General best practices</li> </ul> <p>Limitation: May not meet all compliance requirements (supplemental scanning needed)</p>"},{"location":"gcp/security/web-security-scanner/#exam-focus","title":"Exam Focus","text":""},{"location":"gcp/security/web-security-scanner/#core-concepts_1","title":"Core Concepts","text":"<ul> <li>Automated vulnerability scanning</li> <li>OWASP Top 10 detection</li> <li>Pre-production testing</li> <li>Continuous monitoring</li> </ul>"},{"location":"gcp/security/web-security-scanner/#use-cases","title":"Use Cases","text":"<ul> <li>CI/CD integration</li> <li>Compliance scanning</li> <li>Pre-deployment validation</li> <li>Continuous security testing</li> </ul>"},{"location":"gcp/security/web-security-scanner/#limitations_1","title":"Limitations","text":"<ul> <li>No SQLi testing (safety)</li> <li>No penetration testing replacement</li> <li>Web UI only (not APIs)</li> <li>False positives possible</li> <li>Limited auth support</li> </ul>"},{"location":"gcp/security/web-security-scanner/#architecture","title":"Architecture","text":"<ul> <li>Managed vs custom scans</li> <li>Multi-environment strategy</li> <li>Integration with Security Command Center</li> <li>Defense in depth layer</li> </ul>"},{"location":"gcp/security/web-security-scanner/#best-practices_1","title":"Best Practices","text":"<ul> <li>Scan staging first</li> <li>Regular scheduled scans</li> <li>Combine with manual testing</li> <li>Integrate into CI/CD</li> <li>Prompt remediation</li> </ul>"},{"location":"general/different_types_of_scaling/","title":"Different Types of Scaling in IT Systems","text":"<p>Scaling is a critical aspect of managing IT systems and applications. As workloads fluctuate, organizations need to ensure that resources are scaled effectively to maintain performance and optimize costs. Scaling strategies can be categorized into various types, each suited to specific use cases. In this article, we\u2019ll explore different types of scaling, including reactive scaling, proactive scaling, predictive scaling, and others.</p>"},{"location":"general/different_types_of_scaling/#reactive-scaling","title":"Reactive Scaling","text":"<p>Reactive scaling, also known as on-demand scaling, adjusts resources in response to observed changes in workload or performance metrics. It is typically triggered by predefined thresholds or alerts.</p> <p>This approach monitors real-time metrics such as CPU usage, memory utilization, or request latency. When these metrics cross predefined thresholds, additional resources are automatically added or removed. While it\u2019s an efficient way to address sudden workload changes, there may be slight delays as the system adjusts.</p> <p>Reactive scaling is commonly used for applications with variable traffic, such as web services, or batch processing systems where workloads are predictable but vary in size.</p>"},{"location":"general/different_types_of_scaling/#proactive-scaling","title":"Proactive Scaling","text":"<p>Proactive scaling involves scheduling resource adjustments based on anticipated workload patterns. This method works well when workloads follow predictable cycles, such as business hours or seasonal traffic.</p> <p>By analyzing historical data or known patterns, proactive scaling schedules resource changes in advance. This ensures resources are provisioned before workload increases occur, reducing the risk of performance bottlenecks. However, it requires a thorough understanding of workload behavior and may result in resource inefficiency if patterns change unexpectedly.</p> <p>Proactive scaling is ideal for scenarios like e-commerce sites during holiday seasons or corporate applications primarily used during business hours.</p>"},{"location":"general/different_types_of_scaling/#predictive-scaling","title":"Predictive Scaling","text":"<p>Predictive scaling leverages machine learning or advanced analytics to forecast future resource needs and adjust resources accordingly. It combines elements of both proactive and reactive scaling to provide a more sophisticated approach.</p> <p>This method analyzes historical data and trends to predict workload fluctuations, enabling the system to provision resources in advance. Predictive scaling minimizes latency while maintaining cost-efficiency. However, its effectiveness relies on accurate predictions, which can be challenging in highly volatile environments.</p> <p>Predictive scaling is well-suited for dynamic environments such as streaming services with varying demand or financial systems experiencing periodic transaction spikes.</p>"},{"location":"general/different_types_of_scaling/#manual-scaling","title":"Manual Scaling","text":"<p>Manual scaling relies on human intervention to adjust resources based on observed or anticipated workload changes. This approach is often used in smaller or less dynamic environments where changes are infrequent.</p> <p>In this method, administrators monitor system performance and manually allocate or deallocate resources as needed. While it provides full control over resource allocation, manual scaling can be time-consuming and is less effective for handling rapid workload fluctuations.</p> <p>Manual scaling is typically employed in development or staging environments and by small businesses with predictable workloads.</p>"},{"location":"general/different_types_of_scaling/#horizontal-vs-vertical-scaling","title":"Horizontal vs. Vertical Scaling","text":"<p>Scaling strategies can also be categorized as horizontal or vertical, depending on how resources are adjusted.</p>"},{"location":"general/different_types_of_scaling/#horizontal-scaling","title":"Horizontal Scaling:","text":"<p>Horizontal scaling involves adding or removing instances of a resource, such as servers or containers. This approach is commonly used in distributed systems and cloud environments, offering high fault tolerance and scalability. Applications need to be designed to support distribution, typically requiring a stateless architecture.</p>"},{"location":"general/different_types_of_scaling/#vertical-scaling","title":"Vertical Scaling:","text":"<p>Vertical scaling increases or decreases the capacity of existing resources, such as adding more CPU or memory to a server. It is simpler to implement for monolithic applications and does not require changes to application architecture. However, it is limited by hardware constraints and can create a single point of failure.</p>"},{"location":"general/different_types_of_scaling/#choosing-the-right-scaling-strategy","title":"Choosing the Right Scaling Strategy","text":"<p>The choice of scaling strategy depends on several factors, including workload predictability, application architecture, and cost considerations:</p> <ol> <li>Use reactive scaling for unpredictable workloads with real-time monitoring needs.</li> <li>Opt for proactive scaling when workloads follow consistent patterns.</li> <li>Implement predictive scaling for dynamic environments where forecasting can improve efficiency.</li> <li>Rely on manual scaling for environments where changes are infrequent and manageable.</li> <li>Combine horizontal and vertical scaling based on your system\u2019s architecture and constraints.</li> </ol> <p>Scaling is a fundamental aspect of modern IT systems. By understanding and applying the right scaling strategies, organizations can ensure optimal performance, cost efficiency, and reliability.</p>"},{"location":"kubernetes/admission_controller/","title":"Admission Controller","text":"<p>An Admission Controller is a component in Kubernetes that intercepts API requests to the Kubernetes API server before the objects are persisted in etcd. Admission controllers can modify, validate, or reject these requests based on custom logic or policies.</p>"},{"location":"kubernetes/admission_controller/#purpose","title":"Purpose","text":"<ul> <li>To enforce policies and best practices for resources created or updated in the Kubernetes cluster.</li> <li>To validate and mutate incoming API requests.</li> </ul>"},{"location":"kubernetes/admission_controller/#how-it-works","title":"How It Works","text":"<ol> <li>A user sends a request to the Kubernetes API server (e.g., create a Pod or Deployment).</li> <li>The request goes through authentication and authorization checks.</li> <li>The request is processed by admission controllers, which:</li> <li>Mutate the request (e.g., add default values or labels).</li> <li>Validate the request against policies.</li> <li>Approve or reject the request based on the outcome.</li> </ol>"},{"location":"kubernetes/admission_controller/#types-of-admission-controllers","title":"Types of Admission Controllers","text":"<ol> <li> <p>Mutating Admission Controllers:</p> </li> <li> <p>Modify the incoming request before it is persisted.</p> </li> <li> <p>Example: Adding default resource limits to Pods.</p> </li> <li> <p>Validating Admission Controllers:</p> </li> <li> <p>Validate the request and either approve or reject it.</p> </li> <li>Example: Ensuring that Pods do not use privileged containers.</li> </ol>"},{"location":"kubernetes/admission_controller/#built-in-admission-controllers","title":"Built-in Admission Controllers","text":"<p>Some common admission controllers in Kubernetes include:</p> <ul> <li>PodSecurity: Implements Pod Security Admission (PSA).</li> <li>NamespaceLifecycle: Ensures that objects are created only in active namespaces.</li> <li>LimitRanger: Enforces resource limits on Pods and containers.</li> <li>ResourceQuota: Ensures that resource quotas are not exceeded in a namespace.</li> </ul>"},{"location":"kubernetes/admission_controller/#custom-admission-controllers","title":"Custom Admission Controllers","text":"<ul> <li>Kubernetes allows you to define Dynamic Admission Controllers using Admission Webhooks.</li> <li>MutatingAdmissionWebhook and ValidatingAdmissionWebhook allow you to create custom logic to process API requests.</li> </ul>"},{"location":"kubernetes/aggregation_layer/","title":"Kubernetes Aggregation Layer","text":"<p>The Kubernetes Aggregation Layer is a feature that allows you to extend the Kubernetes API by integrating custom APIs into the Kubernetes API server. It enables you to provide additional functionality by deploying custom API servers alongside the main Kubernetes API server and exposing them through the same API endpoint (<code>/apis</code>).</p>"},{"location":"kubernetes/aggregation_layer/#purpose","title":"Purpose","text":"<ul> <li>Extend Kubernetes capabilities without modifying the core API server.</li> <li>Enable custom API resources and operations tailored to specific use cases or applications.</li> <li>Provide a unified interface to interact with both native and custom APIs.</li> </ul>"},{"location":"kubernetes/aggregation_layer/#how-it-works","title":"How It Works","text":"<p>The Aggregation Layer allows Kubernetes to route API requests to additional API servers. Here\u2019s how it works:</p> <ol> <li> <p>Custom API Servers:</p> </li> <li> <p>Deploy additional API servers in your cluster to handle specific custom APIs.</p> </li> <li> <p>These servers define their own resources and operations.</p> </li> <li> <p>APIService Objects:</p> </li> <li> <p>Use <code>APIService</code> resources to register custom API servers with the Kubernetes API server.</p> </li> <li> <p>The <code>APIService</code> object specifies how the API server should handle requests for a specific group/version.</p> </li> <li> <p>Routing:</p> </li> <li>When a request is made to the Kubernetes API server for a registered API, the API server proxies the request to the appropriate custom API server.</li> </ol>"},{"location":"kubernetes/aggregation_layer/#example-workflow","title":"Example Workflow","text":""},{"location":"kubernetes/aggregation_layer/#step-1-deploy-a-custom-api-server","title":"Step 1: Deploy a Custom API Server","text":"<ul> <li>Deploy a custom API server in the cluster to handle a specific group/version of APIs.</li> </ul>"},{"location":"kubernetes/aggregation_layer/#step-2-register-the-apiservice","title":"Step 2: Register the APIService","text":"<ul> <li>Create an <code>APIService</code> object to register the custom API server with the Kubernetes API server.</li> </ul>"},{"location":"kubernetes/aggregation_layer/#example-apiservice-yaml","title":"Example <code>APIService</code> YAML:","text":"<pre><code>apiVersion: apiregistration.k8s.io/v1\nkind: APIService\nmetadata:\n  name: v1beta1.custom.example.com\nspec:\n  service:\n    name: custom-api-service\n    namespace: custom-namespace\n  group: custom.example.com\n  version: v1beta1\n  insecureSkipTLSVerify: true\n  groupPriorityMinimum: 1000\n  versionPriority: 10\n</code></pre> <ul> <li><code>group</code>: The API group served by the custom API server.</li> <li><code>version</code>: The API version handled by the custom API server.</li> <li><code>service</code>: Specifies the Kubernetes service that proxies requests to the custom API server.</li> </ul>"},{"location":"kubernetes/aggregation_layer/#step-3-access-the-api","title":"Step 3: Access the API","text":"<ul> <li>After registration, the custom API becomes available through the main Kubernetes API endpoint, e.g.:   <pre><code>https://&lt;kube-apiserver&gt;/apis/custom.example.com/v1beta1\n</code></pre></li> </ul>"},{"location":"kubernetes/aggregation_layer/#key-components","title":"Key Components","text":"<ol> <li> <p>APIService Resource:</p> </li> <li> <p>Registers a custom API with the Kubernetes API server.</p> </li> <li> <p>Specifies routing information and priorities.</p> </li> <li> <p>Custom API Server:</p> </li> <li> <p>Implements and serves custom resources and operations.</p> </li> <li> <p>Typically deployed as a Deployment and exposed via a Kubernetes Service.</p> </li> <li> <p>Proxying:</p> </li> <li>The Kubernetes API server acts as a reverse proxy, forwarding requests to the registered custom API servers.</li> </ol>"},{"location":"kubernetes/aggregation_layer/#use-cases","title":"Use Cases","text":"<ol> <li> <p>Custom Resource APIs:</p> </li> <li> <p>Expose advanced APIs for custom applications, such as machine learning pipelines, workflow management, or CI/CD systems.</p> </li> <li> <p>External Integrations:</p> </li> <li> <p>Integrate external systems into Kubernetes with custom APIs, e.g., managing cloud resources.</p> </li> <li> <p>Enhanced Functionality:</p> </li> <li>Provide functionality that extends Kubernetes\u2019 default behavior, such as advanced metrics aggregation or policy enforcement.</li> </ol>"},{"location":"kubernetes/aggregation_layer/#benefits","title":"Benefits","text":"<ul> <li> <p>Extensibility:</p> </li> <li> <p>Add new APIs without modifying or rebuilding the core Kubernetes API server.</p> </li> <li> <p>Unified Interface:</p> </li> <li> <p>Expose custom APIs alongside Kubernetes\u2019 native APIs for a consistent developer experience.</p> </li> <li> <p>Scalability:</p> </li> <li>Scale custom API servers independently from the core Kubernetes API server.</li> </ul>"},{"location":"kubernetes/aggregation_layer/#security-considerations","title":"Security Considerations","text":"<ol> <li> <p>Authentication and Authorization:</p> </li> <li> <p>Ensure proper authentication and authorization mechanisms for custom APIs.</p> </li> <li> <p>Integrate with Kubernetes RBAC if possible.</p> </li> <li> <p>TLS Encryption:</p> </li> <li> <p>Use secure TLS connections between the Kubernetes API server and custom API servers.</p> </li> <li> <p>Validation:</p> </li> <li>Validate input and responses for custom APIs to prevent misuse.</li> </ol>"},{"location":"kubernetes/aggregation_layer/#comparison-with-crds-custom-resource-definitions","title":"Comparison with CRDs (Custom Resource Definitions)","text":"Feature Aggregation Layer Custom Resource Definitions (CRDs) Purpose Adds custom APIs with new endpoints Extends Kubernetes with new resource types under <code>/apis</code> Complexity Higher (requires custom API servers) Lower (uses built-in Kubernetes mechanisms) Flexibility Fully custom API operations and logic Resource-based CRUD operations Use Case Advanced custom APIs Simple extensions to Kubernetes resources"},{"location":"kubernetes/aggregation_layer/#conclusion","title":"Conclusion","text":"<p>The Kubernetes Aggregation Layer is a powerful feature for extending Kubernetes functionality by adding custom APIs. While it is more complex to implement than CRDs, it provides greater flexibility and control, making it suitable for advanced use cases like integrating external systems or providing custom services. By using the Aggregation Layer, organizations can leverage Kubernetes as a unified platform for both native and extended capabilities.</p>"},{"location":"kubernetes/cilium/","title":"Cilium","text":""},{"location":"kubernetes/cilium/#introduction","title":"Introduction","text":"<p>Cilium is a cloud-native networking, observability, and security solution built on top of eBPF (extended Berkeley Packet Filter). It provides advanced networking capabilities for containerized applications, particularly in Kubernetes environments, while offering enhanced security and observability features.</p>"},{"location":"kubernetes/cilium/#core-components","title":"Core Components","text":""},{"location":"kubernetes/cilium/#1-cilium-agent","title":"1. Cilium Agent","text":"<ul> <li>Role: Primary component running as a DaemonSet on each Kubernetes node</li> <li>Responsibilities:</li> <li>Manages eBPF programs on the node</li> <li>Handles network policy enforcement</li> <li>Maintains local identity and endpoint management</li> <li>Communicates with Cilium Operator and etcd/Kubernetes API</li> <li>Location: Runs in userspace but interacts heavily with kernel space</li> </ul>"},{"location":"kubernetes/cilium/#2-cilium-operator","title":"2. Cilium Operator","text":"<ul> <li>Role: Cluster-wide management component</li> <li>Responsibilities:</li> <li>Manages cluster-wide resources (CiliumNetworkPolicy, CiliumClusterwideNetworkPolicy, Cluster Mesh)</li> <li>Handles IPAM (IP Address Management) coordination</li> <li>Manages CRD lifecycle and validation</li> <li>Coordinates with cloud provider APIs for advanced features</li> <li>Deployment: Typically runs as a Deployment with 1-2 replicas</li> </ul>"},{"location":"kubernetes/cilium/#3-envoy-proxy","title":"3. Envoy Proxy","text":"<ul> <li>Role: Manage L7 traffic</li> <li>Responsibilities:</li> <li>Manage L7 Network Policies</li> <li>Location: Part of Clilum Agent or standalone pod (daemon)</li> </ul>"},{"location":"kubernetes/cilium/#4-cilium-cni-plugin-crd","title":"4. Cilium CNI Plugin (CRD)","text":"<ul> <li>Role: Container Network Interface implementation</li> <li>Responsibilities:</li> <li>Pod network setup and teardown</li> <li>IP address assignment</li> <li>Network namespace configuration</li> <li>Integration with container runtime</li> <li>Location: Binary installed on each node</li> </ul>"},{"location":"kubernetes/cilium/#5-hubble","title":"5. Hubble","text":"<ul> <li>Role: Network observability and security monitoring</li> <li>Components:</li> <li>Hubble Server: Runs alongside Cilium agent, exposes gRPC API and collects flows and visibility data using eBPF</li> <li>Hubble Relay: Cluster-wide aggregation service</li> <li>Hubble UI: Web-based interface for network visualization</li> <li>Hubble CLI: Command-line tool for querying network flows throught thw Hubble relay</li> </ul>"},{"location":"kubernetes/cilium/#6-ebpf-programs","title":"6. eBPF Programs","text":"<ul> <li>Role: Manage L3/L4 traffic</li> <li>Responsibilities:</li> <li>Manage L3/L4 Network Policies</li> <li>Location: Loaded into that node\u2019s kernel</li> </ul>"},{"location":"kubernetes/cilium/#ebpf-foundation","title":"eBPF Foundation","text":""},{"location":"kubernetes/cilium/#what-is-ebpf","title":"What is eBPF?","text":"<ul> <li>Extended Berkeley Packet Filter - a kernel technology</li> <li>Allows running sandboxed programs in kernel space without changing kernel source</li> <li>Provides high-performance, programmable packet processing</li> <li>Enables advanced networking, security, and observability features</li> </ul>"},{"location":"kubernetes/cilium/#ciliums-ebpf-usage","title":"Cilium\u2019s eBPF Usage","text":""},{"location":"kubernetes/cilium/#network-datapath","title":"Network Datapath","text":"<ul> <li>TC (Traffic Control) Programs: Attached to network interfaces for ingress/egress processing</li> <li>XDP (eXpress Data Path) Programs: Ultra-fast packet processing at driver level</li> <li>Socket Operations: Intercept and redirect socket operations</li> <li>Kernel Tracing: Monitor system calls and kernel functions</li> </ul>"},{"location":"kubernetes/cilium/#key-ebpf-maps","title":"Key eBPF Maps","text":"<ul> <li>Identity Map: Maps security identities to numeric IDs</li> <li>Policy Map: Stores network policy decisions</li> <li>Endpoint Map: Tracks local endpoints and their properties</li> <li>Service Map: Load balancing and service discovery information</li> </ul>"},{"location":"kubernetes/cilium/#cilium-features","title":"Cilium Features","text":""},{"location":"kubernetes/cilium/#ipam","title":"IPAM","text":"<p>Two available modes:</p> <ul> <li> <p>Kubernetes Host Scope: kube-controllermanager assign PodCIDR to each nodes. Set the resource Node <pre><code>ipam:\n  mode: \"kubernetes\"\nk8s:\n  requireIPv4PodCIDR: true\n  requireIPv6PodCIDR: true\n</code></pre></p> </li> <li> <p>Cluster Scope (default): Cilium Operator assign PodCIDR. Use the resource CiliumNode <pre><code>ipam:\n  mode: \"cluster-pool\"\nk8s:\n  clusterPoolIPv4PodCIDRList: [\"10.0.0.0/8\"]\n  clusterPoolIPv4MaskSize: 24\n  clusterPoolIPv6PodCIDRList: [\"fd00::/104\"]\n  clusterPoolIPv6MaskSize: 120\n</code></pre></p> </li> </ul> <p>To get all assigned CIDRs:</p> <pre><code>cilium-dbg status --all-addresses\n</code></pre> <p>Do not change IPAM mode on live clusters. Instead, deploy a new cluster with desired IPAM mode and migrate workloads.</p>"},{"location":"kubernetes/cilium/#routing-modes","title":"Routing modes","text":"<p>Cilium supports following routing modes that determine how packets are forwarded between pods across nodes:</p> <ol> <li> <p>Native Routing Uses the host\u2019s existing routing table and network stack <pre><code>routingMode: \"native\"\nipV4NativeRoutingCIDR: \"10.244.0.0/16\"\nipV6NativeRoutingCIDR: \"\"\n</code></pre></p> </li> <li> <p>Tunnel - Encapsulating (Default) Encapsulates pod traffic in VXLAN or Geneve tunnels Overhead: ~50 bytes per packet (VXLAN + UDP + IP headers) <pre><code>routingMode: \"tunnel\"\n\ntunnelProtocol: \"vxlan\" # default\n# tunnelProtocol: \"geneve\"\n\ntunnelPort: 8472 # vxlan\n# tunnelPort: 6081 # geneve\n</code></pre></p> </li> </ol>"},{"location":"kubernetes/cilium/#kube-proxy-replacement","title":"Kube-Proxy Replacement","text":"<p>Cilium can completely replace kube-proxy with a more efficient eBPF-based implementation for Kubernetes Service load balancing. Kube-proxy can degrades with large numbers of services/endpoints. eBPF-based has linear scaling regardless of service count. <pre><code>kubeProxyReplacement: false # kube-proxy\n---\nkubeProxyReplacement: true # cilium\nk8sServiceHost: \"host-ip-control-plane\"\nk8sServicePort: \"6443\"\n</code></pre></p> <p>Check current config: <pre><code>cilium-dbg status\n</code></pre></p>"},{"location":"kubernetes/cilium/#cilium-ingress","title":"Cilium Ingress","text":"<p>Cilium provides L7 HTTP/HTTPS ingress capabilities without needing a separate ingress controller. <pre><code>nodePort:\n  enabled: true\ningressController:\n  enabled: true\n  default: true\n  loadBalancerMode: dedicated # or \"shared\"\n</code></pre> Ingress mode:</p> <ul> <li>Dedicated: one loadbalancer for each ingress</li> <li>Shared: one single loadbalancer for all ingress</li> </ul> <p>Ingress components:</p> <ul> <li>GatewayClass: created by Infra team</li> <li>Gateway: created by cluster operator</li> <li>HTTPRoute (TCP, GRPC): created by developer</li> </ul>"},{"location":"kubernetes/cilium/#encryption","title":"Encryption","text":"<p>If enabled, traffic between clusters will be encryted, traffic within cluster not encrypted.</p> <ul> <li>IPSec <pre><code>encryption:\n  enabled: true\n  type: ipsec\n</code></pre></li> <li>WireGuard <pre><code>encryption:\n  enabled: true\n  type: wireguard\n</code></pre></li> </ul> <p>check encryption status: <pre><code>cilium-dbg encryption status\n</code></pre></p>"},{"location":"kubernetes/cilium/#mtls","title":"mTLS","text":"<p>Applied with SPIFFE, implemented by Spire.</p> <ul> <li>A spire-server will be deployed in the cluster</li> <li>A spire-agent in every node</li> </ul> <p>Encryption must be enabled. <pre><code>encryption:\n  enabled: true\n  type: ipsec # or wireguard\n\nauthentication:\n  enabled: true\n  mutual:\n    spire:\n      enabled: true\n      install:\n        enabled: true\n</code></pre></p> <p>In the CiliumNetworkPolicy: <pre><code>apiVersion: \"cilium.io/v2\"\nkind: CiliumNetworkPolicy\nmetadata:\n name: frontend-policy\n namespace: default\nspec:\n endpointSelector:\n   matchLabels:\n     app: frontend\n ingress:\n - fromEndpoints:\n   - matchLabels:\n       app: gateway\n   toPorts:\n   - ports:\n     - port: \"8080\"\n       protocol: TCP\n     authentication: # &lt;---\n       mode: \"required\"\n</code></pre></p>"},{"location":"kubernetes/cilium/#cluster-mesh","title":"Cluster Mesh","text":""},{"location":"kubernetes/cilium/#prerequisites","title":"Prerequisites","text":"<ul> <li>Clusters must have same routing mode</li> <li>Non overlapping Pod CIDRs</li> </ul> <pre><code>cilium clustermesh enable --context $CLUSTER1 # Enable, for every cluster\ncilium clustermesh connect --context $CLUSTER1 --destination-context $CLUSTER2 # connect clusters\ncilium clustermesh status # check status\n</code></pre>"},{"location":"kubernetes/cilium/#kvstoremesh","title":"KVStoreMesh","text":"<p>Cross-cluster networking without requiring a shared key-value store (etcd). Replaces shared etcd with direct cluster-to-cluster communication. Clusters exchange identity and service information directly. No central store: Eliminates shared etcd dependency</p>"},{"location":"kubernetes/cilium/#global-services","title":"Global Services","text":"<p>Global Services enable cross-cluster load balancing, a single service that distributes traffic across pods in multiple clusters. Service exists in multiple clusters with same name/namespace Cilium merges endpoints from all clusters into one global service Traffic is load balanced across all clusters automatically Failover: Automatically excludes unhealthy clusters</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: global-service\n  namespace: default\n  annotations:\n    service.cilium.io/global: \"true\"  # Makes service global (one service for all clusters)\n    service.cilium.io/affinity: \"local\" # Prioritize local clusters (or \"remote\")\n    service.cilium.io/shared: \"false\" # Make service local (one service per cluster)\nspec:\n  type: ClusterIP\n  selector:\n    app: my-app\n  ports:\n  - port: 80\n    targetPort: 8080\n</code></pre>"},{"location":"kubernetes/cilium/#observability-architecture","title":"Observability Architecture","text":""},{"location":"kubernetes/cilium/#hubble","title":"Hubble","text":"<p>To enable: <pre><code>hubble:\n  enabled: true\n\n  relay:\n    enabled: true\n\n  ui:\n    enabled: true\n</code></pre></p> <p>To use hubble CLI: <pre><code>cilium hubble port-forward # start communication\n\nhubble observe --pod pod_name # show in/out traffic for pod\nhubble observe --from-pod pod_name # show out traffic from pod\nhubble observe --to-pod pod_name # show in traffic to pod\n\nhubble observe --to-pod pod_name --port 3000 --protocol http --VERDICT [FORWARDED|DROPPED...] # filter by port, protocol and result\n</code></pre></p> <p>To use hubble UI: <pre><code>cilium hubble ui # start server\n</code></pre></p>"},{"location":"kubernetes/cilium/#bgp-external-networking","title":"BGP &amp; External Networking","text":""},{"location":"kubernetes/cilium/#egress-gateway","title":"Egress gateway","text":"<p>Provides centralized egress traffic control, routing pod traffic through specific gateway nodes with predictable source IPs.</p> <p>To enable: <pre><code>bpf:\n  masquerade: true\n\nkubeProxyReplacement: true\n\negressGateway:\n  enable: true\n</code></pre></p> <p>Configuration: <pre><code>apiVersion: cilium.io/v2\nkind: CiliumEgressGatewayPolicy\nmetadata:\n  name: egress-policy\nspec:\n  selectors:\n  - podSelector:\n      matchLabels:\n        app: frontend\n  destinationCIDRs:\n  - \"10.10.10.0/24\"  # External service subnet\n  gatewayConfig:\n    nodeSelector:\n      matchLabels:\n        egress-gateway: \"true\"  # Designated gateway nodes\n</code></pre></p> <p>Verification: <pre><code>cilium-dbg bpf egress list\n</code></pre></p>"},{"location":"kubernetes/cilium/#loadbalancer-ipam","title":"LoadBalancer IPAM","text":"<p>Automatically assigns IP addresses to Kubernetes LoadBalancer services from predefined pools, eliminating dependency on cloud provider load balancers. Cilium allocates IPs from configured pools to LoadBalancer services.</p> <pre><code>apiVersion: \"cilium.io/v2alpha1\"\nkind: CiliumLoadBalancerIPPool\nmetadata:\n  name: \"main-pool\"\nspec:\n  blocks:\n  - cidr: \"10.0.10.0/24\"\n  serviceSelector:\n    matchLabels:\n      color: red\n</code></pre>"},{"location":"kubernetes/cilium/#bgp","title":"BGP","text":"<p>Enables dynamic route advertisement to network infrastructure, making pod/service IPs reachable from outside the cluster. - Used with routing mode native. - Cilium nodes peer with BGP routers - Advertise routes for pod CIDRs, service IPs, and LoadBalancer IPs - Network infrastructure learns routes and forwards traffic to correct nodes</p> <p>To enable: <pre><code>bgpControlPlane:\n  enabled: true\n</code></pre></p> <p>Configuration: <pre><code>apiVersion: \"cilium.io/v2alpha1\"\nkind: CiliumBGPClusterConfig\nmetadata:\n  name: cilium-bgp\nspec:\n  nodeSelector:\n    matchLabels:\n      bgp: \"true\" # every node must have this label to use bgp\n  bgpInstances:\n  - name: \"instance-64000\"\n    localASN: 64000\n    peers:\n    - name: \"peer-1\"\n      peerASN: 65000\n      peerAddress: \"10.0.0.1\"  # Router switch IP\n      peerConfigRef:\n        name: \"cilium-peer\" # same as CiliumBGPPeerConfig name\n---\napiVersion: \"cilium.io/v2alpha1\"\nkind: CiliumBGPPeerConfig\nmetadata:\n  name: cilium-peer\nspec:\n  timers:\n    holdTimeSeconds: 9\n    keepAliveTimeSeconds: 3\nebgpMultiHop: 5\n  families:\n  - afi: ipv4\n    safi: unicast\n    advertisments:\n      matchLabels:\n        advertise: bgp # must be the same of CiliumBGPAdvertisement labels\n---\napiVersion: \"cilium.io/v2alpha1\"\nkind: CiliumBGPAdvertisement\nmetadata:\n  name: bgp-advertisement\n  labels:\n    advertise: bgp\nspec:\n  advertisements:\n  - advertisementType: PodCIDR\n    attributes:\n      communities:\n        standard: [ \"65000:99\" ]\n      localPreference: 99\n  - advertisementType: Service\n    service:\n      addresses:\n      - ClusterIP\n      - ExternalIP\n      - LoadBalancerIP\n    selector:\n      matchExpressions:\n      - key: type\n        operator: In\n        values: [\"LoadBalancer\"]\n</code></pre></p> <p>Troubleshoot: <pre><code>cilium bgp peers # nodes where bgp is running and all other info\ncilium bgp routes available # routes that the Cilium agent has learned\ncilium bgp routes advertised # routes that the Cilium instances has advertised\n</code></pre></p>"},{"location":"kubernetes/cilium/#l2announcement","title":"L2Announcement","text":"<p>Makes LoadBalancer service IPs reachable by responding to ARP requests on the local network segment, without requiring BGP infrastructure.</p> <p>To enable: <pre><code>kubeProxyReplacement: true\n\nl2announcements:\n  enable: true\n</code></pre></p> <p>Configuration: <pre><code>apiVersion: \"cilium.io/v2alpha1\"\nkind: CiliumL2AnnouncementPolicy\nmetadata:\n  name: \"l2-policy\"\nspec:\n  serviceSelector:\n    matchLabels:\n      app: myapp\n  nodeSelector:\n    matchLabels:\n    - key: node-role.kubernetes.io/control-plane\n      operator: DoesNotExist\n  interfaces:\n  - \"^eth[0-9]+\"  # Network interfaces to announce on\n  externalIPs: true\n  loadBalancerIPs: true\n</code></pre></p>"},{"location":"kubernetes/encryptionconfig/","title":"EncryptionConfig in Kubernetes","text":"<p>EncryptionConfig is a Kubernetes feature that allows you to encrypt sensitive data stored in etcd. Kubernetes uses etcd as its backend storage for cluster data, and while etcd supports encryption at the disk level, EncryptionConfig provides additional protection by encrypting specific Kubernetes resources at the application layer.</p>"},{"location":"kubernetes/encryptionconfig/#why-use-encryptionconfig","title":"Why Use EncryptionConfig?","text":"<ol> <li> <p>Enhanced Security:</p> </li> <li> <p>Protect sensitive data such as Secrets, ConfigMaps, and other resources stored in etcd.</p> </li> <li> <p>Prevent unauthorized access to sensitive information in case etcd backups or snapshots are compromised.</p> </li> <li> <p>Compliance:</p> </li> <li> <p>Helps meet regulatory requirements by encrypting data at rest in etcd.</p> </li> <li> <p>Granular Control:</p> </li> <li>Allows encryption of specific resources or resource types.</li> </ol>"},{"location":"kubernetes/encryptionconfig/#how-it-works","title":"How It Works","text":"<ol> <li> <p>Encryption Providers:</p> </li> <li> <p>Kubernetes uses encryption providers to specify the type of encryption used.</p> </li> <li> <p>Supported providers include:</p> <ul> <li>AES-CBC: Encrypts data using the AES algorithm in Cipher Block Chaining mode.</li> <li>SecretBox: Uses the NaCl SecretBox algorithm for encryption.</li> <li>Identity: No encryption; the data is stored as plaintext.</li> </ul> </li> <li> <p>EncryptionConfig File:</p> </li> <li> <p>An <code>EncryptionConfig</code> file specifies which resources should be encrypted and the encryption method.</p> </li> <li> <p>Decryption on Access:</p> </li> <li>Encrypted data is decrypted automatically when accessed via the Kubernetes API server.</li> </ol>"},{"location":"kubernetes/encryptionconfig/#example-encryptionconfig","title":"Example EncryptionConfig","text":"<p>The following example encrypts Secrets using the AES-CBC encryption provider:</p> <pre><code>apiVersion: apiserver.config.k8s.io/v1\nkind: EncryptionConfiguration\nresources:\n  - resources:\n      - secrets\n    providers:\n      - aescbc:\n          keys:\n            - name: key1\n              secret: &lt;base64-encoded-encryption-key&gt;\n      - identity: {}\n</code></pre>"},{"location":"kubernetes/encryptionconfig/#explanation","title":"Explanation:","text":"<ul> <li><code>resources</code>: Specifies the resource types to encrypt (e.g., Secrets).</li> <li><code>aescbc</code>: Indicates the AES-CBC encryption provider.</li> <li><code>keys</code>: Contains the encryption keys, with the <code>secret</code> field containing a base64-encoded key.</li> <li><code>identity</code>: Falls back to plaintext storage for resources not encrypted by <code>aescbc</code>.</li> </ul>"},{"location":"kubernetes/encryptionconfig/#steps-to-configure-encryptionconfig","title":"Steps to Configure EncryptionConfig","text":""},{"location":"kubernetes/encryptionconfig/#1-create-the-encryptionconfig-file","title":"1. Create the EncryptionConfig File","text":"<ul> <li>Write the <code>EncryptionConfig</code> file as shown above, specifying the resources to encrypt and the encryption providers.</li> </ul>"},{"location":"kubernetes/encryptionconfig/#2-enable-encryption-in-the-api-server","title":"2. Enable Encryption in the API Server","text":"<ul> <li>Update the API server manifest (e.g., <code>/etc/kubernetes/manifests/kube-apiserver.yaml</code>) to include the <code>--encryption-provider-config</code> flag:</li> </ul> <pre><code>- --encryption-provider-config=/path/to/encryption-config.yaml\n</code></pre>"},{"location":"kubernetes/encryptionconfig/#3-restart-the-api-server","title":"3. Restart the API Server","text":"<ul> <li>Restart the API server for the changes to take effect:</li> </ul> <pre><code>sudo systemctl restart kube-apiserver\n</code></pre>"},{"location":"kubernetes/encryptionconfig/#4-migrate-existing-data","title":"4. Migrate Existing Data","text":"<ul> <li>Run the <code>kubectl get</code> and <code>kubectl apply</code> commands to re-encrypt existing resources:</li> </ul> <pre><code>kubectl get secrets --all-namespaces -o yaml | kubectl apply -f -\n</code></pre>"},{"location":"kubernetes/encryptionconfig/#verification","title":"Verification","text":"<p>To confirm that encryption is working:</p> <ol> <li>Inspect the etcd data and verify that encrypted resources are not in plaintext.</li> <li>Use <code>etcdctl</code> to view raw etcd contents:</li> </ol> <pre><code>etcdctl get /registry/secrets/default/my-secret\n</code></pre> <ul> <li>Encrypted data will appear as a ciphered string instead of plaintext values.</li> </ul>"},{"location":"kubernetes/encryptionconfig/#considerations","title":"Considerations","text":"<ol> <li> <p>Key Management:</p> </li> <li> <p>Rotate encryption keys regularly for security.</p> </li> <li> <p>Backup keys securely, as loss of encryption keys may result in data inaccessibility.</p> </li> <li> <p>Resource Overhead:</p> </li> <li> <p>Encryption and decryption introduce additional CPU and memory usage on the API server.</p> </li> <li> <p>Backup Compatibility:</p> </li> <li> <p>Ensure etcd backups include encryption keys to allow data recovery.</p> </li> <li> <p>Fallback to Identity:</p> </li> <li>If decryption fails or a key is lost, resources with <code>identity</code> provider remain accessible as plaintext.</li> </ol>"},{"location":"kubernetes/encryptionconfig/#common-use-cases","title":"Common Use Cases","text":"<ul> <li>Encrypting Secrets to secure sensitive information such as API keys, passwords, and certificates.</li> <li>Encrypting ConfigMaps or other sensitive application configurations.</li> <li>Ensuring compliance with data security regulations.</li> </ul>"},{"location":"kubernetes/encryptionconfig/#conclusion","title":"Conclusion","text":"<p>EncryptionConfig is an essential feature for securing sensitive data in Kubernetes clusters. By encrypting data at the application layer, it adds a robust layer of protection against unauthorized access and meets compliance standards. Proper key management and regular testing are critical to maintaining a secure and reliable encryption setup.</p>"},{"location":"kubernetes/endpoint/","title":"Kubernetes Endpoint Resource","text":"<p>In Kubernetes, an Endpoint resource represents the network addresses (IP and port combinations) of the Pods that are associated with a Kubernetes Service. Endpoints enable the Service to route traffic to the appropriate Pods, acting as a bridge between the abstract Service and the concrete Pods that implement it.</p>"},{"location":"kubernetes/endpoint/#how-endpoints-work","title":"How Endpoints Work","text":"<ol> <li> <p>Service-Pod Association:</p> </li> <li> <p>When you create a Service, Kubernetes automatically creates an associated Endpoint resource.</p> </li> <li> <p>The Endpoint contains a list of IP addresses and ports of the Pods that match the Service\u2019s <code>selector</code>.</p> </li> <li> <p>Dynamic Updates:</p> </li> <li> <p>The Endpoint is updated dynamically by the Kubernetes controller as Pods are added, removed, or their status changes.</p> </li> <li> <p>Routing:</p> </li> <li>The Endpoint resource provides the information necessary for the Service to route traffic to the correct Pods.</li> </ol>"},{"location":"kubernetes/endpoint/#structure-of-an-endpoint-resource","title":"Structure of an Endpoint Resource","text":"<p>The <code>Endpoints</code> object in Kubernetes has the following structure:</p> <pre><code>apiVersion: v1\nkind: Endpoints\nmetadata:\n  name: my-service\n  namespace: default\nsubsets:\n  - addresses:\n      - ip: 10.244.1.5\n      - ip: 10.244.1.6\n    ports:\n      - port: 80\n        protocol: TCP\n</code></pre>"},{"location":"kubernetes/endpoint/#key-fields","title":"Key Fields:","text":"<ul> <li><code>addresses</code>:</li> <li>A list of IP addresses representing the Pods associated with the Service.</li> <li><code>ports</code>:</li> <li>A list of port numbers available on the Pods.</li> </ul>"},{"location":"kubernetes/endpoint/#endpoints-vs-endpointslice","title":"Endpoints vs EndpointSlice","text":"<ul> <li> <p>Endpoints:</p> </li> <li> <p>A legacy resource that lists all IP addresses and ports associated with a Service.</p> </li> <li> <p>Can become inefficient for large-scale clusters with many endpoints.</p> </li> <li> <p>EndpointSlice:</p> </li> <li>Introduced in Kubernetes 1.17 as a scalable alternative.</li> <li>Divides endpoints into smaller chunks for better performance and scalability.</li> </ul>"},{"location":"kubernetes/endpoint/#common-use-cases","title":"Common Use Cases","text":"<ol> <li> <p>Service Discovery:</p> </li> <li> <p>Endpoints help Services discover and communicate with the Pods implementing the Service.</p> </li> <li> <p>Debugging Service Issues:</p> </li> <li> <p>You can inspect the Endpoint resource to verify which Pods are associated with a Service.</p> </li> </ol> <pre><code>kubectl get endpoints my-service -o yaml\n</code></pre> <ol> <li>Custom Routing:</li> <li>Applications or custom controllers can use the Endpoint resource for custom traffic routing logic.</li> </ol>"},{"location":"kubernetes/endpoint/#manually-creating-endpoints","title":"Manually Creating Endpoints","text":"<p>In some scenarios (e.g., external services or legacy applications), you may want to create an Endpoint resource manually.</p>"},{"location":"kubernetes/endpoint/#example","title":"Example:","text":"<pre><code>apiVersion: v1\nkind: Endpoints\nmetadata:\n  name: custom-endpoint\nsubsets:\n  - addresses:\n      - ip: 192.168.1.100\n    ports:\n      - port: 8080\n        protocol: TCP\n</code></pre>"},{"location":"kubernetes/endpoint/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Use EndpointSlices for Scalability:</p> </li> <li> <p>For clusters with large numbers of Services or Pods, enable EndpointSlices for better performance.</p> </li> <li> <p>Avoid Manual Endpoint Management:</p> </li> <li> <p>Let Kubernetes manage Endpoints automatically through Services unless there\u2019s a specific need.</p> </li> <li> <p>Monitor and Debug:</p> </li> <li>Regularly monitor Endpoint resources to ensure Pods are correctly associated with Services.</li> </ol>"},{"location":"kubernetes/endpoint/#troubleshooting-endpoints","title":"Troubleshooting Endpoints","text":"<ol> <li>Check Endpoint Status:</li> </ol> <pre><code>kubectl describe endpoints my-service\n</code></pre> <ol> <li> <p>Verify Service Selectors:</p> </li> <li> <p>Ensure the Service selector matches the labels of the intended Pods.</p> </li> <li> <p>Inspect Pod Readiness:</p> </li> <li>Only Pods in the Ready state are included in the Endpoint resource.</li> </ol>"},{"location":"kubernetes/endpoint/#conclusion","title":"Conclusion","text":"<p>Kubernetes Endpoint resources are crucial for routing traffic within the cluster, providing the linkage between Services and their underlying Pods. While they serve as the backbone for internal service discovery and traffic management, EndpointSlices are the recommended solution for handling large-scale clusters due to their improved scalability and efficiency.</p>"},{"location":"kubernetes/envoy/","title":"What is Envoy?","text":"<p>Envoy is an open-source, high-performance edge and service proxy designed for cloud-native applications. Originally developed by Lyft and now part of the Cloud Native Computing Foundation (CNCF), Envoy is a critical building block for modern service meshes, API gateways, and microservices-based architectures.</p> <p>Envoy acts as a L4/L7 proxy that abstracts networking concerns, enabling reliable, scalable, and observable service-to-service communication.</p>"},{"location":"kubernetes/envoy/#key-features-of-envoy","title":"Key Features of Envoy","text":"<ol> <li> <p>High-Performance Proxy:</p> </li> <li> <p>Envoy is written in C++, ensuring low-latency and high-throughput proxying.</p> </li> <li> <p>Layer 4 (L4) and Layer 7 (L7) Proxy:</p> </li> <li> <p>Supports both transport-level (TCP) and application-level (HTTP/HTTPS) communication.</p> </li> <li> <p>Service Discovery and Load Balancing:</p> </li> <li> <p>Dynamic service discovery and advanced load balancing algorithms (e.g., round-robin, least-request).</p> </li> <li> <p>Observability:</p> </li> <li> <p>Provides detailed metrics, tracing, and logging to monitor service communication.</p> </li> <li> <p>Integrates with tools like Prometheus, Grafana, and Jaeger.</p> </li> <li> <p>Fault Injection and Resilience:</p> </li> <li> <p>Supports retries, circuit breakers, timeouts, and fault injection for improving resilience.</p> </li> <li> <p>mTLS (Mutual TLS):</p> </li> <li> <p>Provides secure communication between services using mutual TLS.</p> </li> <li> <p>Extensibility:</p> </li> <li> <p>Envoy can be extended using filters and works seamlessly with service mesh solutions like Istio.</p> </li> </ol>"},{"location":"kubernetes/envoy/#how-envoy-works","title":"How Envoy Works","text":"<p>Envoy operates as a sidecar proxy alongside application services or as an edge proxy. It intercepts traffic, manages routing, and ensures reliability.</p>"},{"location":"kubernetes/envoy/#1-service-to-service-communication","title":"1. Service-to-Service Communication","text":"<p>Envoy handles requests between services in a microservice architecture, managing load balancing, retries, and failures.</p>"},{"location":"kubernetes/envoy/#2-observability-and-telemetry","title":"2. Observability and Telemetry","text":"<p>Envoy generates telemetry data, including metrics and distributed traces, providing visibility into traffic and performance.</p>"},{"location":"kubernetes/envoy/#3-api-gateway","title":"3. API Gateway","text":"<p>At the edge of a system, Envoy serves as an API gateway, managing external requests, rate limiting, and security.</p>"},{"location":"kubernetes/envoy/#use-cases-for-envoy","title":"Use Cases for Envoy","text":"<ol> <li> <p>Service Mesh:</p> </li> <li> <p>Envoy acts as a sidecar proxy for communication in service meshes like Istio, Consul, and Linkerd.</p> </li> <li> <p>API Gateway:</p> </li> <li> <p>Envoy can manage external API traffic, handle routing, and enforce security policies.</p> </li> <li> <p>Edge Proxy:</p> </li> <li> <p>Envoy can be deployed at the network edge to handle external traffic and load balancing.</p> </li> <li> <p>Observability and Monitoring:</p> </li> <li> <p>Envoy collects and exposes metrics, logs, and traces for performance monitoring.</p> </li> <li> <p>Resilience:</p> </li> <li>Implements features like retries, timeouts, rate limiting, and circuit breakers to ensure system stability.</li> </ol>"},{"location":"kubernetes/envoy/#example-envoy-configuration","title":"Example Envoy Configuration","text":"<p>Here is a sample configuration for routing HTTP requests to a backend service:</p> <pre><code>static_resources:\n  listeners:\n    - name: listener_0\n      address:\n        socket_address: { address: 0.0.0.0, port_value: 8080 }\n      filter_chains:\n        - filters:\n            - name: envoy.filters.network.http_connection_manager\n              typed_config:\n                \"@type\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\n                codec_type: AUTO\n                stat_prefix: ingress_http\n                route_config:\n                  name: local_route\n                  virtual_hosts:\n                    - name: backend_service\n                      domains: [\"*\"]\n                      routes:\n                        - match: { prefix: \"/\" }\n                          route: { cluster: backend_cluster }\n                http_filters:\n                  - name: envoy.filters.http.router\n\n  clusters:\n    - name: backend_cluster\n      connect_timeout: 0.25s\n      type: LOGICAL_DNS\n      lb_policy: ROUND_ROBIN\n      load_assignment:\n        cluster_name: backend_cluster\n        endpoints:\n          - lb_endpoints:\n              - endpoint:\n                  address:\n                    socket_address: { address: backend, port_value: 80 }\n</code></pre>"},{"location":"kubernetes/envoy/#explanation","title":"Explanation:","text":"<ul> <li>Listeners: Define where Envoy listens for incoming requests.</li> <li>Routes: Specify routing rules for requests.</li> <li>Clusters: Define upstream services where traffic is sent.</li> </ul>"},{"location":"kubernetes/envoy/#why-use-envoy","title":"Why Use Envoy?","text":"<ul> <li>Scalability: Designed for modern, large-scale distributed systems.</li> <li>Extensibility: Highly configurable and supports custom extensions.</li> <li>Observability: Detailed metrics and tracing for full visibility.</li> <li>Resilience: Implements retries, circuit breakers, and load balancing for fault tolerance.</li> <li>Compatibility: Integrates seamlessly with service meshes, cloud-native tools, and Kubernetes.</li> </ul>"},{"location":"kubernetes/envoy/#conclusion","title":"Conclusion","text":"<p>Envoy is a versatile, high-performance proxy that enables reliable, observable, and secure communication in modern cloud-native systems. Whether used as an API gateway, edge proxy, or sidecar proxy in a service mesh, Envoy is a powerful tool for managing microservice architectures and distributed systems.</p>"},{"location":"kubernetes/kubefed/","title":"KubeFed (Kubernetes Federation)","text":"<p>KubeFed is a Kubernetes project that enables federation of multiple Kubernetes clusters. Federation allows you to manage multiple clusters as a single entity, providing centralized control over the resources and configurations across clusters.</p>"},{"location":"kubernetes/kubefed/#key-features-of-kubefed","title":"Key Features of KubeFed","text":"<ol> <li> <p>Multi-Cluster Management:</p> </li> <li> <p>Allows administrators to manage multiple Kubernetes clusters from a single control plane.</p> </li> <li> <p>Synchronizes resources and configurations across clusters.</p> </li> <li> <p>Workload Distribution:</p> </li> <li> <p>Enables workload distribution across clusters for improved availability, fault tolerance, and geographic coverage.</p> </li> <li> <p>Cross-Cluster Resource Sharing:</p> </li> <li> <p>Allows shared resources, such as ConfigMaps and Secrets, to be replicated across clusters.</p> </li> <li> <p>Policy Enforcement:</p> </li> <li>Ensures consistent policies and configurations across all federated clusters.</li> </ol>"},{"location":"kubernetes/kubefed/#use-cases","title":"Use Cases","text":"<ul> <li>Disaster recovery and high availability by distributing workloads across multiple regions.</li> <li>Multi-cloud or hybrid cloud deployments to avoid vendor lock-in.</li> <li>Scaling workloads geographically to reduce latency for end-users.</li> </ul>"},{"location":"kubernetes/kubefed/#how-it-works","title":"How It Works","text":"<ul> <li>Control Plane: A central KubeFed control plane manages multiple clusters.</li> <li>Federated Resources: Resources such as Deployments, Services, or ConfigMaps are created in a federated namespace and propagated to member clusters.</li> </ul>"},{"location":"kubernetes/kubefed/#example-federated-deployment","title":"Example: Federated Deployment","text":"<p>A Deployment managed by KubeFed can run replicas of an application across three clusters (e.g., one in the US, one in Europe, and one in Asia).</p>"},{"location":"kubernetes/kubernetes_components/","title":"Components of Kubernetes","text":"<p>Kubernetes is composed of several key components that work together to orchestrate containerized applications. Below is a list of the core components, grouped into control plane components and node components, along with their descriptions.</p>"},{"location":"kubernetes/kubernetes_components/#control-plane-components","title":"Control Plane Components","text":"<p>The control plane manages the Kubernetes cluster and makes global decisions about scheduling, scaling, and maintaining the cluster\u2019s state.</p>"},{"location":"kubernetes/kubernetes_components/#1-api-server-kube-apiserver","title":"1. API Server (<code>kube-apiserver</code>)","text":"<ul> <li>Description: Acts as the central control plane component, exposing the Kubernetes API. It serves as the primary point of communication for users, administrators, and all cluster components.</li> <li>Key Features:</li> <li>Processes API requests (e.g., <code>kubectl</code> commands).</li> <li>Provides authentication, authorization, and admission control.</li> <li>Persists the cluster\u2019s state to etcd.</li> <li>Example: Handles commands like <code>kubectl apply</code> to create resources.</li> </ul>"},{"location":"kubernetes/kubernetes_components/#2-etcd","title":"2. etcd","text":"<ul> <li>Description: A distributed key-value store used as the primary database for storing all cluster data.</li> <li>Key Features:</li> <li>Stores information about nodes, Pods, ConfigMaps, Secrets, and more.</li> <li>Ensures data consistency across the cluster.</li> <li>Example: Stores the desired state of a Deployment and its associated Pods.</li> </ul>"},{"location":"kubernetes/kubernetes_components/#3-scheduler-kube-scheduler","title":"3. Scheduler (<code>kube-scheduler</code>)","text":"<ul> <li>Description: Determines on which node a Pod should run based on resource requirements, constraints, and available capacity.</li> <li>Key Features:</li> <li>Uses policies and priorities to select the optimal node.</li> <li>Factors in taints, tolerations, node affinity, and Pod affinity/anti-affinity.</li> <li>Example: Assigns a Pod to a node with sufficient memory and CPU.</li> </ul>"},{"location":"kubernetes/kubernetes_components/#4-controller-manager-kube-controller-manager","title":"4. Controller Manager (<code>kube-controller-manager</code>)","text":"<ul> <li>Description: Runs multiple controllers that regulate the cluster\u2019s state by watching the API server and taking action to meet the desired state.</li> <li>Key Controllers:</li> <li>Node Controller: Manages node health and lifecycle.</li> <li>Replication Controller: Ensures the correct number of Pod replicas are running.</li> <li>Service Controller: Maintains network load balancers for services.</li> <li>Endpoint Controller: Updates Endpoints for services.</li> <li>Example: Ensures a Deployment with three replicas always has three Pods running.</li> </ul>"},{"location":"kubernetes/kubernetes_components/#5-cloud-controller-manager","title":"5. Cloud Controller Manager","text":"<ul> <li>Description: Integrates Kubernetes with cloud provider-specific APIs to manage resources like load balancers, storage, and networking.</li> <li>Key Controllers:</li> <li>Node Controller: Manages cloud-based node operations.</li> <li>Route Controller: Configures routes in the cloud for cluster networking.</li> <li>Service Controller: Manages external load balancers.</li> <li>Example: Creates a cloud load balancer for a Kubernetes Service of type <code>LoadBalancer</code>.</li> </ul>"},{"location":"kubernetes/kubernetes_components/#node-components","title":"Node Components","text":"<p>Node components run on each worker node and manage workloads, ensuring that containers operate as specified.</p>"},{"location":"kubernetes/kubernetes_components/#1-kubelet","title":"1. Kubelet","text":"<ul> <li>Description: An agent running on each node that communicates with the API server to ensure containers are running as instructed.</li> <li>Key Features:</li> <li>Manages Pods and their containers.</li> <li>Monitors Pod health and resource usage.</li> <li>Interacts with the container runtime (e.g., Docker, containerd).</li> <li>Example: Starts and stops containers as defined in a Pod spec.</li> </ul>"},{"location":"kubernetes/kubernetes_components/#2-kube-proxy","title":"2. Kube-Proxy","text":"<ul> <li>Description: A network proxy running on each node to manage networking for services.</li> <li>Key Features:</li> <li>Implements Kubernetes Services by forwarding traffic to the correct Pods.</li> <li>Supports protocols like TCP, UDP, and SCTP.</li> <li>Example: Routes external requests to the appropriate backend Pod in a Service.</li> </ul>"},{"location":"kubernetes/kubernetes_components/#3-container-runtime","title":"3. Container Runtime","text":"<ul> <li>Description: The software responsible for running containers on a node.</li> <li>Supported Runtimes:</li> <li>Docker (deprecated as of Kubernetes 1.20+).</li> <li>containerd.</li> <li>CRI-O.</li> <li>Any runtime that implements the Kubernetes Container Runtime Interface (CRI).</li> <li>Example: Pulls a container image from a registry and starts it.</li> </ul>"},{"location":"kubernetes/kubernetes_components/#add-ons","title":"Add-Ons","text":"<p>Add-ons provide additional functionality that is not part of the core Kubernetes components but is essential for a fully functional cluster.</p>"},{"location":"kubernetes/kubernetes_components/#1-coredns","title":"1. CoreDNS","text":"<ul> <li>Description: Provides DNS for Kubernetes services and Pods.</li> <li>Example: Resolves service names to IP addresses (e.g., <code>my-service.default.svc.cluster.local</code>).</li> </ul>"},{"location":"kubernetes/kubernetes_components/#2-dashboard","title":"2. Dashboard","text":"<ul> <li>Description: A web-based user interface for managing and monitoring the cluster.</li> <li>Example: Provides a visual representation of workloads, resources, and cluster status.</li> </ul>"},{"location":"kubernetes/kubernetes_components/#3-metrics-server","title":"3. Metrics Server","text":"<ul> <li>Description: Collects resource usage data (e.g., CPU, memory) for Pods and nodes.</li> <li>Example: Enables horizontal Pod autoscaling based on CPU usage.</li> </ul>"},{"location":"kubernetes/kubernetes_components/#4-ingress-controller","title":"4. Ingress Controller","text":"<ul> <li>Description: Manages HTTP and HTTPS routing to services within the cluster.</li> <li>Example: Routes external traffic to a service using a custom domain (e.g., <code>example.com</code>).</li> </ul>"},{"location":"kubernetes/kubernetes_components/#5-logging-and-monitoring-tools","title":"5. Logging and Monitoring Tools","text":"<ul> <li>Examples:</li> <li>Prometheus/Grafana: Collect and visualize metrics.</li> <li>ELK Stack (Elasticsearch, Logstash, Kibana): Aggregate and analyze logs.</li> </ul>"},{"location":"kubernetes/kubernetes_components/#interaction-of-components","title":"Interaction of Components","text":"<ol> <li>A user submits a request to the API server (e.g., <code>kubectl apply</code>).</li> <li>The API server validates the request and persists the desired state in etcd.</li> <li>The Scheduler assigns the Pod to an appropriate node.</li> <li>The Controller Manager ensures the desired state matches the actual state (e.g., launching Pods, scaling Deployments).</li> <li>The Kubelet on the assigned node pulls the container image, starts the container, and reports status to the API server.</li> <li>Kube-Proxy manages networking so traffic can reach the Pods.</li> </ol>"},{"location":"kubernetes/kubernetes_components/#conclusion","title":"Conclusion","text":"<p>These components work together to manage the lifecycle of applications, maintain the desired state, and ensure scalability and high availability in Kubernetes clusters. Understanding these components is essential for effectively deploying and managing containerized workloads.</p>"},{"location":"kubernetes/kubernetes_serverless_platforms/","title":"Kubernetes Serverless Platforms","text":""},{"location":"kubernetes/kubernetes_serverless_platforms/#answer","title":"Answer","text":"<p>Kubernetes serverless platforms are frameworks or tools that extend Kubernetes\u2019 capabilities to support serverless computing. These platforms enable developers to deploy and manage functions or applications that scale automatically based on demand, including scaling to zero when idle. The platforms abstract many of Kubernetes\u2019 complexities, allowing developers to focus on writing code instead of managing infrastructure.</p>"},{"location":"kubernetes/kubernetes_serverless_platforms/#key-features-of-kubernetes-serverless-platforms","title":"Key Features of Kubernetes Serverless Platforms:","text":"<ol> <li>Auto-Scaling: Automatically scales workloads based on demand.</li> <li>Event-Driven: Supports triggering workloads based on events like HTTP requests, Kafka messages, or scheduled tasks.</li> <li>Scale-to-Zero: Reduces costs by shutting down workloads when they\u2019re not in use.</li> <li>Portability: Most platforms work across different Kubernetes distributions, making them highly portable.</li> </ol> <p>Some of the popular Kubernetes serverless platforms:</p>"},{"location":"kubernetes/kubernetes_serverless_platforms/#1-knative","title":"1. Knative","text":"<p>Knative is an open-source Kubernetes-based platform designed for building, deploying, and managing serverless applications. It provides two core components: Knative Serving for deploying stateless services and Knative Eventing for building event-driven architectures.</p>"},{"location":"kubernetes/kubernetes_serverless_platforms/#pros","title":"Pros:","text":"<ul> <li>Fully integrates with Kubernetes, leveraging its native features.</li> <li>Supports advanced auto-scaling, including scale-to-zero.</li> <li>Flexible eventing model for complex workflows.</li> <li>Works with containerized workloads, not limited to functions.</li> </ul>"},{"location":"kubernetes/kubernetes_serverless_platforms/#cons","title":"Cons:","text":"<ul> <li>Can be complex to set up and manage.</li> <li>Requires a strong understanding of Kubernetes to use effectively.</li> <li>Resource-intensive for small-scale deployments.</li> </ul>"},{"location":"kubernetes/kubernetes_serverless_platforms/#2-openfaas","title":"2. OpenFaaS","text":"<p>OpenFaaS (Open Function as a Service) is a lightweight and developer-friendly serverless framework that runs on Kubernetes and Docker Swarm. It focuses on simplicity and portability, allowing developers to deploy functions in any language using templates.</p>"},{"location":"kubernetes/kubernetes_serverless_platforms/#pros_1","title":"Pros:","text":"<ul> <li>Easy to use with intuitive CLI tools and templates.</li> <li>Language-agnostic, supporting any runtime.</li> <li>Supports Kubernetes and Docker Swarm, making it versatile.</li> <li>Built-in Prometheus integration for monitoring.</li> </ul>"},{"location":"kubernetes/kubernetes_serverless_platforms/#cons_1","title":"Cons:","text":"<ul> <li>Limited eventing capabilities compared to Knative.</li> <li>Lacks native scale-to-zero support (requires external tools).</li> <li>Less suited for large-scale enterprise environments.</li> </ul>"},{"location":"kubernetes/kubernetes_serverless_platforms/#3-kubeless","title":"3. Kubeless","text":"<p>Kubeless is a Kubernetes-native serverless framework that uses Custom Resource Definitions (CRDs) to deploy and manage functions. It is lightweight and integrates closely with Kubernetes\u2019 architecture.</p>"},{"location":"kubernetes/kubernetes_serverless_platforms/#pros_2","title":"Pros:","text":"<ul> <li>Simple and lightweight; uses Kubernetes-native resources.</li> <li>Event triggers via Kafka, HTTP, or cron jobs.</li> <li>Minimal configuration required for basic use cases.</li> </ul>"},{"location":"kubernetes/kubernetes_serverless_platforms/#cons_2","title":"Cons:","text":"<ul> <li>Limited community support and slower development compared to other platforms.</li> <li>Lacks advanced features like scale-to-zero.</li> <li>Less extensible for complex workflows.</li> </ul>"},{"location":"kubernetes/kubernetes_serverless_platforms/#4-fission","title":"4. Fission","text":"<p>Fission is a fast, Kubernetes-native serverless framework optimized for low-latency function execution. It pre-warms environments to eliminate cold starts and supports multiple languages out of the box.</p>"},{"location":"kubernetes/kubernetes_serverless_platforms/#pros_3","title":"Pros:","text":"<ul> <li>Extremely fast execution with pre-warmed environments.</li> <li>Simple YAML-based configuration for functions.</li> <li>Supports event-driven triggers like HTTP, Kafka, and cron.</li> <li>Lightweight and easy to install.</li> </ul>"},{"location":"kubernetes/kubernetes_serverless_platforms/#cons_3","title":"Cons:","text":"<ul> <li>Limited scalability for large-scale, complex systems.</li> <li>Fewer integrations compared to Knative.</li> <li>Not as feature-rich for workflows and advanced eventing.</li> </ul>"},{"location":"kubernetes/kubernetes_serverless_platforms/#5-red-hat-openshift-serverless","title":"5. Red Hat OpenShift Serverless","text":"<p>Red Hat OpenShift Serverless is based on Knative and tailored for Red Hat\u2019s OpenShift Kubernetes platform. It offers enterprise-grade serverless capabilities with enhanced security and compliance features.</p>"},{"location":"kubernetes/kubernetes_serverless_platforms/#pros_4","title":"Pros:","text":"<ul> <li>Enterprise-ready with strong security and compliance.</li> <li>Seamless integration with Red Hat OpenShift.</li> <li>Full support for Knative features (Serving and Eventing).</li> </ul>"},{"location":"kubernetes/kubernetes_serverless_platforms/#cons_4","title":"Cons:","text":"<ul> <li>Requires OpenShift, limiting portability to non-OpenShift Kubernetes clusters.</li> <li>Higher cost due to the OpenShift licensing model.</li> <li>More complex setup compared to simpler frameworks like OpenFaaS.</li> </ul>"},{"location":"kubernetes/kubernetes_serverless_platforms/#6-apache-openwhisk-self-hosted","title":"6. Apache OpenWhisk (Self-Hosted)","text":"<p>Apache OpenWhisk is an open-source, distributed serverless platform that can run on Kubernetes. It supports event-driven workloads and provides a flexible runtime for functions.</p>"},{"location":"kubernetes/kubernetes_serverless_platforms/#pros_5","title":"Pros:","text":"<ul> <li>Highly extensible and customizable.</li> <li>Supports multiple event triggers, including HTTP and Kafka.</li> <li>Language-agnostic, supporting various runtimes.</li> </ul>"},{"location":"kubernetes/kubernetes_serverless_platforms/#cons_5","title":"Cons:","text":"<ul> <li>Complex setup and management.</li> <li>Resource-intensive for smaller environments.</li> <li>Limited built-in Kubernetes integrations compared to other platforms.</li> </ul>"},{"location":"kubernetes/kubernetes_serverless_platforms/#7-google-cloud-run-for-anthos","title":"7. Google Cloud Run for Anthos","text":"<p>Google Cloud Run for Anthos extends Knative\u2019s capabilities to hybrid Kubernetes environments. It enables serverless containers to run on Anthos, Google\u2019s hybrid and multi-cloud platform.</p>"},{"location":"kubernetes/kubernetes_serverless_platforms/#pros_6","title":"Pros:","text":"<ul> <li>Based on Knative, providing robust auto-scaling and event-driven capabilities.</li> <li>Seamless integration with Google Cloud services.</li> <li>Ideal for hybrid cloud environments.</li> </ul>"},{"location":"kubernetes/kubernetes_serverless_platforms/#cons_6","title":"Cons:","text":"<ul> <li>Requires Anthos, which adds complexity and cost.</li> <li>Less suitable for non-Google Cloud Kubernetes environments.</li> <li>Limited to containerized workloads.</li> </ul>"},{"location":"kubernetes/kubernetes_serverless_platforms/#conclusion","title":"Conclusion","text":"<p>Kubernetes serverless platforms provide powerful tools for running scalable, event-driven workloads. Choosing the right platform depends on your use case, environment, and skill level. For example:</p> <ul> <li>Use Knative if you need a feature-rich, Kubernetes-native solution with advanced eventing.</li> <li>Choose OpenFaaS or Fission for simplicity and fast deployments.</li> <li>Opt for Red Hat OpenShift Serverless or Google Cloud Run for Anthos for enterprise-grade hybrid cloud solutions.</li> </ul> <p>Understanding the strengths and weaknesses of each platform ensures you select the one that best aligns with your application\u2019s requirements and organizational goals.</p>"},{"location":"kubernetes/observability_tools/","title":"Observability Tools in Kubernetes","text":""},{"location":"kubernetes/observability_tools/#use-case-observability","title":"Use Case: Observability","text":"<p>Observability tools are vital for maintaining the health and performance of Kubernetes clusters. They enable operators to monitor system metrics, identify bottlenecks, and troubleshoot issues effectively. By providing both real-time and historical insights, these tools ensure that workloads remain optimized and reliable, even in dynamic cloud-native environments.</p>"},{"location":"kubernetes/observability_tools/#tools","title":"Tools:","text":""},{"location":"kubernetes/observability_tools/#1-thanos","title":"1. Thanos","text":"<ul> <li>Description: Thanos extends Prometheus by enabling long-term metrics storage, high availability, and cross-cluster queries. It aggregates data from multiple Prometheus instances, allowing operators to view metrics across clusters. With its support for object storage systems like S3 and Azure Blob, Thanos ensures scalable and cost-effective metrics retention.</li> <li>Best For: Large-scale environments requiring unified monitoring across clusters, long-term storage, and robust querying capabilities.</li> </ul>"},{"location":"kubernetes/observability_tools/#2-cortex","title":"2. Cortex","text":"<ul> <li>Description: Cortex is a multi-tenant, horizontally scalable backend for Prometheus designed for cloud-native observability. It enables advanced metrics management by offering isolation for different teams or projects, long-term storage in object stores, and seamless integration with tools like Grafana. Cortex is highly optimized for enterprises running Prometheus-as-a-service.</li> <li>Best For: Organizations needing centralized metrics storage and management with robust multi-tenancy and scalability to support large teams and complex workloads.</li> </ul>"},{"location":"kubernetes/pod_disruption_budget/","title":"PodDisruptionBudget (PDB) in Kubernetes","text":"<p>A PodDisruptionBudget (PDB) is a Kubernetes resource that helps ensure a certain number or percentage of Pods remain available during voluntary disruptions. These disruptions can include node maintenance, cluster scaling, or rolling updates.</p>"},{"location":"kubernetes/pod_disruption_budget/#purpose-of-poddisruptionbudget","title":"Purpose of PodDisruptionBudget","text":"<ul> <li>To protect application availability during planned events.</li> <li>To enforce a minimum number of Pods running or restrict the maximum number of Pods disrupted simultaneously.</li> <li>To balance the needs of system administrators and application reliability.</li> </ul>"},{"location":"kubernetes/pod_disruption_budget/#key-features","title":"Key Features","text":"<ol> <li> <p>Voluntary Disruptions:</p> </li> <li> <p>PDB applies only to voluntary disruptions, such as:</p> <ul> <li>Node draining for maintenance.</li> <li>Rolling updates.</li> <li>Scaling events.</li> </ul> </li> <li> <p>Minimum Availability:</p> </li> <li> <p>Ensures that a certain number of Pods remain available during disruptions.</p> </li> <li> <p>Maximum Disruption:</p> </li> <li> <p>Restricts the maximum number of Pods that can be disrupted simultaneously.</p> </li> <li> <p>Integration with Controllers:</p> </li> <li>Works with Deployments, StatefulSets, ReplicaSets, and other controllers.</li> </ol>"},{"location":"kubernetes/pod_disruption_budget/#how-poddisruptionbudget-works","title":"How PodDisruptionBudget Works","text":"<ul> <li> <p><code>minAvailable</code>:</p> </li> <li> <p>Specifies the minimum number of Pods that must remain available during disruptions.</p> </li> <li> <p><code>maxUnavailable</code>:</p> </li> <li> <p>Specifies the maximum number of Pods that can be disrupted simultaneously.</p> </li> <li> <p>Scope:</p> </li> <li>PDB is applied to a group of Pods matching the specified label selector.</li> </ul>"},{"location":"kubernetes/pod_disruption_budget/#example-poddisruptionbudget","title":"Example PodDisruptionBudget","text":""},{"location":"kubernetes/pod_disruption_budget/#1-minimum-available-pods","title":"1. Minimum Available Pods","text":"<p>This PDB ensures at least 2 Pods are always running during voluntary disruptions.</p> <pre><code>apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: my-app-pdb\n  namespace: my-namespace\nspec:\n  minAvailable: 2\n  selector:\n    matchLabels:\n      app: my-app\n</code></pre>"},{"location":"kubernetes/pod_disruption_budget/#2-maximum-unavailable-pods","title":"2. Maximum Unavailable Pods","text":"<p>This PDB ensures that no more than 1 Pod can be disrupted at any time.</p> <pre><code>apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: my-app-pdb\n  namespace: my-namespace\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app: my-app\n</code></pre>"},{"location":"kubernetes/pod_disruption_budget/#use-cases","title":"Use Cases","text":"<ol> <li> <p>High Availability:</p> </li> <li> <p>Ensures critical applications remain operational during cluster maintenance.</p> </li> <li> <p>Rolling Updates:</p> </li> <li> <p>Controls the pace of Pod evictions to prevent service downtime.</p> </li> <li> <p>Stateful Applications:</p> </li> <li>Protects databases or StatefulSets that require a specific number of Pods for consistency.</li> </ol>"},{"location":"kubernetes/pod_disruption_budget/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Plan for Downtime:</p> </li> <li> <p>Use <code>minAvailable</code> or <code>maxUnavailable</code> based on the application\u2019s availability requirements.</p> </li> <li> <p>Label Pods Consistently:</p> </li> <li> <p>Ensure Pods have appropriate labels to match the PDB\u2019s selector.</p> </li> <li> <p>Combine with Monitoring:</p> </li> <li> <p>Use monitoring tools to track PDB effectiveness during disruptions.</p> </li> <li> <p>Test Scenarios:</p> </li> <li>Simulate node drains and rolling updates to verify PDB behavior.</li> </ol>"},{"location":"kubernetes/pod_disruption_budget/#limitations","title":"Limitations","text":"<ol> <li> <p>Voluntary Disruptions Only:</p> </li> <li> <p>PDB does not apply to involuntary disruptions, such as crashes or node failures.</p> </li> <li> <p>No Guarantee of Scheduling:</p> </li> <li>PDB ensures Pods are not evicted below the threshold but does not guarantee new Pods can be scheduled.</li> </ol>"},{"location":"kubernetes/pod_disruption_budget/#conclusion","title":"Conclusion","text":"<p>PodDisruptionBudget is a vital tool in Kubernetes for ensuring application availability during planned events like maintenance or updates. By setting appropriate thresholds with <code>minAvailable</code> or <code>maxUnavailable</code>, you can balance operational flexibility with application reliability.</p>"},{"location":"kubernetes/pod_priority_and_preemption/","title":"Implementing Pod Priority and Preemption in Kubernetes","text":"<p>Pod Priority and Preemption is a feature in Kubernetes that allows you to assign different levels of importance to Pods. Higher-priority Pods can preempt (evict) lower-priority Pods to make room for critical workloads when cluster resources are scarce.</p>"},{"location":"kubernetes/pod_priority_and_preemption/#steps-to-implement-pod-priority-and-preemption","title":"Steps to Implement Pod Priority and Preemption","text":""},{"location":"kubernetes/pod_priority_and_preemption/#step-1-enable-priority-and-preemption","title":"Step 1: Enable Priority and Preemption","text":"<p>Pod Priority and Preemption are enabled by default in Kubernetes (v1.14+). Ensure it is not disabled in your cluster by checking the <code>--enable-admission-plugins</code> flag on the API server. The <code>Priority</code> admission plugin must be enabled.</p> <pre><code>kubectl get podsecuritypolicy\n# Verify the admission plugins include \"Priority\".\n</code></pre>"},{"location":"kubernetes/pod_priority_and_preemption/#step-2-create-priorityclasses","title":"Step 2: Create PriorityClasses","text":"<p>PriorityClasses define the priority level for Pods. A higher <code>value</code> indicates higher priority.</p>"},{"location":"kubernetes/pod_priority_and_preemption/#example-yaml-for-priorityclasses","title":"Example YAML for PriorityClasses:","text":"<pre><code>apiVersion: scheduling.k8s.io/v1\nkind: PriorityClass\nmetadata:\n  name: high-priority\nvalue: 1000\nglobalDefault: false\ndescription: \"This priority is for critical Pods.\"\n---\napiVersion: scheduling.k8s.io/v1\nkind: PriorityClass\nmetadata:\n  name: low-priority\nvalue: 500\nglobalDefault: false\ndescription: \"This priority is for less important Pods.\"\n</code></pre> <p>Apply the PriorityClasses:</p> <pre><code>kubectl apply -f priorityclasses.yaml\n</code></pre>"},{"location":"kubernetes/pod_priority_and_preemption/#step-3-assign-priority-to-pods","title":"Step 3: Assign Priority to Pods","text":"<p>Use the <code>priorityClassName</code> field in the Pod specification to assign a priority to your Pods.</p>"},{"location":"kubernetes/pod_priority_and_preemption/#example-high-priority-pod","title":"Example: High-Priority Pod:","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: high-priority-pod\n  namespace: default\nspec:\n  containers:\n    - name: nginx\n      image: nginx\n  priorityClassName: high-priority\n</code></pre>"},{"location":"kubernetes/pod_priority_and_preemption/#example-low-priority-pod","title":"Example: Low-Priority Pod:","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: low-priority-pod\n  namespace: default\nspec:\n  containers:\n    - name: nginx\n      image: nginx\n  priorityClassName: low-priority\n</code></pre> <p>Apply the Pod definitions:</p> <pre><code>kubectl apply -f high-priority-pod.yaml\nkubectl apply -f low-priority-pod.yaml\n</code></pre>"},{"location":"kubernetes/pod_priority_and_preemption/#step-4-test-preemption","title":"Step 4: Test Preemption","text":"<ol> <li>Simulate a resource-scarce scenario by scheduling multiple low-priority Pods to consume available resources.</li> <li>Schedule a high-priority Pod. Kubernetes will preempt (evict) the low-priority Pods if necessary to make room for the high-priority Pod.</li> </ol>"},{"location":"kubernetes/pod_priority_and_preemption/#verify-preemption","title":"Verify Preemption:","text":"<p>Check the status of the evicted Pods:</p> <pre><code>kubectl get pods -o wide\n</code></pre> <p>Evicted Pods will show a status of <code>Evicted</code> or <code>Pending</code>.</p>"},{"location":"kubernetes/pod_priority_and_preemption/#additional-considerations","title":"Additional Considerations","text":"<ol> <li> <p>Preemption Delay:</p> </li> <li> <p>Preemption is not immediate. Kubernetes waits for evicted Pods to terminate before scheduling high-priority Pods.</p> </li> <li> <p>Avoid Overuse of High Priority:</p> </li> <li> <p>Overusing high-priority Pods can lead to instability by preempting essential workloads.</p> </li> <li> <p>Graceful Eviction:</p> </li> <li> <p>Kubernetes respects the <code>terminationGracePeriodSeconds</code> of evicted Pods to allow graceful termination.</p> </li> <li> <p>Default Priority:</p> </li> <li>You can define a <code>globalDefault: true</code> PriorityClass, which will be used for Pods without an explicit <code>priorityClassName</code>.</li> </ol>"},{"location":"kubernetes/pod_priority_and_preemption/#benefits","title":"Benefits","text":"<ul> <li>Ensures critical workloads are prioritized during resource contention.</li> <li>Helps maintain cluster reliability by protecting important services.</li> </ul>"},{"location":"kubernetes/pod_priority_and_preemption/#use-cases","title":"Use Cases","text":"<ul> <li>Assigning higher priority to system Pods (e.g., DNS, monitoring).</li> <li>Ensuring critical workloads are scheduled even in overloaded clusters.</li> <li>Preempting non-essential workloads for disaster recovery operations.</li> </ul> <p>By carefully designing your PriorityClasses and assigning them appropriately, you can efficiently manage resource allocation in your Kubernetes cluster.</p>"},{"location":"kubernetes/pod_priority_and_preemption/#system-cluster-critical-priorityclass","title":"<code>system-cluster-critical</code> PriorityClass","text":"<p><code>system-cluster-critical</code> is a predefined PriorityClass in Kubernetes. It is used for system-critical Pods that are essential for the overall functionality of the cluster. This PriorityClass ensures that critical system Pods have the highest priority and can preempt less critical workloads to maintain cluster health.</p>"},{"location":"kubernetes/pod_priority_and_preemption/#key-features","title":"Key Features","text":"<ol> <li> <p>High Priority:</p> </li> <li> <p><code>system-cluster-critical</code> is one of the highest priority levels in Kubernetes, ensuring that critical system Pods can always run, even under resource contention.</p> </li> <li> <p>Reserved for System Pods:</p> </li> <li> <p>Intended for Pods required for cluster management, such as DNS, network plugins, or monitoring systems.</p> </li> <li> <p>Preemption:</p> </li> <li>Pods with this PriorityClass can preempt lower-priority Pods to free up resources.</li> </ol>"},{"location":"kubernetes/pod_security_admission/","title":"Pod Security Admission (PSA)","text":"<p>Pod Security Admission (PSA) is a Kubernetes feature that enforces security policies at the namespace level to control how Pods are created and managed based on predefined security standards. It is the successor to the deprecated Pod Security Policies (PSPs) and provides a simpler way to apply security controls.</p>"},{"location":"kubernetes/pod_security_admission/#purpose","title":"Purpose","text":"<ul> <li>To enforce security best practices for Kubernetes Pods.</li> <li>To prevent potentially unsafe Pod configurations (e.g., privilege escalation, use of host namespaces).</li> </ul>"},{"location":"kubernetes/pod_security_admission/#how-it-works","title":"How It Works","text":"<ul> <li>PSA evaluates Pod specifications during the admission phase (before the Pod is created) to ensure compliance with the security standards.</li> <li>Security policies are defined by labeling namespaces with one of three predefined security levels:</li> <li>Privileged: Minimal restrictions, suitable for trusted environments.</li> <li>Baseline: Basic restrictions to enforce reasonable security defaults.</li> <li>Restricted: Strong restrictions for high-security environments.</li> </ul>"},{"location":"kubernetes/pod_security_admission/#key-features","title":"Key Features","text":"<ol> <li>Namespace-Level Control:</li> <li>PSA applies policies based on namespace labels, making it simple to manage security across the cluster.</li> <li>Three Modes:</li> <li>Enforce: Rejects Pods that violate the policy.</li> <li>Audit: Logs violations but does not block Pod creation.</li> <li>Warn: Issues warnings to users creating non-compliant Pods.</li> </ol>"},{"location":"kubernetes/pod_security_admission/#example-namespace-labels","title":"Example Namespace Labels","text":"<pre><code>kubectl label namespace dev pod-security.kubernetes.io/enforce=restricted\nkubectl label namespace dev pod-security.kubernetes.io/audit=baseline\nkubectl label namespace dev pod-security.kubernetes.io/warn=privileged\n</code></pre>"},{"location":"kubernetes/submariner/","title":"Submariner","text":"<p>Submariner is a tool that facilitates network connectivity across multiple Kubernetes clusters. It provides secure and seamless communication between Pods and Services across different clusters, even if they are in separate networks.</p>"},{"location":"kubernetes/submariner/#key-features-of-submariner","title":"Key Features of Submariner","text":"<ol> <li> <p>Cross-Cluster Networking:</p> </li> <li> <p>Establishes network connectivity between Kubernetes clusters without requiring them to share the same network.</p> </li> <li> <p>Service Discovery:</p> </li> <li> <p>Enables Pods in one cluster to discover and communicate with Services in another cluster.</p> </li> <li> <p>Secure Communication:</p> </li> <li> <p>Uses IPsec or WireGuard for secure communication between clusters.</p> </li> <li> <p>Load Balancing:</p> </li> <li>Provides efficient load balancing for cross-cluster traffic.</li> </ol>"},{"location":"kubernetes/submariner/#use-cases","title":"Use Cases","text":"<ul> <li>Multi-cluster deployments where clusters are in different networks or regions.</li> <li>Enabling cross-cluster communication for hybrid or multi-cloud setups.</li> <li>Building multi-cluster service meshes for advanced traffic control.</li> </ul>"},{"location":"kubernetes/submariner/#how-it-works","title":"How It Works","text":"<ul> <li>Gateway Nodes: Submariner designates a gateway node in each cluster to handle inter-cluster traffic.</li> <li>Tunnel Creation: Secure tunnels are established between the gateway nodes of participating clusters.</li> <li>Routing: Submariner ensures that Pods and Services can route traffic seamlessly across clusters.</li> </ul>"},{"location":"kubernetes/submariner/#example-service-connectivity","title":"Example: Service Connectivity","text":"<p>A Pod in Cluster A can directly access a Service in Cluster B using the standard Service DNS name, such as <code>my-service.my-namespace.svc.cluster.local</code>.</p>"},{"location":"kubernetes/cka/cri/","title":"CRI","text":""},{"location":"kubernetes/cka/cri/#install-a-container-runtime-interface","title":"Install a container runtime interface","text":""},{"location":"kubernetes/cka/cri/#containerd","title":"Containerd","text":"<p>Install <pre><code>sudo apt update\nsudo apt install -y containerd\n</code></pre></p> <p>Setup <pre><code>sudo mkdir -p /etc/containerd\ncontainerd config default | sudo tee /etc/containerd/config.toml\n</code></pre></p> <p>Restart <pre><code>sudo systemctl restart containerd\nsudo systemctl enable containerd\n</code></pre></p> <p>Check if Kubernetes is using containerd <pre><code>ps aux | grep kubelet | grep container-runtime\n</code></pre></p> <p>If it\u2019s not using it, set it up: <pre><code>sudo systemctl enable --now containerd\n</code></pre> or: <pre><code>sudo mkdir -p /etc/systemd/system/kubelet.service.d\nsudo tee /etc/systemd/system/kubelet.service.d/10-kubeadm.conf &lt;&lt;EOF\n[Service]\nEnvironment=\"KUBELET_EXTRA_ARGS=--container-runtime=remote --container-runtime-endpoint=unix:///run/containerd/containerd.sock\"\nEOF\n</code></pre></p> <p>Check which CRI is in use: <pre><code>crictl info\n</code></pre></p>"},{"location":"kubernetes/cka/etcd/","title":"ETDC","text":"<p>Backup ETCD database:</p> <ol> <li> <p>Get certicates paths: <pre><code>kubectl get pod etcd-controlplane -n kube-system -o yaml\n</code></pre></p> </li> <li> <p>Create snapshot <pre><code>ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 --cacert=&lt;trusted-ca-file&gt; --cert=&lt;cert-file&gt; --key=&lt;key-file&gt; snapshot save &lt;backup-file-location&gt;\n</code></pre></p> </li> <li> <p>Restore snapshot <pre><code>ETCDCTL_API=3 etcdctl --data-dir &lt;data-dir-location&gt; snapshot restore &lt;backup-file-location&gt;\n</code></pre></p> </li> </ol>"},{"location":"kubernetes/cka/helm/","title":"Helm","text":"<p>Some useful commands:</p> <ol> <li> <p>Verify deployed release <pre><code>helm list\n</code></pre></p> </li> <li> <p>Get values used by a release <pre><code>helm get values &lt;release_name&gt; --all\n</code></pre></p> </li> <li> <p>Get available values of a chart <pre><code>helm show values &lt;chart&gt;\n</code></pre></p> </li> <li> <p>Upgrade release (using local chart) <pre><code>helm upgrade --install &lt;release_name&gt; &lt;chart_path&gt;\n</code></pre></p> </li> <li> <p>Upgrade and set custom values <pre><code>helm upgrade --install &lt;release_name&gt; &lt;chart_path&gt; --set key=value\nhelm upgrade --install &lt;release_name&gt; &lt;chart_path&gt; --values &lt;values_file_path&gt;\n</code></pre></p> </li> <li> <p>Add repo <pre><code>helm repo add &lt;repo_name&gt; &lt;repo_url&gt;\n</code></pre></p> </li> <li> <p>Install chart from repo <pre><code>helm install &lt;name&gt; &lt;chart&gt;\n</code></pre></p> </li> </ol>"},{"location":"kubernetes/cka/jsonpath/","title":"Jsonpath and custom-columns","text":""},{"location":"kubernetes/cka/jsonpath/#jsonpath","title":"Jsonpath","text":"<p>Print all pods names: <pre><code>kubectl get pods -o jsonpath='{.items[*].metadata.name}'\n</code></pre></p> <p>Loop over multiple objects with <code>range</code> <pre><code>kubectl get pods -o jsonpath='{range .items[*]}{.metadata.name}{\" - \"}{.status.phase}{\"\\n\"}{end}'\n</code></pre></p> <p>Filter with condition: <pre><code>kubectl get pods -o jsonpath='{.items[?(@.status.phase==\"Running\")].metadata.name}'\n</code></pre></p>"},{"location":"kubernetes/cka/jsonpath/#custom-columns","title":"Custom columns","text":"<p>Get name and status in a table like format: <pre><code>kubectl get pods -o custom-columns=\"NAME:.metadata.name,STATUS:.status.phase\"\n</code></pre></p>"},{"location":"kubernetes/cka/kubelet_issues/","title":"Kubelet Issues","text":"<p>Search logs: <pre><code>journalctl -u kubelet --no-pager\ncat /var/log/syslog | grep kubelet\n</code></pre></p> <p>Check status: <pre><code>systemctl status kubelet\n</code></pre></p> <p>Restart: <pre><code>systemctl daemon-reload\nsystemctl restart kubelet\n</code></pre></p> <p>Configuration files: <pre><code>/var/lib/kubelet/\n/etc/kubernetes/kubelet.conf\n/etc/systemd/system/kubelet.service.d/10-kubeadm.conf\n</code></pre></p>"},{"location":"kubernetes/cka/openssl/","title":"OpenSSL commands","text":""},{"location":"kubernetes/cka/openssl/#create-new-user","title":"Create new user","text":"<ol> <li> <p>Generate private key: <pre><code>openssl genrsa -out &lt;private_key_name&gt;.key 2048\n</code></pre></p> </li> <li> <p>Create Certificate Signing Request: <pre><code>openssl req -new -key &lt;private_key_name&gt;.key -subj \"/CN=&lt;user_name&gt;\" -out &lt;user_name&gt;.csr\n</code></pre></p> </li> <li> <p>Create a CSR <pre><code>apiVersion: certificates.k8s.io/v1\nkind: CertificateSigningRequest\nmetadata:\n  name: &lt;user_name&gt;\nspec:\n  groups:\n  - system:authenticated\n  request: $REQUEST\n  signerName: kubernetes.io/kube-apiserver-client\n  usages:\n  - client auth\n</code></pre></p> </li> </ol> <p><code>$REQUEST</code> should contains the encoded .csr file: <pre><code>cat &lt;user_name&gt;.csr | base64 | tr -d '\\n'\n</code></pre></p> <ol> <li> <p>Approve CSR: <pre><code>kubectl certificate approve &lt;user_name&gt;\n</code></pre></p> </li> <li> <p>Save client certificate: <pre><code>k get csr &lt;user_name&gt; -o jsonpath='{.status.certificate}' | base64 -d &gt; &lt;user_name&gt;.crt\n</code></pre></p> </li> <li> <p>Add user to kubeconfig <pre><code>k config set-credentials &lt;user_name&gt; --client-key=&lt;user_name&gt;.key --client-certificate=carlton.crt --embed-certs\n</code></pre></p> </li> </ol>"},{"location":"kubernetes/cka/openssl/#check-certificate-content","title":"Check certificate content","text":"<pre><code>openssl x509 -in &lt;certificate_file_name&gt;.crt -text -noout\n</code></pre>"},{"location":"kubernetes/cka/utilities/","title":"Utility commands","text":""},{"location":"kubernetes/cka/utilities/#move-file-from-node-to-node","title":"Move file from node to node","text":"<pre><code>scp /path/to/local-file user@remote-node:/path/to/destination/\n</code></pre>"},{"location":"kubernetes/cka/utilities/#write-logs-to-file","title":"Write logs to file","text":"<pre><code>tail -f /var/log/container.log\n</code></pre>"},{"location":"kubernetes/cka/vpa/","title":"Vertical Pod Autoscaler","text":"<p>Example of VPA manifest: <pre><code>apiVersion: autoscaling.k8s.io/v1\nkind: VerticalPodAutoscaler\nmetadata:\n  name: my-app-vpa\n  namespace: default\nspec:\n  targetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: my-app\n  updatePolicy:\n    updateMode: \"Auto\" # Options: \"Off\", \"Initial\", \"Auto\", \"Recreate\"\n  resourcePolicy:\n    containerPolicies:\n      - containerName: '*'\n        minAllowed:\n          cpu: 100m\n          memory: 128Mi\n        maxAllowed:\n          cpu: 1\n          memory: 1Gi\n        controlledResources: [\"cpu\", \"memory\"]\n</code></pre></p>"},{"location":"kubernetes/cks/admission_controller_config/","title":"Admission Controller Configuration","text":"<p>This document provides the necessary configuration files for setting up the ImagePolicyWebhook admission controller in Kubernetes. The ImagePolicyWebhook allows you to enforce policies on which container images can be used in your cluster by validating them against an external webhook service.</p>"},{"location":"kubernetes/cks/admission_controller_config/#1-admission-configuration-file","title":"1. Admission Configuration File","text":"<p>Create a file named <code>admission-configuration.yaml</code>:</p> <pre><code>apiVersion: apiserver.config.k8s.io/v1\nkind: AdmissionConfiguration\nplugins:\n- name: ImagePolicyWebhook\n  configuration:\n    imagePolicy:\n      kubeConfigFile: /etc/kubernetes/image-policy-webhook/kubeconfig.yaml\n      allowTTL: 50\n      denyTTL: 50\n      retryBackoff: 500\n      defaultAllow: false\n</code></pre> <p>Save this file to <code>/etc/kubernetes/admission-configuration.yaml</code> on all control plane nodes.</p>"},{"location":"kubernetes/cks/admission_controller_config/#2-webhook-kubeconfig-file","title":"2. Webhook KubeConfig File","text":"<p>Create a file named <code>kubeconfig.yaml</code> with your webhook service configuration:</p> <pre><code>apiVersion: v1\nkind: Config\n# clusters refers to the remote service\nclusters:\n- name: image-policy-webhook\n  cluster:\n    certificate-authority: /etc/kubernetes/image-policy-webhook/ca.crt\n    server: https://image-policy-webhook.example.com/image-policy\n# users refers to the API server's webhook configuration\nusers:\n- name: api-server\n  user:\n    client-certificate: /etc/kubernetes/image-policy-webhook/apiserver-client.crt\n    client-key: /etc/kubernetes/image-policy-webhook/apiserver-client.key\n# kubeconfig files require a context, current-context refers to the context to use\ncurrent-context: webhook\ncontexts:\n- context:\n    cluster: image-policy-webhook\n    user: api-server\n  name: webhook\n</code></pre> <p>Save this file to <code>/etc/kubernetes/image-policy-webhook/kubeconfig.yaml</code> on all control plane nodes.</p>"},{"location":"kubernetes/cks/admission_controller_config/#3-certificate-setup","title":"3. Certificate Setup","text":"<p>You\u2019ll need to set up TLS certificates for secure communication between the API server and your webhook service:</p> <ol> <li> <p>Create a directory for certificates: <pre><code>mkdir -p /etc/kubernetes/image-policy-webhook\n</code></pre></p> </li> <li> <p>Generate a CA certificate (if you don\u2019t have one already): <pre><code>openssl genrsa -out /etc/kubernetes/image-policy-webhook/ca.key 2048\nopenssl req -x509 -new -nodes -key /etc/kubernetes/image-policy-webhook/ca.key \\\n  -subj \"/CN=image-policy-webhook-ca\" \\\n  -days 3650 -out /etc/kubernetes/image-policy-webhook/ca.crt\n</code></pre></p> </li> <li> <p>Generate client certificates for the API server: <pre><code>openssl genrsa -out /etc/kubernetes/image-policy-webhook/apiserver-client.key 2048\nopenssl req -new -key /etc/kubernetes/image-policy-webhook/apiserver-client.key \\\n  -subj \"/CN=api-server\" \\\n  -out /etc/kubernetes/image-policy-webhook/apiserver-client.csr\nopenssl x509 -req -in /etc/kubernetes/image-policy-webhook/apiserver-client.csr \\\n  -CA /etc/kubernetes/image-policy-webhook/ca.crt \\\n  -CAkey /etc/kubernetes/image-policy-webhook/ca.key \\\n  -CAcreateserial \\\n  -days 3650 \\\n  -out /etc/kubernetes/image-policy-webhook/apiserver-client.crt\n</code></pre></p> </li> <li> <p>Set appropriate permissions: <pre><code>chmod 600 /etc/kubernetes/image-policy-webhook/*.key\n</code></pre></p> </li> </ol>"},{"location":"kubernetes/cks/admission_controller_config/#4-api-server-configuration","title":"4. API Server Configuration","text":"<p>Enable the ImagePolicyWebhook admission controller in your API server configuration:</p> <p>For kubeadm installations, edit <code>/etc/kubernetes/manifests/kube-apiserver.yaml</code>:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-apiserver\n  namespace: kube-system\nspec:\n  containers:\n  - name: kube-apiserver\n    image: k8s.gcr.io/kube-apiserver:v1.23.0\n    command:\n    - kube-apiserver\n    - --enable-admission-plugins=NodeRestriction,ImagePolicyWebhook\n    - --admission-control-config-file=/etc/kubernetes/admission-configuration.yaml\n    # Other API server flags here...\n    volumeMounts:\n    - mountPath: /etc/kubernetes/admission-configuration.yaml\n      name: admission-configuration\n      readOnly: true\n    - mountPath: /etc/kubernetes/image-policy-webhook\n      name: image-policy-webhook\n      readOnly: true\n  volumes:\n  - hostPath:\n      path: /etc/kubernetes/admission-configuration.yaml\n      type: File\n    name: admission-configuration\n  - hostPath:\n      path: /etc/kubernetes/image-policy-webhook\n      type: Directory\n    name: image-policy-webhook\n</code></pre> <p>For non-kubeadm installations, update the API server flags in the appropriate systemd service file:</p> <pre><code>--enable-admission-plugins=NodeRestriction,ImagePolicyWebhook\n--admission-control-config-file=/etc/kubernetes/admission-configuration.yaml\n</code></pre>"},{"location":"kubernetes/cks/admission_controller_config/#5-webhook-service-implementation","title":"5. Webhook Service Implementation","text":"<p>Your webhook service should implement the admission review API and respond with an admission review response. Here\u2019s a sample response format from your webhook service:</p> <pre><code>{\n  \"apiVersion\": \"imagepolicy.k8s.io/v1alpha1\",\n  \"kind\": \"ImageReview\",\n  \"status\": {\n    \"allowed\": true,\n    \"reason\": \"Image is from an allowed registry\"\n  }\n}\n</code></pre> <p>Or to deny an image:</p> <pre><code>{\n  \"apiVersion\": \"imagepolicy.k8s.io/v1alpha1\",\n  \"kind\": \"ImageReview\",\n  \"status\": {\n    \"allowed\": false,\n    \"reason\": \"Image from untrusted registry\"\n  }\n}\n</code></pre>"},{"location":"kubernetes/cks/admission_controller_config/#6-understanding-configuration-options","title":"6. Understanding Configuration Options","text":"<p>The ImagePolicyWebhook configuration has several important options:</p> <ul> <li>kubeConfigFile: Path to the kubeconfig file for connecting to the webhook</li> <li>allowTTL: Duration in seconds to cache \u2018allow\u2019 responses</li> <li>denyTTL: Duration in seconds to cache \u2018deny\u2019 responses</li> <li>retryBackoff: Duration in milliseconds to wait between retries</li> <li>defaultAllow: Whether to allow all images if the webhook service is unavailable</li> <li><code>true</code>: All images are allowed when the webhook is unavailable</li> <li><code>false</code>: All images are denied when the webhook is unavailable (more secure)</li> </ul>"},{"location":"kubernetes/cks/apparmor/","title":"AppArmor Profile Management","text":"<p>AppArmor is a Linux Security Module that allows system administrators to restrict programs\u2019 capabilities with per-program profiles.</p>"},{"location":"kubernetes/cks/apparmor/#verifying-apparmor-status","title":"Verifying AppArmor Status","text":"<p>After installation, verify AppArmor is running:</p> <pre><code>sudo aa-status\n</code></pre>"},{"location":"kubernetes/cks/apparmor/#understanding-apparmor-profiles","title":"Understanding AppArmor Profiles","text":"<p>AppArmor profiles define what system resources a program can access and what actions it can perform. Profiles can be in one of several modes:</p> <ul> <li>enforce: Restricts the program according to the profile and logs violations</li> <li>complain: Does not restrict the program, but logs actions that would be prevented in enforce mode</li> <li>disabled: The profile is loaded but not applied</li> </ul> <p>Profiles are stored in <code>/etc/apparmor.d/</code> and have a specific syntax for defining permissions.</p>"},{"location":"kubernetes/cks/apparmor/#creating-apparmor-profiles","title":"Creating AppArmor Profiles","text":""},{"location":"kubernetes/cks/apparmor/#method-1-using-aa-genprof-recommended-for-beginners","title":"Method 1: Using aa-genprof (Recommended for Beginners)","text":"<ol> <li>Start the profile generation tool:</li> </ol> <pre><code>sudo aa-genprof /path/to/application\n</code></pre> <ol> <li> <p>Run the application to generate typical usage patterns.</p> </li> <li> <p>When done, press \u2018S\u2019 to save the profile.</p> </li> </ol>"},{"location":"kubernetes/cks/apparmor/#method-2-creating-a-profile-manually","title":"Method 2: Creating a Profile Manually","text":"<ol> <li>Create a new file in <code>/etc/apparmor.d/</code> named after your application:</li> </ol> <pre><code>sudo nano /etc/apparmor.d/my.application\n</code></pre> <ol> <li>Add the profile content. Here\u2019s a basic example:</li> </ol> <pre><code>#include &lt;tunables/global&gt;\n\nprofile my.application /path/to/application {\n  #include &lt;abstractions/base&gt;\n\n  # Allow basic functionality\n  /path/to/application mr,\n  /usr/lib/** mr,\n  /lib/** mr,\n\n  # Allow reading of specific files\n  /etc/my-app/** r,\n\n  # Allow writing to specific directories\n  /var/log/my-app/** w,\n}\n</code></pre>"},{"location":"kubernetes/cks/apparmor/#method-3-using-aa-logprof-to-generate-from-logs","title":"Method 3: Using aa-logprof to Generate from Logs","text":"<ol> <li>Set an existing profile to complain mode:</li> </ol> <pre><code>sudo aa-complain /path/to/application\n</code></pre> <ol> <li> <p>Run the application to generate logs.</p> </li> <li> <p>Use aa-logprof to analyze logs and update the profile:</p> </li> </ol> <pre><code>sudo aa-logprof\n</code></pre>"},{"location":"kubernetes/cks/apparmor/#loading-and-enabling-profiles","title":"Loading and Enabling Profiles","text":""},{"location":"kubernetes/cks/apparmor/#loading-a-new-profile","title":"Loading a New Profile","text":"<p>After creating a profile, load it with:</p> <pre><code>sudo apparmor_parser -r /etc/apparmor.d/my.application\n</code></pre>"},{"location":"kubernetes/cks/apparmor/#setting-profile-mode","title":"Setting Profile Mode","text":"<p>Set a profile to enforce mode:</p> <pre><code>sudo aa-enforce /path/to/application\n</code></pre> <p>Set a profile to complain mode:</p> <pre><code>sudo aa-complain /path/to/application\n</code></pre> <p>Disable a profile:</p> <pre><code>sudo ln -s /etc/apparmor.d/my.application /etc/apparmor.d/disable/\nsudo apparmor_parser -R /etc/apparmor.d/my.application\n</code></pre>"},{"location":"kubernetes/cks/apparmor/#managing-profiles","title":"Managing Profiles","text":""},{"location":"kubernetes/cks/apparmor/#listing-profiles","title":"Listing Profiles","text":"<p>List all profiles and their status:</p> <pre><code>sudo aa-status\n</code></pre>"},{"location":"kubernetes/cks/apparmor/#using-apparmor-with-containers","title":"Using AppArmor with Containers","text":"<p>For Kubernetes, you need to:</p> <ol> <li> <p>Create a profile on all worker nodes</p> </li> <li> <p>Load the profile on all nodes:</p> </li> </ol> <pre><code>sudo apparmor_parser -r /etc/apparmor.d/k8s-myprofile\n</code></pre> <ol> <li>Apply the AppArmor profile to your Pod/container using one of two methods:</li> </ol>"},{"location":"kubernetes/cks/apparmor/#method-1-using-annotations-beta-api","title":"Method 1: Using Annotations (Beta API)","text":"<p>The original beta implementation uses annotations:</p> <pre><code>metadata:\n  annotations:\n    container.apparmor.security.beta.kubernetes.io/container-name: localhost/k8s-myprofile\n</code></pre> <p>Where <code>container-name</code> is the name of your container, and <code>k8s-myprofile</code> is your AppArmor profile name.</p>"},{"location":"kubernetes/cks/apparmor/#method-2-using-securitycontext-preferred","title":"Method 2: Using securityContext (Preferred)","text":"<p>The newer, more structured approach uses securityContext:</p> <pre><code># At the container level\nspec:\n  containers:\n  - name: my-container\n    securityContext:\n      appArmorProfile:\n        type: Localhost\n        localhostProfile: k8s-myprofile\n\n# OR at the pod level\nspec:\n  securityContext:\n    appArmorProfile:\n      type: Localhost\n      localhostProfile: k8s-myprofile\n</code></pre> <p>The securityContext approach is recommended for new deployments as it follows Kubernetes conventions for security features and provides better validation.</p>"},{"location":"kubernetes/cks/audit_logging/","title":"Audit Logging","text":"<p>Audit logging in Kubernetes is crucial for security monitoring and compliance. This guide explains how to configure the API server to enable comprehensive audit logging.</p>"},{"location":"kubernetes/cks/audit_logging/#create-an-audit-policy-file","title":"Create an Audit Policy File","text":"<p>First, create an audit policy file that defines what events should be recorded:</p> <pre><code>mkdir -p /etc/kubernetes/audit\n</code></pre> <p>Create the audit policy file at <code>/etc/kubernetes/audit/policy.yaml</code>:</p> <pre><code>apiVersion: audit.k8s.io/v1\nkind: Policy\nrules:\n  # Log pod changes at RequestResponse level\n  - level: RequestResponse\n    resources:\n    - group: \"\"\n      resources: [\"pods\"]\n\n  # Log persistent volume changes\n  - level: RequestResponse\n    resources:\n    - group: \"\"\n      resources: [\"persistentvolumes\"]\n      verbs: [\"create\", \"delete\", \"update\"]\n\n  # Log auth at Metadata level\n  - level: Metadata\n    resources:\n    - group: \"authentication.k8s.io\"\n      resources: [\"*\"]\n\n  # Log all other resources at the Metadata level\n  - level: Metadata\n    resources:\n    - group: \"\"\n      resources: [\"*\"]\n    - group: \"apps\"\n      resources: [\"*\"]\n    - group: \"rbac.authorization.k8s.io\"\n      resources: [\"*\"]\n\n  # A catch-all rule to log all other events at the Metadata level\n  - level: Metadata\n    omitStages:\n      - \"RequestReceived\"\n</code></pre>"},{"location":"kubernetes/cks/audit_logging/#configure-the-api-server","title":"Configure the API Server","text":"<pre><code>nano /etc/kubernetes/manifests/kube-apiserver.yaml\n</code></pre> <p>Add the audit policy and log path parameters:</p> <pre><code>spec:\n  containers:\n  - command:\n    - kube-apiserver\n    - --audit-policy-file=/etc/kubernetes/audit/policy.yaml\n    - --audit-log-path=/var/log/kubernetes/audit/audit.log\n    - --audit-log-maxage=30\n    - --audit-log-maxbackup=10\n    - --audit-log-maxsize=100\n    # ... other existing parameters\n    volumeMounts:\n    - mountPath: /etc/kubernetes/audit\n      name: audit-config\n      readOnly: true\n    - mountPath: /var/log/kubernetes/audit\n      name: audit-log\n  volumes:\n  - hostPath:\n      path: /etc/kubernetes/audit\n      type: DirectoryOrCreate\n    name: audit-config\n  - hostPath:\n      path: /var/log/kubernetes/audit\n      type: DirectoryOrCreate\n    name: audit-log\n</code></pre>"},{"location":"kubernetes/cks/audit_logging/#audit-log-levels","title":"Audit Log Levels","text":"<p>The policy file uses these audit levels:</p> <ul> <li>None: Don\u2019t log events matching this rule</li> <li>Metadata: Log request metadata but not request or response body</li> <li>Request: Log event metadata and request body</li> <li>RequestResponse: Log event metadata, request and response bodies</li> </ul>"},{"location":"kubernetes/cks/audit_logging/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"kubernetes/cks/audit_logging/#configure-log-backend-format","title":"Configure Log Backend Format","text":"<p>For JSON format (better for processing):</p> <pre><code>--audit-log-format=json\n</code></pre>"},{"location":"kubernetes/cks/audit_logging/#webhook-backend","title":"Webhook Backend","text":"<p>To send audit logs to an external webhook:</p> <pre><code>--audit-webhook-config-file=/etc/kubernetes/audit/webhook-config.yaml\n--audit-webhook-batch-max-size=10000\n--audit-webhook-batch-max-wait=5s\n</code></pre> <p>Example webhook configuration:</p> <pre><code>apiVersion: v1\nkind: Config\nclusters:\n- name: audit-webhook\n  cluster:\n    server: https://audit.example.com/webhook\ncontexts:\n- context:\n    cluster: audit-webhook\n    user: \"\"\n  name: default-context\ncurrent-context: default-context\npreferences: {}\nusers: []\n</code></pre>"},{"location":"kubernetes/cks/cert_auth/","title":"Create User Certificate Authentication","text":"<p>How to create and use client certificates for authenticating users to a Kubernetes cluster. Certificate-based authentication is one of the standard methods for controlling access to your Kubernetes API server.</p>"},{"location":"kubernetes/cks/cert_auth/#prerequisites","title":"Prerequisites","text":"<ul> <li>Access to a running Kubernetes cluster</li> <li><code>kubectl</code> command-line tool installed</li> <li><code>openssl</code> command-line tool installed</li> <li>Admin access to the Kubernetes cluster (to approve certificate signing requests)</li> </ul>"},{"location":"kubernetes/cks/cert_auth/#step-1-create-a-private-key-for-the-user","title":"Step 1: Create a Private Key for the User","text":"<p>First, create a private key for the user using OpenSSL:</p> <pre><code>openssl genrsa -out jane.key 2048\n</code></pre> <p>This command generates a 2048-bit RSA key and saves it to a file named <code>jane.key</code>.</p>"},{"location":"kubernetes/cks/cert_auth/#step-2-create-a-certificate-signing-request-csr","title":"Step 2: Create a Certificate Signing Request (CSR)","text":"<p>Create a certificate signing request using the private key:</p> <pre><code>openssl req -new -key jane.key -out jane.csr -subj \"/CN=jane/O=engineering\"\n</code></pre> <p>In this command: - <code>/CN=jane</code> specifies the username - <code>/O=engineering</code> specifies the group the user belongs to (you can specify multiple groups using multiple <code>/O=</code> entries)</p>"},{"location":"kubernetes/cks/cert_auth/#step-3-encode-the-csr-in-base64","title":"Step 3: Encode the CSR in Base64","text":"<p>Encode the CSR file in base64 format:</p> <pre><code>cat jane.csr | base64 | tr -d '\\n'\n</code></pre> <p>Copy the output for use in the next step.</p>"},{"location":"kubernetes/cks/cert_auth/#step-4-create-a-certificatesigningrequest-object-in-kubernetes","title":"Step 4: Create a CertificateSigningRequest Object in Kubernetes","text":"<p>Create a file named <code>jane-csr.yaml</code> with the following content:</p> <pre><code>apiVersion: certificates.k8s.io/v1\nkind: CertificateSigningRequest\nmetadata:\n  name: jane\nspec:\n  request: &lt;base64-encoded-csr&gt;\n  signerName: kubernetes.io/kube-apiserver-client\n  expirationSeconds: 86400  # 24 hours\n  usages:\n  - client auth\n</code></pre> <p>Replace <code>&lt;base64-encoded-csr&gt;</code> with the output from Step 3.</p> <p>Apply this configuration to your cluster:</p> <pre><code>kubectl apply -f jane-csr.yaml\n</code></pre>"},{"location":"kubernetes/cks/cert_auth/#step-5-approve-the-certificate-signing-request","title":"Step 5: Approve the Certificate Signing Request","text":"<p>As a cluster administrator, approve the CSR:</p> <pre><code>kubectl certificate approve jane\n</code></pre>"},{"location":"kubernetes/cks/cert_auth/#step-6-retrieve-the-signed-certificate","title":"Step 6: Retrieve the Signed Certificate","text":"<p>Retrieve the approved certificate:</p> <pre><code>kubectl get csr jane -o jsonpath='{.status.certificate}' | base64 --decode &gt; jane.crt\n</code></pre> <p>This command extracts the signed certificate from the CSR object and decodes it into a file named <code>jane.crt</code>.</p>"},{"location":"kubernetes/cks/cert_auth/#step-7-create-a-kubeconfig-file-for-the-user","title":"Step 7: Create a kubeconfig File for the User","text":"<p>Now you need to create a kubeconfig file that the user can use for authentication:</p> <pre><code>kubectl config set-credentials jane --client-certificate=&lt;client-cert&gt; --client-key=&lt;client-key&gt; --embed-certs=true\nkubectl config set-context &lt;context-name&gt; --cluster=&lt;cluster-name&gt; --user=jane\nkubectl config use-context jane@&lt;cluster-name&gt;\n</code></pre>"},{"location":"kubernetes/cks/cert_auth/#step-8-set-appropriate-permissions-with-rbac","title":"Step 8: Set Appropriate Permissions with RBAC","text":"<p>The user now has a valid certificate, but they need to be granted permissions to perform actions in the cluster. Create a Role or ClusterRole and a corresponding RoleBinding or ClusterRoleBinding:</p> <p>Example RoleBinding (for namespace-specific access):</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: jane-engineering-rolebinding\n  namespace: development\nsubjects:\n- kind: User\n  name: jane\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: Role\n  name: developer\n  apiGroup: rbac.authorization.k8s.io\n</code></pre> <p>Example ClusterRoleBinding (for cluster-wide access):</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: jane-engineering-clusterrolebinding\nsubjects:\n- kind: User\n  name: jane\n  apiGroup: rbac.authorization.k8s.io\n- kind: Group\n  name: engineering\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: ClusterRole\n  name: view\n  apiGroup: rbac.authorization.k8s.io\n</code></pre> <p>Apply the binding using kubectl:</p> <pre><code>kubectl apply -f jane-rolebinding.yaml\n</code></pre>"},{"location":"kubernetes/cks/cilium/","title":"Cilium","text":"<p>Cilium is an open-source software for providing, securing, and observing network connectivity between container workloads. It\u2019s designed specifically for Kubernetes and leverages a powerful Linux kernel technology called eBPF (extended Berkeley Packet Filter) to deliver its features with high performance and minimal overhead.</p> <p>Cilium operates at Layer \u00be (IP/TCP) as well as Layer 7 (HTTP, gRPC, Kafka), allowing it to secure modern API-driven microservices while also supporting traditional network security approaches.</p>"},{"location":"kubernetes/cks/cilium/#core-concepts","title":"Core Concepts","text":""},{"location":"kubernetes/cks/cilium/#ebpf","title":"eBPF","text":"<p>eBPF is a revolutionary technology that allows programs to run in the Linux kernel without changing kernel source code or loading kernel modules. Cilium leverages eBPF to:</p> <ul> <li>Intercept and control network traffic at the kernel level</li> <li>Apply security policies with minimal performance overhead</li> <li>Provide deep visibility into network and application behavior</li> <li>Dynamically program network flows without service disruption</li> </ul> <p>Unlike traditional networking solutions that use iptables, eBPF programs are JIT-compiled and highly efficient, making Cilium particularly well-suited for large-scale environments.</p>"},{"location":"kubernetes/cks/cilium/#identity-based-security","title":"Identity-Based Security","text":"<p>Cilium introduces the concept of security identities, which are derived from Kubernetes labels. This enables:</p> <ul> <li>Security policies based on service identities rather than IP addresses</li> <li>Persistent security even when pods are rescheduled to different nodes</li> <li>Simplified policy management that aligns with Kubernetes native concepts</li> </ul>"},{"location":"kubernetes/cks/cilium/#network-policy","title":"Network Policy","text":"<p>Cilium extends the Kubernetes NetworkPolicy API with CiliumNetworkPolicy, which adds:</p> <ul> <li>Layer 7 (application protocol) visibility and filtering</li> <li>Support for cluster-wide policies with ClusterwideCiliumNetworkPolicy</li> <li>FQDN/DNS based filtering for external services</li> <li>Deny policies and more advanced rule semantics</li> </ul>"},{"location":"kubernetes/cks/cilium/#key-features","title":"Key Features","text":""},{"location":"kubernetes/cks/cilium/#networking","title":"Networking","text":"<ul> <li>CNI Plugin: Implements the Container Network Interface for Kubernetes</li> <li>Multi-cluster Routing: Connect multiple Kubernetes clusters</li> <li>IPv4/IPv6 Support: Dual-stack networking capabilities</li> <li>VXLAN, Geneve, or Direct Routing: Flexible overlay or native routing options</li> <li>Bandwidth Management: QoS and rate limiting capabilities</li> </ul>"},{"location":"kubernetes/cks/cilium/#security","title":"Security","text":"<ul> <li>Network Policies: Layer 3-4 (IP/ports) filtering</li> <li>Application Policies: Layer 7 filtering for HTTP, gRPC, Kafka, etc.</li> <li>Transparent Encryption: IPsec or WireGuard for node-to-node traffic</li> <li>Service Authorization: Control access to services</li> <li>DNS Security: Filter outbound connections based on DNS names</li> </ul>"},{"location":"kubernetes/cks/cilium/#observability","title":"Observability","text":"<ul> <li>Hubble: Dedicated observability platform built into Cilium</li> <li>Flow Logs: Detailed network flow information</li> <li>Service Maps: Visual representation of service dependencies</li> <li>Policy Verdicts: See which policies accept or deny connections</li> <li>Metrics: Prometheus integration for monitoring</li> </ul>"},{"location":"kubernetes/cks/cilium/#load-balancing","title":"Load Balancing","text":"<ul> <li>Kubernetes Services: Implementation of kube-proxy functionality</li> <li>Direct Server Return (DSR): Optimized load balancing path</li> <li>Session Affinity: Consistent hashing for stable connections</li> <li>Global Services: Services spanning multiple clusters</li> <li>XDP Acceleration: High-performance packet processing</li> </ul>"},{"location":"kubernetes/cks/cilium/#basic-configuration","title":"Basic Configuration","text":""},{"location":"kubernetes/cks/cilium/#network-policies","title":"Network Policies","text":"<p>Cilium extends Kubernetes NetworkPolicy with additional features. Here\u2019s an example of a basic CiliumNetworkPolicy:</p> <pre><code>apiVersion: \"cilium.io/v2\"\nkind: CiliumNetworkPolicy\nmetadata:\n  name: \"allow-web-from-frontend\"\nspec:\n  endpointSelector:\n    matchLabels:\n      app: web\n  ingress:\n  - fromEndpoints:\n    - matchLabels:\n        app: frontend\n    toPorts:\n    - ports:\n      - port: \"80\"\n        protocol: TCP\n      rules:\n        http:\n        - method: \"GET\"\n          path: \"/api/v1/.*\"\n</code></pre> <p>This policy allows frontend pods to make HTTP GET requests to web pods on port 80, specifically for paths matching <code>/api/v1/.*</code>.</p>"},{"location":"kubernetes/cks/cilium/#service-mesh","title":"Service Mesh","text":"<p>Cilium can be used as a service mesh alternative by enabling the following features:</p> <pre><code>helm upgrade cilium cilium/cilium --namespace kube-system \\\n  --reuse-values \\\n  --set hubble.enabled=true \\\n  --set hubble.metrics.enabled=\"{dns,drop,tcp,flow,port-distribution,icmp,http}\"\n</code></pre> <p>This provides many service mesh features without a sidecar proxy, including: - Service-to-service communication security - Traffic monitoring and metrics - L7 visibility</p>"},{"location":"kubernetes/cks/cilium/#cluster-mesh","title":"Cluster Mesh","text":"<p>To connect multiple Kubernetes clusters with Cilium:</p> <ol> <li>Enable Cluster Mesh on each cluster:</li> </ol> <pre><code>cilium clustermesh enable\n</code></pre> <ol> <li>Connect the clusters:</li> </ol> <pre><code>cilium clustermesh connect --destination-context=cluster2\n</code></pre>"},{"location":"kubernetes/cks/cilium/#hubble-observability-platform","title":"Hubble: Observability Platform","text":""},{"location":"kubernetes/cks/cilium/#installing-hubble","title":"Installing Hubble","text":"<p>Hubble is Cilium\u2019s observability platform:</p> <pre><code># Enable Hubble with UI\ncilium hubble enable --ui\n\n# Verify Hubble is properly installed\ncilium hubble status\n</code></pre>"},{"location":"kubernetes/cks/cilium/#using-hubble-ui","title":"Using Hubble UI","text":"<p>Access the Hubble UI by port-forwarding:</p> <pre><code>cilium hubble ui\n</code></pre> <p>This will open a browser window with the Hubble UI, displaying: - Service dependency map - Real-time network flows - HTTP, DNS, and TCP metrics - Detailed flow information</p>"},{"location":"kubernetes/cks/cilium/#hubble-cli","title":"Hubble CLI","text":"<p>Hubble CLI provides command-line access to observability data:</p> <pre><code># Install Hubble CLI\nexport HUBBLE_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/hubble/master/stable.txt)\ncurl -L --remote-name-all https://github.com/cilium/hubble/releases/download/$HUBBLE_VERSION/hubble-linux-amd64.tar.gz\nsudo tar xzvfC hubble-linux-amd64.tar.gz /usr/local/bin\n\n# Set up Hubble CLI\ncilium hubble port-forward&amp;\n\n# Observe flows in real-time\nhubble observe --follow\n\n# Filter flows for specific pods\nhubble observe --pod frontend\n\n# Show HTTP metrics\nhubble observe --protocol http\n</code></pre>"},{"location":"kubernetes/cks/cilium/#advanced-features","title":"Advanced Features","text":""},{"location":"kubernetes/cks/cilium/#transparent-encryption","title":"Transparent Encryption","text":"<p>Enable transparent encryption between nodes using IPsec or WireGuard:</p> <pre><code># Using Helm with IPsec\nhelm upgrade cilium cilium/cilium --namespace kube-system \\\n  --reuse-values \\\n  --set encryption.enabled=true \\\n  --set encryption.type=ipsec\n\n# Using Cilium CLI with WireGuard\ncilium config set encryption.enabled=true\ncilium config set encryption.type=wireguard\n</code></pre>"},{"location":"kubernetes/cks/cilium/#host-firewall","title":"Host Firewall","text":"<p>Protect the Kubernetes nodes themselves with Cilium\u2019s host firewall:</p> <pre><code>helm upgrade cilium cilium/cilium --namespace kube-system \\\n  --reuse-values \\\n  --set hostFirewall.enabled=true\n</code></pre> <p>Example policy to protect hosts:</p> <pre><code>apiVersion: \"cilium.io/v2\"\nkind: CiliumClusterwideNetworkPolicy\nmetadata:\n  name: \"host-policy\"\nspec:\n  nodeSelector:\n    matchLabels:\n      node-role.kubernetes.io/control-plane: \"\"\n  ingress:\n  - fromEntities:\n    - cluster\n    toPorts:\n    - ports:\n      - port: \"6443\"\n        protocol: TCP\n</code></pre>"},{"location":"kubernetes/cks/cilium/#kubernetes-without-kube-proxy","title":"Kubernetes Without kube-proxy","text":"<p>Cilium can replace kube-proxy for better performance:</p> <pre><code>helm upgrade cilium cilium/cilium --namespace kube-system \\\n  --reuse-values \\\n  --set kubeProxyReplacement=strict \\\n  --set k8sServiceHost=&lt;API_SERVER_IP&gt; \\\n  --set k8sServicePort=&lt;API_SERVER_PORT&gt;\n</code></pre> <p>Benefits include: - eBPF-based service implementation - Lower latency - Better scalability - Support for DSR (Direct Server Return)</p>"},{"location":"kubernetes/cks/cilium/#multi-cluster-connectivity","title":"Multi-Cluster Connectivity","text":"<p>Create global services spanning multiple clusters:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: global-service\n  annotations:\n    service.cilium.io/global: \"true\"\nspec:\n  type: ClusterIP\n  selector:\n    app: web\n  ports:\n  - port: 80\n</code></pre>"},{"location":"kubernetes/cks/cilium/#performance-considerations","title":"Performance Considerations","text":"<p>Cilium is designed for high performance:</p> <ul> <li>eBPF vs iptables: Significant performance improvements, especially at scale</li> <li>Kernel Bypass: XDP acceleration for load balancing</li> <li>Direct Routing: Improved performance compared to overlay networks</li> <li>Hubble Overhead: Minimal when sampling is configured appropriately</li> </ul> <p>Optimizing Cilium for performance:</p> <pre><code># Direct routing for better performance\nhelm upgrade cilium cilium/cilium --namespace kube-system \\\n  --reuse-values \\\n  --set tunnel=disabled \\\n  --set autoDirectNodeRoutes=true\n\n# Enable XDP acceleration for service load balancing\nhelm upgrade cilium cilium/cilium --namespace kube-system \\\n  --reuse-values \\\n  --set loadBalancer.acceleration=native\n</code></pre>"},{"location":"kubernetes/cks/cilium/#troubleshooting","title":"Troubleshooting","text":"<p>Common troubleshooting commands:</p> <pre><code># Check Cilium status\ncilium status\n\n# Validate Cilium installation\ncilium connectivity test\n\n# Troubleshoot a specific endpoint\ncilium endpoint get &lt;endpoint-id&gt;\n\n# Check Cilium agent logs\nkubectl -n kube-system logs -l k8s-app=cilium\n\n# Restart Cilium on a node\nkubectl -n kube-system delete pod -l k8s-app=cilium -l kubernetes.io/hostname=&lt;node-name&gt;\n\n# Verify network policies\ncilium policy get\n</code></pre>"},{"location":"kubernetes/cks/falco/","title":"Falco","text":"<p>Falco is an open-source, cloud-native runtime security project that detects unexpected behavior, configuration changes, intrusions, and data theft in real-time. Originally created by Sysdig and now a CNCF (Cloud Native Computing Foundation) graduated project, Falco acts as a security camera for your system, alerting you to potentially malicious activity.</p> <p>Key features include: - System call monitoring for process behavior - Container and Kubernetes monitoring - Extensible rule language - Multiple output options (syslog, file, program, HTTP, etc.) - Low performance overhead</p>"},{"location":"kubernetes/cks/falco/#basic-configuration","title":"Basic Configuration","text":""},{"location":"kubernetes/cks/falco/#configuration-file-structure","title":"Configuration File Structure","text":"<p>The main Falco configuration file is typically located at <code>/etc/falco/falco.yaml</code>. Here are the key sections:</p> <pre><code># Rules file(s) or directories\nrules_file:\n  - /etc/falco/falco_rules.yaml\n  - /etc/falco/falco_rules.local.yaml\n  - /etc/falco/rules.d\n\n# Where security notifications should be written\noutput:\n  rate: 1\n  max_burst: 1000\n  enabled: true\n\n# Logging settings\nlog_stderr: true\nlog_syslog: true\nlog_level: info\n\n# Monitoring settings\nsyscall_event_drops:\n  actions:\n    - log\n    - alert\n</code></pre>"},{"location":"kubernetes/cks/falco/#output-methods","title":"Output Methods","text":"<p>Configure various output methods in <code>/etc/falco/falco.yaml</code>:</p>"},{"location":"kubernetes/cks/falco/#standard-output","title":"Standard Output","text":"<pre><code>stdout_output:\n  enabled: true\n</code></pre>"},{"location":"kubernetes/cks/falco/#file-output","title":"File Output","text":"<pre><code>file_output:\n  enabled: true\n  filename: /var/log/falco.log\n  keep_alive: false\n</code></pre>"},{"location":"kubernetes/cks/falco/#syslog-output","title":"Syslog Output","text":"<pre><code>syslog_output:\n  enabled: true\n</code></pre>"},{"location":"kubernetes/cks/falco/#program-output","title":"Program Output","text":"<pre><code>program_output:\n  enabled: true\n  program: \"curl -d @- -X POST https://example.com/falco-alerts\"\n</code></pre>"},{"location":"kubernetes/cks/falco/#http-output","title":"HTTP Output","text":"<pre><code>http_output:\n  enabled: true\n  url: https://alerts.example.com/falco\n  user_agent: falco/5.0.0\n</code></pre>"},{"location":"kubernetes/cks/falco/#working-with-falco-rules","title":"Working with Falco Rules","text":""},{"location":"kubernetes/cks/falco/#rule-anatomy","title":"Rule Anatomy","text":"<p>A typical Falco rule consists of:</p> <ol> <li>Rule definition with a name, description, and priority</li> <li>Condition that triggers the rule</li> <li>Output format for alerts</li> </ol> <p>Example:</p> <pre><code>- rule: Terminal Shell in Container\n  desc: A shell was used as the entrypoint/exec point into a container\n  condition: &gt;\n    container.id != host and\n    proc.name = bash and\n    container\n  output: &gt;\n    Shell executed in a container (user=%user.name %container.info)\n  priority: WARNING\n  tags: [container, shell]\n</code></pre>"},{"location":"kubernetes/cks/falco/#creating-custom-rules","title":"Creating Custom Rules","text":"<p>Create a new file for custom rules:</p> <pre><code>nano /etc/falco/rules.d/custom_rules.yaml\n</code></pre> <p>Add your rules following this structure:</p> <pre><code>- macro: my_custom_macro\n  condition: evt.type=open\n\n- list: my_sensitive_files\n  items: [/etc/shadow, /etc/passwd]\n\n- rule: Read Sensitive Files\n  desc: Detect attempts to read sensitive files\n  condition: &gt;\n    open_read and\n    fd.name in (my_sensitive_files) and\n    not proc.name in (my_trusted_processes)\n  output: &gt;\n    Sensitive file opened for reading (user=%user.name file=%fd.name)\n  priority: WARNING\n  tags: [filesystem]\n</code></pre> <p>After adding rules, restart Falco:</p> <pre><code>systemctl restart falco\n</code></pre>"},{"location":"kubernetes/cks/falco/#testing-rules","title":"Testing Rules","text":"<p>Use <code>falco-tester</code> to validate your rules:</p> <pre><code>falco --validate /etc/falco/falco_rules.yaml\n\n# Test with specific rule file\nfalco -r /etc/falco/rules.d/custom_rules.yaml -V\n</code></pre> <p>To test with real events:</p> <pre><code># Run Falco interactively with your rules\nfalco -r /etc/falco/rules.d/custom_rules.yaml\n\n# In another terminal, perform actions that should trigger rules\ncat /etc/shadow\n</code></pre>"},{"location":"kubernetes/cks/falco/#checking-and-analyzing-logs","title":"Checking and Analyzing Logs","text":""},{"location":"kubernetes/cks/falco/#standard-host-logs","title":"Standard Host Logs","text":""},{"location":"kubernetes/cks/falco/#systemd-journal","title":"Systemd Journal","text":"<pre><code># View real-time logs\njournalctl -u falco -f\n\n# View logs with specific priority\njournalctl -u falco -p warning -f\n\n# View logs from a time period\njournalctl -u falco --since \"2023-04-25 10:00:00\" --until \"2023-04-25 11:00:00\"\n</code></pre>"},{"location":"kubernetes/cks/falco/#log-files","title":"Log Files","text":"<p>If configured to use file output:</p> <pre><code># View the entire log file\ncat /var/log/falco.log\n\n# Follow logs in real-time\ntail -f /var/log/falco.log\n\n# Filter logs by priority\ngrep -i \"critical\\|error\" /var/log/falco.log\n\n# Filter logs by rule name\ngrep \"Terminal Shell in Container\" /var/log/falco.log\n</code></pre>"},{"location":"kubernetes/cks/falco/#syslog","title":"Syslog","text":"<pre><code># Check syslog for Falco events\ngrep falco /var/log/syslog\n</code></pre>"},{"location":"kubernetes/cks/falco/#kubernetes-logs","title":"Kubernetes Logs","text":"<pre><code># List Falco pods\nkubectl get pods -n falco\n\n# Follow logs for a specific pod\nkubectl logs -n falco falco-abcd1234 -f\n\n# Follow logs for all Falco pods\nkubectl logs -n falco -l app=falco -f\n\n# Filter logs by severity\nkubectl logs -n falco -l app=falco | grep -i \"warning\\|error\\|critical\"\n</code></pre>"},{"location":"kubernetes/cks/falco/#log-formats-and-analysis","title":"Log Formats and Analysis","text":""},{"location":"kubernetes/cks/falco/#json-output","title":"JSON Output","text":"<p>Configure JSON output for easier parsing:</p> <pre><code># In falco.yaml\njson_output: true\n</code></pre> <p>Then analyze with tools like <code>jq</code>:</p> <pre><code># Extract specific fields\ncat /var/log/falco.log | jq 'select(.priority == \"CRITICAL\")'\ncat /var/log/falco.log | jq 'select(.rule == \"Terminal Shell in Container\")'\n\n# Count alerts by rule\ncat /var/log/falco.log | jq -r '.rule' | sort | uniq -c | sort -nr\n</code></pre>"},{"location":"kubernetes/cks/falco/#time-based-analysis","title":"Time-Based Analysis","text":"<pre><code># Events in the last hour\ngrep \"$(date -d '1 hour ago' +'%Y-%m-%d %H:')\" /var/log/falco.log\n\n# Create a timeline of events\ncat /var/log/falco.log | jq -r '[.time, .rule, .output] | @tsv' | sort\n</code></pre>"},{"location":"kubernetes/cks/falco/#alert-management","title":"Alert Management","text":"<p>Configure alert throttling to prevent alert fatigue:</p> <pre><code># In falco.yaml\noutputs:\n  rate: 0.03333  # One alert every 30 seconds\n  max_burst: 10   # Allow bursts of up to 10 alerts\n</code></pre>"},{"location":"kubernetes/cks/falco/#advanced-usage","title":"Advanced Usage","text":""},{"location":"kubernetes/cks/falco/#custom-field-extraction","title":"Custom Field Extraction","text":"<p>Define custom fields for use in rules:</p> <pre><code># In a custom rules file\n- required_engine_version: 11\n\n- source: syscall\n\n- list: trusted_users\n  items: [\"root\", \"admin\"]\n\n- rule: Custom Field Example\n  desc: Using custom fields\n  condition: &gt;\n    spawned_process and\n    not user.name in (trusted_users) and\n    proc.args contains \"-i\"\n  output: &gt;\n    Process started with interactive flag (user=%user.name command=%proc.cmdline custom_field=%evt.rawtime)\n  priority: WARNING\n</code></pre>"},{"location":"kubernetes/cks/falco/#lua-support","title":"Lua Support","text":"<p>Enhance rules with Lua functions:</p> <pre><code># Define a Lua function\n- macro: is_suspicious_path\n  condition: &gt;\n    (evt.type=open and fd.name startswith \"/tmp/evil\" and evt.dir=&lt;)\n\n# Use the function in a rule\n- rule: Suspicious File Access\n  desc: Accessing potentially malicious files\n  condition: is_suspicious_path\n  output: Suspicious file accessed (file=%fd.name user=%user.name)\n  priority: WARNING\n</code></pre>"},{"location":"kubernetes/cks/falco/#performance-monitoring","title":"Performance Monitoring","text":"<p>Monitor Falco\u2019s performance impact:</p> <pre><code># Check CPU and memory usage\ntop -p $(pgrep -f falco)\n\n# Check dropped events\ngrep \"Dropped events\" /var/log/falco.log\n\n# Use internal metrics (if enabled)\ncurl http://localhost:8765/metrics\n</code></pre>"},{"location":"kubernetes/cks/image_scanning/","title":"Image Scanning and Security","text":""},{"location":"kubernetes/cks/image_scanning/#introduction","title":"Introduction","text":"<p>As software supply chain attacks increase in frequency and sophistication, organizations are seeking effective ways to secure their software development lifecycle. Two critical components in this effort are Software Bills of Materials (SBOMs) and vulnerability scanning. This guide focuses on Trivy, a powerful open-source scanner, and how it can be used to generate and analyze SBOMs for enhanced security posture.</p>"},{"location":"kubernetes/cks/image_scanning/#understanding-sbom","title":"Understanding SBOM","text":""},{"location":"kubernetes/cks/image_scanning/#what-is-an-sbom","title":"What is an SBOM?","text":"<p>A Software Bill of Materials (SBOM) is a formal, machine-readable inventory of software components and dependencies, information about those components, and their hierarchical relationships. Think of it as a comprehensive ingredient list for software, similar to the nutrition label on food products.</p> <p>An SBOM provides transparency into: - Components: All libraries, modules, and packages included in the software - Versions: Specific versions of each component - Dependencies: Relationships between components - Suppliers: Origin of components - Licensing: License information for each component</p>"},{"location":"kubernetes/cks/image_scanning/#sbom-formats","title":"SBOM Formats","text":"<p>There are several SBOM formats available, with the following being the most widely adopted:</p>"},{"location":"kubernetes/cks/image_scanning/#1-cyclonedx","title":"1. CycloneDX","text":"<p>CycloneDX is a lightweight SBOM specification designed by OWASP for application security contexts and supply chain component analysis.</p> <p>Key characteristics: - Designed specifically for cybersecurity use cases - Supports multiple formats: JSON, XML, Protocol Buffers - Lightweight and focused on security-relevant metadata - Includes vulnerability reporting capabilities - Supports VEX (Vulnerability Exploitability eXchange)</p> <p>Sample CycloneDX JSON (simplified):</p> <pre><code>{\n  \"bomFormat\": \"CycloneDX\",\n  \"specVersion\": \"1.4\",\n  \"serialNumber\": \"urn:uuid:3e671687-395b-41f5-a30f-a58921a69b79\",\n  \"version\": 1,\n  \"components\": [\n    {\n      \"type\": \"library\",\n      \"name\": \"acme-library\",\n      \"version\": \"1.0.0\",\n      \"purl\": \"pkg:npm/acme-library@1.0.0\"\n    }\n  ]\n}\n</code></pre>"},{"location":"kubernetes/cks/image_scanning/#2-spdx-software-package-data-exchange","title":"2. SPDX (Software Package Data Exchange)","text":"<p>SPDX is an open standard for communicating software bill of materials information, including components, licenses, copyrights, and security references.</p> <p>Key characteristics: - ISO/IEC 5962:2021 standard - Comprehensive license information - Detailed metadata capabilities - Mature standard with strong industry adoption - Originally focused on license compliance, now expanded for security</p> <p>Sample SPDX JSON (simplified):</p> <pre><code>{\n  \"spdxVersion\": \"SPDX-2.2\",\n  \"dataLicense\": \"CC0-1.0\",\n  \"SPDXID\": \"SPDXRef-DOCUMENT\",\n  \"name\": \"example-project\",\n  \"documentNamespace\": \"http://spdx.org/spdxdocs/example-project\",\n  \"packages\": [\n    {\n      \"name\": \"acme-library\",\n      \"SPDXID\": \"SPDXRef-Package-1\",\n      \"versionInfo\": \"1.0.0\",\n      \"downloadLocation\": \"https://example.com/acme-library-1.0.0.tar.gz\",\n      \"licenseConcluded\": \"MIT\"\n    }\n  ]\n}\n</code></pre>"},{"location":"kubernetes/cks/image_scanning/#3-swid-software-identification-tags","title":"3. SWID (Software Identification Tags)","text":"<p>SWID tags provide identification information for software, primarily used for inventory and asset management.</p> <p>Key characteristics: - Standardized by ISO/IEC 19770-2 - Focused on software identification and inventory - Less detailed than CycloneDX or SPDX - Primarily used for IT asset management</p>"},{"location":"kubernetes/cks/image_scanning/#introduction-to-trivy","title":"Introduction to Trivy","text":""},{"location":"kubernetes/cks/image_scanning/#what-is-trivy","title":"What is Trivy?","text":"<p>Trivy is a comprehensive and versatile security scanner developed by Aqua Security. It is designed to find vulnerabilities, misconfigurations, secrets, and generate SBOMs across various targets including containers, filesystems, git repositories, and Kubernetes clusters.</p>"},{"location":"kubernetes/cks/image_scanning/#working-with-trivy","title":"Working with Trivy","text":""},{"location":"kubernetes/cks/image_scanning/#basic-commands","title":"Basic Commands","text":"<p>Trivy has several subcommands for different scanning targets:</p> <ul> <li><code>image</code>: Scan container images</li> <li><code>filesystem</code> (or <code>fs</code>): Scan local filesystem</li> <li><code>repository</code> (or <code>repo</code>): Scan a remote git repository</li> <li><code>kubernetes</code> (or <code>k8s</code>): Scan Kubernetes resources</li> <li><code>config</code>: Scan IaC configuration files</li> <li><code>sbom</code>: Scan SBOM files</li> </ul>"},{"location":"kubernetes/cks/image_scanning/#scanning-container-images","title":"Scanning Container Images","text":""},{"location":"kubernetes/cks/image_scanning/#basic-image-scanning","title":"Basic Image Scanning","text":"<pre><code># Scan an image for vulnerabilities (pull from remote registry)\ntrivy image nginx:latest\n\n# Scan a locally built image\ntrivy image my-local-image:tag\n\n# Scan and filter by severity\ntrivy image --severity HIGH,CRITICAL nginx:latest\n\n# Output results in JSON format\ntrivy image --format json --output results.json nginx:latest\n</code></pre>"},{"location":"kubernetes/cks/image_scanning/#advanced-image-scanning-options","title":"Advanced Image Scanning Options","text":"<pre><code># Ignore unfixed vulnerabilities\ntrivy image --ignore-unfixed nginx:latest\n\n# Include package information in the report\ntrivy image --list-all-pkgs nginx:latest\n\n# Scan with custom policy\ntrivy image --policy=./policy/ nginx:latest\n\n# Show dependency tree for vulnerabilities\ntrivy image --dependency-tree nginx:latest\n</code></pre>"},{"location":"kubernetes/cks/image_scanning/#scanning-kubernetes-clusters","title":"Scanning Kubernetes Clusters","text":"<pre><code># Scan a Kubernetes cluster (requires kubectl context)\ntrivy k8s --report summary cluster\n\n# Scan a specific namespace\ntrivy k8s --namespace default all\n\n# Scan a specific workload\ntrivy k8s deployment/my-deployment\n</code></pre>"},{"location":"kubernetes/cks/image_scanning/#scanning-infrastructure-as-code","title":"Scanning Infrastructure as Code","text":"<pre><code># Scan IaC files for misconfigurations\ntrivy config ./terraform/\n\n# Scan Kubernetes YAML files\ntrivy config ./kubernetes-manifests/\n\n# Scan with custom policies\ntrivy config --policy=./policy/ ./terraform/\n</code></pre>"},{"location":"kubernetes/cks/image_scanning/#generating-sboms-with-trivy","title":"Generating SBOMs with Trivy","text":""},{"location":"kubernetes/cks/image_scanning/#sbom-output-formats","title":"SBOM Output Formats","text":"<p>Trivy can generate SBOMs in the following formats:</p> <ul> <li>CycloneDX: Using <code>--format cyclonedx</code></li> <li>SPDX: Using <code>--format spdx-json</code> or <code>--format spdx</code></li> </ul>"},{"location":"kubernetes/cks/image_scanning/#command-examples","title":"Command Examples","text":""},{"location":"kubernetes/cks/image_scanning/#generate-cyclonedx-sbom-for-a-container-image","title":"Generate CycloneDX SBOM for a Container Image","text":"<pre><code># Generate a CycloneDX SBOM for a container image\ntrivy image --format cyclonedx --output sbom.cdx.json nginx:latest\n\n# Include full dependency tree\ntrivy image --format cyclonedx --output sbom.cdx.json --dependency-tree nginx:latest\n\n# Include package info for all packages\ntrivy image --format cyclonedx --output sbom.cdx.json --list-all-pkgs nginx:latest\n</code></pre>"},{"location":"kubernetes/cks/istio_mtls/","title":"mTLS Pod-to-Pod Communication with Istio","text":""},{"location":"kubernetes/cks/istio_mtls/#introduction","title":"Introduction","text":"<p>Mutual TLS (mTLS) is one of the most powerful security features offered by Istio service mesh. Unlike regular TLS where only the server authenticates itself to the client, in mutual TLS both parties verify each other\u2019s identity. </p>"},{"location":"kubernetes/cks/istio_mtls/#understanding-istio-mtls","title":"Understanding Istio mTLS","text":""},{"location":"kubernetes/cks/istio_mtls/#how-mtls-works-in-istio","title":"How mTLS Works in Istio","text":"<p>Istio implements mTLS through its sidecar proxies (Envoy). When a service with an Istio sidecar communicates with another service in the mesh, the following happens:</p> <ol> <li>The client-side sidecar proxy initiates a connection to the server</li> <li>The client-side proxy performs a TLS handshake with the server-side proxy, exchanging certificates</li> <li>The client-side proxy verifies the server\u2019s identity</li> <li>The client and server establish a mutual TLS connection</li> <li>The client proxy forwards the request to the server through the encrypted channel</li> <li>The server-side proxy forwards the request to the application container</li> </ol> <p>This entire process happens transparently to your application.</p>"},{"location":"kubernetes/cks/istio_mtls/#mtls-modes-in-istio","title":"mTLS Modes in Istio","text":"<p>Istio offers three modes for mTLS:</p> <ol> <li>PERMISSIVE: Accepts both plaintext and mTLS traffic (default)</li> <li>STRICT: Only accepts mTLS traffic</li> <li>DISABLED: Only accepts plaintext traffic</li> </ol>"},{"location":"kubernetes/cks/istio_mtls/#implementing-mtls-between-pods","title":"Implementing mTLS Between Pods","text":""},{"location":"kubernetes/cks/istio_mtls/#enable-strict-mtls","title":"Enable Strict mTLS","text":"<p>Enable strict mTLS for the namespaces using PeerAuthentication:</p> <pre><code># save this as peer-authentication.yaml\napiVersion: security.istio.io/v1beta1\nkind: PeerAuthentication\nmetadata:\n  name: default\n  namespace: foo\nspec:\n  mtls:\n    mode: STRICT\n---\napiVersion: security.istio.io/v1beta1\nkind: PeerAuthentication\nmetadata:\n  name: default\n  namespace: bar\nspec:\n  mtls:\n    mode: STRICT\n</code></pre> <p>Apply the configuration:</p> <pre><code>kubectl apply -f peer-authentication.yaml\n</code></pre>"},{"location":"kubernetes/cks/istio_mtls/#configure-destination-rules","title":"Configure Destination Rules","text":"<p>Destination rules define how traffic is routed to a service after virtual service routing rules are evaluated.  When you think about Istio\u2019s traffic management, it\u2019s important to understand the sequence:</p> <ul> <li>First, traffic is routed (often using VirtualService)</li> <li>Then, DestinationRule policies are applied to that routed traffic</li> </ul> <p>Create destination rules to enforce mTLS for outbound traffic:</p> <pre><code># save this as destination-rule.yaml\napiVersion: networking.istio.io/v1beta1\nkind: DestinationRule\nmetadata:\n  name: httpbin-foo\n  namespace: foo\nspec:\n  host: \"httpbin.foo.svc.cluster.local\"\n  trafficPolicy:\n    tls:\n      mode: ISTIO_MUTUAL\n---\napiVersion: networking.istio.io/v1beta1\nkind: DestinationRule\nmetadata:\n  name: httpbin-bar\n  namespace: bar\nspec:\n  host: \"httpbin.bar.svc.cluster.local\"\n  trafficPolicy:\n    tls:\n      mode: ISTIO_MUTUAL\n</code></pre> <p>Apply the configuration:</p> <pre><code>kubectl apply -f destination-rule.yaml\n</code></pre>"},{"location":"kubernetes/cks/istio_mtls/#handling-non-mesh-services","title":"Handling Non-Mesh Services","text":"<p>For services outside the mesh or services that don\u2019t support mTLS, you can configure exceptions to the mTLS policy.</p>"},{"location":"kubernetes/cks/istio_mtls/#workload-specific-mtls-policies","title":"Workload-Specific mTLS Policies","text":"<p>Create workload-specific PeerAuthentication policies to override namespace or mesh-wide policies:</p> <pre><code>apiVersion: security.istio.io/v1beta1\nkind: PeerAuthentication\nmetadata:\n  name: workload-policy\n  namespace: foo\nspec:\n  selector:\n    matchLabels:\n      app: httpbin\n  mtls:\n    mode: PERMISSIVE\n</code></pre>"},{"location":"kubernetes/cks/istio_mtls/#configure-destination-rules-for-external-services","title":"Configure Destination Rules for External Services","text":"<p>For external services, set up a destination rule to disable mTLS:</p> <pre><code>apiVersion: networking.istio.io/v1beta1\nkind: DestinationRule\nmetadata:\n  name: external-service\nspec:\n  host: external-service.example.com\n  trafficPolicy:\n    tls:\n      mode: DISABLE  # or SIMPLE for regular TLS\n</code></pre>"},{"location":"kubernetes/cks/istio_mtls/#istio-sidecar-injection","title":"Istio Sidecar Injection","text":"<p>Istio sidecar injection is the process of automatically adding an Envoy proxy container (the \u201csidecar\u201d) to Kubernetes pods. This sidecar proxy is what enables Istio\u2019s service mesh functionality, as it intercepts and manages all network traffic going in and out of your application pods.</p> <p>When you deploy applications in a Kubernetes cluster with Istio, each pod gets its own sidecar proxy that handles all the networking concerns like traffic routing, load balancing, circuit breaking, and implementing security policies.</p>"},{"location":"kubernetes/cks/istio_mtls/#how-sidecar-injection-works","title":"How Sidecar Injection Works","text":"<p>Istio uses a Kubernetes feature called \u201cmutating webhook admission controller\u201d to modify pod specifications at creation time. When the webhook is enabled and a namespace is properly labeled, any new pods created in that namespace will automatically have the Istio sidecar proxy container added to them.</p> <p>The injection can happen in two ways:</p> <ol> <li>Automatic injection: Configured at the namespace level</li> <li>Manual injection: Applied directly to deployments using the <code>istioctl</code> command</li> </ol>"},{"location":"kubernetes/cks/istio_mtls/#how-to-enable-sidecar-injection","title":"How to Enable Sidecar Injection","text":""},{"location":"kubernetes/cks/istio_mtls/#method-1-automatic-injection-namespace-level","title":"Method 1: Automatic Injection (Namespace-Level)","text":"<p>This is the most common approach:</p> <pre><code># Label the namespace to enable automatic injection\nkubectl label namespace &lt;your-namespace&gt; istio-injection=enabled\n</code></pre> <p>Once you apply this label, all new workloads deployed in the namespace will automatically have the Istio sidecar injected. Note that existing workloads won\u2019t be affected\u2014you\u2019ll need to redeploy them to get the sidecar.</p> <p>You can verify the label is applied with:</p> <pre><code>kubectl get namespace &lt;your-namespace&gt; --show-labels\n</code></pre>"},{"location":"kubernetes/cks/istio_mtls/#method-2-manual-injection-deployment-level","title":"Method 2: Manual Injection (Deployment-Level)","text":"<p>If you prefer to inject sidecars on specific deployments:</p> <pre><code># Manually inject the sidecar into a deployment\nistioctl kube-inject -f your-deployment.yaml | kubectl apply -f -\n</code></pre> <p>This approach modifies the deployment YAML directly, adding the sidecar configuration before applying it to the cluster.</p>"},{"location":"kubernetes/cks/istio_mtls/#how-to-verify-injection-worked","title":"How to Verify Injection Worked","text":"<p>You can verify that a pod has the sidecar injected by checking the READY column:</p> <pre><code>kubectl get pods\n</code></pre> <p>A pod with the sidecar injected will show <code>2/2</code> in the READY column, indicating that there are two containers running (your application and the Istio proxy).</p> <p>You can also describe the pod to see the <code>istio-proxy</code> container:</p> <pre><code>kubectl describe pod &lt;pod-name&gt;\n</code></pre> <p>Or check for the presence of the sidecar container directly:</p> <pre><code>kubectl get pod &lt;pod-name&gt; -o jsonpath='{.spec.containers[*].name}'\n</code></pre>"},{"location":"kubernetes/cks/istio_mtls/#disabling-sidecar-injection","title":"Disabling Sidecar Injection","text":"<p>If you need to exclude a specific workload from injection, you can add an annotation to that workload:</p> <pre><code>metadata:\n  annotations:\n    sidecar.istio.io/inject: \"false\"\n</code></pre> <p>Alternatively, the newer approach uses a label instead of an annotation:</p> <pre><code>metadata:\n  labels:\n    sidecar.istio.io/inject: \"false\"\n</code></pre> <p>This is particularly useful for jobs, CronJobs, or any workload that shouldn\u2019t be part of the service mesh.</p>"},{"location":"kubernetes/cks/k8s_dashboard/","title":"Kubernetes Dashboard","text":"<p>The Kubernetes Dashboard is a web-based UI for Kubernetes clusters that allows users to manage and troubleshoot applications. While convenient, it requires proper security measures to prevent unauthorized access.</p>"},{"location":"kubernetes/cks/k8s_dashboard/#1-use-rbac-role-based-access-control","title":"1. Use RBAC (Role-Based Access Control)","text":"<p>RBAC is critical for limiting dashboard access to authorized users only:</p> <pre><code># Example: Create a restricted dashboard role\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: kubernetes-dashboard-restricted\n  namespace: kubernetes-dashboard\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\", \"services\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n</code></pre> <p>Link roles to users with RoleBindings:</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: dashboard-user-binding\n  namespace: kubernetes-dashboard\nsubjects:\n- kind: User\n  name: dashboard-user\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: Role\n  name: kubernetes-dashboard-restricted\n  apiGroup: rbac.authorization.k8s.io\n</code></pre>"},{"location":"kubernetes/cks/k8s_dashboard/#2-never-expose-dashboard-publicly","title":"2. Never Expose Dashboard Publicly","text":"<p>Never expose your dashboard directly to the internet. Use secure access methods:</p> <ul> <li> <p>Kubectl Proxy:   <pre><code>kubectl proxy\n# Access at http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/\n</code></pre></p> </li> <li> <p>Port Forwarding:   <pre><code>kubectl port-forward -n kubernetes-dashboard service/kubernetes-dashboard 8443:443\n# Access at https://localhost:8443\n</code></pre></p> </li> </ul>"},{"location":"kubernetes/cks/k8s_dashboard/#3-use-token-authentication","title":"3. Use Token Authentication","text":"<p>Create service accounts with limited permissions for dashboard access:</p> <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: dashboard-viewer\n  namespace: kubernetes-dashboard\n</code></pre> <p>Bind appropriate roles:</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: dashboard-viewer\nsubjects:\n- kind: ServiceAccount\n  name: dashboard-viewer\n  namespace: kubernetes-dashboard\nroleRef:\n  kind: ClusterRole\n  name: view\n  apiGroup: rbac.authorization.k8s.io\n</code></pre> <p>Generate and retrieve tokens:</p> <pre><code># For Kubernetes v1.24+\nkubectl create token dashboard-viewer -n kubernetes-dashboard\n\n# For older versions\nkubectl -n kubernetes-dashboard describe secret $(kubectl -n kubernetes-dashboard get secret | grep dashboard-viewer | awk '{print $1}')\n</code></pre>"},{"location":"kubernetes/cks/k8s_dashboard/#4-enable-httpstls","title":"4. Enable HTTPS/TLS","text":"<p>Always use HTTPS with valid certificates:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kubernetes-dashboard\n  namespace: kubernetes-dashboard\nspec:\n  template:\n    spec:\n      containers:\n      - name: kubernetes-dashboard\n        args:\n        - --auto-generate-certificates\n        - --namespace=kubernetes-dashboard\n</code></pre>"},{"location":"kubernetes/cks/kube_bench/","title":"kube-bench","text":""},{"location":"kubernetes/cks/kube_bench/#introduction-to-kube-bench","title":"Introduction to kube-bench","text":"<p>kube-bench is an open-source tool that checks whether Kubernetes is deployed securely by running the checks documented in the CIS Kubernetes Benchmark. The tool helps cluster administrators ensure their Kubernetes deployments meet industry security standards.</p> <p>kube-bench automates security checks against the Center for Internet Security (CIS) Kubernetes Benchmark, which provides guidelines for configuring Kubernetes securely. These benchmarks are widely recognized as security standards for configuring various systems.</p> <p>Key features: - Ability to run checks for multiple Kubernetes components - Support for different Kubernetes versions - Support for various deployment environments - Integration with CI/CD pipelines - Customizable test configurations</p>"},{"location":"kubernetes/cks/kube_bench/#deployment-options","title":"Deployment Options","text":""},{"location":"kubernetes/cks/kube_bench/#running-as-a-kubernetes-job","title":"Running as a Kubernetes Job","text":"<p>A Kubernetes job is a good option for one-time assessments:</p> <pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: kube-bench\nspec:\n  template:\n    spec:\n      hostPID: true\n      containers:\n        - name: kube-bench\n          image: aquasec/kube-bench:latest\n          securityContext:\n            privileged: true\n          volumeMounts:\n          - name: var-lib-kubelet\n            mountPath: /var/lib/kubelet\n            readOnly: true\n          - name: etc-systemd\n            mountPath: /etc/systemd\n            readOnly: true\n          - name: etc-kubernetes\n            mountPath: /etc/kubernetes\n            readOnly: true\n      restartPolicy: Never\n      volumes:\n      - name: var-lib-kubelet\n        hostPath:\n          path: \"/var/lib/kubelet\"\n      - name: etc-systemd\n        hostPath:\n          path: \"/etc/systemd\"\n      - name: etc-kubernetes\n        hostPath:\n          path: \"/etc/kubernetes\"\n</code></pre>"},{"location":"kubernetes/cks/kube_bench/#running-as-a-daemonset","title":"Running as a DaemonSet","text":"<p>To run kube-bench on every node in your cluster, use a DaemonSet:</p> <pre><code>apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kube-bench\n  namespace: security\nspec:\n  selector:\n    matchLabels:\n      app: kube-bench\n  template:\n    metadata:\n      labels:\n        app: kube-bench\n    spec:\n      hostPID: true\n      containers:\n      - name: kube-bench\n        image: aquasec/kube-bench:latest\n        command: [\"kube-bench\", \"--json\", \"--logtostderr=true\", \"node\"]\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: var-lib-kubelet\n          mountPath: /var/lib/kubelet\n          readOnly: true\n        - name: etc-systemd\n          mountPath: /etc/systemd\n          readOnly: true\n        - name: etc-kubernetes\n          mountPath: /etc/kubernetes\n          readOnly: true\n        - name: output\n          mountPath: /output\n      volumes:\n      - name: var-lib-kubelet\n        hostPath:\n          path: \"/var/lib/kubelet\"\n      - name: etc-systemd\n        hostPath:\n          path: \"/etc/systemd\"\n      - name: etc-kubernetes\n        hostPath:\n          path: \"/etc/kubernetes\"\n      - name: output\n        hostPath:\n          path: \"/tmp/kube-bench\"\n          type: DirectoryOrCreate\n</code></pre>"},{"location":"kubernetes/cks/kube_bench/#running-locally-on-nodes","title":"Running Locally on Nodes","text":"<p>For direct execution on a node:</p> <pre><code># If installed as a package\nkube-bench\n\n# Using the binary\n./kube-bench\n</code></pre>"},{"location":"kubernetes/cks/kube_bench/#basic-usage","title":"Basic Usage","text":""},{"location":"kubernetes/cks/kube_bench/#running-default-checks","title":"Running Default Checks","text":"<p>To run all checks against your cluster:</p> <pre><code>kube-bench\n</code></pre> <p>This will automatically detect your Kubernetes version and run the appropriate CIS benchmark checks.</p>"},{"location":"kubernetes/cks/kube_bench/#targeting-specific-components","title":"Targeting Specific Components","text":"<p>You can target specific Kubernetes components:</p> <pre><code># Master node checks\nkube-bench run --targets master\n\n# Worker node checks\nkube-bench run --targets node\n\n# etcd node checks\nkube-bench run --targets etcd\n\n# Multiple targets\nkube-bench run --targets master,node\n\n# Control plane components\nkube-bench run --targets control-plane\n\n# Policies\nkube-bench run --targets policies\n</code></pre>"},{"location":"kubernetes/cks/kube_bench/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"kubernetes/cks/kube_bench/#custom-configuration-files","title":"Custom Configuration Files","text":"<p>You can customize the checks using your own configuration files:</p> <pre><code>kube-bench --config-dir /path/to/custom/configs\n</code></pre> <p>The structure should match the default config structure:</p> <pre><code>/path/to/custom/configs/\n\u251c\u2500\u2500 config.yaml\n\u251c\u2500\u2500 controlplane.yaml\n\u251c\u2500\u2500 etcd.yaml\n\u251c\u2500\u2500 master.yaml\n\u251c\u2500\u2500 node.yaml\n\u2514\u2500\u2500 policies.yaml\n</code></pre>"},{"location":"kubernetes/cks/kube_bench/#excluding-specific-tests","title":"Excluding Specific Tests","text":"<p>To exclude certain tests:</p> <pre><code>kube-bench run --targets master --exclude 1.1.2,1.2.1\n</code></pre>"},{"location":"kubernetes/cks/kube_bench/#running-specific-test-groups","title":"Running Specific Test Groups","text":"<p>To run only specific test groups or checks:</p> <pre><code># Run only section 1 tests on master\nkube-bench run --targets master --check 1\n\n# Run specific checks\nkube-bench run --targets master --check 1.1.1,1.1.2\n</code></pre>"},{"location":"kubernetes/cks/kube_bench/#output-formats","title":"Output Formats","text":""},{"location":"kubernetes/cks/kube_bench/#default-output","title":"Default Output","text":"<p>By default, kube-bench outputs results in a human-readable format:</p> <pre><code>kube-bench\n</code></pre> <p>Example output: <pre><code>[INFO] 1 Master Node Security Configuration\n[INFO] 1.1 Master Node Configuration Files\n[PASS] 1.1.1 Ensure that the API server pod specification file permissions are set to 644 or more restrictive (Automated)\n[FAIL] 1.1.2 Ensure that the API server pod specification file ownership is set to root:root (Automated)\n</code></pre></p>"},{"location":"kubernetes/cks/kube_bench/#json-output","title":"JSON Output","text":"<p>For programmatic processing, use JSON output:</p> <pre><code>kube-bench --json &gt; kube-bench-results.json\n</code></pre> <p>Example structure: <pre><code>{\n  \"controls\": [\n    {\n      \"id\": \"1\",\n      \"text\": \"Master Node Security Configuration\",\n      \"tests\": [\n        {\n          \"section\": \"1.1\",\n          \"type\": \"manual\",\n          \"pass\": true,\n          \"text\": \"Ensure that the API server pod specification file permissions are set to 644 or more restrictive\"\n        }\n      ]\n    }\n  ]\n}\n</code></pre></p>"},{"location":"kubernetes/cks/kube_bench/#junit-xml-for-cicd","title":"JUnit XML for CI/CD","text":"<p>For CI/CD integration, use JUnit XML format:</p> <pre><code>kube-bench --junit &gt; kube-bench-results.xml\n</code></pre>"},{"location":"kubernetes/cks/kube_bench/#interpreting-results","title":"Interpreting Results","text":""},{"location":"kubernetes/cks/kube_bench/#understanding-severity-levels","title":"Understanding Severity Levels","text":"<p>Results are categorized with these severities:</p> <ul> <li>PASS: The check was successful</li> <li>FAIL: The check failed and needs remediation</li> <li>WARN: The check found something that might need attention</li> <li>INFO: Informational only, no action needed</li> <li>NOTE: Additional information about a test</li> </ul>"},{"location":"kubernetes/cks/kube_bench/#reading-test-output","title":"Reading Test Output","text":"<p>For each check, kube-bench provides:</p> <ol> <li>The CIS benchmark ID (e.g., \u201c1.1.1\u201d)</li> <li>Description of the check</li> <li>Result (PASS/FAIL/WARN/INFO)</li> <li>Remediation suggestions for failed checks</li> </ol>"},{"location":"kubernetes/cks/kube_bench/#remediation-steps","title":"Remediation Steps","text":"<p>For each failed check, kube-bench provides remediation instructions:</p> <pre><code>[FAIL] 1.1.20 Ensure that the Kubernetes PKI directory and file ownership is set to root:root (Automated)\n        [remediation]\n        Run the below command (based on the file location on your system) on the master node.\n        For example,\n        chown -R root:root /etc/kubernetes/pki/\n</code></pre>"},{"location":"kubernetes/cks/opa/","title":"OPA and Gatekeeper","text":""},{"location":"kubernetes/cks/opa/#introduction","title":"Introduction","text":"<p>Open Policy Agent (OPA) is a general-purpose policy engine that enables unified policy enforcement across the entire stack. When combined with Gatekeeper, OPA becomes a powerful tool for enforcing policies in Kubernetes environments. This guide will take you through practical implementations of OPA and Gatekeeper, from basic concepts to advanced use cases, preparing you for real-world scenarios and practical exams.</p>"},{"location":"kubernetes/cks/opa/#understanding-opa-and-gatekeeper","title":"Understanding OPA and Gatekeeper","text":"<p>OPA is a general-purpose policy engine that enables unified policy enforcement across various systems. Gatekeeper is the Kubernetes-native implementation of OPA, providing a way to enforce policies on Kubernetes resources during creation and update operations.</p>"},{"location":"kubernetes/cks/opa/#key-concepts","title":"Key Concepts","text":"<ul> <li>Policy as Code: Define policies in a declarative language (Rego) rather than hardcoding them in applications.</li> <li>Admission Control: Gatekeeper works as a validating admission controller in Kubernetes.</li> <li>Constraint Framework: A system for defining, applying, and monitoring policy compliance.</li> </ul>"},{"location":"kubernetes/cks/opa/#the-relationship-between-opa-and-gatekeeper","title":"The Relationship Between OPA and Gatekeeper","text":"<p>Gatekeeper extends OPA\u2019s capabilities specifically for Kubernetes:</p> <ul> <li>It provides Kubernetes custom resources for defining policies</li> <li>It integrates with the Kubernetes admission controller</li> <li>It enables audit functionality for existing resources</li> </ul>"},{"location":"kubernetes/cks/opa/#installing-gatekeeper","title":"Installing Gatekeeper","text":"<p>Let\u2019s begin with installing Gatekeeper on a Kubernetes cluster:</p>"},{"location":"kubernetes/cks/opa/#using-helm","title":"Using Helm","text":"<pre><code># Add the Gatekeeper Helm repository\nhelm repo add gatekeeper https://open-policy-agent.github.io/gatekeeper/charts\n\n# Update the Helm repositories\nhelm repo update\n\n# Install Gatekeeper\nhelm install gatekeeper gatekeeper/gatekeeper --namespace gatekeeper-system --create-namespace\n</code></pre>"},{"location":"kubernetes/cks/opa/#using-kubectl","title":"Using kubectl","text":"<pre><code>kubectl apply -f https://raw.githubusercontent.com/open-policy-agent/gatekeeper/release-3.11/deploy/gatekeeper.yaml\n</code></pre>"},{"location":"kubernetes/cks/opa/#verify-the-installation","title":"Verify the Installation","text":"<pre><code>kubectl get pods -n gatekeeper-system\n</code></pre> <p>You should see pods like <code>gatekeeper-audit-*</code>, <code>gatekeeper-controller-manager-*</code>, etc., all in a Running state.</p>"},{"location":"kubernetes/cks/opa/#constrainttemplates-and-constraints","title":"ConstraintTemplates and Constraints","text":"<p>Gatekeeper uses two custom resources to define and enforce policies:</p>"},{"location":"kubernetes/cks/opa/#constrainttemplates","title":"ConstraintTemplates","text":"<p>A ConstraintTemplate defines the logic of the policy using Rego and the schema for the Constraint that will implement the policy. Think of it as a reusable policy definition that can be applied in different contexts.</p>"},{"location":"kubernetes/cks/opa/#anatomy-of-a-constrainttemplate","title":"Anatomy of a ConstraintTemplate","text":"<pre><code>apiVersion: templates.gatekeeper.sh/v1\nkind: ConstraintTemplate\nmetadata:\n  name: k8srequiredlabels  # Name of the template\nspec:\n  crd:\n    spec:\n      names:\n        kind: K8sRequiredLabels  # Kind of the Constraint this template creates\n      validation:\n        # Schema for the `parameters` field in the Constraint\n        openAPIV3Schema:\n          type: object\n          properties:\n            labels:\n              type: array\n              items:\n                type: string\n  targets:\n    - target: admission.k8s.gatekeeper.sh\n      rego: |\n        package k8srequiredlabels\n\n        violation[{\"msg\": msg, \"details\": {\"missing_labels\": missing}}] {\n          provided := {label | input.review.object.metadata.labels[label]}\n          required := {label | label := input.parameters.labels[_]}\n          missing := required - provided\n          count(missing) &gt; 0\n          msg := sprintf(\"you must provide labels: %v\", [missing])\n        }\n</code></pre> <p>Let\u2019s break this down:</p> <ol> <li>CRD Specification: Defines the kind name and validation schema for the Constraint.</li> <li>Target: Specifies where the policy will be enforced. For Kubernetes, this is usually <code>admission.k8s.gatekeeper.sh</code>.</li> <li>Rego Policy: The policy logic written in Rego language.</li> </ol>"},{"location":"kubernetes/cks/opa/#constraints","title":"Constraints","text":"<p>A Constraint is an instance of a ConstraintTemplate that enforces the policy defined in the template. It allows you to specify which resources the policy should apply to and provide parameters to customize the policy.</p>"},{"location":"kubernetes/cks/opa/#example-constraint","title":"Example Constraint","text":"<pre><code>apiVersion: constraints.gatekeeper.sh/v1beta1\nkind: K8sRequiredLabels  # This must match the kind defined in the ConstraintTemplate\nmetadata:\n  name: require-team-label\nspec:\n  match:\n    kinds:\n      - apiGroups: [\"\"]\n        kinds: [\"Namespace\"]  # This policy applies to Namespaces\n  parameters:\n    labels: [\"team\"]  # Require 'team' label on all Namespaces\n</code></pre> <p>Let\u2019s break this down:</p> <ol> <li>Kind: Must match the kind defined in the ConstraintTemplate.</li> <li>Match: Defines which resources the Constraint applies to.</li> <li>Parameters: Values passed to the Rego policy in the ConstraintTemplate.</li> </ol>"},{"location":"kubernetes/cks/opa/#practical-examples","title":"Practical Examples","text":"<p>Let\u2019s walk through some practical examples of ConstraintTemplates and Constraints that solve real-world problems:</p>"},{"location":"kubernetes/cks/opa/#example-1-require-specific-labels","title":"Example 1: Require Specific Labels","text":"<p>This example ensures that all Namespaces have required labels:</p> <pre><code># First, create the ConstraintTemplate\napiVersion: templates.gatekeeper.sh/v1\nkind: ConstraintTemplate\nmetadata:\n  name: k8srequiredlabels\nspec:\n  crd:\n    spec:\n      names:\n        kind: K8sRequiredLabels\n      validation:\n        openAPIV3Schema:\n          type: object\n          properties:\n            labels:\n              type: array\n              items:\n                type: string\n  targets:\n    - target: admission.k8s.gatekeeper.sh\n      rego: |\n        package k8srequiredlabels\n\n        violation[{\"msg\": msg, \"details\": {\"missing_labels\": missing}}] {\n          provided := {label | input.review.object.metadata.labels[label]}\n          required := {label | label := input.parameters.labels[_]}\n          missing := required - provided\n          count(missing) &gt; 0\n          msg := sprintf(\"you must provide labels: %v\", [missing])\n        }\n</code></pre> <pre><code># Then, create the Constraint\napiVersion: constraints.gatekeeper.sh/v1beta1\nkind: K8sRequiredLabels\nmetadata:\n  name: ns-must-have-env\nspec:\n  match:\n    kinds:\n      - apiGroups: [\"\"]\n        kinds: [\"Namespace\"]\n  parameters:\n    labels: [\"environment\"]\n</code></pre> <p>Now, if you try to create a Namespace without the <code>environment</code> label:</p> <pre><code>kubectl create namespace test\n</code></pre> <p>It will fail with an error like:</p> <pre><code>Error from server ([ns-must-have-env] you must provide labels: {\"environment\"}): admission webhook \"validation.gatekeeper.sh\" denied the request: [ns-must-have-env] you must provide labels: {\"environment\"}\n</code></pre>"},{"location":"kubernetes/cks/opa/#example-2-block-latest-image-tags","title":"Example 2: Block Latest Image Tags","text":"<p>This example blocks the use of the <code>:latest</code> tag for container images:</p> <pre><code># ConstraintTemplate\napiVersion: templates.gatekeeper.sh/v1\nkind: ConstraintTemplate\nmetadata:\n  name: k8sblocklatestimages\nspec:\n  crd:\n    spec:\n      names:\n        kind: K8sBlockLatestImages\n      validation:\n        openAPIV3Schema:\n          type: object\n  targets:\n    - target: admission.k8s.gatekeeper.sh\n      rego: |\n        package k8sblocklatestimages\n\n        violation[{\"msg\": msg}] {\n          container := input.review.object.spec.containers[_]\n          endswith(container.image, \":latest\")\n          msg := sprintf(\"container &lt;%v&gt; uses the latest tag\", [container.name])\n        }\n\n        violation[{\"msg\": msg}] {\n          container := input.review.object.spec.initContainers[_]\n          endswith(container.image, \":latest\")\n          msg := sprintf(\"initContainer &lt;%v&gt; uses the latest tag\", [container.name])\n        }\n</code></pre> <pre><code># Constraint\napiVersion: constraints.gatekeeper.sh/v1beta1\nkind: K8sBlockLatestImages\nmetadata:\n  name: block-latest-images\nspec:\n  match:\n    kinds:\n      - apiGroups: [\"\"]\n        kinds: [\"Pod\"]\n      - apiGroups: [\"apps\"]\n        kinds: [\"Deployment\", \"StatefulSet\", \"DaemonSet\"]\n</code></pre>"},{"location":"kubernetes/cks/opa/#example-3-require-resource-limits","title":"Example 3: Require Resource Limits","text":"<p>This example ensures all containers have CPU and memory limits:</p> <pre><code># ConstraintTemplate\napiVersion: templates.gatekeeper.sh/v1\nkind: ConstraintTemplate\nmetadata:\n  name: k8srequirelimits\nspec:\n  crd:\n    spec:\n      names:\n        kind: K8sRequireLimits\n      validation:\n        openAPIV3Schema:\n          type: object\n  targets:\n    - target: admission.k8s.gatekeeper.sh\n      rego: |\n        package k8srequirelimits\n\n        violation[{\"msg\": msg}] {\n          container := input.review.object.spec.containers[_]\n          not container.resources.limits.cpu\n          msg := sprintf(\"container &lt;%v&gt; has no CPU limit\", [container.name])\n        }\n\n        violation[{\"msg\": msg}] {\n          container := input.review.object.spec.containers[_]\n          not container.resources.limits.memory\n          msg := sprintf(\"container &lt;%v&gt; has no memory limit\", [container.name])\n        }\n</code></pre> <pre><code># Constraint\napiVersion: constraints.gatekeeper.sh/v1beta1\nkind: K8sRequireLimits\nmetadata:\n  name: require-resource-limits\nspec:\n  match:\n    kinds:\n      - apiGroups: [\"\"]\n        kinds: [\"Pod\"]\n      - apiGroups: [\"apps\"]\n        kinds: [\"Deployment\", \"StatefulSet\"]\n</code></pre>"},{"location":"kubernetes/cks/opa/#advanced-use-cases","title":"Advanced Use Cases","text":""},{"location":"kubernetes/cks/opa/#mutating-admission-control","title":"Mutating Admission Control","text":"<p>While traditionally Gatekeeper has been used for validation, recent versions support mutation as well. Here\u2019s an example of a MutatingWebhook that adds default resource limits:</p> <pre><code>apiVersion: mutations.gatekeeper.sh/v1\nkind: AssignMetadata\nmetadata:\n  name: add-team-label\nspec:\n  match:\n    scope: Namespaced\n    kinds:\n      - apiGroups: [\"\"]\n        kinds: [\"Namespace\"]\n  location: \"metadata.labels.team\"\n  parameters:\n    assign:\n      value: \"default-team\"\n</code></pre>"},{"location":"kubernetes/cks/opa/#external-data","title":"External Data","text":"<p>Gatekeeper can use data from external sources through the <code>data.inventory</code> object:</p> <pre><code>apiVersion: templates.gatekeeper.sh/v1\nkind: ConstraintTemplate\nmetadata:\n  name: k8suniqueservices\nspec:\n  crd:\n    spec:\n      names:\n        kind: K8sUniqueServices\n  targets:\n    - target: admission.k8s.gatekeeper.sh\n      rego: |\n        package k8suniqueservices\n\n        violation[{\"msg\": msg}] {\n          input.review.kind.kind == \"Service\"\n          input.review.object.spec.type == \"LoadBalancer\"\n          port := input.review.object.spec.ports[_]\n          existing := data.inventory.namespace[namespace].Service[name]\n          existing.spec.type == \"LoadBalancer\"\n          existing_port := existing.spec.ports[_]\n          port.port == existing_port.port\n          not (input.review.object.metadata.name == existing.metadata.name)\n          msg := sprintf(\"Service port %v already in use by service %v\", [port.port, existing.metadata.name])\n        }\n</code></pre>"},{"location":"kubernetes/cks/opa/#exemptions","title":"Exemptions","text":"<p>You can create exemptions for your policies using the <code>excludedNamespaces</code> field in the Constraint:</p> <pre><code>apiVersion: constraints.gatekeeper.sh/v1beta1\nkind: K8sRequireLimits\nmetadata:\n  name: require-resource-limits\nspec:\n  match:\n    kinds:\n      - apiGroups: [\"\"]\n        kinds: [\"Pod\"]\n    excludedNamespaces: [\"kube-system\", \"gatekeeper-system\"]\n</code></pre>"},{"location":"kubernetes/cks/runtimeclass/","title":"RuntimeClass","text":""},{"location":"kubernetes/cks/runtimeclass/#introduction","title":"Introduction","text":"<p>RuntimeClass is a Kubernetes feature that allows you to select the container runtime configuration used to run a Pod\u2019s containers. It\u2019s particularly valuable in scenarios where you need to provide different levels of isolation, security, or performance for specific workloads.  The RuntimeClass resource is cluster-scoped (non-namespaced) and maps a runtime class name to the corresponding configuration used by the container runtime to run a container.</p>"},{"location":"kubernetes/cks/runtimeclass/#how-runtimeclass-works","title":"How RuntimeClass Works","text":"<p>RuntimeClass leverages the Container Runtime Interface (CRI) to expose different runtime options to Kubernetes. The workflow is:</p> <ol> <li>An administrator configures the CRI implementation on nodes (such as containerd or CRI-O).</li> <li>The administrator creates RuntimeClass objects that reference handlers for these configurations.</li> <li>Users specify a <code>runtimeClassName</code> in their Pod specs.</li> <li>Kubelet uses the referenced RuntimeClass to determine which runtime handler to use for the Pod.</li> </ol>"},{"location":"kubernetes/cks/runtimeclass/#setting-up-runtimeclass","title":"Setting Up RuntimeClass","text":""},{"location":"kubernetes/cks/runtimeclass/#step-1-configure-cri-implementation-on-nodes","title":"Step 1: Configure CRI Implementation on Nodes","text":"<p>The specific configuration depends on your CRI implementation:</p>"},{"location":"kubernetes/cks/runtimeclass/#for-containerd-v12","title":"For containerd (v1.2+)","text":"<p>Edit <code>/etc/containerd/config.toml</code> to define runtime handlers:</p> <pre><code>[plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes]\n  [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc]\n    runtime_type = \"io.containerd.runc.v2\"\n    [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc.options]\n      SystemdCgroup = true\n\n  # Example configuration for Kata Containers\n  [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.kata]\n    runtime_type = \"io.containerd.kata.v2\"\n    [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.kata.options]\n      ConfigPath = \"/opt/kata/share/defaults/kata-containers/configuration.toml\"\n\n  # Example configuration for gVisor\n  [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.gvisor]\n    runtime_type = \"io.containerd.runsc.v1\"\n</code></pre> <p>After modifying the configuration, restart containerd:</p> <pre><code>sudo systemctl restart containerd\n</code></pre>"},{"location":"kubernetes/cks/runtimeclass/#for-cri-o","title":"For CRI-O","text":"<p>Edit <code>/etc/crio/crio.conf</code> to define runtime handlers:</p> <pre><code>[crio.runtime.runtimes.runc]\nruntime_path = \"/usr/local/bin/runc\"\n\n[crio.runtime.runtimes.kata]\nruntime_path = \"/usr/bin/kata-runtime\"\n\n[crio.runtime.runtimes.runsc]\nruntime_path = \"/usr/local/bin/runsc\"\n</code></pre> <p>After modifying the configuration, restart CRI-O:</p> <pre><code>sudo systemctl restart crio\n</code></pre>"},{"location":"kubernetes/cks/runtimeclass/#step-2-create-runtimeclass-objects","title":"Step 2: Create RuntimeClass Objects","text":"<p>Create RuntimeClass objects to reference your configured handlers:</p> <pre><code># runc-runtimeclass.yaml\napiVersion: node.k8s.io/v1\nkind: RuntimeClass\nmetadata:\n  name: runc\nhandler: runc\n</code></pre> <pre><code># kata-runtimeclass.yaml\napiVersion: node.k8s.io/v1\nkind: RuntimeClass\nmetadata:\n  name: kata\nhandler: kata\n</code></pre> <pre><code># gvisor-runtimeclass.yaml\napiVersion: node.k8s.io/v1\nkind: RuntimeClass\nmetadata:\n  name: gvisor\nhandler: gvisor\n</code></pre> <p>Apply the RuntimeClass objects:</p> <pre><code>kubectl apply -f runc-runtimeclass.yaml\nkubectl apply -f kata-runtimeclass.yaml\nkubectl apply -f gvisor-runtimeclass.yaml\n</code></pre> <p>Verify the RuntimeClass objects:</p> <pre><code>kubectl get runtimeclass\n</code></pre>"},{"location":"kubernetes/cks/runtimeclass/#using-runtimeclass-in-pods","title":"Using RuntimeClass in Pods","text":"<p>Once RuntimeClasses are defined, you can use them in your Pod specifications:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-kata\nspec:\n  runtimeClassName: kata\n  containers:\n  - name: nginx\n    image: nginx:latest\n</code></pre> <p>When you create this Pod, the kubelet will use the specified RuntimeClass (in this case, \u201ckata\u201d) to run the Pod\u2019s containers. The kubelet will use the \u201ckata\u201d handler which maps to Kata Containers runtime.</p>"},{"location":"kubernetes/cks/runtimeclass/#scheduling","title":"Scheduling","text":"<p>RuntimeClass also supports scheduling constraints to ensure Pods are scheduled to nodes that support the specified runtime.</p> <pre><code>apiVersion: node.k8s.io/v1\nkind: RuntimeClass\nmetadata:\n  name: kata\nhandler: kata\nscheduling:\n  nodeSelector:\n    runtime-kata: \"true\"\n  tolerations:\n  - key: \"runtime-kata\"\n    operator: \"Exists\"\n    effect: \"NoSchedule\"\n</code></pre> <p>With this configuration, Pods using the \u201ckata\u201d RuntimeClass will only be scheduled on nodes with the label <code>runtime-kata: \"true\"</code>, and they will tolerate the NoSchedule taint with the key \u201cruntime-kata\u201d.</p>"},{"location":"kubernetes/cks/runtimeclass/#practical-examples","title":"Practical Examples","text":"<p>This section provides concrete examples for setting up and using various RuntimeClasses.</p>"},{"location":"kubernetes/cks/runtimeclass/#example-1-using-gvisor-for-untrusted-workloads","title":"Example 1: Using gVisor for Untrusted Workloads","text":"<p>gVisor is a container runtime that provides an additional layer of isolation by implementing a user-space kernel. Here\u2019s how to set it up and use it.</p> <ol> <li> <p>Install gVisor on your nodes (commands will vary based on your operating system).</p> </li> <li> <p>Configure containerd:</p> </li> </ol> <pre><code>[plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runsc]\n  runtime_type = \"io.containerd.runsc.v1\"\n</code></pre> <ol> <li>Create a RuntimeClass:</li> </ol> <pre><code>apiVersion: node.k8s.io/v1\nkind: RuntimeClass\nmetadata:\n  name: gvisor\nhandler: runsc\n</code></pre> <ol> <li>Deploy a Pod that uses gVisor:</li> </ol> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-gvisor\nspec:\n  runtimeClassName: gvisor\n  containers:\n  - name: nginx\n    image: nginx:latest\n</code></pre>"},{"location":"kubernetes/cks/runtimeclass/#example-2-using-kata-containers-for-hardware-isolation","title":"Example 2: Using Kata Containers for Hardware Isolation","text":"<p>Kata Containers provide stronger isolation by running containers in lightweight VMs.</p> <ol> <li> <p>Install Kata Containers on your nodes.</p> </li> <li> <p>Configure containerd:</p> </li> </ol> <pre><code>[plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.kata-fc]\n  runtime_type = \"io.containerd.kata.v2\"\n  [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.kata-fc.options]\n    ConfigPath = \"/opt/kata/share/defaults/kata-containers/configuration-fc.toml\"\n</code></pre> <ol> <li>Create a RuntimeClass with scheduling and overhead:</li> </ol> <pre><code>apiVersion: node.k8s.io/v1\nkind: RuntimeClass\nmetadata:\n  name: kata-fc\nhandler: kata-fc\noverhead:\n  podFixed:\n    memory: \"130Mi\"\n    cpu: \"250m\"\nscheduling:\n  nodeSelector:\n    katacontainers.io/kata-runtime: \"true\"\n</code></pre> <ol> <li>Label nodes that support Kata Containers:</li> </ol> <pre><code>kubectl label nodes worker1 katacontainers.io/kata-runtime=true\n</code></pre> <ol> <li>Deploy a Pod that uses Kata Containers:</li> </ol> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: redis-kata\nspec:\n  runtimeClassName: kata-fc\n  containers:\n  - name: redis\n    image: redis:latest\n</code></pre>"},{"location":"kubernetes/cks/seccomp/","title":"Creating Seccomp Audit Profile","text":""},{"location":"kubernetes/cks/seccomp/#seccomp","title":"Seccomp","text":"<p>Seccomp (secure computing mode) is a Linux kernel feature that allows restricting the system calls that a process can make. In container environments, seccomp profiles provide an additional layer of security by limiting what actions containers can perform at the system call level.</p> <p>Audit mode allows you to log all system calls without blocking any of them, which is useful for: - Understanding what system calls an application requires - Creating baseline profiles for applications - Detecting potential malicious activities - Debugging permission issues</p>"},{"location":"kubernetes/cks/seccomp/#creating-an-audit-seccomp-profile","title":"Creating an Audit Seccomp Profile","text":"<p>Create a JSON file named <code>audit-seccomp.json</code> with the following content:</p> <pre><code>{\n  \"defaultAction\": \"SCMP_ACT_ALLOW\",\n  \"architectures\": [\n    \"SCMP_ARCH_X86_64\",\n    \"SCMP_ARCH_X86\",\n    \"SCMP_ARCH_AARCH64\"\n  ],\n  \"syscalls\": [\n    {\n      \"names\": [\n        \"open\",\n        \"openat\",\n        \"read\",\n        \"write\",\n        \"connect\",\n        \"socket\",\n        \"execve\",\n        \"clone\"\n      ],\n      \"action\": \"SCMP_ACT_LOG\"\n    }\n  ]\n}\n</code></pre> <p>This profile sets the default action to <code>SCMP_ACT_LOG</code>, which logs all system calls without blocking them. The <code>syscalls</code> array of this profile would only log the specified system calls and silently allow the rest. An empty <code>syscalls</code> array means no system calls have special handling - they\u2019re all logged.</p> <ul> <li>defaultAction: The action to take by default (in this case, log but allow all syscalls)</li> <li>architectures: The CPU architectures the profile applies to</li> <li>syscalls: A list of system calls with specific actions (empty in this audit profile)</li> </ul>"},{"location":"kubernetes/cks/seccomp/#installing-the-profile-on-a-worker-node","title":"Installing the Profile on a Worker Node","text":"<pre><code>sudo mkdir -p /var/lib/kubelet/seccomp/profiles\nsudo cp audit-seccomp.json /var/lib/kubelet/seccomp/profiles/\nsudo chmod 644 /var/lib/kubelet/seccomp/profiles/audit-seccomp.json\n</code></pre>"},{"location":"kubernetes/cks/seccomp/#applying-the-profile-to-containers","title":"Applying the Profile to Containers","text":""},{"location":"kubernetes/cks/seccomp/#method-1-using-annotations-older-approach","title":"Method 1: Using annotations (older approach)","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: audit-pod\n  annotations:\n    seccomp.security.alpha.kubernetes.io/pod: \"localhost/audit-seccomp.json\"\nspec:\n  containers:\n  - name: my-container\n    image: nginx\n</code></pre>"},{"location":"kubernetes/cks/seccomp/#method-2-using-securitycontext-recommended","title":"Method 2: Using securityContext (recommended)","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: audit-pod\nspec:\n  securityContext:\n    seccompProfile:\n      type: Localhost\n      localhostProfile: audit-seccomp.json\n  containers:\n  - name: my-container\n    image: nginx\n</code></pre>"},{"location":"kubernetes/cks/secure_docker_daemon/","title":"Secure Docker Daemon","text":"<p>The Docker daemon is a critical component that manages and coordinates all Docker operations. Since it often runs with elevated privileges, securing it properly is essential to protect your containerized applications and host systems from potential security threats.</p>"},{"location":"kubernetes/cks/secure_docker_daemon/#understanding-docker-daemon-security-risks","title":"Understanding Docker Daemon Security Risks","text":"<p>Before implementing security measures, it\u2019s important to understand the potential risks:</p> <ol> <li> <p>Privilege Escalation: The Docker daemon typically runs with root privileges, making it a prime target for attackers seeking to gain elevated access to your system.</p> </li> <li> <p>Unauthorized Access: An unsecured Docker daemon can be accessed by unauthorized users, potentially leading to data breaches or system compromise.</p> </li> <li> <p>Container Breakouts: Vulnerabilities in the Docker daemon can lead to container breakouts, where attackers escape container isolation and access the host system.</p> </li> <li> <p>API Exposure: The Docker API, if exposed unsafely, can be exploited for unauthorized operations.</p> </li> </ol>"},{"location":"kubernetes/cks/secure_docker_daemon/#configuring-and-securing-communication-sockets","title":"Configuring and Securing Communication Sockets","text":"<p>Docker daemon can communicate through different types of sockets. Understanding and properly configuring these communication channels is essential for security.</p>"},{"location":"kubernetes/cks/secure_docker_daemon/#1-unix-socket-default-and-recommended","title":"1. Unix Socket (Default and Recommended)","text":"<p>By default, Docker runs through a non-networked UNIX socket. It can also optionally communicate using SSH or a TLS (HTTPS) socket. The Unix socket is located at <code>/var/run/docker.sock</code> and is the most secure option for local communications.</p> <pre><code># The default configuration in daemon.json [/etc/docker/daemon.json] (often doesn't need to be specified)\n{\n  \"hosts\": [\"unix:///var/run/docker.sock\"]\n}\n</code></pre> <p>Unix sockets are more secure than TCP sockets because:</p> <ul> <li>They\u2019re not exposed to the network</li> <li>They use standard Unix file permissions for access control</li> <li>They\u2019re not prone to cross-site request forgery attacks that can happen with TCP sockets</li> </ul>"},{"location":"kubernetes/cks/secure_docker_daemon/#2-tcp-socket-for-remote-access","title":"2. TCP Socket (For Remote Access)","text":"<p>If you need remote access to the Docker daemon, you can configure it to listen on a TCP socket, but this should always be protected with TLS.</p> <p>When using a TCP socket, the Docker daemon provides un-encrypted and un-authenticated direct access to the Docker daemon by default. You should secure the daemon either using the built in HTTPS encrypted socket, or by putting a secure web proxy in front of it.</p> <pre><code># Example daemon.json [/etc/docker/daemon.json] with both Unix socket and secure TCP socket\n{\n  \"hosts\": [\n    \"unix:///var/run/docker.sock\",\n    \"tcp://0.0.0.0:2376\"\n  ],\n  \"tls\": true,\n  \"tlscacert\": \"/path/to/ca.pem\",\n  \"tlscert\": \"/path/to/server-cert.pem\",\n  \"tlskey\": \"/path/to/server-key.pem\",\n  \"tlsverify\": true\n}\n</code></pre> <p>Important:  When using the TCP socket:</p> <ul> <li>Never expose the Docker daemon to the internet without TLS encryption</li> <li>Docker over TLS should run on TCP port 2376 (not 2375, which is unencrypted)</li> <li>Use client certificate authentication</li> <li>Implement proper network firewall rules</li> </ul>"},{"location":"kubernetes/cks/secure_docker_daemon/#3-setting-up-tls-for-docker-daemon","title":"3. Setting Up TLS for Docker Daemon","text":"<p>To set up TLS for secure remote access:</p> <ol> <li>Generate CA, server, and client certificates</li> </ol> <pre><code># Create a CA key and certificate\nopenssl genrsa -out ca-key.pem 4096\nopenssl req -new -x509 -days 365 -key ca-key.pem -sha256 -out ca.pem\n\n# Create a server key and certificate signing request (CSR)\n# Set $HOST to the DNS name or IP of your Docker host\nopenssl genrsa -out server-key.pem 4096\nopenssl req -subj \"/CN=$HOST\" -sha256 -new -key server-key.pem -out server.csr\n\n# Sign the server certificate\n# Include IP addresses and DNS names that will be used to connect to your Docker host\necho \"subjectAltName = DNS:$HOST,IP:127.0.0.1,IP:$PUBLIC_IP\" &gt;&gt; extfile.cnf\necho \"extendedKeyUsage = serverAuth\" &gt;&gt; extfile.cnf\nopenssl x509 -req -days 365 -sha256 -in server.csr -CA ca.pem -CAkey ca-key.pem \\\n  -CAcreateserial -out server-cert.pem -extfile extfile.cnf\n\n# Create a client key and certificate signing request\nopenssl genrsa -out client-key.pem 4096\nopenssl req -subj '/CN=client' -new -key client-key.pem -out client.csr\n\n# Sign the client certificate\necho \"extendedKeyUsage = clientAuth\" &gt; extfile-client.cnf\nopenssl x509 -req -days 365 -sha256 -in client.csr -CA ca.pem -CAkey ca-key.pem \\\n  -CAcreateserial -out client-cert.pem -extfile extfile-client.cnf\n</code></pre> <ol> <li>Configure the Docker daemon to use TLS</li> </ol> <p>Update the <code>/etc/docker/daemon.json</code> (/etc/docker/daemon.json) file:</p> <pre><code>{\n  \"hosts\": [\"unix:///var/run/docker.sock\", \"tcp://0.0.0.0:2376\"],\n  \"tls\": true,\n  \"tlsverify\": true,\n  \"tlscacert\": \"/path/to/ca.pem\",\n  \"tlscert\": \"/path/to/server-cert.pem\",\n  \"tlskey\": \"/path/to/server-key.pem\"\n}\n</code></pre> <ol> <li>Configure your client to use TLS</li> </ol> <p>Replace all instances of $HOST in the following example with the DNS name of your Docker daemon\u2019s host.</p> <pre><code># When connecting, specify the TLS certificates\ndocker --tlsverify \\\n  --tlscacert=ca.pem \\\n  --tlscert=client-cert.pem \\\n  --tlskey=client-key.pem \\\n  -H=$HOST:2376 version\n\n# Or set environment variables\nexport DOCKER_HOST=tcp://$HOST:2376\nexport DOCKER_TLS_VERIFY=1\nexport DOCKER_CERT_PATH=/path/to/client/certificates\n</code></pre>"},{"location":"kubernetes/cks/secure_docker_daemon/#4-using-systemd-configuration-alternative-method","title":"4. Using systemd Configuration (Alternative Method)","text":"<p>If you\u2019re using systemd, you can also configure the Docker daemon through the service file:</p> <pre><code># Create a systemd override file\nsudo mkdir -p /etc/systemd/system/docker.service.d\nsudo touch /etc/systemd/system/docker.service.d/override.conf\n\n# Edit the override file\nsudo nano /etc/systemd/system/docker.service.d/override.conf\n</code></pre> <p>Add the following content to the <code>override.conf</code> file:</p> <pre><code>[Service]\nExecStart=\nExecStart=/usr/bin/dockerd -H fd:// -H unix:///var/run/docker.sock -H tcp://0.0.0.0:2376 --tlsverify --tlscacert=/path/to/ca.pem --tlscert=/path/to/server-cert.pem --tlskey=/path/to/server-key.pem\n</code></pre> <p>Reload systemd and restart Docker:</p> <pre><code>sudo systemctl daemon-reload\nsudo systemctl restart docker\n</code></pre>"},{"location":"kubernetes/cks/secure_docker_daemon/#5-verifying-socket-configuration","title":"5. Verifying Socket Configuration","text":"<p>After configuring your Docker daemon, verify that it\u2019s listening on the specified sockets:</p> <pre><code># Check for Unix socket\nls -la /var/run/docker.sock\n\n# Check for TCP socket\nsudo netstat -tulpn | grep dockerd\n</code></pre>"},{"location":"kubernetes/cks/secure_docker_daemon/#6-listening-on-both-unix-and-tcp-sockets","title":"6. Listening on Both Unix and TCP Sockets","text":"<p>When configuring Docker to listen on both types of sockets, be aware that the \u201chosts\u201d option overrides the Docker default value rather than appends to it. You must specify both the Unix socket and TCP socket in your configuration.</p> <pre><code>{\n  \"hosts\": [\"unix:///var/run/docker.sock\", \"tcp://0.0.0.0:2376\"]\n}\n</code></pre> <p>You can also verify the configuration is working with:</p> <pre><code>sudo netstat -lntp | grep dockerd\n</code></pre> <p>If properly configured, you should see Docker listening on both the Unix socket and the specified TCP port.</p>"},{"location":"kubernetes/cks/secure_docker_daemon/#3-implement-rootless-mode","title":"3. Implement Rootless Mode","text":"<p>Rootless mode allows running the Docker daemon and containers as a non-root user to mitigate potential vulnerabilities in the daemon and the container runtime.</p> <p>Rootless mode ensures that the Docker daemon and containers are running as an unprivileged user, which means that even if an attacker breaks out of the container, they will not have root privileges on the host, which in turn substantially limits the attack surface.</p> <pre><code># Set up rootless mode\ndockerd-rootless-setuptool.sh install\n</code></pre>"},{"location":"kubernetes/cks/secure_docker_daemon/#4-secure-the-docker-socket","title":"4. Secure the Docker Socket","text":"<p>Docker communicates with a UNIX domain socket called /var/run/docker.sock. This is the main entry point for the Docker API. Anyone who has access to the Docker daemon socket also has unrestricted root access.</p> <p>Best practices for socket security:</p> <ol> <li>Never expose the Docker socket to containers unless absolutely necessary</li> <li>Use proper Unix file permissions to restrict access</li> <li>If socket sharing is required, consider using a proxy like Docker Socket Proxy</li> </ol> <pre><code># Set proper permissions for the Docker socket\nsudo chmod 660 /var/run/docker.sock\nsudo chown root:docker /var/run/docker.sock\n</code></pre>"},{"location":"kubernetes/cks/secure_docker_daemon/#5-configure-user-namespace-remapping","title":"5. Configure User Namespace Remapping","text":"<p>User namespace remapping is a Docker feature that converts host UIDs to a different unprivileged range inside your containers. This helps to prevent privilege escalation attacks, where a process running in a container gains the same privileges as its UID has on your host.</p> <pre><code># Configure user namespace remapping in daemon.json\n{\n  \"userns-remap\": \"default\"\n}\n</code></pre>"},{"location":"kubernetes/cks/secure_docker_daemon/#6-restrict-inter-container-communication","title":"6. Restrict Inter-Container Communication","text":"<p>Docker normally allows arbitrary communication between the containers running on your host. Each new container is automatically added to the docker0 bridge network, which allows it to discover and contact its peers. Keeping inter-container communication (ICC) enabled is risky because it could permit a malicious process to launch an attack against neighboring containers.</p> <pre><code># Disable inter-container communication in daemon.json\n{\n  \"icc\": false\n}\n</code></pre>"},{"location":"kubernetes/cks/secure_docker_daemon/#7-implement-resource-limitations","title":"7. Implement Resource Limitations","text":"<p>Set appropriate resource constraints to prevent denial-of-service attacks:</p> <pre><code># Configure default resource constraints in daemon.json\n{\n  \"default-ulimits\": {\n    \"nofile\": {\n      \"Name\": \"nofile\",\n      \"Hard\": 64000,\n      \"Soft\": 64000\n    }\n  },\n  \"default-shm-size\": \"64M\",\n  \"default-memory-swap\": \"-1\",\n  \"default-memory\": \"1G\",\n  \"default-cpu-shares\": 1024\n}\n</code></pre>"},{"location":"kubernetes/cks/secure_docker_daemon/#8-enable-content-trust","title":"8. Enable Content Trust","text":"<p>Docker Engine can be configured to run only signed images, enhancing security through image signature verification. This feature, set up in the Docker configuration file (daemon.json), gives you control over enforcing security policies related to image usage.</p> <pre><code># Enable Docker Content Trust in daemon.json\n{\n  \"content-trust\": {\n    \"trust-pinning\": {\n      \"official-library-images\": true\n    }\n  }\n}\n</code></pre>"},{"location":"kubernetes/cks/secure_docker_daemon/#9-enable-logging-and-monitoring","title":"9. Enable Logging and Monitoring","text":"<p>Collecting Docker daemon logs is instrumental in identifying and responding to security incidents. These logs offer comprehensive insights into Docker\u2019s system-level operations, encompassing critical aspects such as container lifecycle events, network configurations, image management, and incoming API requests.</p> <pre><code># Configure logging in daemon.json\n{\n  \"log-driver\": \"json-file\",\n  \"log-opts\": {\n    \"max-size\": \"10m\",\n    \"max-file\": \"3\"\n  },\n  \"log-level\": \"info\"\n}\n</code></pre>"},{"location":"kubernetes/cks/secure_docker_daemon/#10-use-security-enhanced-systems","title":"10. Use Security Enhanced Systems","text":"<p>Ensuring OS-level security systems are active helps defend against malicious activity originating inside containers and the Docker daemon. Docker supports policies for SELinux, Seccomp, and AppArmor; keeping them enabled ensures sane defaults are applied to your containers, including restrictions for dangerous system calls.</p> <pre><code># Don't disable security-enhanced systems\n# Incorrect configuration (do not use):\n# {\n#   \"selinux-enabled\": false,\n#   \"apparmor-enabled\": false\n# }\n\n# Correct configuration:\n{\n  \"selinux-enabled\": true,\n  \"apparmor-enabled\": true\n}\n</code></pre>"},{"location":"kubernetes/cks/secure_docker_daemon/#comprehensive-etcdockerdaemonjson-example","title":"Comprehensive <code>/etc/docker/daemon.json</code> Example","text":"<p>Here\u2019s a comprehensive example of a secure <code>daemon.json</code> configuration that incorporates many of the best practices:</p> <pre><code>{\n  \"icc\": false,\n  \"log-level\": \"info\",\n  \"iptables\": true,\n  \"live-restore\": true,\n  \"userland-proxy\": false,\n  \"no-new-privileges\": true,\n  \"userns-remap\": \"default\",\n  \"storage-driver\": \"overlay2\",\n  \"default-ulimits\": {\n    \"nofile\": {\n      \"Name\": \"nofile\",\n      \"Hard\": 64000,\n      \"Soft\": 64000\n    }\n  },\n  \"selinux-enabled\": true,\n  \"apparmor-enabled\": true,\n  \"default-runtime\": \"runc\",\n  \"log-driver\": \"json-file\",\n  \"log-opts\": {\n    \"max-size\": \"10m\",\n    \"max-file\": \"3\"\n  },\n  \"content-trust\": {\n    \"trust-pinning\": {\n      \"official-library-images\": true\n    }\n  }\n}\n</code></pre>"},{"location":"kubernetes/cks/exercises/cilium/","title":"Cilium Network Policies Certification Guide","text":""},{"location":"kubernetes/cks/exercises/cilium/#layer-3-network-policies","title":"Layer 3 Network Policies","text":"<p>Layer 3 policies control traffic based on IP addresses and CIDR blocks.</p>"},{"location":"kubernetes/cks/exercises/cilium/#basic-l3-policy-allow-specific-ip-ranges","title":"Basic L3 Policy - Allow Specific IP Ranges","text":"<pre><code>apiVersion: cilium.io/v2\nkind: CiliumNetworkPolicy\nmetadata:\n  name: l3-allow-specific-ips\n  namespace: production\nspec:\n  endpointSelector:\n    matchLabels:\n      app: web-server\n  ingress:\n  - fromCIDR:\n    - \"10.0.1.0/24\"    # Allow from specific subnet\n    - \"192.168.1.100/32\" # Allow from specific IP\n  egress:\n  - toCIDR:\n    - \"10.0.2.0/24\"    # Allow to database subnet\n  - toFQDNs:\n    - matchName: \"api.external-service.com\"\n</code></pre>"},{"location":"kubernetes/cks/exercises/cilium/#l3-policy-block-external-traffic","title":"L3 Policy - Block External Traffic","text":"<pre><code>apiVersion: cilium.io/v2\nkind: CiliumNetworkPolicy\nmetadata:\n  name: l3-block-external\n  namespace: secure-zone\nspec:\n  endpointSelector:\n    matchLabels:\n      security: high\n  ingress:\n  - fromCIDR:\n    - \"10.0.0.0/8\"     # Only allow internal traffic\n    - \"172.16.0.0/12\"\n    - \"192.168.0.0/16\"\n  egress:\n  - toCIDR:\n    - \"10.0.0.0/8\"     # Only allow internal egress\n    - \"172.16.0.0/12\"\n    - \"192.168.0.0/16\"\n</code></pre>"},{"location":"kubernetes/cks/exercises/cilium/#l3-policy-with-node-selection","title":"L3 Policy with Node Selection","text":"<pre><code>apiVersion: cilium.io/v2\nkind: CiliumNetworkPolicy\nmetadata:\n  name: l3-node-based\nspec:\n  endpointSelector:\n    matchLabels:\n      app: monitoring\n  ingress:\n  - fromEntities:\n    - \"host\"           # Allow from node\n  - fromNodes:\n    - matchLabels:\n        node-role: \"worker\"\n</code></pre>"},{"location":"kubernetes/cks/exercises/cilium/#layer-4-network-policies","title":"Layer 4 Network Policies","text":"<p>Layer 4 policies control traffic based on ports and protocols.</p>"},{"location":"kubernetes/cks/exercises/cilium/#basic-l4-policy-port-specific-access","title":"Basic L4 Policy - Port-specific Access","text":"<pre><code>apiVersion: cilium.io/v2\nkind: CiliumNetworkPolicy\nmetadata:\n  name: l4-web-server-ports\n  namespace: web-app\nspec:\n  endpointSelector:\n    matchLabels:\n      app: nginx\n  ingress:\n  - fromEndpoints:\n    - matchLabels:\n        app: frontend\n    toPorts:\n    - ports:\n      - port: \"80\"\n        protocol: TCP\n      - port: \"443\"\n        protocol: TCP\n  - fromEndpoints:\n    - matchLabels:\n        app: monitoring\n    toPorts:\n    - ports:\n      - port: \"9090\"    # Metrics port\n        protocol: TCP\n</code></pre>"},{"location":"kubernetes/cks/exercises/cilium/#l4-policy-database-access-control","title":"L4 Policy - Database Access Control","text":"<pre><code>apiVersion: cilium.io/v2\nkind: CiliumNetworkPolicy\nmetadata:\n  name: l4-database-access\n  namespace: database\nspec:\n  endpointSelector:\n    matchLabels:\n      app: mysql\n  ingress:\n  - fromEndpoints:\n    - matchLabels:\n        tier: backend\n    toPorts:\n    - ports:\n      - port: \"3306\"\n        protocol: TCP\n  # Deny all other ingress traffic (implicit)\n  egress:\n  - {} # Allow all egress for database operations\n</code></pre>"},{"location":"kubernetes/cks/exercises/cilium/#l4-policy-multi-protocol-support","title":"L4 Policy - Multi-Protocol Support","text":"<pre><code>apiVersion: cilium.io/v2\nkind: CiliumNetworkPolicy\nmetadata:\n  name: l4-multi-protocol\nspec:\n  endpointSelector:\n    matchLabels:\n      app: dns-server\n  ingress:\n  - fromEndpoints:\n    - matchLabels:\n        role: client\n    toPorts:\n    - ports:\n      - port: \"53\"\n        protocol: TCP\n      - port: \"53\"\n        protocol: UDP\n    - ports:\n      - port: \"853\"      # DNS over TLS\n        protocol: TCP\n</code></pre>"},{"location":"kubernetes/cks/exercises/cilium/#icmp-policies","title":"ICMP Policies","text":"<p>ICMP policies control ping, traceroute, and other ICMP traffic.</p>"},{"location":"kubernetes/cks/exercises/cilium/#allow-icmp-for-monitoring","title":"Allow ICMP for Monitoring","text":"<pre><code>apiVersion: cilium.io/v2\nkind: CiliumNetworkPolicy\nmetadata:\n  name: icmp-monitoring\n  namespace: infrastructure\nspec:\n  endpointSelector:\n    matchLabels:\n      role: server\n  ingress:\n  - fromEndpoints:\n    - matchLabels:\n        app: monitoring\n    icmps:\n    - fields:\n      - type: 8          # Echo Request (ping)\n        code: 0\n  - fromEndpoints:\n    - matchLabels:\n        app: network-tools\n    icmps:\n    - fields:\n      - type: 8          # Echo Request\n      - type: 11         # Time Exceeded (traceroute)\n</code></pre>"},{"location":"kubernetes/cks/exercises/cilium/#block-icmp-from-external-sources","title":"Block ICMP from External Sources","text":"<pre><code>apiVersion: cilium.io/v2\nkind: CiliumNetworkPolicy\nmetadata:\n  name: icmp-block-external\nspec:\n  endpointSelector:\n    matchLabels:\n      exposure: internal\n  ingress:\n  - fromCIDR:\n    - \"10.0.0.0/8\"\n    icmps:\n    - fields:\n      - type: 8          # Allow internal ping\n  # ICMP from external sources blocked (no rule = deny)\n</code></pre>"},{"location":"kubernetes/cks/exercises/cilium/#icmp-troubleshooting-policy","title":"ICMP Troubleshooting Policy","text":"<pre><code>apiVersion: cilium.io/v2\nkind: CiliumNetworkPolicy\nmetadata:\n  name: icmp-troubleshooting\nspec:\n  endpointSelector:\n    matchLabels:\n      debug: enabled\n  ingress:\n  - fromEndpoints:\n    - matchLabels:\n        role: admin\n    icmps:\n    - fields:\n      - type: 8          # Echo Request\n      - type: 0          # Echo Reply\n      - type: 3          # Destination Unreachable\n      - type: 11         # Time Exceeded\n      - type: 12         # Parameter Problem\n</code></pre>"},{"location":"kubernetes/cks/exercises/cilium/#layer-7-application-layer-policies","title":"Layer 7 (Application Layer) Policies","text":"<p>Layer 7 policies provide HTTP/gRPC/Kafka protocol-aware filtering.</p>"},{"location":"kubernetes/cks/exercises/cilium/#http-based-l7-policy","title":"HTTP-based L7 Policy","text":"<pre><code>apiVersion: cilium.io/v2\nkind: CiliumNetworkPolicy\nmetadata:\n  name: l7-http-api-access\n  namespace: api-gateway\nspec:\n  endpointSelector:\n    matchLabels:\n      app: api-server\n  ingress:\n  - fromEndpoints:\n    - matchLabels:\n        app: frontend\n    toPorts:\n    - ports:\n      - port: \"8080\"\n        protocol: TCP\n      rules:\n        http:\n        - method: \"GET\"\n          path: \"/api/v1/users\"\n        - method: \"POST\"\n          path: \"/api/v1/users\"\n        - method: \"GET\"\n          path: \"/api/v1/health\"\n  - fromEndpoints:\n    - matchLabels:\n        app: admin-panel\n    toPorts:\n    - ports:\n      - port: \"8080\"\n        protocol: TCP\n      rules:\n        http:\n        - method: \"GET\"\n          path: \"/api/v1/.*\"    # Regex for any v1 endpoint\n        - method: \"POST\"\n          path: \"/api/v1/.*\"\n        - method: \"PUT\"\n          path: \"/api/v1/.*\"\n        - method: \"DELETE\"\n          path: \"/api/v1/.*\"\n</code></pre>"},{"location":"kubernetes/cks/exercises/cilium/#l7-policy-with-headers","title":"L7 Policy with Headers","text":"<pre><code>apiVersion: cilium.io/v2\nkind: CiliumNetworkPolicy\nmetadata:\n  name: l7-http-headers\nspec:\n  endpointSelector:\n    matchLabels:\n      app: auth-service\n  ingress:\n  - fromEndpoints:\n    - matchLabels:\n        app: web-app\n    toPorts:\n    - ports:\n      - port: \"80\"\n        protocol: TCP\n      rules:\n        http:\n        - method: \"POST\"\n          path: \"/auth/login\"\n          headers:\n          - \"Content-Type: application/json\"\n        - method: \"GET\"\n          path: \"/auth/verify\"\n          headers:\n          - \"Authorization: Bearer .*\"\n</code></pre>"},{"location":"kubernetes/cks/exercises/cilium/#l7-grpc-policy","title":"L7 gRPC Policy","text":"<pre><code>apiVersion: cilium.io/v2\nkind: CiliumNetworkPolicy\nmetadata:\n  name: l7-grpc-service\nspec:\n  endpointSelector:\n    matchLabels:\n      app: grpc-service\n  ingress:\n  - fromEndpoints:\n    - matchLabels:\n        app: grpc-client\n    toPorts:\n    - ports:\n      - port: \"9090\"\n        protocol: TCP\n      rules:\n        grpc:\n        - service: \"user.UserService\"\n          method: \"GetUser\"\n        - service: \"user.UserService\"\n          method: \"ListUsers\"\n</code></pre>"},{"location":"kubernetes/cks/exercises/cilium/#l7-dns-policy","title":"L7 DNS Policy","text":"<pre><code>apiVersion: cilium.io/v2\nkind: CiliumNetworkPolicy\nmetadata:\n  name: l7-dns-filtering\nspec:\n  endpointSelector:\n    matchLabels:\n      app: web-crawler\n  egress:\n  - toEndpoints:\n    - matchLabels:\n        k8s:io.kubernetes.pod.namespace: kube-system\n        k8s:app: kube-dns\n    toPorts:\n    - ports:\n      - port: \"53\"\n        protocol: UDP\n      rules:\n        dns:\n        - matchPattern: \"*.example.com\"\n        - matchPattern: \"api.allowed-service.org\"\n        - matchName: \"specific-host.com\"\n</code></pre>"},{"location":"kubernetes/cks/exercises/cilium/#deny-policies","title":"Deny Policies","text":"<p>Cilium supports explicit deny policies for enhanced security.</p>"},{"location":"kubernetes/cks/exercises/cilium/#explicit-deny-policy","title":"Explicit Deny Policy","text":"<pre><code>apiVersion: cilium.io/v2\nkind: CiliumNetworkPolicy\nmetadata:\n  name: deny-suspicious-traffic\nspec:\n  endpointSelector:\n    matchLabels:\n      security: strict\n  ingressDeny:\n  - fromCIDR:\n    - \"192.168.100.0/24\"  # Block specific subnet\n  - fromEndpoints:\n    - matchLabels:\n        reputation: suspicious\n  egressDeny:\n  - toPorts:\n    - ports:\n      - port: \"22\"        # Block SSH\n        protocol: TCP\n      - port: \"3389\"      # Block RDP\n        protocol: TCP\n</code></pre>"},{"location":"kubernetes/cks/exercises/cilium/#deny-with-l7-rules","title":"Deny with L7 Rules","text":"<pre><code>apiVersion: cilium.io/v2\nkind: CiliumNetworkPolicy\nmetadata:\n  name: deny-admin-endpoints\nspec:\n  endpointSelector:\n    matchLabels:\n      app: web-server\n  ingressDeny:\n  - fromEndpoints:\n    - matchLabels:\n        role: user\n    toPorts:\n    - ports:\n      - port: \"80\"\n        protocol: TCP\n      rules:\n        http:\n        - method: \"GET\"\n          path: \"/admin/.*\"\n        - method: \"POST\"\n          path: \"/admin/.*\"\n</code></pre>"},{"location":"kubernetes/cks/exercises/cilium/#time-based-deny-advanced","title":"Time-based Deny (Advanced)","text":"<pre><code>apiVersion: cilium.io/v2\nkind: CiliumNetworkPolicy\nmetadata:\n  name: deny-after-hours\nspec:\n  endpointSelector:\n    matchLabels:\n      access: business-hours\n  ingressDeny:\n  - fromEndpoints:\n    - matchLabels:\n        role: employee\n    toPorts:\n    - ports:\n      - port: \"443\"\n        protocol: TCP\n      rules:\n        http:\n        - method: \".*\"\n          path: \"/.*\"\n          headers:\n          - \"X-Time-Restriction: after-hours\"\n</code></pre>"},{"location":"kubernetes/cks/exercises/cilium/#allow-policies","title":"Allow Policies","text":"<p>Allow policies explicitly permit traffic and can work with deny policies.</p>"},{"location":"kubernetes/cks/exercises/cilium/#basic-allow-policy","title":"Basic Allow Policy","text":"<pre><code>apiVersion: cilium.io/v2\nkind: CiliumNetworkPolicy\nmetadata:\n  name: allow-essential-services\nspec:\n  endpointSelector:\n    matchLabels:\n      app: microservice\n  ingress:\n  - fromEndpoints:\n    - matchLabels:\n        app: gateway\n    toPorts:\n    - ports:\n      - port: \"8080\"\n        protocol: TCP\n  egress:\n  - toEndpoints:\n    - matchLabels:\n        app: database\n    toPorts:\n    - ports:\n      - port: \"5432\"\n        protocol: TCP\n  - toFQDNs:\n    - matchName: \"logging.external.com\"\n</code></pre>"},{"location":"kubernetes/cks/exercises/cilium/#allow-with-service-mesh-integration","title":"Allow with Service Mesh Integration","text":"<pre><code>apiVersion: cilium.io/v2\nkind: CiliumNetworkPolicy\nmetadata:\n  name: allow-istio-sidecar\nspec:\n  endpointSelector:\n    matchLabels:\n      app: bookinfo\n  ingress:\n  - fromEndpoints:\n    - matchLabels:\n        \"k8s:io.kubernetes.pod.namespace\": istio-system\n  - fromEndpoints:\n    - {} # Allow from any pod in same namespace\n    toPorts:\n    - ports:\n      - port: \"15001\"     # Istio proxy\n        protocol: TCP\n</code></pre>"},{"location":"kubernetes/cks/exercises/cilium/#advanced-scenarios","title":"Advanced Scenarios","text":""},{"location":"kubernetes/cks/exercises/cilium/#multi-tier-application-security","title":"Multi-tier Application Security","text":"<pre><code># Frontend tier\napiVersion: cilium.io/v2\nkind: CiliumNetworkPolicy\nmetadata:\n  name: frontend-policy\n  namespace: web-app\nspec:\n  endpointSelector:\n    matchLabels:\n      tier: frontend\n  ingress:\n  - fromCIDR:\n    - \"0.0.0.0/0\"        # Allow from internet\n    toPorts:\n    - ports:\n      - port: \"80\"\n        protocol: TCP\n      - port: \"443\"\n        protocol: TCP\n  egress:\n  - toEndpoints:\n    - matchLabels:\n        tier: backend\n    toPorts:\n    - ports:\n      - port: \"8080\"\n        protocol: TCP\n      rules:\n        http:\n        - method: \"GET\"\n          path: \"/api/.*\"\n        - method: \"POST\"\n          path: \"/api/.*\"\n\n---\n# Backend tier\napiVersion: cilium.io/v2\nkind: CiliumNetworkPolicy\nmetadata:\n  name: backend-policy\n  namespace: web-app\nspec:\n  endpointSelector:\n    matchLabels:\n      tier: backend\n  ingress:\n  - fromEndpoints:\n    - matchLabels:\n        tier: frontend\n    toPorts:\n    - ports:\n      - port: \"8080\"\n        protocol: TCP\n  egress:\n  - toEndpoints:\n    - matchLabels:\n        tier: database\n    toPorts:\n    - ports:\n      - port: \"5432\"\n        protocol: TCP\n\n---\n# Database tier\napiVersion: cilium.io/v2\nkind: CiliumNetworkPolicy\nmetadata:\n  name: database-policy\n  namespace: web-app\nspec:\n  endpointSelector:\n    matchLabels:\n      tier: database\n  ingress:\n  - fromEndpoints:\n    - matchLabels:\n        tier: backend\n    toPorts:\n    - ports:\n      - port: \"5432\"\n        protocol: TCP\n  # No egress rules = deny all egress\n</code></pre>"},{"location":"kubernetes/cks/exercises/cilium/#cross-namespace-communication","title":"Cross-Namespace Communication","text":"<pre><code>apiVersion: cilium.io/v2\nkind: CiliumClusterwideNetworkPolicy\nmetadata:\n  name: cross-namespace-api\nspec:\n  endpointSelector:\n    matchLabels:\n      app: shared-api\n  ingress:\n  - fromEndpoints:\n    - matchLabels:\n        \"k8s:io.kubernetes.pod.namespace\": \"frontend\"\n        app: web-app\n    - matchLabels:\n        \"k8s:io.kubernetes.pod.namespace\": \"mobile\"\n        app: mobile-api\n    toPorts:\n    - ports:\n      - port: \"8080\"\n        protocol: TCP\n      rules:\n        http:\n        - method: \"GET\"\n          path: \"/shared/.*\"\n</code></pre>"},{"location":"kubernetes/cks/exercises/cilium/#zero-trust-network","title":"Zero Trust Network","text":"<pre><code>apiVersion: cilium.io/v2\nkind: CiliumClusterwideNetworkPolicy\nmetadata:\n  name: zero-trust-default-deny\nspec:\n  endpointSelector: {}   # Apply to all pods\n  ingressDeny:\n  - fromEntities:\n    - \"all\"\n  egressDeny:\n  - toEntities:\n    - \"all\"\n\n---\n# Explicit allow for essential services\napiVersion: cilium.io/v2\nkind: CiliumNetworkPolicy\nmetadata:\n  name: allow-dns\n  namespace: kube-system\nspec:\n  endpointSelector:\n    matchLabels:\n      k8s-app: kube-dns\n  ingress:\n  - fromEntities:\n    - \"cluster\"\n    toPorts:\n    - ports:\n      - port: \"53\"\n        protocol: UDP\n      - port: \"53\"\n        protocol: TCP\n</code></pre>"},{"location":"kubernetes/cks/exercises/docker-socket-security/","title":"Editing Docker Socket Service File to Change Group Ownership to Root","text":""},{"location":"kubernetes/cks/exercises/docker-socket-security/#problem-statement","title":"Problem Statement","text":"<p>Edit the docker.socket service file to make the group owned by root instead of the default docker group.</p>"},{"location":"kubernetes/cks/exercises/docker-socket-security/#solution","title":"Solution","text":"<p>The Docker socket file (<code>docker.socket</code>) defines permissions for the Docker daemon socket. By default, this socket is owned by root:docker, which allows any user in the docker group to interact with the Docker daemon with essentially root privileges. Changing the group ownership to root restricts Docker access to only the root user, enhancing security.</p>"},{"location":"kubernetes/cks/exercises/docker-socket-security/#step-1-locate-the-dockersocket-service-file","title":"Step 1: Locate the docker.socket Service File","text":"<p>First, let\u2019s find the exact location of the docker.socket service file:</p> <pre><code>find /lib/systemd/system /usr/lib/systemd/system -name \"docker.socket\"\n</code></pre> <p>This typically returns: <pre><code>/lib/systemd/system/docker.socket\n</code></pre></p>"},{"location":"kubernetes/cks/exercises/docker-socket-security/#step-2-examine-the-current-configuration","title":"Step 2: Examine the Current Configuration","text":"<p>Before making changes, examine the current configuration:</p> <pre><code>cat /lib/systemd/system/docker.socket\n</code></pre> <p>The file should look something like this:</p> <pre><code>[Unit]\nDescription=Docker Socket for the API\n\n[Socket]\nListenStream=/var/run/docker.sock\nSocketMode=0660\nSocketUser=root\nSocketGroup=docker\n\n[Install]\nWantedBy=sockets.target\n</code></pre> <p>Notice that the <code>SocketGroup</code> is set to <code>docker</code>.</p>"},{"location":"kubernetes/cks/exercises/docker-socket-security/#step-3-edit-the-dockersocket-file","title":"Step 3: Edit the docker.socket File","text":"<p>Now edit the file to change the group ownership to root:</p> <pre><code>sudo vi /lib/systemd/system/docker.socket\n</code></pre> <p>Modify the <code>SocketGroup</code> line to use root instead of docker:</p> <pre><code>[Unit]\nDescription=Docker Socket for the API\n\n[Socket]\nListenStream=/var/run/docker.sock\nSocketMode=0660\nSocketUser=root\nSocketGroup=root\n\n[Install]\nWantedBy=sockets.target\n</code></pre> <p>Save and exit the editor.</p>"},{"location":"kubernetes/cks/exercises/docker-socket-security/#step-4-reload-the-systemd-daemon","title":"Step 4: Reload the Systemd Daemon","text":"<p>After editing the service file, reload the systemd daemon to recognize the changes:</p> <pre><code>sudo systemctl daemon-reload\n</code></pre>"},{"location":"kubernetes/cks/exercises/docker-socket-security/#step-5-restart-the-docker-socket-and-service","title":"Step 5: Restart the Docker Socket and Service","text":"<p>Restart both the Docker socket and the Docker service to apply the changes:</p> <pre><code>sudo systemctl restart docker.socket\nsudo systemctl restart docker\n</code></pre>"},{"location":"kubernetes/cks/exercises/docker-socket-security/#step-6-verify-the-change","title":"Step 6: Verify the Change","text":"<p>Verify that the Docker socket is now owned by root:root:</p> <pre><code>ls -la /var/run/docker.sock\n</code></pre> <p>You should see output similar to this:</p> <pre><code>srw-rw---- 1 root root 0 May 11 10:24 /var/run/docker.sock\n</code></pre> <p>Notice that both the user and group are now \u201croot\u201d instead of \u201croot docker\u201d.</p>"},{"location":"kubernetes/cks/exercises/docker-socket-security/#step-7-test-docker-functionality","title":"Step 7: Test Docker Functionality","text":"<p>Test that Docker still works when accessed with sudo:</p> <pre><code>sudo docker ps\n</code></pre> <p>This should execute successfully and list running containers if any.</p> <p>Try to run Docker as a non-root user:</p> <pre><code>docker ps\n</code></pre> <p>This should fail with a permissions error, confirming that only root can now access Docker.</p>"},{"location":"kubernetes/cks/exercises/docker-socket-security/#security-implications","title":"Security Implications","text":"<ol> <li> <p>Improved Security: Only the root user can now interact with the Docker daemon, reducing the attack surface.</p> </li> <li> <p>Restricted Access: Non-root users (even those in the docker group) can no longer run Docker commands directly.</p> </li> <li> <p>Operational Impact: Any applications or services that rely on docker group access will need to be reconfigured to use sudo or another privilege escalation method.</p> </li> <li> <p>Principle of Least Privilege: This change implements the security principle of granting only the minimum necessary privileges.</p> </li> </ol>"},{"location":"kubernetes/cks/exercises/docker-socket-security/#conclusion","title":"Conclusion","text":"<p>By changing the Docker socket group ownership from <code>docker</code> to <code>root</code>, we\u2019ve significantly enhanced the security of the Docker daemon by restricting access to only the root user. This modification aligns with security best practices for systems where Docker access should be tightly controlled.</p> <p>Remember that after this change, all Docker commands must be executed with sudo or by the root user, which might require adjustments to scripts, CI/CD pipelines, or user workflows that previously relied on docker group membership.</p>"},{"location":"kubernetes/cks/exercises/falco-rule/","title":"Falco Rule to Detect Container Access to /dev/mem","text":""},{"location":"kubernetes/cks/exercises/falco-rule/#problem-statement","title":"Problem Statement","text":"<p>Create a Falco rule to detect which container is accessing the <code>/dev/mem</code> folder. The rule output should include Kubernetes namespace name and pod/container ID information.</p>"},{"location":"kubernetes/cks/exercises/falco-rule/#solution","title":"Solution","text":"<p>Falco is an open-source cloud-native runtime security tool that can detect anomalous behavior in your containers and applications. For this task, we need to create a rule that specifically monitors for container access to the <code>/dev/mem</code> device, which could indicate a privileged container attempting to access physical memory directly (a potential security concern).</p>"},{"location":"kubernetes/cks/exercises/falco-rule/#rule-implementation","title":"Rule Implementation","text":"<pre><code>- rule: Container Accessing /dev/mem\n  desc: Detect attempts by containers to access /dev/mem device\n  condition: &gt;\n    open_read and \n    container and \n    fd.name = \"/dev/mem\" and \n    not falco_privileged_containers\n  output: &gt;\n    Container accessed /dev/mem device \n    (user=%user.name user_uid=%user.uid \n    command=%proc.cmdline %container.info \n    pod=%k8s.pod.name ns=%k8s.ns.name container_id=%container.id image=%container.image.repository)\n  priority: WARNING\n  tags: [container, access, memory, dev, k8s]\n</code></pre>"},{"location":"kubernetes/cks/exercises/falco-rule/#rule-explanation","title":"Rule Explanation","text":"<p>Let\u2019s break down the components of this rule:</p>"},{"location":"kubernetes/cks/exercises/falco-rule/#rule-metadata","title":"Rule Metadata","text":"<ul> <li>rule: Name of the rule - \u201cContainer Accessing /dev/mem\u201d</li> <li>desc: Brief description of what the rule detects</li> <li>priority: Set to WARNING as this is potentially suspicious activity </li> <li>tags: Keywords for organizing and categorizing the rule</li> </ul>"},{"location":"kubernetes/cks/exercises/falco-rule/#condition","title":"Condition","text":"<p>The condition defines when the rule should trigger:</p> <pre><code>open_read and container and fd.name = \"/dev/mem\" and not falco_privileged_containers\n</code></pre> <p>This condition will match when: - An <code>open_read</code> syscall occurs (a file is opened for reading) - The activity happens within a container - The file being accessed is specifically <code>/dev/mem</code> - The container is not already in the list of known privileged containers (to reduce false positives)</p>"},{"location":"kubernetes/cks/exercises/falco-rule/#output","title":"Output","text":"<p>The output specifies what information to include in the alert:</p> <pre><code>Container accessed /dev/mem device (user=%user.name user_uid=%user.uid command=%proc.cmdline %container.info pod=%k8s.pod.name ns=%k8s.ns.name container_id=%container.id image=%container.image.repository)\n</code></pre> <p>This will output: - The username and UID of the process - The command that was run - Container information - Pod name (as required) - Kubernetes namespace (as required) - Container ID (as required) - Container image information</p>"},{"location":"kubernetes/cks/exercises/falco-rule/#testing-the-rule","title":"Testing the Rule","text":"<ol> <li>Save the rule to a file (e.g., <code>dev-mem-access.yaml</code>)</li> <li>Deploy it to your Falco configuration:    <pre><code>sudo cp dev-mem-access.yaml /etc/falco/rules.d/\n</code></pre></li> <li> <p>Restart Falco:    <pre><code>sudo systemctl restart falco\n</code></pre></p> </li> <li> <p>To test the rule, you can run a privileged container that attempts to access <code>/dev/mem</code>:    <pre><code>kubectl run test-pod --image=alpine --overrides='{\"spec\": {\"containers\": [{\"name\": \"test-pod\", \"image\": \"alpine\", \"command\": [\"sh\", \"-c\", \"sleep 3 &amp;&amp; cat /dev/mem 2&gt;/dev/null || echo 'Access attempted'\"], \"securityContext\": {\"privileged\": true}}]}}'\n</code></pre></p> </li> </ol>"},{"location":"kubernetes/cks/exercises/falco-rule/#example-alert-output","title":"Example Alert Output","text":"<p>When the rule is triggered, you should see an alert in the Falco logs that looks like:</p> <pre><code>Warning Container Accessing /dev/mem: Container accessed /dev/mem device (user=root user_uid=0 command=cat /dev/mem container=test-pod pod=test-pod-6f7d9c5b9-xv7tj ns=default container_id=e8913b725fd3 image=alpine)\n</code></pre>"},{"location":"kubernetes/cks/exercises/falco-rule/#additional-considerations","title":"Additional Considerations","text":"<ol> <li> <p>This rule might generate false positives in environments where legitimate access to <code>/dev/mem</code> is needed.</p> </li> <li> <p>Consider adding a macro if you have specific containers that legitimately need to access <code>/dev/mem</code>:    <pre><code>- macro: authorized_mem_access_containers\n  condition: (container.name in (authorized-container-1, authorized-container-2))\n</code></pre></p> </li> </ol> <p>Then modify the rule condition:    <pre><code>condition: open_read and container and fd.name = \"/dev/mem\" and not falco_privileged_containers and not authorized_mem_access_containers\n</code></pre></p> <ol> <li>For production environments, you might want to increase the priority to ERROR if this activity is strictly prohibited in your security policies.</li> </ol>"},{"location":"kubernetes/cks/exercises/falco-rule/#conclusion","title":"Conclusion","text":"<p>This Falco rule effectively monitors and alerts on container access to the <code>/dev/mem</code> device with the required Kubernetes namespace and container identification information included in the output.</p>"},{"location":"kubernetes/cks/exercises/hostpath-only/","title":"Creating a Pod with Host Path Access and Falco Detection","text":""},{"location":"kubernetes/cks/exercises/hostpath-only/#problem-statement","title":"Problem Statement","text":"<p>Create a deployment that can read and write to a specific path in the underlying host machine\u2019s filesystem using a normal volume added to the container specs. The solution must ensure that access to any other host paths is denied. After creating the deployment and verifying it works, create Falco rules to detect this activity, identifying which container is performing the actions and displaying the output via journalctl.</p>"},{"location":"kubernetes/cks/exercises/hostpath-only/#solution","title":"Solution","text":""},{"location":"kubernetes/cks/exercises/hostpath-only/#part-1-creating-a-deployment-with-host-path-access","title":"Part 1: Creating a Deployment with Host Path Access","text":"<p>Let\u2019s create a deployment that can access the host filesystem:</p> <pre><code># host-path-deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: host-path-access\n  labels:\n    app: host-path-access\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: host-path-access\n  template:\n    metadata:\n      labels:\n        app: host-path-access\n    spec:\n      securityContext:\n        runAsUser: 1000\n        runAsGroup: 1000\n        fsGroup: 1000\n      containers:\n      - name: host-path-container\n        image: ubuntu:20.04\n        command: [\"sleep\", \"infinity\"]\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop: [\"ALL\"]\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: host-volume\n          mountPath: /host-data\n      volumes:\n      - name: host-volume\n        hostPath:\n          path: /opt/app-data  # The specific host path we want to access\n          type: DirectoryOrCreate\n</code></pre> <p>Apply the deployment: <pre><code>kubectl apply -f host-path-deployment.yaml\n</code></pre></p>"},{"location":"kubernetes/cks/exercises/hostpath-only/#part-2-verify-host-path-access","title":"Part 2: Verify Host Path Access","text":"<pre><code># Get the pod name\nPOD_NAME=$(kubectl get pods -l app=host-path-access -o jsonpath='{.items[0].metadata.name}')\n\n# Exec into the pod\nkubectl exec -it $POD_NAME -- bash\n\n# Inside the pod, verify access to the host path\nls -la /host-data\necho \"Test file for host path access\" &gt; /host-data/test.txt\ncat /host-data/test.txt\n\n# Verify we can't write to other locations\necho \"This should fail\" &gt; /etc/test.txt  # Should fail\ntouch /bin/test-file  # Should fail\n\n# Exit the pod\nexit\n</code></pre>"},{"location":"kubernetes/cks/exercises/hostpath-only/#part-3-creating-falco-rules-to-detect-host-path-access","title":"Part 3: Creating Falco Rules to Detect Host Path Access","text":"<p>Now, let\u2019s create Falco rules to detect when containers access this specific host path:</p> <pre><code># host-path-detection.yaml\n- rule: Container Accessing Host Path\n  desc: Detects when a container accesses the specific host path\n  condition: &gt;\n    container \n    and fd.name startswith \"/host-data\"\n    and (evt.type=open or evt.type=openat)\n  output: &gt;\n    Container accessing host path \n    (user=%user.name user_uid=%user.uid command=%proc.cmdline \n    file=%fd.name access_type=%evt.is_open_read,%evt.is_open_write\n    container_id=%container.id container_name=%container.name\n    image=%container.image.repository pod=%k8s.pod.name ns=%k8s.ns.name)\n  priority: INFO\n  tags: [filesystem, access, allowed]\n\n- rule: Container Writing To Host Path\n  desc: Detects when a container writes to the specific host path\n  condition: &gt;\n    container \n    and fd.name startswith \"/host-data\"\n    and (evt.type=open or evt.type=openat)\n    and evt.is_open_write=true\n  output: &gt;\n    Container WRITING to host path \n    (user=%user.name user_uid=%user.uid command=%proc.cmdline \n    file=%fd.name container_id=%container.id container_name=%container.name\n    image=%container.image.repository pod=%k8s.pod.name ns=%k8s.ns.name)\n  priority: INFO\n  tags: [filesystem, modification, allowed]\n\n- rule: Container Accessing Disallowed Path\n  desc: Detects when a container attempts to access any path other than the allowed host path\n  condition: &gt;\n    container \n    and fd.name startswith \"/\"\n    and not fd.name startswith \"/host-data\"\n    and not fd.name in (/proc, /dev, /sys) \n    and (evt.type=open or evt.type=openat)\n    and evt.is_open_write=true\n  output: &gt;\n    Suspicious: Container attempting to write to DISALLOWED path \n    (user=%user.name user_uid=%user.uid command=%proc.cmdline \n    file=%fd.name container_id=%container.id container_name=%container.name\n    image=%container.image.repository pod=%k8s.pod.name ns=%k8s.ns.name)\n  priority: WARNING\n  tags: [filesystem, modification, suspicious]\n</code></pre>"},{"location":"kubernetes/cks/exercises/hostpath-only/#part-4-deploy-the-falco-rules","title":"Part 4: Deploy the Falco Rules","text":"<p>If you\u2019re using Falco as a DaemonSet in Kubernetes:</p> <pre><code># Create a ConfigMap with the rules\nkubectl create configmap falco-hostpath-rules --from-file=host-path-detection.yaml\n\n# Update the Falco DaemonSet to use this ConfigMap\nkubectl -n falco patch daemonset falco --type=json -p='[\n  {\n    \"op\": \"add\",\n    \"path\": \"/spec/template/spec/volumes/-\",\n    \"value\": {\n      \"name\": \"hostpath-rules\",\n      \"configMap\": {\n        \"name\": \"falco-hostpath-rules\"\n      }\n    }\n  },\n  {\n    \"op\": \"add\",\n    \"path\": \"/spec/template/spec/containers/0/volumeMounts/-\",\n    \"value\": {\n      \"mountPath\": \"/etc/falco/rules.d/\",\n      \"name\": \"hostpath-rules\"\n    }\n  }\n]'\n</code></pre> <p>If you\u2019re running Falco directly on the host:</p> <pre><code># Copy the rules file to the Falco rules directory\nsudo cp host-path-detection.yaml /etc/falco/rules.d/\n\n# Restart Falco\nsudo systemctl restart falco\n</code></pre>"},{"location":"kubernetes/cks/exercises/hostpath-only/#part-5-monitor-falco-alerts-with-journalctl","title":"Part 5: Monitor Falco Alerts with journalctl","text":"<pre><code># Follow Falco logs in journalctl\nsudo journalctl -fu falco\n</code></pre>"},{"location":"kubernetes/cks/exercises/hostpath-only/#part-6-test-the-falco-detection","title":"Part 6: Test the Falco Detection","text":"<pre><code># Trigger allowed access to host path\nkubectl exec -it $POD_NAME -- bash -c \"echo 'Allowed write to host path' &gt; /host-data/falco-test.txt\"\n\n# Attempt disallowed access\nkubectl exec -it $POD_NAME -- bash -c \"touch /etc/not-allowed.txt || echo 'Access denied as expected'\"\n</code></pre> <p>You should see Falco alerts in the journalctl output that identify which container is accessing the host path.</p>"},{"location":"kubernetes/cks/exercises/hostpath-only/#conclusion","title":"Conclusion","text":"<p>This solution demonstrates:</p> <ol> <li> <p>How to create a deployment that can access a specific host path using a direct hostPath volume</p> </li> <li> <p>How to restrict file system access to only that specific path</p> </li> <li> <p>How to configure Falco rules to:</p> </li> <li>Detect access to the allowed host path</li> <li>Alert on attempts to access disallowed paths</li> <li> <p>Identify which container is performing these actions</p> </li> <li> <p>How to monitor these activities in real-time using journalctl</p> </li> </ol> <p>The solution fulfills the requirements by providing a deployment with access to a specific host path and proper Falco detection configured to monitor this activity.</p>"},{"location":"kubernetes/cks/exercises/istio-mtls/","title":"Securing Pod-to-Pod Encryption with mTLS Using Istio and PeerAuthentication","text":""},{"location":"kubernetes/cks/exercises/istio-mtls/#problem-statement","title":"Problem Statement","text":"<p>Implement mutual TLS (mTLS) encryption for pod-to-pod communication within a specific namespace using Istio and the PeerAuthentication resource.</p>"},{"location":"kubernetes/cks/exercises/istio-mtls/#solution","title":"Solution","text":"<p>Mutual TLS (mTLS) provides strong security for service-to-service communication by ensuring both the client and server verify each other\u2019s identity. Istio\u2019s PeerAuthentication resource makes it easy to enable and enforce mTLS across a namespace. This solution demonstrates how to implement mTLS for all services within a specific namespace.</p>"},{"location":"kubernetes/cks/exercises/istio-mtls/#prerequisites","title":"Prerequisites","text":"<ol> <li>A Kubernetes cluster with Istio installed</li> <li><code>kubectl</code> configured to interact with your cluster</li> <li>Administrative access to the cluster</li> <li>Istio\u2019s control plane (istiod) running</li> </ol>"},{"location":"kubernetes/cks/exercises/istio-mtls/#step-1-verify-istio-installation","title":"Step 1: Verify Istio Installation","text":"<p>First, let\u2019s verify that Istio is properly installed in your cluster:</p> <pre><code># Check if Istio components are installed and running\nkubectl get pods -n istio-system\n\n# Sample output:\n# NAME                                    READY   STATUS    RESTARTS   AGE\n# istio-ingressgateway-6b9c847cf-m4xb7    1/1     Running   0          24h\n# istiod-5d49996999-8rvdl                 1/1     Running   0          24h\n</code></pre>"},{"location":"kubernetes/cks/exercises/istio-mtls/#step-2-create-or-identify-the-target-namespace","title":"Step 2: Create or Identify the Target Namespace","text":"<p>For this example, let\u2019s create a namespace called <code>secure-apps</code> where we\u2019ll enforce mTLS:</p> <pre><code># Create the namespace\nkubectl create namespace secure-apps\n\n# Label the namespace for Istio injection\nkubectl label namespace secure-apps istio-injection=enabled\n</code></pre> <p>The <code>istio-injection=enabled</code> label ensures that the Istio sidecar proxy is automatically injected into all pods created in this namespace, which is required for mTLS to function.</p>"},{"location":"kubernetes/cks/exercises/istio-mtls/#step-3-apply-peerauthentication-policy-for-mtls","title":"Step 3: Apply PeerAuthentication Policy for mTLS","text":"<p>Now, let\u2019s create a PeerAuthentication resource that enforces STRICT mTLS for all services in the <code>secure-apps</code> namespace:</p> <pre><code># Save this as mtls-policy.yaml\napiVersion: security.istio.io/v1beta1\nkind: PeerAuthentication\nmetadata:\n  name: default\n  namespace: secure-apps\nspec:\n  mtls:\n    mode: STRICT\n</code></pre> <p>Apply this configuration:</p> <pre><code>kubectl apply -f mtls-policy.yaml\n</code></pre> <p>Let\u2019s understand the important parts of this configuration:</p> <ul> <li><code>kind: PeerAuthentication</code>: Specifies that this is an Istio PeerAuthentication resource</li> <li><code>metadata.namespace: secure-apps</code>: The policy applies to the secure-apps namespace</li> <li><code>mtls.mode: STRICT</code>: Enforces strict mTLS, meaning:</li> <li>All traffic to services in this namespace must use mTLS</li> <li>Plaintext connections will be rejected</li> <li>Communications are encrypted and authenticated</li> </ul>"},{"location":"kubernetes/cks/exercises/istio-mtls/#step-4-verify-mtls-enforcement","title":"Step 4: Verify mTLS Enforcement","text":"<p>Let\u2019s verify that mTLS is properly configured and enforced:</p> <pre><code># Check the PeerAuthentication policy status\nkubectl get peerauthentication -n secure-apps\n\n# Sample output:\n# NAME      MODE     AGE\n# default   STRICT   30s\n</code></pre> <p>For a more detailed verification, deploy test applications in the secure-apps namespace and verify mTLS is working:</p> <pre><code># Deploy a simple application in the secure-apps namespace\nkubectl apply -f - &lt;&lt;EOF\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sleep\n  namespace: secure-apps\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sleep\n  template:\n    metadata:\n      labels:\n        app: sleep\n    spec:\n      containers:\n      - name: sleep\n        image: curlimages/curl\n        command: [\"/bin/sleep\", \"3650d\"]\n        imagePullPolicy: IfNotPresent\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: sleep\n  namespace: secure-apps\nspec:\n  ports:\n  - port: 80\n    name: http\n  selector:\n    app: sleep\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: httpbin\n  namespace: secure-apps\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: httpbin\n  template:\n    metadata:\n      labels:\n        app: httpbin\n    spec:\n      containers:\n      - image: kennethreitz/httpbin\n        imagePullPolicy: IfNotPresent\n        name: httpbin\n        ports:\n        - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: httpbin\n  namespace: secure-apps\nspec:\n  ports:\n  - name: http\n    port: 8000\n    targetPort: 80\n  selector:\n    app: httpbin\nEOF\n</code></pre> <p>Now you can verify mTLS is working by checking the connection from sleep to httpbin:</p> <pre><code># Get the sleep pod name\nSLEEP_POD=$(kubectl get pod -l app=sleep -n secure-apps -o jsonpath={.items..metadata.name})\n\n# Check the TLS stats to verify mTLS is active\nkubectl exec -n secure-apps $SLEEP_POD -c istio-proxy -- curl -s http://localhost:15000/stats | grep tls_inspector\n\n# Try a connection and check if it's using mTLS\nkubectl exec -n secure-apps $SLEEP_POD -c istio-proxy -- curl http://httpbin.secure-apps:8000/headers -s | grep X-Forwarded-Client-Cert\n</code></pre> <p>If the connection is encrypted with mTLS, you\u2019ll see the X-Forwarded-Client-Cert header in the response, which contains the client certificate information.</p>"},{"location":"kubernetes/cks/exercises/istio-mtls/#step-5-visualize-mtls-with-kiali-optional","title":"Step 5: Visualize mTLS with Kiali (Optional)","text":"<p>If you have Kiali installed as part of your Istio setup, you can visualize the mTLS status:</p> <pre><code># If Kiali is not exposed, you can port-forward to access it\nkubectl port-forward svc/kiali 20001:20001 -n istio-system\n</code></pre> <p>Then open your browser to http://localhost:20001, log in, and navigate to the Graph view. You should see secure (padlock) icons on the connections between services in the secure-apps namespace.</p>"},{"location":"kubernetes/cks/exercises/istio-mtls/#understanding-mtls-modes","title":"Understanding mTLS Modes","text":"<p>Istio provides different mTLS modes in PeerAuthentication:</p> <ol> <li>STRICT: All connections must use mTLS. Non-mTLS connections are rejected.</li> <li>PERMISSIVE: Both mTLS and plaintext connections are allowed (useful during migration).</li> <li>DISABLE: mTLS is disabled, only plaintext connections are allowed.</li> </ol> <p>For our example, we\u2019ve used STRICT to enforce full encryption.</p>"},{"location":"kubernetes/cks/exercises/istio-mtls/#fine-tuning-mtls-optional","title":"Fine-tuning mTLS (Optional)","text":"<p>If you need more granular control, you can apply mTLS settings at the service level:</p> <pre><code>apiVersion: security.istio.io/v1beta1\nkind: PeerAuthentication\nmetadata:\n  name: service-specific\n  namespace: secure-apps\nspec:\n  selector:\n    matchLabels:\n      app: httpbin  # This policy applies only to the httpbin service\n  mtls:\n    mode: STRICT\n</code></pre>"},{"location":"kubernetes/cks/exercises/istio-mtls/#conclusion","title":"Conclusion","text":"<p>We\u2019ve successfully configured pod-to-pod encryption using mTLS in the <code>secure-apps</code> namespace with Istio\u2019s PeerAuthentication resource. The configuration ensures that:</p> <ol> <li>All communication between services is encrypted and authenticated</li> <li>Only services with valid certificates can communicate with each other</li> <li>The entire namespace is protected with a single policy</li> </ol> <p>This approach provides strong security for microservices communication without requiring changes to the application code, as Istio\u2019s service mesh handles the TLS negotiation, certificate management, and encryption transparently.</p>"},{"location":"kubernetes/cks/exercises/istio-mtls/#additional-security-considerations","title":"Additional Security Considerations","text":"<ul> <li>Rotation: Istio automatically rotates certificates (default validity is 24 hours)</li> <li>Monitoring: Implement monitoring for failed authentication attempts using Istio telemetry</li> <li>Gradual Rollout: For production environments, consider starting with PERMISSIVE mode, then monitoring, before switching to STRICT mode</li> <li>API Gateway: Consider configuring an AuthorizationPolicy alongside PeerAuthentication for additional security</li> </ul> <p>By implementing mTLS across your namespace, you\u2019ve significantly improved your security posture by ensuring all service-to-service communication is encrypted and authenticated.</p>"},{"location":"kubernetes/cks/exercises/p2p-encryption-istio-cilium/","title":"Enabling Pod-to-Pod Encryption with Istio and Cilium","text":"<p>This document provides solutions for implementing pod-to-pod (P2P) encryption in Kubernetes using two different approaches: Istio service mesh and Cilium network policy engine. Both methods offer robust security for intra-cluster communications.</p>"},{"location":"kubernetes/cks/exercises/p2p-encryption-istio-cilium/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Solution 1: P2P Encryption with Istio</li> <li>Solution 2: P2P Encryption with Cilium</li> <li>Comparison and Best Practices</li> </ol>"},{"location":"kubernetes/cks/exercises/p2p-encryption-istio-cilium/#solution-1-p2p-encryption-with-istio","title":"Solution 1: P2P Encryption with Istio","text":"<p>Istio provides pod-to-pod encryption through mutual TLS (mTLS) authentication, where both the client and server authenticate each other\u2019s identity.</p>"},{"location":"kubernetes/cks/exercises/p2p-encryption-istio-cilium/#prerequisites","title":"Prerequisites","text":"<ul> <li>Kubernetes cluster (v1.16+)</li> <li>Istio (v1.9+) installed</li> <li><code>kubectl</code> and <code>istioctl</code> CLI tools</li> </ul>"},{"location":"kubernetes/cks/exercises/p2p-encryption-istio-cilium/#step-1-install-istio-with-mtls-enabled","title":"Step 1: Install Istio with mTLS Enabled","text":"<p>If starting fresh:</p> <pre><code># Download Istio\ncurl -L https://istio.io/downloadIstio | sh -\ncd istio-*\n\n# Install Istio with strict mTLS profile\nistioctl install --set profile=default \\\n  --set meshConfig.enableAutoMtls=true \\\n  --set values.global.mtls.enabled=true\n</code></pre>"},{"location":"kubernetes/cks/exercises/p2p-encryption-istio-cilium/#step-2-configure-namespace-for-istio-injection","title":"Step 2: Configure Namespace for Istio Injection","text":"<pre><code># Label the namespace for automatic sidecar injection\nkubectl label namespace default istio-injection=enabled\n</code></pre>"},{"location":"kubernetes/cks/exercises/p2p-encryption-istio-cilium/#step-3-define-a-peerauthentication-policy-for-strict-mtls","title":"Step 3: Define a PeerAuthentication Policy for Strict mTLS","text":"<p>Create a file named <code>strict-mtls-policy.yaml</code>:</p> <pre><code>apiVersion: \"security.istio.io/v1beta1\"\nkind: \"PeerAuthentication\"\nmetadata:\n  name: \"default\"\n  namespace: \"istio-system\"  # Apply mesh-wide\nspec:\n  mtls:\n    mode: STRICT\n</code></pre> <p>Apply the policy:</p> <pre><code>kubectl apply -f strict-mtls-policy.yaml\n</code></pre>"},{"location":"kubernetes/cks/exercises/p2p-encryption-istio-cilium/#step-4-verify-mtls-encryption","title":"Step 4: Verify mTLS Encryption","text":"<p>After deploying some applications, verify that mTLS is working:</p> <pre><code># Check mTLS status\nistioctl x authz check &lt;pod-name&gt;.&lt;namespace&gt;\n\n# View real-time mTLS status in Kiali\nkubectl port-forward -n istio-system svc/kiali 20001:20001\n</code></pre> <p>Visit <code>http://localhost:20001</code> in your browser to visualize secure connections.</p>"},{"location":"kubernetes/cks/exercises/p2p-encryption-istio-cilium/#step-5-debugging-and-validating-mtls","title":"Step 5: Debugging and Validating mTLS","text":"<p>To verify mTLS encryption is actively enforced:</p> <pre><code># Deploy a sample application\nkubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml\n\n# Create a testing pod\nkubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Pod\nmetadata:\n  name: sleep\n  labels:\n    app: sleep\nspec:\n  containers:\n  - name: sleep\n    image: curlimages/curl\n    command: [\"/bin/sleep\", \"3650d\"]\nEOF\n\n# Test communication (should succeed with mTLS)\nkubectl exec sleep -- curl -s productpage:9080 | grep -o \"&lt;title&gt;.*&lt;/title&gt;\"\n\n# View TLS certificate information\nkubectl exec -it sleep -- curl -v productpage:9080 | grep \"SSL connection\"\n</code></pre>"},{"location":"kubernetes/cks/exercises/p2p-encryption-istio-cilium/#solution-2-p2p-encryption-with-cilium","title":"Solution 2: P2P Encryption with Cilium","text":"<p>Cilium implements transparent encryption using IPsec or WireGuard for pod-to-pod traffic.</p>"},{"location":"kubernetes/cks/exercises/p2p-encryption-istio-cilium/#prerequisites_1","title":"Prerequisites","text":"<ul> <li>Kubernetes cluster</li> <li>Helm (v3+)</li> <li><code>kubectl</code> CLI tool</li> </ul>"},{"location":"kubernetes/cks/exercises/p2p-encryption-istio-cilium/#step-1-install-cilium-with-encryption-enabled","title":"Step 1: Install Cilium with Encryption Enabled","text":""},{"location":"kubernetes/cks/exercises/p2p-encryption-istio-cilium/#using-ipsec-encryption","title":"Using IPsec Encryption:","text":"<pre><code># Create a secret for IPsec encryption keys\nkubectl create -n kube-system secret generic cilium-ipsec-keys \\\n  --from-literal=keys=\"3 rfc4106(gcm(aes)) $(echo $(dd if=/dev/urandom count=20 bs=1 2&gt; /dev/null | xxd -p -c 64)) 128\"\n\n# Install Cilium with IPsec encryption enabled\nhelm install cilium cilium/cilium --version 1.12.0 \\\n  --namespace kube-system \\\n  --set encryption.enabled=true \\\n  --set encryption.type=ipsec\n</code></pre>"},{"location":"kubernetes/cks/exercises/p2p-encryption-istio-cilium/#using-wireguard-encryption-alternative","title":"Using WireGuard Encryption (alternative):","text":"<pre><code># Install Cilium with WireGuard encryption enabled\nhelm install cilium cilium/cilium --version 1.12.0 \\\n  --namespace kube-system \\\n  --set encryption.enabled=true \\\n  --set encryption.type=wireguard\n</code></pre>"},{"location":"kubernetes/cks/exercises/p2p-encryption-istio-cilium/#step-2-verify-cilium-installation-with-encryption","title":"Step 2: Verify Cilium Installation with Encryption","text":"<pre><code># Check if Cilium is running correctly\nkubectl -n kube-system get pods -l k8s-app=cilium\n\n# Check Cilium status including encryption\nkubectl -n kube-system exec cilium-xxxx -- cilium status | grep Encryption\n</code></pre>"},{"location":"kubernetes/cks/exercises/p2p-encryption-istio-cilium/#step-3-create-a-ciliumnetworkpolicy-to-enforce-encryption","title":"Step 3: Create a CiliumNetworkPolicy to Enforce Encryption","text":"<p>Cilium allows selective encryption using CiliumNetworkPolicy. Create a file named <code>encryption-policy.yaml</code>:</p> <pre><code>apiVersion: \"cilium.io/v2\"\nkind: CiliumNetworkPolicy\nmetadata:\n  name: \"encrypt-all-traffic\"\nspec:\n  endpointSelector:\n    matchLabels:\n      app: secure-app\n  egress:\n  - toEndpoints:\n    - matchLabels:\n        app: secure-backend\n    toPorts:\n    - ports:\n      - port: \"8080\"\n        protocol: TCP\n    encrypted: true\n</code></pre> <p>Apply the policy:</p> <pre><code>kubectl apply -f encryption-policy.yaml\n</code></pre>"},{"location":"kubernetes/cks/exercises/p2p-encryption-istio-cilium/#step-4-testing-and-validating-cilium-encryption","title":"Step 4: Testing and Validating Cilium Encryption","text":"<p>Deploy test applications:</p> <pre><code># Deploy a test application\nkubectl apply -f - &lt;&lt;EOF\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: secure-app\nspec:\n  selector:\n    matchLabels:\n      app: secure-app\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: secure-app\n    spec:\n      containers:\n      - name: web\n        image: nginx\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: secure-backend\nspec:\n  selector:\n    matchLabels:\n      app: secure-backend\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: secure-backend\n    spec:\n      containers:\n      - name: web\n        image: nginx\nEOF\n</code></pre> <p>Verify encryption:</p> <pre><code># For IPsec encryption, verify ESP packets\nkubectl exec -n kube-system cilium-xxxx -- cilium bpf tunnel list\n\n# For WireGuard encryption, verify WireGuard statistics\nkubectl exec -n kube-system cilium-xxxx -- cilium status --verbose | grep WireGuard\n</code></pre>"},{"location":"kubernetes/cks/exercises/p2p-encryption-istio-cilium/#comparison-and-best-practices","title":"Comparison and Best Practices","text":""},{"location":"kubernetes/cks/exercises/p2p-encryption-istio-cilium/#istio-mtls-vs-cilium-encryption","title":"Istio mTLS vs. Cilium Encryption","text":"Feature Istio mTLS Cilium Encryption Layer Application (L7) Network (L3) Encryption Method TLS IPsec or WireGuard Performance Impact Moderate Low-Moderate Identity Verification Yes (x509 certs) Limited (IPsec PSK) Selective Encryption Yes (via policies) Yes (via policies) Observability High (detailed metrics) Basic"},{"location":"kubernetes/cks/exercises/p2p-encryption-istio-cilium/#best-practices","title":"Best Practices","text":"<ol> <li>Choose based on requirements:</li> <li>Use Istio for comprehensive service mesh features beyond encryption</li> <li> <p>Use Cilium for network-level security with lower overhead</p> </li> <li> <p>Layered Security:</p> </li> <li>Consider using both solutions for defense in depth</li> <li>Istio for service-to-service authentication</li> <li> <p>Cilium for network-level encryption</p> </li> <li> <p>Key Rotation:</p> </li> <li>For Istio: Configure automated certificate rotation (default: 24h)</li> <li> <p>For Cilium IPsec: Rotate encryption keys periodically</p> </li> <li> <p>Monitoring:</p> </li> <li>For Istio: Use Kiali, Grafana, and Prometheus for visibility</li> <li> <p>For Cilium: Use Hubble for network flow visibility</p> </li> <li> <p>Policy Testing:</p> </li> <li>Test encryption policies in non-production environments first</li> <li>Use Cilium\u2019s dry-run mode or Istio\u2019s permissive mode initially</li> </ol> <p>By implementing either or both of these solutions, you can ensure that pod-to-pod communications within your Kubernetes cluster remain secure and protected from eavesdropping or man-in-the-middle attacks.</p>"},{"location":"kubernetes/cks/exercises/pod-security-standards/","title":"Pod Security Standards (PSS): Overview and Implementation Guide","text":""},{"location":"kubernetes/cks/exercises/pod-security-standards/#introduction-to-pod-security-standards","title":"Introduction to Pod Security Standards","text":"<p>Pod Security Standards (PSS) is a feature in Kubernetes that defines different levels of security for pods. It replaced the deprecated PodSecurityPolicy (PSP) in Kubernetes 1.25. The PSS provides a standardized way to enforce security controls for pods, helping cluster administrators ensure workloads meet specific security requirements without needing to create and manage custom policies.</p>"},{"location":"kubernetes/cks/exercises/pod-security-standards/#pss-profiles","title":"PSS Profiles","text":"<p>PSS defines three security profiles with increasing levels of restriction:</p> <ol> <li> <p>Privileged: Unrestricted policy, providing the widest possible level of permissions. This profile has essentially no restrictions on pod configuration.</p> </li> <li> <p>Baseline: Minimally restrictive policy which prevents known privilege escalations. Allows the default (minimally specified) pod configuration.</p> </li> <li> <p>Restricted: Heavily restricted policy, following current pod hardening best practices. This is the most secure profile, designed to enforce security best practices.</p> </li> </ol>"},{"location":"kubernetes/cks/exercises/pod-security-standards/#pss-implementation-methods","title":"PSS Implementation Methods","text":"<p>There are three ways to implement Pod Security Standards:</p> <ol> <li> <p>Pod Security Admission Controller: Built into Kubernetes since v1.23, it can enforce the standards at the namespace level.</p> </li> <li> <p>Namespace Labels: Used to configure the Pod Security Admission Controller with different modes (enforce, audit, warn).</p> </li> <li> <p>3<sup>rd</sup> Party Solutions: Various tools like OPA/Gatekeeper, Kyverno, etc. can enforce PSS.</p> </li> </ol>"},{"location":"kubernetes/cks/exercises/pod-security-standards/#pss-modes","title":"PSS Modes","text":"<p>Each profile can be applied in one of three modes:</p> <ol> <li>enforce: Policy violations will cause the pod to be rejected</li> <li>audit: Policy violations trigger audit annotations, but are allowed</li> <li>warn: Policy violations trigger user-facing warnings, but are allowed</li> </ol>"},{"location":"kubernetes/cks/exercises/pod-security-standards/#exam-task-example-making-a-deployment-compatible-with-pss-restricted","title":"Exam Task Example: Making a Deployment Compatible with PSS-Restricted","text":""},{"location":"kubernetes/cks/exercises/pod-security-standards/#task-description","title":"Task Description","text":"<p>Create a deployment that is compatible with the Pod Security Standard \u201crestricted\u201d profile in enforce mode. The deployment should run a container that: 1. Reads and writes to a persistent volume 2. Exposes port 8080 3. Runs as a non-root user 4. Meets all the requirements of the restricted profile</p>"},{"location":"kubernetes/cks/exercises/pod-security-standards/#solution","title":"Solution","text":""},{"location":"kubernetes/cks/exercises/pod-security-standards/#step-1-configure-namespace-with-pss","title":"Step 1: Configure Namespace with PSS","text":"<p>First, let\u2019s create a namespace with the restricted profile in enforce mode:</p> <pre><code>kubectl create namespace pss-restricted\nkubectl label --overwrite ns pss-restricted \\\n  pod-security.kubernetes.io/enforce=restricted \\\n  pod-security.kubernetes.io/warn=restricted \\\n  pod-security.kubernetes.io/audit=restricted\n</code></pre>"},{"location":"kubernetes/cks/exercises/pod-security-standards/#step-2-create-a-compliant-deployment","title":"Step 2: Create a Compliant Deployment","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: pss-restricted-app\n  namespace: pss-restricted\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: pss-restricted-app\n  template:\n    metadata:\n      labels:\n        app: pss-restricted-app\n    spec:\n      # Security Context at Pod level\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: app\n        image: nginx:1.21\n        # Container Security Context\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          runAsUser: 1000\n          runAsGroup: 3000\n          seccompProfile:\n            type: RuntimeDefault\n          readOnlyRootFilesystem: true\n        ports:\n        - containerPort: 8080\n        resources:\n          limits:\n            cpu: \"500m\"\n            memory: \"512Mi\"\n          requests:\n            cpu: \"100m\"\n            memory: \"128Mi\"\n        volumeMounts:\n        - name: data-volume\n          mountPath: /data\n          readOnly: false\n        - name: tmp-volume\n          mountPath: /tmp\n      volumes:\n      - name: data-volume\n        persistentVolumeClaim:\n          claimName: pss-restricted-pvc\n      - name: tmp-volume\n        emptyDir: {}\n</code></pre>"},{"location":"kubernetes/cks/exercises/pod-security-standards/#step-3-create-the-persistent-volume-claim","title":"Step 3: Create the Persistent Volume Claim","text":"<pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: pss-restricted-pvc\n  namespace: pss-restricted\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi\n  storageClassName: standard\n</code></pre>"},{"location":"kubernetes/cks/exercises/pod-security-standards/#step-4-apply-the-configuration","title":"Step 4: Apply the Configuration","text":"<pre><code>kubectl apply -f pss-pvc.yaml\nkubectl apply -f pss-deployment.yaml\n</code></pre>"},{"location":"kubernetes/cks/exercises/pod-security-standards/#step-5-verify-deployment-status","title":"Step 5: Verify Deployment Status","text":"<pre><code>kubectl get deployment -n pss-restricted\nkubectl describe deployment pss-restricted-app -n pss-restricted\n</code></pre>"},{"location":"kubernetes/cks/exercises/pod-security-standards/#key-pss-restricted-requirements-explained","title":"Key PSS Restricted Requirements Explained","text":"<p>When the Pod Security Standard enforces the restricted profile, the following security requirements must be met:</p> <ol> <li>Pod-level Requirements:</li> <li><code>securityContext.runAsNonRoot: true</code>: Ensures pods run as non-root users</li> <li> <p><code>securityContext.seccompProfile.type: RuntimeDefault</code>: Enables default seccomp profile</p> </li> <li> <p>Container-level Requirements:</p> </li> <li><code>securityContext.allowPrivilegeEscalation: false</code>: Prevents privilege escalation</li> <li><code>securityContext.capabilities.drop: [\"ALL\"]</code>: Drops all Linux capabilities</li> <li><code>securityContext.runAsUser</code> and <code>runAsGroup</code>: Explicit non-zero user/group IDs</li> <li><code>securityContext.seccompProfile.type: RuntimeDefault</code>: Enables seccomp at container level</li> <li> <p><code>readOnlyRootFilesystem: true</code>: Makes root filesystem read-only</p> </li> <li> <p>Volume-related Considerations:</p> </li> <li>When a read-only root filesystem is used, you must provide writable volumes for paths like <code>/tmp</code></li> <li> <p>PersistentVolumeClaims are allowed and need appropriate mount paths</p> </li> <li> <p>Prohibited configurations:</p> </li> <li><code>hostPath</code> volumes</li> <li><code>hostNetwork</code>, <code>hostIPC</code>, and <code>hostPID</code> set to true</li> <li><code>privileged</code> containers</li> <li>Adding capabilities beyond a minimal set</li> <li>HostPort usage</li> </ol>"},{"location":"kubernetes/cks/exercises/pod-security-standards/#common-issues-and-troubleshooting","title":"Common Issues and Troubleshooting","text":"<p>When working with PSS in restricted mode, you might encounter the following issues:</p> <ol> <li> <p>Pod Rejection Errors: If a pod is rejected, check the error message from kubectl, which will specify which PSS policy was violated.</p> </li> <li> <p>Runtime Failures: Even if a pod starts, it might fail if it needs to write to locations that are now read-only.</p> </li> <li> <p>Legacy Applications: Older applications often assume root access or specific capabilities, requiring refactoring.</p> </li> </ol>"},{"location":"kubernetes/cks/exercises/pod-security-standards/#troubleshooting-commands","title":"Troubleshooting Commands","text":"<pre><code># Check if pod was rejected due to PSS violations\nkubectl get events -n pss-restricted\n\n# Check audit annotations for violations\nkubectl get pod &lt;pod-name&gt; -n pss-restricted -o yaml | grep \"audit\"\n\n# Check warning events\nkubectl get events -n pss-restricted | grep Warning\n</code></pre>"},{"location":"kubernetes/cks/exercises/pod-security-standards/#additional-exam-task-examples","title":"Additional Exam Task Examples","text":""},{"location":"kubernetes/cks/exercises/pod-security-standards/#task-1-configure-a-namespace-for-pss-audit-mode","title":"Task 1: Configure a Namespace for PSS Audit Mode","text":"<p>Create a namespace that audits according to the restricted profile but only enforces the baseline profile.</p> <pre><code>kubectl create namespace mixed-pss\nkubectl label --overwrite ns mixed-pss \\\n  pod-security.kubernetes.io/enforce=baseline \\\n  pod-security.kubernetes.io/audit=restricted \\\n  pod-security.kubernetes.io/warn=restricted\n</code></pre>"},{"location":"kubernetes/cks/exercises/pod-security-standards/#task-2-troubleshoot-a-pod-failing-due-to-pss-restrictions","title":"Task 2: Troubleshoot a Pod Failing Due to PSS Restrictions","text":"<p>Given a pod that is failing to deploy in a namespace with PSS restricted enforcement, identify and fix the issues.</p> <p>Original pod manifest: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: privileged-pod\n  namespace: pss-restricted\nspec:\n  containers:\n  - name: web\n    image: nginx\n    ports:\n    - containerPort: 80\n    securityContext:\n      privileged: true  # Violation! \n</code></pre></p> <p>Fixed pod manifest: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: compliant-pod\n  namespace: pss-restricted\nspec:\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n  containers:\n  - name: web\n    image: nginx\n    ports:\n    - containerPort: 80\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n      runAsUser: 101  # nginx user\n      runAsGroup: 101\n      seccompProfile:\n        type: RuntimeDefault\n</code></pre></p>"},{"location":"kubernetes/cks/exercises/pod-security-standards/#conclusion","title":"Conclusion","text":"<p>Pod Security Standards provide a robust framework for securing Kubernetes workloads. Understanding the different profiles and their requirements is essential for creating secure deployments that can run in restricted environments. When working with the restricted profile:</p> <ol> <li>Always specify non-root users</li> <li>Drop ALL capabilities at the container level</li> <li>Prevent privilege escalation</li> <li>Use appropriate seccomp profiles</li> <li>Make root filesystems read-only when possible</li> <li>Use emptyDir volumes for temporary writable storage</li> </ol> <p>By following these practices, you can ensure your workloads are compatible with Kubernetes security best practices and can run in environments with strict security requirements.</p>"},{"location":"kubernetes/cks/exercises/security-static-analysis/","title":"Static Security Analysis","text":""},{"location":"kubernetes/cks/exercises/security-static-analysis/#dockerfile-security-analysis","title":"Dockerfile Security Analysis","text":""},{"location":"kubernetes/cks/exercises/security-static-analysis/#key-security-issues-to-look-for","title":"Key Security Issues to Look For","text":""},{"location":"kubernetes/cks/exercises/security-static-analysis/#1-running-as-root-user","title":"1. Running as Root User","text":"<pre><code># \u274c BAD - Running as root\nFROM ubuntu:20.04\nRUN apt-get update &amp;&amp; apt-get install -y nginx\nCMD [\"nginx\", \"-g\", \"daemon off;\"]\n\n# \u2705 GOOD - Non-root user\nFROM ubuntu:20.04\nRUN apt-get update &amp;&amp; apt-get install -y nginx\nRUN useradd -r -s /bin/false nginx-user\nUSER nginx-user\nCMD [\"nginx\", \"-g\", \"daemon off;\"]\n</code></pre>"},{"location":"kubernetes/cks/exercises/security-static-analysis/#2-secrets-in-image-layers","title":"2. Secrets in Image Layers","text":"<pre><code># \u274c BAD - Hardcoded secrets\nFROM node:14\nENV API_KEY=sk-1234567890abcdef\nENV DATABASE_PASSWORD=super-secret-password\nCOPY . /app\n\n# \u2705 GOOD - No secrets in Dockerfile\nFROM node:14\n# Secrets should be passed at runtime via K8s secrets\nCOPY . /app\n</code></pre>"},{"location":"kubernetes/cks/exercises/security-static-analysis/#3-unnecessary-packages-and-attack-surface","title":"3. Unnecessary Packages and Attack Surface","text":"<pre><code># \u274c BAD - Full OS with unnecessary packages\nFROM ubuntu:20.04\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    curl wget git vim nano ssh openssh-server\n\n# \u2705 GOOD - Minimal base image\nFROM node:14-alpine\n# Only install what's needed\nRUN apk add --no-cache dumb-init\n</code></pre>"},{"location":"kubernetes/cks/exercises/security-static-analysis/#4-improper-file-permissions","title":"4. Improper File Permissions","text":"<pre><code># \u274c BAD - World-writable files\nFROM alpine\nCOPY --chmod=777 app.sh /usr/local/bin/\nCOPY --chmod=666 config.json /etc/\n\n# \u2705 GOOD - Restrictive permissions\nFROM alpine\nCOPY --chmod=755 app.sh /usr/local/bin/\nCOPY --chmod=644 config.json /etc/\n</code></pre>"},{"location":"kubernetes/cks/exercises/security-static-analysis/#5-using-latest-tags","title":"5. Using Latest Tags","text":"<pre><code># \u274c BAD - Unpredictable base image\nFROM node:latest\nFROM ubuntu:latest\n\n# \u2705 GOOD - Specific versions\nFROM node:14.21.3-alpine\nFROM ubuntu:20.04\n</code></pre>"},{"location":"kubernetes/cks/exercises/security-static-analysis/#kubernetes-manifest-security-analysis","title":"Kubernetes Manifest Security Analysis","text":""},{"location":"kubernetes/cks/exercises/security-static-analysis/#key-security-issues-to-look-for_1","title":"Key Security Issues to Look For","text":""},{"location":"kubernetes/cks/exercises/security-static-analysis/#1-missing-security-context","title":"1. Missing Security Context","text":"<pre><code># \u274c BAD - No security context\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: insecure-app\nspec:\n  template:\n    spec:\n      containers:\n      - name: app\n        image: nginx:1.20\n\n# \u2705 GOOD - Proper security context\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: secure-app\nspec:\n  template:\n    spec:\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 1000\n        runAsGroup: 1000\n        fsGroup: 1000\n      containers:\n      - name: app\n        image: nginx:1.20\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          capabilities:\n            drop:\n            - ALL\n</code></pre>"},{"location":"kubernetes/cks/exercises/security-static-analysis/#2-privileged-containers","title":"2. Privileged Containers","text":"<pre><code># \u274c BAD - Privileged container\ncontainers:\n- name: dangerous-app\n  image: app:1.0\n  securityContext:\n    privileged: true\n\n# \u2705 GOOD - Non-privileged with specific capabilities\ncontainers:\n- name: safe-app\n  image: app:1.0\n  securityContext:\n    privileged: false\n    capabilities:\n      add:\n      - NET_BIND_SERVICE\n      drop:\n      - ALL\n</code></pre>"},{"location":"kubernetes/cks/exercises/security-static-analysis/#3-missing-resource-limits","title":"3. Missing Resource Limits","text":"<pre><code># \u274c BAD - No resource limits\ncontainers:\n- name: resource-hog\n  image: app:1.0\n\n# \u2705 GOOD - Resource limits set\ncontainers:\n- name: limited-app\n  image: app:1.0\n  resources:\n    limits:\n      memory: \"512Mi\"\n      cpu: \"500m\"\n    requests:\n      memory: \"256Mi\"\n      cpu: \"250m\"\n</code></pre>"},{"location":"kubernetes/cks/exercises/security-static-analysis/#4-secrets-in-environment-variables","title":"4. Secrets in Environment Variables","text":"<pre><code># \u274c BAD - Secrets in plain text env vars\ncontainers:\n- name: app\n  image: myapp:1.0\n  env:\n  - name: DB_PASSWORD\n    value: \"super-secret-password\"\n  - name: API_KEY\n    value: \"sk-1234567890\"\n\n# \u2705 GOOD - Using Kubernetes secrets\ncontainers:\n- name: app\n  image: myapp:1.0\n  env:\n  - name: DB_PASSWORD\n    valueFrom:\n      secretKeyRef:\n        name: db-secret\n        key: password\n  - name: API_KEY\n    valueFrom:\n      secretKeyRef:\n        name: api-secret\n        key: api-key\n</code></pre>"},{"location":"kubernetes/cks/exercises/security-static-analysis/#5-network-policies-missing","title":"5. Network Policies Missing","text":"<pre><code># \u274c BAD - No network restrictions\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-app\n# No NetworkPolicy defined\n\n# \u2705 GOOD - With NetworkPolicy\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: web-app-netpol\nspec:\n  podSelector:\n    matchLabels:\n      app: web-app\n  policyTypes:\n  - Ingress\n  - Egress\n  ingress:\n  - from:\n    - podSelector:\n        matchLabels:\n          app: frontend\n    ports:\n    - protocol: TCP\n      port: 8080\n</code></pre>"},{"location":"kubernetes/cks/exercises/security-static-analysis/#tools-for-static-analysis","title":"Tools for Static Analysis","text":""},{"location":"kubernetes/cks/exercises/security-static-analysis/#1-trivy-recommended-for-exams","title":"1. Trivy (Recommended for exams)","text":"<pre><code># Scan Dockerfile\ntrivy config Dockerfile\n\n# Scan Kubernetes manifests\ntrivy config k8s-manifests/\n\n# Scan specific manifest\ntrivy config deployment.yaml\n</code></pre>"},{"location":"kubernetes/cks/exercises/security-static-analysis/#2-checkov","title":"2. Checkov","text":"<pre><code># Install\npip install checkov\n\n# Scan Dockerfile\ncheckov -f Dockerfile\n\n# Scan Kubernetes manifests\ncheckov -d k8s-manifests/\n</code></pre>"},{"location":"kubernetes/cks/exercises/security-static-analysis/#3-kubesec","title":"3. Kubesec","text":"<pre><code># Online tool\ncurl -sSX POST --data-binary @deployment.yaml https://v2.kubesec.io/scan\n\n# Local installation\nkubesec scan deployment.yaml\n</code></pre>"},{"location":"kubernetes/cks/exercises/security-static-analysis/#4-kube-score","title":"4. Kube-score","text":"<pre><code># Install\nkubectl krew install score\n\n# Analyze manifest\nkubectl score deployment.yaml\n</code></pre>"},{"location":"kubernetes/cks/exercises/security-static-analysis/#common-exam-questions","title":"Common Exam Questions","text":""},{"location":"kubernetes/cks/exercises/security-static-analysis/#question-type-1-identify-security-issues","title":"Question Type 1: \u201cIdentify Security Issues\u201d","text":"<p>Prompt: \u201cAnalyze the following Dockerfile and identify at least 3 security vulnerabilities\u201d</p> <p>Approach: 1. Look for root user usage 2. Check for hardcoded secrets 3. Verify base image tags 4. Check file permissions 5. Look for unnecessary packages</p>"},{"location":"kubernetes/cks/exercises/security-static-analysis/#question-type-2-fix-security-issues","title":"Question Type 2: \u201cFix Security Issues\u201d","text":"<p>Prompt: \u201cFix the security issues in the following Kubernetes deployment manifest\u201d</p> <p>Approach: 1. Add securityContext 2. Set resource limits 3. Use secrets instead of plain env vars 4. Ensure non-root execution 5. Add readiness/liveness probes</p>"},{"location":"kubernetes/cks/exercises/security-static-analysis/#question-type-3-use-tool-for-analysis","title":"Question Type 3: \u201cUse Tool for Analysis\u201d","text":"<p>Prompt: \u201cUse trivy to scan the provided manifest and fix the HIGH severity issues\u201d</p> <p>Approach: 1. Run trivy scan 2. Identify HIGH severity findings 3. Apply fixes systematically 4. Re-scan to verify</p>"},{"location":"kubernetes/cks/exercises/security-static-analysis/#step-by-step-solutions","title":"Step-by-Step Solutions","text":""},{"location":"kubernetes/cks/exercises/security-static-analysis/#scenario-1-dockerfile-analysis","title":"Scenario 1: Dockerfile Analysis","text":"<p>Given Dockerfile: <pre><code>FROM ubuntu:latest\nRUN apt-get update &amp;&amp; apt-get install -y curl wget git\nENV SECRET_KEY=abc123\nCOPY app.py /app/\nRUN chmod 777 /app/app.py\nCMD [\"python\", \"/app/app.py\"]\n</code></pre></p> <p>Step 1: Identify Issues <pre><code># Run trivy scan\ntrivy config Dockerfile\n</code></pre></p> <p>Step 2: Issues Found - Using <code>latest</code> tag (unpredictable) - Running as root user - Hardcoded secret in ENV - Overly permissive file permissions (777) - Unnecessary packages installed</p> <p>Step 3: Fixed Dockerfile <pre><code>FROM ubuntu:20.04\nRUN apt-get update &amp;&amp; apt-get install -y python3 &amp;&amp; \\\n    apt-get clean &amp;&amp; rm -rf /var/lib/apt/lists/*\nRUN useradd -r -s /bin/false appuser\nCOPY --chown=appuser:appuser --chmod=644 app.py /app/\nUSER appuser\nCMD [\"python3\", \"/app/app.py\"]\n</code></pre></p>"},{"location":"kubernetes/cks/exercises/security-static-analysis/#scenario-2-kubernetes-manifest-analysis","title":"Scenario 2: Kubernetes Manifest Analysis","text":"<p>Given Deployment: <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: web-app\n  template:\n    metadata:\n      labels:\n        app: web-app\n    spec:\n      containers:\n      - name: web\n        image: nginx:latest\n        env:\n        - name: DB_PASSWORD\n          value: \"secretpassword\"\n</code></pre></p> <p>Step 1: Scan with Tools <pre><code>trivy config deployment.yaml\nkubesec scan deployment.yaml\n</code></pre></p> <p>Step 2: Issues Identified - No security context - Using latest tag - Secret in plain text env var - No resource limits - Missing health checks</p> <p>Step 3: Fixed Deployment <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: web-app\n  template:\n    metadata:\n      labels:\n        app: web-app\n    spec:\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 1000\n        fsGroup: 1000\n      containers:\n      - name: web\n        image: nginx:1.21.6-alpine\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          capabilities:\n            drop:\n            - ALL\n        env:\n        - name: DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: db-secret\n              key: password\n        resources:\n          limits:\n            memory: \"512Mi\"\n            cpu: \"500m\"\n          requests:\n            memory: \"256Mi\"\n            cpu: \"250m\"\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 8080\n          initialDelaySeconds: 5\n        volumeMounts:\n        - name: tmp-volume\n          mountPath: /tmp\n        - name: var-cache\n          mountPath: /var/cache/nginx\n      volumes:\n      - name: tmp-volume\n        emptyDir: {}\n      - name: var-cache\n        emptyDir: {}\n</code></pre></p>"},{"location":"kubernetes/cks/exercises/security-static-analysis/#remediation-examples","title":"Remediation Examples","text":""},{"location":"kubernetes/cks/exercises/security-static-analysis/#security-context-best-practices","title":"Security Context Best Practices","text":"<pre><code># Complete security context example\nsecurityContext:\n  # Pod-level\n  runAsNonRoot: true\n  runAsUser: 1000\n  runAsGroup: 1000\n  fsGroup: 1000\n  seccompProfile:\n    type: RuntimeDefault\n\ncontainers:\n- name: app\n  securityContext:\n    # Container-level\n    allowPrivilegeEscalation: false\n    readOnlyRootFilesystem: true\n    runAsNonRoot: true\n    runAsUser: 1000\n    capabilities:\n      drop:\n      - ALL\n      add:\n      - NET_BIND_SERVICE  # Only if needed\n</code></pre>"},{"location":"kubernetes/cks/exercises/security-static-analysis/#resource-management","title":"Resource Management","text":"<pre><code>resources:\n  limits:\n    memory: \"512Mi\"\n    cpu: \"500m\"\n    ephemeral-storage: \"1Gi\"\n  requests:\n    memory: \"256Mi\"\n    cpu: \"250m\"\n    ephemeral-storage: \"500Mi\"\n</code></pre>"},{"location":"kubernetes/cks/exercises/security-static-analysis/#secrets-management","title":"Secrets Management","text":"<pre><code># Create secret first\napiVersion: v1\nkind: Secret\nmetadata:\n  name: app-secrets\ntype: Opaque\ndata:\n  api-key: &lt;base64-encoded-value&gt;\n  db-password: &lt;base64-encoded-value&gt;\n\n# Reference in deployment\nenv:\n- name: API_KEY\n  valueFrom:\n    secretKeyRef:\n      name: app-secrets\n      key: api-key\n</code></pre>"},{"location":"kubernetes/cks/exercises/security-static-analysis/#exam-tips","title":"Exam Tips","text":""},{"location":"kubernetes/cks/exercises/security-static-analysis/#time-management","title":"Time Management","text":"<ol> <li>Quick Scan First: Use automated tools to identify obvious issues</li> <li>Prioritize: Focus on HIGH/CRITICAL severity issues first</li> <li>Systematic Approach: Check each category methodically</li> </ol>"},{"location":"kubernetes/cks/exercises/security-static-analysis/#common-commands-to-remember","title":"Common Commands to Remember","text":"<pre><code># Trivy scanning\ntrivy config .\ntrivy image nginx:latest\n\n# Kubesec analysis\nkubesec scan pod.yaml\n\n# Kubectl dry-run for validation\nkubectl apply --dry-run=client -f manifest.yaml\n\n# Check security context\nkubectl get pod -o jsonpath='{.spec.securityContext}'\n</code></pre>"},{"location":"kubernetes/cks/exercises/security-static-analysis/#checklist-for-quick-review","title":"Checklist for Quick Review","text":"<ul> <li>[ ] Non-root user execution</li> <li>[ ] No hardcoded secrets</li> <li>[ ] Specific image tags (not latest)</li> <li>[ ] Resource limits set</li> <li>[ ] Security context configured</li> <li>[ ] Minimal attack surface</li> <li>[ ] Health checks present</li> <li>[ ] Network policies defined</li> </ul>"},{"location":"kubernetes/cks/exercises/security-static-analysis/#practice-exercises","title":"Practice Exercises","text":""},{"location":"kubernetes/cks/exercises/security-static-analysis/#exercise-1-fix-this-dockerfile","title":"Exercise 1: Fix This Dockerfile","text":"<pre><code>FROM node:latest\nRUN apt-get update &amp;&amp; apt-get install -y vim curl wget git\nENV JWT_SECRET=mysecretkey123\nCOPY . /app\nWORKDIR /app\nRUN npm install\nEXPOSE 3000\nCMD [\"npm\", \"start\"]\n</code></pre>"},{"location":"kubernetes/cks/exercises/security-static-analysis/#exercise-2-secure-this-deployment","title":"Exercise 2: Secure This Deployment","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api-server\nspec:\n  template:\n    spec:\n      containers:\n      - name: api\n        image: myapi:latest\n        env:\n        - name: DATABASE_URL\n          value: \"postgres://user:password@db:5432/mydb\"\n        ports:\n        - containerPort: 8080\n</code></pre> <p>Solutions available upon request or can be worked through using the guidelines above.</p>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/","title":"Kubelet Configuration Exam Exercises","text":""},{"location":"kubernetes/cks/exercises/update-kubelet-config/#overview","title":"Overview","text":"<p>This document contains comprehensive exam exercises for modifying and managing Kubelet configuration. Each exercise covers different aspects of Kubelet configuration management, from basic parameter changes to advanced cluster-wide configuration scenarios.</p>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#exercise-1-basic-kubelet-configuration-file-management","title":"Exercise 1: Basic Kubelet Configuration File Management","text":""},{"location":"kubernetes/cks/exercises/update-kubelet-config/#scenario","title":"Scenario","text":"<p>You need to modify the Kubelet configuration on a worker node to change basic operational parameters.</p>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#tasks","title":"Tasks","text":"<ol> <li>Locate the current Kubelet configuration file</li> <li>Modify the following parameters:</li> <li>Change the cluster DNS to <code>10.96.0.10</code></li> <li>Set the maximum number of pods per node to 200</li> <li>Enable CPU and Memory manager policies</li> <li>Configure log rotation settings</li> </ol>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#configuration-file-example","title":"Configuration File Example","text":"<pre><code>apiVersion: kubelet.config.k8s.io/v1beta1\nkind: KubeletConfiguration\naddress: 0.0.0.0\nport: 10250\nserializeImagePulls: false\nclusterDNS:\n- 10.96.0.10\nclusterDomain: cluster.local\nmaxPods: 200\ncpuManagerPolicy: static\nmemoryManagerPolicy: Static\nlogRotateMaxSize: 100Mi\nlogRotateMaxBackups: 5\nlogRotateMaxAge: 7\n</code></pre>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#steps-to-complete","title":"Steps to Complete","text":"<ol> <li> <p>Back up the existing configuration: <pre><code>sudo cp /var/lib/kubelet/config.yaml /var/lib/kubelet/config.yaml.backup\n</code></pre></p> </li> <li> <p>Edit the configuration file: <pre><code>sudo vim /var/lib/kubelet/config.yaml\n</code></pre></p> </li> <li> <p>Restart the Kubelet service: <pre><code>sudo systemctl restart kubelet\n</code></pre></p> </li> <li> <p>Verify the changes: <pre><code>sudo systemctl status kubelet\nkubectl get nodes -o wide\n</code></pre></p> </li> </ol>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#expected-outcomes","title":"Expected Outcomes","text":"<ul> <li>Kubelet restarts successfully with new configuration</li> <li>Node shows updated capacity and allocatable resources</li> <li>DNS resolution works with the new cluster DNS</li> <li>Pod limit is enforced according to new maxPods setting</li> </ul>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#exercise-2-kubeadm-upgrade-node-phase-configuration","title":"Exercise 2: Kubeadm Upgrade Node Phase Configuration","text":""},{"location":"kubernetes/cks/exercises/update-kubelet-config/#scenario_1","title":"Scenario","text":"<p>Use kubeadm to upgrade the kubelet configuration during a cluster upgrade process.</p>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#tasks_1","title":"Tasks","text":"<ol> <li>Perform kubelet configuration upgrade using kubeadm</li> <li>Verify the upgraded configuration</li> <li>Handle any conflicts between old and new configurations</li> </ol>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#pre-upgrade-preparation","title":"Pre-upgrade Preparation","text":"<pre><code># Check current kubelet version\nkubelet --version\n\n# Check current node configuration\nkubectl get node $(hostname) -o yaml | grep kubeletVersion\n\n# Backup current configuration\nsudo cp /var/lib/kubelet/config.yaml /var/lib/kubelet/config.yaml.pre-upgrade\n</code></pre>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#kubeadm-upgrade-process","title":"Kubeadm Upgrade Process","text":"<pre><code># Download the new kubelet configuration from the cluster\nsudo kubeadm upgrade node phase kubelet-config\n\n# Restart kubelet to apply the new configuration\nsudo systemctl restart kubelet\n\n# Verify the upgrade\nsudo systemctl status kubelet\nkubectl get nodes\n</code></pre>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#post-upgrade-verification","title":"Post-upgrade Verification","text":"<pre><code># Check if kubelet is running with new config\nps aux | grep kubelet\n\n# Verify node readiness\nkubectl describe node $(hostname)\n\n# Check for any configuration differences\ndiff /var/lib/kubelet/config.yaml.pre-upgrade /var/lib/kubelet/config.yaml\n</code></pre>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#expected-outcomes_1","title":"Expected Outcomes","text":"<ul> <li>Kubelet configuration is updated to match cluster version</li> <li>Node remains in Ready state after upgrade</li> <li>No configuration conflicts exist</li> <li>Kubelet version matches the target cluster version</li> </ul>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#exercise-3-resource-management-configuration","title":"Exercise 3: Resource Management Configuration","text":""},{"location":"kubernetes/cks/exercises/update-kubelet-config/#scenario_2","title":"Scenario","text":"<p>Configure Kubelet for optimal resource management in a high-performance computing environment.</p>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#tasks_2","title":"Tasks","text":"<ol> <li>Configure CPU Manager with static policy</li> <li>Set up Memory Manager with static policy</li> <li>Configure Topology Manager for NUMA awareness</li> <li>Set up custom resource reservations for system components</li> </ol>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#advanced-resource-configuration","title":"Advanced Resource Configuration","text":"<pre><code>apiVersion: kubelet.config.k8s.io/v1beta1\nkind: KubeletConfiguration\n# CPU Management\ncpuManagerPolicy: static\ncpuManagerPolicyOptions:\n  full-pcpus-only: true\ncpuManagerReconcilePeriod: 10s\n\n# Memory Management\nmemoryManagerPolicy: Static\n\n# Topology Management\ntopologyManagerPolicy: single-numa-node\ntopologyManagerScope: pod\n\n# Resource Reservations\nsystemReserved:\n  cpu: 500m\n  memory: 1Gi\n  ephemeral-storage: 2Gi\nkubeReserved:\n  cpu: 500m\n  memory: 1Gi\n  ephemeral-storage: 1Gi\nevictionHard:\n  memory.available: \"100Mi\"\n  nodefs.available: \"10%\"\n  nodefs.inodesFree: \"5%\"\n  imagefs.available: \"15%\"\n\n# Container Runtime\ncontainerRuntimeEndpoint: unix:///var/run/containerd/containerd.sock\nimageServiceEndpoint: unix:///var/run/containerd/containerd.sock\n\n# Feature Gates\nfeatureGates:\n  CPUManager: true\n  MemoryManager: true\n  TopologyManager: true\n  KubeletPodResources: true\n</code></pre>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#pre-configuration-setup","title":"Pre-configuration Setup","text":"<pre><code># Clear CPU manager state (if changing policy)\nsudo systemctl stop kubelet\nsudo rm -f /var/lib/kubelet/cpu_manager_state\nsudo rm -f /var/lib/kubelet/memory_manager_state\n\n# Apply the new configuration\nsudo cp new-kubelet-config.yaml /var/lib/kubelet/config.yaml\n\n# Restart kubelet\nsudo systemctl start kubelet\n</code></pre>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#verification-commands","title":"Verification Commands","text":"<pre><code># Check CPU manager status\nls -la /var/lib/kubelet/cpu_manager_state\ncat /var/lib/kubelet/cpu_manager_state\n\n# Verify resource allocation\nkubectl describe nodes\n\n# Check feature gates\nkubectl get --raw /api/v1/nodes/$(hostname)/proxy/configz | jq '.kubeletconfig.featureGates'\n</code></pre>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#expected-outcomes_2","title":"Expected Outcomes","text":"<ul> <li>CPU Manager allocates exclusive CPU cores to guaranteed pods</li> <li>Memory Manager provides NUMA-local memory allocation</li> <li>Topology Manager ensures NUMA affinity</li> <li>System and Kube reserved resources are properly allocated</li> </ul>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#exercise-4-security-and-authentication-configuration","title":"Exercise 4: Security and Authentication Configuration","text":""},{"location":"kubernetes/cks/exercises/update-kubelet-config/#scenario_3","title":"Scenario","text":"<p>Harden Kubelet security by configuring authentication, authorization, and TLS settings.</p>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#tasks_3","title":"Tasks","text":"<ol> <li>Enable Webhook authentication and authorization</li> <li>Configure TLS certificate rotation</li> <li>Set up admission controllers</li> <li>Configure security contexts and AppArmor profiles</li> </ol>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#security-configuration","title":"Security Configuration","text":"<pre><code>apiVersion: kubelet.config.k8s.io/v1beta1\nkind: KubeletConfiguration\n# Authentication\nauthentication:\n  anonymous:\n    enabled: false\n  webhook:\n    enabled: true\n    cacheTTL: 30s\n  x509:\n    clientCAFile: /etc/kubernetes/pki/ca.crt\n\n# Authorization\nauthorization:\n  mode: Webhook\n  webhook:\n    cacheAuthorizedTTL: 5m\n    cacheUnauthorizedTTL: 30s\n\n# TLS Configuration\ntlsCertFile: /var/lib/kubelet/pki/kubelet.crt\ntlsPrivateKeyFile: /var/lib/kubelet/pki/kubelet.key\nrotateCertificates: true\nserverTLSBootstrap: true\n\n# Security Settings\nprotectKernelDefaults: true\nmakeIPTablesUtilChains: true\niptablesMasqueradeBit: 14\niptablesDropBit: 15\n\n# Runtime Security\nallowPrivileged: false\nhostNetworkSources: []\nhostPIDSources: []\nhostIPCSources: []\n\n# Admission Controllers\nenableAdmissionPlugins:\n- NamespaceLifecycle\n- LimitRanger\n- ServiceAccount\n- DefaultStorageClass\n- DefaultTolerationSeconds\n- MutatingAdmissionWebhook\n- ValidatingAdmissionWebhook\n- ResourceQuota\n- NodeRestriction\n</code></pre>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#configuration-update-steps","title":"Configuration Update Steps","text":"<pre><code># Apply the security configuration\nsudo cp security-kubelet-config.yaml /var/lib/kubelet/config.yaml\n\n# Restart kubelet to apply changes\nsudo systemctl restart kubelet\n\n# Verify security settings are active\nsudo systemctl status kubelet\n</code></pre>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#security-verification","title":"Security Verification","text":"<pre><code># Test authentication\ncurl -k --cert /var/lib/kubelet/pki/kubelet.crt \\\n     --key /var/lib/kubelet/pki/kubelet.key \\\n     https://localhost:10250/metrics\n\n# Verify certificate rotation\nkubectl get csr | grep kubelet\n\n# Check security settings\nkubectl auth can-i --list --as=system:node:$(hostname)\n</code></pre>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#expected-outcomes_3","title":"Expected Outcomes","text":"<ul> <li>Anonymous access is disabled</li> <li>Webhook authentication and authorization work</li> <li>TLS certificates are properly configured and rotating</li> <li>Security policies are enforced</li> </ul>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#exercise-5-logging-and-monitoring-configuration","title":"Exercise 5: Logging and Monitoring Configuration","text":""},{"location":"kubernetes/cks/exercises/update-kubelet-config/#scenario_4","title":"Scenario","text":"<p>Configure comprehensive logging and monitoring for Kubelet operations.</p>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#tasks_4","title":"Tasks","text":"<ol> <li>Configure structured logging</li> <li>Set up log rotation and retention</li> <li>Enable metrics collection</li> <li>Configure event recording and retention</li> </ol>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#logging-and-monitoring-configuration","title":"Logging and Monitoring Configuration","text":"<pre><code>apiVersion: kubelet.config.k8s.io/v1beta1\nkind: KubeletConfiguration\n# Logging Configuration\nlogging:\n  format: json\n  flushFrequency: 5s\n  verbosity: 2\n  options:\n    json:\n      infoBufferSize: \"0\"\n\n# Log Rotation\nlogRotateMaxSize: 100Mi\nlogRotateMaxBackups: 5\nlogRotateMaxAge: 7\n\n# Container Log Management\ncontainerLogMaxSize: 50Mi\ncontainerLogMaxFiles: 5\n\n# Event Configuration\neventRecordQPS: 50\neventBurst: 100\neventTTL: 1h\n\n# Metrics\nenableProfilingHandler: true\nenableDebuggingHandlers: true\nmetricsBindAddress: 0.0.0.0:10255\n\n# Health Checks\nhealthzBindAddress: 0.0.0.0:10248\nhealthzPort: 10248\n\n# Node Status\nnodeStatusMaxImages: 50\nnodeStatusUpdateFrequency: 10s\nnodeStatusReportFrequency: 5m\n\n# Runtime Monitoring\nruntimeRequestTimeout: 10m\nimageMinimumGCAge: 2m\nimageGCHighThresholdPercent: 85\nimageGCLowThresholdPercent: 80\n</code></pre>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#apply-configuration","title":"Apply Configuration","text":"<pre><code># Update kubelet configuration\nsudo cp logging-kubelet-config.yaml /var/lib/kubelet/config.yaml\n\n# Restart kubelet\nsudo systemctl restart kubelet\n</code></pre>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#monitoring-commands","title":"Monitoring Commands","text":"<pre><code># Check Kubelet metrics\ncurl http://localhost:10255/metrics\n\n# Monitor health endpoint\ncurl http://localhost:10248/healthz\n\n# View structured logs\njournalctl -u kubelet -o json-pretty\n\n# Check event logs\nkubectl get events --field-selector involvedObject.kind=Node\n\n# Monitor resource usage\nkubectl top nodes\n</code></pre>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#expected-outcomes_4","title":"Expected Outcomes","text":"<ul> <li>Structured JSON logging is enabled</li> <li>Log rotation works correctly</li> <li>Metrics are accessible and properly formatted</li> <li>Events are recorded and retained appropriately</li> </ul>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#exercise-6-container-runtime-configuration","title":"Exercise 6: Container Runtime Configuration","text":""},{"location":"kubernetes/cks/exercises/update-kubelet-config/#scenario_5","title":"Scenario","text":"<p>Configure Kubelet to work with different container runtimes and optimize container operations.</p>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#tasks_5","title":"Tasks","text":"<ol> <li>Configure containerd runtime settings</li> <li>Set up image pull policies and parallel pulls</li> <li>Configure registry authentication</li> <li>Optimize container lifecycle management</li> </ol>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#container-runtime-configuration","title":"Container Runtime Configuration","text":"<pre><code>apiVersion: kubelet.config.k8s.io/v1beta1\nkind: KubeletConfiguration\n# Container Runtime\ncontainerRuntimeEndpoint: unix:///var/run/containerd/containerd.sock\nimageServiceEndpoint: unix:///var/run/containerd/containerd.sock\n\n# Image Management\nimageMinimumGCAge: 2m\nimageGCHighThresholdPercent: 85\nimageGCLowThresholdPercent: 80\nserializeImagePulls: false\nmaxParallelImagePulls: 5\n\n# Registry Configuration\nregistryPullQPS: 10\nregistryBurst: 20\n\n# Container Lifecycle\nstreamingConnectionIdleTimeout: 4h\ndockerDisableSharedPID: false\npodPidsLimit: 4096\n\n# Runtime Class Support\nruntimeRequestTimeout: 10m\n\n# Volume Configuration\nvolumePluginDir: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/\n</code></pre>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#apply-configuration_1","title":"Apply Configuration","text":"<pre><code># Update configuration\nsudo cp runtime-kubelet-config.yaml /var/lib/kubelet/config.yaml\n\n# Restart kubelet\nsudo systemctl restart kubelet\n</code></pre>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#verification-commands_1","title":"Verification Commands","text":"<pre><code># Test runtime connection\nsudo crictl version\n\n# Check image operations\nsudo crictl images\nsudo crictl pull nginx:latest\n\n# Monitor container runtime\nsudo crictl stats\nsudo crictl ps\n</code></pre>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#expected-outcomes_5","title":"Expected Outcomes","text":"<ul> <li>Containerd runtime is properly configured</li> <li>Image pulls work efficiently with parallel downloads</li> <li>Container lifecycle operations are optimized</li> </ul>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#exercise-7-node-taints-labels-and-scheduling-configuration","title":"Exercise 7: Node Taints, Labels, and Scheduling Configuration","text":""},{"location":"kubernetes/cks/exercises/update-kubelet-config/#scenario_6","title":"Scenario","text":"<p>Configure Kubelet to properly handle node scheduling, taints, and labels for workload placement.</p>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#tasks_6","title":"Tasks","text":"<ol> <li>Configure node labels and annotations</li> <li>Set up node taints for specialized workloads</li> <li>Configure scheduling policies</li> <li>Implement node cordoning and draining procedures</li> </ol>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#node-configuration","title":"Node Configuration","text":"<pre><code>apiVersion: kubelet.config.k8s.io/v1beta1\nkind: KubeletConfiguration\n# Node Labels\nnodeLabels:\n  node-type: compute\n  hardware: gpu\n  zone: us-west-1a\n  instance-type: m5.xlarge\n\n# Provider Configuration\nproviderID: aws:///us-west-1a/i-1234567890abcdef0\ncloudProvider: aws\n\n# Scheduling Configuration\nmaxPods: 110\npodsPerCore: 0\n\n# Node Status\nnodeStatusUpdateFrequency: 10s\nnodeStatusReportFrequency: 5m\n</code></pre>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#apply-configuration-and-node-management","title":"Apply Configuration and Node Management","text":"<pre><code># Apply kubelet configuration\nsudo cp node-kubelet-config.yaml /var/lib/kubelet/config.yaml\nsudo systemctl restart kubelet\n\n# Add node labels\nkubectl label nodes worker-node-1 hardware=gpu\nkubectl label nodes worker-node-1 workload-type=ml\n\n# Add node taints\nkubectl taint nodes worker-node-1 dedicated=gpu:NoSchedule\nkubectl taint nodes worker-node-1 gpu=true:NoExecute\n\n# Configure node annotations\nkubectl annotate nodes worker-node-1 cluster-autoscaler.kubernetes.io/scale-down-disabled=true\n</code></pre>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#node-maintenance-commands","title":"Node Maintenance Commands","text":"<pre><code># Cordon node (prevent new pods)\nkubectl cordon worker-node-1\n\n# Drain node (remove existing pods)\nkubectl drain worker-node-1 --ignore-daemonsets --delete-emptydir-data\n\n# Uncordon node (allow scheduling)\nkubectl uncordon worker-node-1\n\n# Check node scheduling status\nkubectl get nodes -o wide\nkubectl describe node worker-node-1\n</code></pre>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#expected-outcomes_6","title":"Expected Outcomes","text":"<ul> <li>Node labels and taints are properly applied</li> <li>Specialized workloads schedule correctly</li> <li>Node maintenance operations work smoothly</li> <li>Scheduling policies are enforced</li> </ul>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#exercise-8-performance-tuning-and-optimization","title":"Exercise 8: Performance Tuning and Optimization","text":""},{"location":"kubernetes/cks/exercises/update-kubelet-config/#scenario_7","title":"Scenario","text":"<p>Optimize Kubelet configuration for high-performance workloads and large-scale deployments.</p>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#tasks_7","title":"Tasks","text":"<ol> <li>Configure for high pod density</li> <li>Optimize garbage collection settings</li> <li>Tune networking and DNS performance</li> <li>Configure for minimal latency</li> </ol>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#high-performance-configuration","title":"High-Performance Configuration","text":"<pre><code>apiVersion: kubelet.config.k8s.io/v1beta1\nkind: KubeletConfiguration\n# High Density Configuration\nmaxPods: 500\npodsPerCore: 10\n\n# Optimized Frequencies\nnodeStatusUpdateFrequency: 4s\nnodeStatusReportFrequency: 1m\nsyncFrequency: 10s\nfileCheckFrequency: 10s\nhttpCheckFrequency: 10s\n\n# Garbage Collection Optimization\nimageGCHighThresholdPercent: 90\nimageGCLowThresholdPercent: 85\nimageMinimumGCAge: 30s\n\n# Container GC\nminimumGCAge: 30s\nmaxPerPodContainerCount: 2\nmaxContainerCount: 500\n\n# Event Optimization\neventRecordQPS: 100\neventBurst: 200\n\n# DNS Optimization\nclusterDNS:\n- 10.96.0.10\nresolverConfig: /etc/resolv.conf\ndnsPolicy: ClusterFirst\n\n# Runtime Optimization\nserializeImagePulls: false\nmaxParallelImagePulls: 10\nregistryPullQPS: 20\nregistryBurst: 40\n\n# Resource Management\nsystemReserved:\n  cpu: 1000m\n  memory: 2Gi\n  ephemeral-storage: 5Gi\nkubeReserved:\n  cpu: 1000m\n  memory: 2Gi\n  ephemeral-storage: 2Gi\n\n# Security Optimizations\nauthentication:\n  webhook:\n    cacheTTL: 2m\nauthorization:\n  webhook:\n    cacheAuthorizedTTL: 10m\n    cacheUnauthorizedTTL: 1m\n</code></pre>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#apply-optimization","title":"Apply Optimization","text":"<pre><code># Apply performance configuration\nsudo cp performance-kubelet-config.yaml /var/lib/kubelet/config.yaml\nsudo systemctl restart kubelet\n</code></pre>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#performance-monitoring","title":"Performance Monitoring","text":"<pre><code># Monitor Kubelet performance\nkubectl top nodes\nkubectl get nodes -o custom-columns=NAME:.metadata.name,PODS:.status.capacity.pods,ALLOCATABLE:.status.allocatable.pods\n\n# Check resource usage\ncurl -s localhost:10255/metrics | grep kubelet_\n\n# Monitor garbage collection\njournalctl -u kubelet | grep -i \"garbage collect\"\n</code></pre>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#expected-outcomes_7","title":"Expected Outcomes","text":"<ul> <li>Increased pod density without performance degradation</li> <li>Optimized resource utilization</li> <li>Reduced latency for pod operations</li> <li>Efficient garbage collection</li> </ul>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#exercise-9-kubeadm-integration-and-cluster-wide-updates","title":"Exercise 9: Kubeadm Integration and Cluster-wide Updates","text":""},{"location":"kubernetes/cks/exercises/update-kubelet-config/#scenario_8","title":"Scenario","text":"<p>Use kubeadm to manage kubelet configuration updates across the entire cluster during upgrades.</p>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#tasks_8","title":"Tasks","text":"<ol> <li>Prepare cluster for kubelet configuration upgrade</li> <li>Update control plane kubelet configuration</li> <li>Update worker node kubelet configuration</li> <li>Verify cluster-wide configuration consistency</li> </ol>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#control-plane-configuration-update","title":"Control Plane Configuration Update","text":"<pre><code># On control plane node\n# Update kubeadm configuration first\nsudo kubeadm upgrade plan\n\n# Upgrade control plane components\nsudo kubeadm upgrade apply v1.29.0\n\n# Update kubelet configuration\nsudo kubeadm upgrade node phase kubelet-config\n\n# Restart kubelet\nsudo systemctl restart kubelet\n\n# Verify control plane\nkubectl get nodes\nkubectl cluster-info\n</code></pre>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#worker-node-configuration-update","title":"Worker Node Configuration Update","text":"<pre><code># On each worker node\n# Download new kubelet configuration\nsudo kubeadm upgrade node phase kubelet-config\n\n# Restart kubelet with new configuration\nsudo systemctl restart kubelet\n\n# Verify node status\nkubectl get nodes\n</code></pre>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#custom-configuration-with-kubeadm","title":"Custom Configuration with Kubeadm","text":"<p>Create a kubeadm configuration file for custom kubelet settings:</p> <pre><code># kubeadm-config.yaml\napiVersion: kubeadm.k8s.io/v1beta3\nkind: ClusterConfiguration\nkubernetesVersion: v1.29.0\n---\napiVersion: kubelet.config.k8s.io/v1beta1\nkind: KubeletConfiguration\nmaxPods: 200\ncpuManagerPolicy: static\nsystemReserved:\n  cpu: 500m\n  memory: 1Gi\nkubeReserved:\n  cpu: 500m\n  memory: 1Gi\n---\napiVersion: kubeproxy.config.k8s.io/v1alpha1\nkind: KubeProxyConfiguration\nmode: ipvs\n</code></pre>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#apply-custom-configuration","title":"Apply Custom Configuration","text":"<pre><code># Apply the configuration during upgrade\nsudo kubeadm upgrade apply --config=kubeadm-config.yaml v1.29.0\n\n# Update nodes with new configuration\nsudo kubeadm upgrade node phase kubelet-config\n\n# Restart kubelet\nsudo systemctl restart kubelet\n</code></pre>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#verification-commands_2","title":"Verification Commands","text":"<pre><code># Check kubelet version across cluster\nkubectl get nodes -o custom-columns=NAME:.metadata.name,VERSION:.status.nodeInfo.kubeletVersion\n\n# Verify configuration consistency\nfor node in $(kubectl get nodes -o name); do\n  echo \"=== $node ===\"\n  kubectl get --raw /api/v1/nodes/${node#node/}/proxy/configz | jq '.kubeletconfig.maxPods'\ndone\n\n# Check cluster health\nkubectl get componentstatuses\nkubectl get nodes -o wide\n</code></pre>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#expected-outcomes_8","title":"Expected Outcomes","text":"<ul> <li>All nodes run the same kubelet version</li> <li>Configuration is consistent across the cluster</li> <li>Cluster remains healthy during updates</li> <li>Custom configurations are properly applied</li> </ul>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#exercise-10-troubleshooting-configuration-issues","title":"Exercise 10: Troubleshooting Configuration Issues","text":""},{"location":"kubernetes/cks/exercises/update-kubelet-config/#scenario_9","title":"Scenario","text":"<p>Diagnose and fix various Kubelet configuration problems that may occur in production environments.</p>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"kubernetes/cks/exercises/update-kubelet-config/#issue-1-kubelet-fails-to-start-after-configuration-change","title":"Issue 1: Kubelet Fails to Start After Configuration Change","text":"<pre><code># Check systemd status\nsudo systemctl status kubelet\n\n# Check logs for errors\njournalctl -u kubelet -n 50\n\n# Validate configuration syntax\nsudo kubelet --config=/var/lib/kubelet/config.yaml --dry-run\n\n# Restore backup configuration if needed\nsudo cp /var/lib/kubelet/config.yaml.backup /var/lib/kubelet/config.yaml\nsudo systemctl restart kubelet\n</code></pre>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#issue-2-node-not-ready-after-configuration","title":"Issue 2: Node Not Ready After Configuration","text":"<pre><code># Check node status\nkubectl get nodes\nkubectl describe node $(hostname)\n\n# Check Kubelet logs\njournalctl -u kubelet -f\n\n# Verify container runtime\nsudo systemctl status containerd\nsudo crictl version\n\n# Check certificates\nopenssl x509 -in /var/lib/kubelet/pki/kubelet.crt -text -noout\n</code></pre>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#issue-3-resource-manager-policy-conflicts","title":"Issue 3: Resource Manager Policy Conflicts","text":"<pre><code># Reset CPU manager state\nsudo systemctl stop kubelet\nsudo rm -f /var/lib/kubelet/cpu_manager_state\nsudo rm -f /var/lib/kubelet/memory_manager_state\n\n# Apply corrected configuration\nsudo cp fixed-kubelet-config.yaml /var/lib/kubelet/config.yaml\nsudo systemctl start kubelet\n\n# Verify policy is active\ncat /var/lib/kubelet/cpu_manager_state\n</code></pre>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#emergency-recovery-procedure","title":"Emergency Recovery Procedure","text":"<pre><code># Complete kubelet reset and reconfiguration\nsudo systemctl stop kubelet\n\n# Reset kubelet configuration to defaults\nsudo kubeadm reset phase cleanup-node\n\n# Rejoin the cluster\nsudo kubeadm join &lt;control-plane-endpoint&gt; --token &lt;token&gt; --discovery-token-ca-cert-hash &lt;hash&gt;\n\n# Verify cluster rejoin\nkubectl get nodes\nkubectl describe node $(hostname)\n</code></pre>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#expected-outcomes_9","title":"Expected Outcomes","text":"<ul> <li>Ability to diagnose configuration issues quickly</li> <li>Successful rollback procedures when needed</li> <li>Proper validation of configuration changes</li> <li>Recovery from various failure scenarios</li> </ul>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#comprehensive-final-exercise-multi-node-configuration-management","title":"Comprehensive Final Exercise: Multi-Node Configuration Management","text":""},{"location":"kubernetes/cks/exercises/update-kubelet-config/#scenario_10","title":"Scenario","text":"<p>Manage kubelet configuration across a heterogeneous cluster with different node types.</p>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#node-specific-configurations","title":"Node-Specific Configurations","text":""},{"location":"kubernetes/cks/exercises/update-kubelet-config/#gpu-node-configuration","title":"GPU Node Configuration","text":"<pre><code>apiVersion: kubelet.config.k8s.io/v1beta1\nkind: KubeletConfiguration\nmaxPods: 100\ncpuManagerPolicy: static\nmemoryManagerPolicy: Static\ntopologyManagerPolicy: single-numa-node\nsystemReserved:\n  cpu: 1000m\n  memory: 4Gi\n  nvidia.com/gpu: 0\nkubeReserved:\n  cpu: 1000m\n  memory: 2Gi\nnodeLabels:\n  accelerator: nvidia-tesla-v100\n  workload-type: ml\nfeatureGates:\n  DevicePlugins: true\n</code></pre>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#edge-node-configuration","title":"Edge Node Configuration","text":"<pre><code>apiVersion: kubelet.config.k8s.io/v1beta1\nkind: KubeletConfiguration\nmaxPods: 30\ncpuManagerPolicy: none\nmemoryManagerPolicy: None\nimageGCHighThresholdPercent: 95\nimageGCLowThresholdPercent: 90\nsystemReserved:\n  cpu: 200m\n  memory: 512Mi\nkubeReserved:\n  cpu: 200m\n  memory: 256Mi\nnodeLabels:\n  node-type: edge\n  location: remote\nevictionHard:\n  memory.available: \"50Mi\"\n  nodefs.available: \"5%\"\n</code></pre>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#configuration-deployment-process","title":"Configuration Deployment Process","text":"<pre><code># For GPU nodes\nsudo cp gpu-kubelet-config.yaml /var/lib/kubelet/config.yaml\nsudo systemctl restart kubelet\nkubectl label nodes gpu-node-1 accelerator=nvidia-tesla-v100\nkubectl taint nodes gpu-node-1 dedicated=gpu:NoSchedule\n\n# For edge nodes  \nsudo cp edge-kubelet-config.yaml /var/lib/kubelet/config.yaml\nsudo systemctl restart kubelet\nkubectl label nodes edge-node-1 node-type=edge\nkubectl taint nodes edge-node-1 edge=true:NoSchedule\n\n# Verify configurations\nkubectl get nodes --show-labels\nkubectl describe nodes\n</code></pre>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#cluster-wide-validation","title":"Cluster-wide Validation","text":"<pre><code># Check all node configurations\nkubectl get nodes -o custom-columns=NAME:.metadata.name,LABELS:.metadata.labels,TAINTS:.spec.taints\n\n# Verify kubelet versions\nkubectl get nodes -o custom-columns=NAME:.metadata.name,VERSION:.status.nodeInfo.kubeletVersion\n\n# Test workload scheduling\nkubectl apply -f test-workloads.yaml\nkubectl get pods -o wide\n\n# Monitor cluster health\nkubectl get componentstatuses\nkubectl top nodes\n</code></pre>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#expected-outcomes_10","title":"Expected Outcomes","text":"<ul> <li>Different node types have appropriate configurations</li> <li>Workloads schedule to correct node types</li> <li>Cluster maintains overall health and stability</li> <li>Configuration changes are applied consistently</li> </ul>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#answer-key-and-validation","title":"Answer Key and Validation","text":""},{"location":"kubernetes/cks/exercises/update-kubelet-config/#general-validation-commands","title":"General Validation Commands","text":"<pre><code># Check kubelet service status\nsudo systemctl status kubelet\n\n# Verify configuration syntax\nsudo kubelet --config=/var/lib/kubelet/config.yaml --dry-run\n\n# Check node readiness\nkubectl get nodes\nkubectl describe node $(hostname)\n\n# Monitor kubelet logs\njournalctl -u kubelet -f\n\n# Verify resource allocation\nkubectl describe node $(hostname) | grep -A 10 \"Allocatable\"\n\n# Check current configuration\nkubectl get --raw /api/v1/nodes/$(hostname)/proxy/configz | jq\n</code></pre>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#common-troubleshooting-steps","title":"Common Troubleshooting Steps","text":"<ol> <li>Always backup configuration before changes</li> <li>Validate YAML syntax before applying</li> <li>Check systemd service status after restart</li> <li>Monitor logs for error messages</li> <li>Verify node remains in Ready state</li> <li>Test pod scheduling functionality</li> </ol>"},{"location":"kubernetes/cks/exercises/update-kubelet-config/#configuration-best-practices","title":"Configuration Best Practices","text":"<ul> <li>Use incremental changes rather than large configuration overhauls</li> <li>Test configurations in development before production</li> <li>Keep backup copies of working configurations</li> <li>Document all configuration changes</li> <li>Use kubeadm for cluster-wide consistency</li> <li>Monitor cluster health after configuration changes</li> </ul>"},{"location":"kubernetes/cks/exercises/use-sbom/","title":"Use SBOM - Finding Images Using Cryptolib","text":""},{"location":"kubernetes/cks/exercises/use-sbom/#problem-statement","title":"Problem Statement","text":"<p>Given 3 container images, identify which one is using cryptolib by using the BOM (Bill of Materials) binary.</p>"},{"location":"kubernetes/cks/exercises/use-sbom/#solution","title":"Solution","text":""},{"location":"kubernetes/cks/exercises/use-sbom/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have the <code>bom</code> binary installed:</p> <pre><code># Install if needed\ngo install sigs.k8s.io/bom/cmd/bom@latest\n\n# Or download from releases\ncurl -L -o bom https://github.com/kubernetes-sigs/bom/releases/download/v0.5.1/bom-amd64-linux\nchmod +x bom\nsudo mv bom /usr/local/bin/\n</code></pre>"},{"location":"kubernetes/cks/exercises/use-sbom/#step-1-examine-each-image-directly-with-bom-and-grep-for-cryptolib","title":"Step 1: Examine each image directly with BOM and grep for \u201ccryptolib\u201d","text":"<p>Since the task specifically asks to find which image is using \u201ccryptolib,\u201d we\u2019ll search for this exact term:</p> <pre><code># For this example, let's assume the images are:\n# image1: registry.example.com/app1:latest\n# image2: registry.example.com/app2:latest\n# image3: registry.example.com/app3:latest\n\n# Check image 1\necho \"Checking image 1 for cryptolib:\"\nbom generate -i registry.example.com/app1:latest | grep -i \"cryptolib\"\n\n# Check image 2\necho \"Checking image 2 for cryptolib:\"\nbom generate -i registry.example.com/app2:latest | grep -i \"cryptolib\"\n\n# Check image 3\necho \"Checking image 3 for cryptolib:\"\nbom generate -i registry.example.com/app3:latest | grep -i \"cryptolib\"\n</code></pre>"},{"location":"kubernetes/cks/exercises/use-sbom/#sample-output","title":"Sample Output","text":"<p>The output might look something like this:</p> <pre><code>Checking image 1 for cryptolib:\n[No output - cryptolib not found]\n\nChecking image 2 for cryptolib:\n[No output - cryptolib not found]\n\nChecking image 3 for cryptolib:\nSPDXRef-Package-npm-cryptolib-1.2.3\nname: cryptolib\nPackage: cryptolib\n</code></pre>"},{"location":"kubernetes/cks/exercises/use-sbom/#conclusion","title":"Conclusion","text":"<p>Based on the output, we can determine that Image 3 (registry.example.com/app3:latest) is using cryptolib, as it contains specific references to this library.</p>"},{"location":"kubernetes/cks/exercises/use-sbom/#notes","title":"Notes","text":"<ol> <li> <p>In an exam scenario, it\u2019s important to search for the exact library mentioned in the question.</p> </li> <li> <p>If no results are found with the exact name, you may need to try variations or check for common abbreviations of the library name:    <pre><code>bom generate -i image_name | grep -i -E 'cryptolib|crypto-lib|cryptojs|cryptography'\n</code></pre></p> </li> <li> <p>You can also examine the full SBOM output if the direct search doesn\u2019t provide results:    <pre><code>bom generate -i image_name &gt; sbom.txt\nless sbom.txt  # Then search within the file\n</code></pre></p> </li> <li> <p>The <code>bom</code> tool can generate output in different formats. If you\u2019re having trouble with the default format, try specifying the format explicitly:    <pre><code>bom generate -f spdx -i image_name | grep -i \"cryptolib\"\n</code></pre></p> </li> </ol>"},{"location":"kubernetes/kcsa/compliance_frameworks/","title":"Compliance Frameworks","text":""},{"location":"kubernetes/kcsa/compliance_frameworks/#gdpr-general-data-protection-regulation","title":"GDPR (General Data Protection Regulation)","text":"<p>European Union regulation that establishes strict requirements for processing personal data of EU citizens, including consent, data minimization, breach notifications, and the right to access/erase personal information.</p>"},{"location":"kubernetes/kcsa/compliance_frameworks/#hipaa-health-insurance-portability-and-accountability-act","title":"HIPAA (Health Insurance Portability and Accountability Act)","text":"<p>US legislation that protects sensitive patient health information by establishing standards for privacy, security, and breach notifications in healthcare.</p>"},{"location":"kubernetes/kcsa/compliance_frameworks/#pci-dss-payment-card-industry-data-security-standard","title":"PCI DSS (Payment Card Industry Data Security Standard)","text":"<p>Security standard for organizations handling credit card information, requiring secure networks, vulnerability management, access controls, and regular testing.</p>"},{"location":"kubernetes/kcsa/compliance_frameworks/#nist-national-institute-of-standards-and-technology","title":"NIST (National Institute of Standards and Technology)","text":"<p>US agency that develops cybersecurity frameworks, guidelines, and standards to help organizations assess and improve their security posture across various industries.</p>"},{"location":"kubernetes/kcsa/compliance_frameworks/#cis-benchmarks","title":"CIS Benchmarks","text":"<p>Industry-standard configuration guidelines developed by the Center for Internet Security that provide best practices for securely configuring operating systems, cloud services, containers, and applications.</p>"},{"location":"kubernetes/kcsa/compliance_frameworks/#fedramp-federal-risk-and-authorization-management-program","title":"FedRAMP (Federal Risk and Authorization Management Program)","text":"<p>FedRAMP is a US government-wide program that provides a standardized approach to security assessment, authorization, and continuous monitoring for cloud products and services.</p>"},{"location":"kubernetes/kcsa/compliance_frameworks/#microsoft-security-development-lifecycle-sdl","title":"Microsoft Security Development Lifecycle (SDL)","text":"<p>The Microsoft Security Development Lifecycle (SDL) is a software development process that helps developers build more secure software by reducing the number and severity of vulnerabilities while reducing development cost.</p> <p>Key Practices:</p> <ul> <li>Threat Modeling: Structured approach to identifying, quantifying, and addressing security risks</li> <li>Secure Coding Guidelines: Standards for writing code that\u2019s resistant to vulnerabilities</li> <li>Static Application Security Testing (SAST): Automated scanning during development</li> <li>Dynamic Application Security Testing (DAST): Runtime security testing</li> <li>Security Reviews: Formal assessments at critical phases</li> <li>Penetration Testing: Simulated attacks to find vulnerabilities</li> </ul>"},{"location":"kubernetes/kcsa/compliance_security_frameworks_tools/","title":"Compliance and Security frameworks","text":""},{"location":"kubernetes/kcsa/compliance_security_frameworks_tools/#distribute-container-manifest","title":"Distribute - Container manifest","text":""},{"location":"kubernetes/kcsa/compliance_security_frameworks_tools/#kubesec","title":"Kubesec","text":"<p>A security risk analysis tool for Kubernetes resources that scans YAML manifests and Helm charts to identify security misconfigurations, vulnerabilities, and non-compliance with best practices.</p>"},{"location":"kubernetes/kcsa/compliance_security_frameworks_tools/#terrascan","title":"Terrascan","text":"<p>An open-source security scanner that detects compliance and security violations across Infrastructure as Code (IaC) tools like Terraform, Kubernetes, Helm, and Dockerfiles to help enforce security best practices.</p>"},{"location":"kubernetes/kcsa/compliance_security_frameworks_tools/#distribute-security-tests","title":"Distribute - Security Tests","text":""},{"location":"kubernetes/kcsa/compliance_security_frameworks_tools/#nuclei","title":"Nuclei","text":"<p>A fast, template-based vulnerability scanner that detects security issues across various attack surfaces by using customizable templates for targeted scanning.</p>"},{"location":"kubernetes/kcsa/compliance_security_frameworks_tools/#trivy","title":"Trivy","text":"<p>An all-in-one open-source scanner for containers, filesystems, Git repositories, and Kubernetes that detects vulnerabilities, misconfigurations, and secrets.</p>"},{"location":"kubernetes/kcsa/compliance_security_frameworks_tools/#snyk","title":"Snyk","text":"<p>A developer security platform that finds, fixes, and monitors vulnerabilities in application code, open source dependencies, containers, and infrastructure as code.</p>"},{"location":"kubernetes/kcsa/compliance_security_frameworks_tools/#clair","title":"Clair","text":"<p>An open-source container vulnerability scanner that statically analyzes container images for known security vulnerabilities in application dependencies.</p>"},{"location":"kubernetes/kcsa/compliance_security_frameworks_tools/#grype","title":"Grype","text":"<p>A vulnerability scanner for container images and filesystems created by Anchore that provides fast, comprehensive detection of package vulnerabilities.</p>"},{"location":"kubernetes/kcsa/compliance_security_frameworks_tools/#sysdig-secure","title":"Sysdig Secure","text":"<p>A cloud-native security platform designed for securing containerized applications, Kubernetes environments, and cloud workloads. It provides comprehensive security capabilities across the entire container lifecycle.</p>"},{"location":"kubernetes/kcsa/compliance_security_frameworks_tools/#distribute-signing-and-trust","title":"Distribute - Signing and Trust","text":""},{"location":"kubernetes/kcsa/compliance_security_frameworks_tools/#in-toto","title":"in-toto","text":"<p>A framework that cryptographically ensures software supply chain integrity by tracking each step in the development process with signed metadata to verify the chain hasn\u2019t been compromised.</p>"},{"location":"kubernetes/kcsa/compliance_security_frameworks_tools/#notation","title":"Notation","text":"<p>A CNCF project for signing and verifying OCI container images and artifacts with a pluggable signature verification architecture.</p>"},{"location":"kubernetes/kcsa/compliance_security_frameworks_tools/#tuf-the-update-framework","title":"TUF (The Update Framework):","text":"<p>A secure framework for software updates that protects against various attacks by using multiple layers of signing keys and metadata verification.</p>"},{"location":"kubernetes/kcsa/compliance_security_frameworks_tools/#sigstore","title":"Sigstore","text":"<p>An open-source project providing free tools for code signing, transparency, and verification to secure software supply chains without managing private keys.</p>"},{"location":"kubernetes/kcsa/compliance_security_frameworks_tools/#deploy-pre-flight-checks","title":"Deploy - Pre-flight checks","text":""},{"location":"kubernetes/kcsa/compliance_security_frameworks_tools/#gatekeeper","title":"Gatekeeper","text":"<p>A Kubernetes admission controller using Open Policy Agent (OPA) that enforces configurable policies to validate, mutate, or reject resources before they\u2019re admitted to the cluster.</p>"},{"location":"kubernetes/kcsa/compliance_security_frameworks_tools/#kyverno","title":"Kyverno","text":"<p>A policy engine designed specifically for Kubernetes that can validate, mutate, and generate resources using policy as code without requiring a new language, providing native YAML/JSON support.</p>"},{"location":"kubernetes/kcsa/compliance_security_frameworks_tools/#distribution-response-investigation","title":"Distribution - Response &amp; Investigation","text":""},{"location":"kubernetes/kcsa/compliance_security_frameworks_tools/#wazuh","title":"Wazuh","text":"<p>An open-source security monitoring platform that provides threat detection, integrity monitoring, and compliance capabilities through log analysis, file integrity checking, and security alerts.</p>"},{"location":"kubernetes/kcsa/compliance_security_frameworks_tools/#snort","title":"Snort","text":"<p>A widely-used open-source network intrusion detection and prevention system (IDPS) that performs real-time traffic analysis and packet logging to detect and prevent network attacks.</p>"},{"location":"kubernetes/kcsa/compliance_security_frameworks_tools/#zeek","title":"Zeek","text":"<p>A powerful network security monitoring tool that analyzes network traffic to identify suspicious activity by generating high-level logs about network behavior rather than just matching signatures.</p>"},{"location":"kubernetes/kcsa/compliance_security_frameworks_tools/#runtime-orchestration","title":"Runtime - Orchestration","text":""},{"location":"kubernetes/kcsa/compliance_security_frameworks_tools/#kube-bench","title":"kube-bench","text":"<p>An open-source tool that checks whether Kubernetes deployments align with CIS Kubernetes Benchmark security recommendations by automatically testing control plane and worker nodes for best practices.</p>"},{"location":"kubernetes/kcsa/compliance_security_frameworks_tools/#trivy_1","title":"Trivy","text":"<p>An all-in-one vulnerability scanner for containers, filesystems, Git repositories, and Kubernetes that detects vulnerabilities, misconfigurations, and secrets.</p>"},{"location":"kubernetes/kcsa/compliance_security_frameworks_tools/#falco","title":"Falco","text":"<p>A cloud-native runtime security tool that detects abnormal behavior and security threats in real-time by monitoring Linux system calls, container activities, and Kubernetes events.</p>"},{"location":"kubernetes/kcsa/compliance_security_frameworks_tools/#spiffe","title":"SPIFFE","text":"<p>A set of open-source standards for securely identifying and authenticating services to each other across heterogeneous environments using platform-agnostic, cryptographic identities.</p>"},{"location":"kubernetes/kcsa/compliance_security_frameworks_tools/#runtime-storage","title":"Runtime - Storage","text":""},{"location":"kubernetes/kcsa/compliance_security_frameworks_tools/#rook","title":"Rook","text":"<p>A cloud-native storage orchestrator for Kubernetes that automates deployment, bootstrapping, configuration, scaling, and recovery of storage services like Ceph.</p>"},{"location":"kubernetes/kcsa/compliance_security_frameworks_tools/#ceph","title":"Ceph","text":"<p>A highly scalable, distributed storage system providing object, block, and file storage in a unified platform designed for high performance, reliability, and no single point of failure.</p>"},{"location":"kubernetes/kcsa/compliance_security_frameworks_tools/#gluster","title":"Gluster","text":"<p>An open-source distributed file system that can scale to several petabytes, handles thousands of clients, and is suitable for data-intensive workloads like cloud storage and media streaming.</p>"},{"location":"kubernetes/kcsa/compliance_security_frameworks_tools/#runtime-access","title":"Runtime - Access","text":""},{"location":"kubernetes/kcsa/compliance_security_frameworks_tools/#keycloak","title":"Keycloak","text":"<p>An open-source identity and access management solution that provides single sign-on, identity federation, social login, and user management with support for standard protocols like OAuth 2.0 and SAML.</p>"},{"location":"kubernetes/kcsa/compliance_security_frameworks_tools/#teleport","title":"Teleport","text":"<p>A security gateway for accessing Kubernetes clusters, servers, and applications that unifies access controls with certificate-based authentication, session recording, and audit logging.</p>"},{"location":"kubernetes/kcsa/compliance_security_frameworks_tools/#vault","title":"Vault","text":"<p>HashiCorp\u2019s secrets management tool that securely stores and tightly controls access to tokens, passwords, certificates, and encryption keys with dynamic secrets generation, data encryption, and leasing/renewal capabilities.</p>"},{"location":"kubernetes/kcsa/kubernetes_auditing_policy/","title":"Kubernetes Auditing Policy","text":"<p>Kubernetes auditing provides a security-relevant chronological set of records documenting the sequence of activities that have affected the system. The audit policy defines what events should be recorded and what data they should include.</p>"},{"location":"kubernetes/kcsa/kubernetes_auditing_policy/#audit-policy-configuration","title":"Audit Policy Configuration","text":"<p>Kubernetes audit policies are defined in YAML format. The policy specifies rules that determine what events should be recorded and the level of detail.</p>"},{"location":"kubernetes/kcsa/kubernetes_auditing_policy/#audit-levels","title":"Audit Levels","text":"<p>Kubernetes supports these audit levels, from least to most verbose:</p> <ul> <li>None: Don\u2019t log events matching this rule</li> <li>Metadata: Log request metadata (user, timestamp, resource, verb) but not request or response body</li> <li>Request: Log event metadata and request body but not response body</li> <li>RequestResponse: Log event metadata, request and response bodies</li> </ul>"},{"location":"kubernetes/kcsa/kubernetes_auditing_policy/#example-audit-policy","title":"Example Audit Policy","text":"<pre><code>apiVersion: audit.k8s.io/v1\nkind: Policy\nrules:\n  # Log pod changes at RequestResponse level\n  - level: RequestResponse\n    resources:\n    - group: \"\"\n      resources: [\"pods\"]\n\n  # Log \"configmaps\" and \"secrets\" at Metadata level\n  - level: Metadata\n    resources:\n    - group: \"\"\n      resources: [\"configmaps\", \"secrets\"]\n\n  # Don't log requests to certain non-resource URL paths\n  - level: None\n    nonResourceURLs:\n    - /api*\n    - /version\n    - /healthz\n\n  # Log everything else at Metadata level\n  - level: Metadata\n</code></pre>"},{"location":"kubernetes/kcsa/kubernetes_auditing_policy/#implementing-audit-logging","title":"Implementing Audit Logging","text":"<p>To enable audit logging in Kubernetes, you need to:</p> <ol> <li> <p>Configure the API server with audit policy file:    <pre><code>--audit-policy-file=/etc/kubernetes/audit-policy.yaml\n</code></pre></p> </li> <li> <p>Specify the log backend:</p> </li> <li>Log to file:      <pre><code>--audit-log-path=/var/log/kubernetes/audit.log\n--audit-log-maxage=30\n--audit-log-maxbackup=10\n</code></pre></li> <li>Or log to webhook:      <pre><code>--audit-webhook-config-file=/etc/kubernetes/audit-webhook.yaml\n</code></pre></li> </ol>"},{"location":"kubernetes/kcsa/kubernetes_auditing_policy/#best-practices","title":"Best Practices","text":"<ol> <li>Focus on sensitive operations: Privilege escalation, auth failures, resource deletion</li> <li>Be selective: Logging everything at RequestResponse level causes significant overhead</li> <li>Consider log storage and rotation: Audit logs can grow very large</li> <li>Monitor audit logs: Integrate with security monitoring tools like SIEM systems</li> <li>Implement a graduated approach: Log sensitive operations at RequestResponse level, less sensitive at Metadata</li> </ol> <p>Properly configured audit logging is critical for security incident detection, forensic investigations, and compliance in Kubernetes clusters.</p>"},{"location":"kubernetes/kcsa/linux_tools/","title":"AppArmor","text":"<p>AppArmor (Application Armor) is a Linux security module that provides mandatory access control (MAC) for processes. It restricts programs\u2019 capabilities by enforcing security policies that limit what actions applications can perform.</p> <pre><code># Check AppArmor status\nsudo aa-status\n\n# List all profiles and their modes\nsudo apparmor_status\n\n# Get version information\nsudo apparmor_parser -V\n\n# Load a profile\nsudo apparmor_parser -r /etc/apparmor.d/profile_name\n\n# Set a profile to enforce mode\nsudo aa-enforce /path/to/binary\n\n# Set a profile to complain mode\nsudo aa-complain /path/to/binary\n\n# Check profile syntax\nsudo apparmor_parser -p /etc/apparmor.d/profile_name\n</code></pre>"},{"location":"kubernetes/kcsa/linux_tools/#kubernetes-apparmor-integration","title":"Kubernetes AppArmor Integration","text":"<p>Kubernetes supports AppArmor profiles through annotations on pods. This allows you to apply different security profiles to different pods based on their specific security requirements.</p>"},{"location":"kubernetes/kcsa/linux_tools/#implementation-details","title":"Implementation Details","text":"<ul> <li>Pod Annotations: AppArmor profiles are specified using annotations on the pod specification</li> <li>Node Requirements: AppArmor must be installed on each worker node</li> <li>Profile Loading: Profiles must be loaded on each node before pods can use them</li> </ul> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: secure-nginx\n  annotations:\n    container.apparmor.security.beta.kubernetes.io/nginx: runtime/default\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n</code></pre>"},{"location":"kubernetes/kcsa/security_context/","title":"Security Context","text":""},{"location":"kubernetes/kcsa/security_context/#examples","title":"Examples","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: mixed-security-context\nspec:\n  securityContext:\n    runAsUser: 1000\n    runAsGroup: 3000\n    fsGroup: 2000  # file system group ownership for volumes mounted in a Pod\n    supplementalGroups: [1001, 1002]  # adds secondary group IDs to the processes running in all containers of a Pod\n    seLinuxOptions:  # SELinux (Security-Enhanced Linux) parameters for containers and pods. SELinux provides mandatory access controls by enforcing security policies that restrict what processes can do\n      level: \"s0:c123,c456\"\n    seccompProfile: # apply seccomp (secure computing mode) profiles to restrict the system calls that containers can make to the Linux kernel\n      type: RuntimeDefault\n  containers:\n  - name: first-container\n    image: nginx\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop: [\"ALL\"]\n  - name: second-container\n    image: busybox\n    command: [\"sh\", \"-c\", \"sleep 3600\"]\n    securityContext:\n      runAsUser: 2000  # Overrides the Pod-level setting\n      capabilities:\n        add: [\"NET_ADMIN\"]\n</code></pre>"},{"location":"kubernetes/kcsa/threat_modeling_frameworks/","title":"Threat Modeling Framework","text":"<p>Threat modeling for Kubernetes involves a structured approach to identifying, categorizing, and mitigating potential security threats.</p>"},{"location":"kubernetes/kcsa/threat_modeling_frameworks/#stride","title":"STRIDE","text":"<p>STRIDE is a commonly used threat modeling framework that can be effectively applied to Kubernetes:</p> <ul> <li>Spoofing: Unauthorized access using stolen credentials or impersonating legitimate Kubernetes components</li> <li>Tampering: Unauthorized modification of Kubernetes resources, configurations, or container images</li> <li>Repudiation: Lack of audit trails for actions performed in the cluster</li> <li>Information Disclosure: Unauthorized access to sensitive data in pods, secrets, or ConfigMaps</li> <li>Denial of Service: Attacks that make Kubernetes services unavailable</li> <li>Elevation of Privilege: Gaining higher levels of access than intended</li> </ul>"},{"location":"kubernetes/kcsa/threat_modeling_frameworks/#mitre-attck-framework","title":"MITRE ATT&amp;CK Framework","text":"<p>MITRE ATT&amp;CK (Adversarial Tactics, Techniques, and Common Knowledge) is a globally accessible knowledge base of adversary tactics and techniques based on real-world observations. It\u2019s a comprehensive framework that systematically documents cyber adversary behavior.</p>"},{"location":"kubernetes/kcsa/threat_modeling_frameworks/#tactical-categories","title":"Tactical Categories","text":"<ul> <li>Initial Access: Methods attackers use to first enter a network or system, such as phishing emails, exploiting vulnerabilities in public-facing applications, or using stolen credentials.</li> <li>Execution: Techniques for running malicious code on a compromised system, including command-line interfaces, scripts, or scheduled tasks to execute the attacker\u2019s code.</li> <li>Persistence: Methods to maintain access to systems despite reboots or credential changes, including backdoors, registry modifications, or startup scripts that ensure attackers can return.</li> <li>Privilege Escalation: Techniques to gain higher-level permissions, such as exploiting vulnerabilities or manipulating access tokens to obtain administrative rights needed for further attack activities.</li> <li>Defense Evasion: Methods to avoid detection, including disabling security tools, clearing logs, encrypting malicious payloads, or disguising malicious activity as legitimate processes.</li> </ul>"},{"location":"kubernetes/kcsa/virtualizers/","title":"Virtualization Technologies","text":""},{"location":"kubernetes/kcsa/virtualizers/#gvisor","title":"gVisor","text":"<p>gVisor is an application kernel developed by Google that provides an additional security layer for containers by intercepting and handling system calls.</p>"},{"location":"kubernetes/kcsa/virtualizers/#key-characteristics","title":"Key Characteristics:","text":"<ul> <li>Acts as a security sandbox between containers and the host kernel</li> <li>Implements a substantial portion of the Linux system call interface in userspace</li> <li>Written primarily in Go</li> <li>Creates an application kernel that mediates access between the container and host</li> </ul>"},{"location":"kubernetes/kcsa/virtualizers/#how-it-works","title":"How it Works:","text":"<ul> <li>Intercepts system calls from containerized applications</li> <li>Implements its own network and filesystem interfaces</li> <li>Provides compatibility with standard container runtimes via OCI integration (runsc)</li> <li>Significantly reduces the attack surface exposed to containers</li> </ul>"},{"location":"kubernetes/kcsa/virtualizers/#use-cases","title":"Use Cases:","text":"<ul> <li>Multi-tenant container deployments</li> <li>Running untrusted or third-party code</li> <li>Enhancing security of web-facing containerized applications</li> </ul> <pre><code>apiVersion: node.k8s.io/v1\nkind: RuntimeClass\nmetadata:\n  name: gvisor\nhandler: runsc\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: gvisor-pod\nspec:\n  runtimeClassName: gvisor\n  containers:\n  - name: nginx\n    image: nginx\n</code></pre>"},{"location":"kubernetes/kcsa/virtualizers/#firecracker","title":"Firecracker","text":"<p>Firecracker is a lightweight virtualization technology developed by AWS that powers AWS Lambda and Fargate services.</p>"},{"location":"kubernetes/kcsa/virtualizers/#key-characteristics_1","title":"Key Characteristics:","text":"<ul> <li>Micro-VM technology combining VM security with container-like performance</li> <li>Minimalist VMM (Virtual Machine Monitor) built on KVM</li> <li>Written in Rust for memory safety</li> <li>Creates lightweight VMs in milliseconds</li> </ul>"},{"location":"kubernetes/kcsa/virtualizers/#how-it-works_1","title":"How it Works:","text":"<ul> <li>Launches micro-VMs with minimal memory footprint (~5MB per instance)</li> <li>Provides a minimal device model (virtio-net, virtio-block, serial console)</li> <li>Uses a RESTful API to manage VM lifecycle</li> <li>Each workload runs in a separate VM with true hardware-based isolation</li> </ul>"},{"location":"kubernetes/kcsa/virtualizers/#use-cases_1","title":"Use Cases:","text":"<ul> <li>Serverless computing platforms</li> <li>Container-as-a-service offerings</li> <li>Secure isolation of workloads</li> <li>High-density computing environments</li> </ul> <pre><code>apiVersion: node.k8s.io/v1\nkind: RuntimeClass\nmetadata:\n  name: kata-fc\nhandler: kata-fc\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: firecracker-pod\nspec:\n  runtimeClassName: kata-fc\n  containers:\n  - name: nginx\n    image: nginx\n</code></pre>"},{"location":"kubernetes/open_standards/cni_components/","title":"Networking Tools in Kubernetes","text":""},{"location":"kubernetes/open_standards/cni_components/#container-network-interface-cni","title":"Container Network Interface (CNI)","text":""},{"location":"kubernetes/open_standards/cni_components/#use-case-networking","title":"Use Case: Networking","text":"<p>Networking tools in Kubernetes play a critical role in ensuring seamless communication within the cluster and with external systems. They provide the backbone for pod-to-pod connectivity, service discovery, load balancing, and traffic routing. Beyond connectivity, these tools enforce security policies, optimize traffic flow, and maintain the reliability and scalability of modern distributed applications.</p>"},{"location":"kubernetes/open_standards/cni_components/#tools","title":"Tools:","text":""},{"location":"kubernetes/open_standards/cni_components/#1-contour","title":"1. Contour","text":"<ul> <li>Description: Contour is a Kubernetes-native ingress controller that uses the Envoy proxy to manage HTTP/HTTPS traffic effectively. It supports real-time configuration updates, enabling zero-downtime changes to routing rules. With advanced features like rate limiting, path rewrites, and secure gRPC support, Contour ensures robust and scalable traffic management.</li> <li>Best For: Teams that need a powerful, flexible ingress solution for managing complex traffic patterns and integrating seamlessly with modern application architectures.</li> </ul>"},{"location":"kubernetes/open_standards/cni_components/#2-calico","title":"2. Calico","text":"<ul> <li>Description: Calico is a robust Kubernetes networking and security solution designed for large-scale clusters. It provides advanced networking capabilities, such as encrypted traffic between pods, fine-grained network policies, and support for multiple networking backends like BGP and VXLAN. Its high scalability makes it a top choice for enterprises seeking enhanced security and performance.</li> <li>Best For: Large-scale Kubernetes deployments requiring comprehensive security policies and flexible networking configurations.</li> </ul>"},{"location":"kubernetes/open_standards/cni_components/#3-kube-router","title":"3. Kube-router","text":"<ul> <li>Description: Kube-router simplifies Kubernetes networking by consolidating routing, network policy enforcement, and service proxying into a single lightweight component. By leveraging IP routing instead of overlays, it minimizes latency and maximizes throughput, making it a preferred choice for performance-critical environments.</li> <li>Best For: Scenarios demanding high-performance networking and minimal overhead, particularly for latency-sensitive applications.</li> </ul>"},{"location":"kubernetes/open_standards/cni_components/#4-metallb","title":"4. MetalLB","text":"<ul> <li>Description: MetalLB is a load balancer designed specifically for bare-metal Kubernetes clusters. It integrates seamlessly with existing network infrastructure, offering cloud-like load balancing features via Layer 2 (local network) or Layer 3 (BGP). MetalLB simplifies external traffic routing, making bare-metal clusters operationally similar to cloud-based environments.</li> <li>Best For: Organizations operating bare-metal Kubernetes clusters that require external traffic handling without native cloud provider integrations.</li> </ul>"},{"location":"kubernetes/open_standards/cni_components/#5-flannel","title":"5. Flannel","text":"<ul> <li>Description: Flannel is a straightforward CNI plugin that provides basic pod networking through an overlay network. With support for multiple backends, it offers flexibility for a variety of cluster setups. Flannel is lightweight, easy to configure, and is often the go-to choice for small-to-medium Kubernetes environments.</li> <li>Best For: Environments prioritizing simplicity and ease of deployment over advanced networking capabilities.</li> </ul>"},{"location":"kubernetes/open_standards/cni_components/#6-weave","title":"6. Weave","text":"<ul> <li>Description: Weave Net offers a versatile networking solution that emphasizes simplicity and security. It includes built-in traffic encryption, multi-cloud support, and automatic service discovery. Weave makes it easy to set up hybrid and multi-cloud Kubernetes environments with minimal configuration, ensuring secure communication across all nodes and regions.</li> <li>Best For: Organizations operating in multi-cloud or hybrid environments that require secure and reliable pod networking.</li> </ul>"},{"location":"kubernetes/open_standards/cpi_components/","title":"Cloud Provider Interface (CPI)","text":"<p>The Cloud Provider Interface (CPI) is a Kubernetes standard that allows Kubernetes clusters to integrate with cloud providers for managing cloud-specific infrastructure. It enables provisioning of resources like nodes, load balancers, and persistent storage by abstracting cloud platform details.</p> <p>Below is an example of integrating a Kubernetes cluster with a Cloud Provider Interface (CPI) for managing a Load Balancer on a cloud provider.</p>"},{"location":"kubernetes/open_standards/cpi_components/#example-use-case-exposing-a-service-using-cloud-load-balancer","title":"Example Use Case: Exposing a Service Using Cloud Load Balancer","text":"<p>This example demonstrates how the Cloud Provider Interface enables Kubernetes to provision a cloud-based load balancer to expose a service.</p>"},{"location":"kubernetes/open_standards/cpi_components/#1-prerequisites","title":"1. Prerequisites","text":"<ul> <li>A Kubernetes cluster running on a supported cloud provider (AWS, GCP, Azure).</li> <li>The Cloud Controller Manager for the cloud provider must be deployed and configured.</li> </ul>"},{"location":"kubernetes/open_standards/cpi_components/#2-deployment-and-service-configuration","title":"2. Deployment and Service Configuration","text":""},{"location":"kubernetes/open_standards/cpi_components/#sample-deployment","title":"Sample Deployment","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n        - name: nginx\n          image: nginx:latest\n          ports:\n            - containerPort: 80\n</code></pre>"},{"location":"kubernetes/open_standards/cpi_components/#service-with-load-balancer","title":"Service with Load Balancer","text":"<p>The following service definition provisions a cloud load balancer using the CPI:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-service\n  annotations:\n    service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\" # AWS-specific annotation (optional)\nspec:\n  type: LoadBalancer\n  selector:\n    app: nginx\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 80\n</code></pre>"},{"location":"kubernetes/open_standards/cpi_components/#explanation","title":"Explanation:","text":"<ul> <li>Type: LoadBalancer: Tells Kubernetes to provision an external cloud load balancer.</li> <li>Annotations: Optional cloud-specific configurations (e.g., specifying the load balancer type in AWS).</li> <li>Selector: Targets pods labeled with <code>app: nginx</code>.</li> </ul>"},{"location":"kubernetes/open_standards/cpi_components/#3-how-it-works","title":"3. How It Works","text":"<ol> <li>When the service is created with <code>type: LoadBalancer</code>, the CPI Cloud Controller Manager interacts with the cloud provider\u2019s API.</li> <li>A cloud load balancer (e.g., AWS Elastic Load Balancer, Azure Load Balancer, or GCP Load Balancer) is provisioned automatically.</li> <li>The load balancer routes external traffic to the Kubernetes service, which forwards requests to the backend pods.</li> </ol>"},{"location":"kubernetes/open_standards/cpi_components/#4-verifying-the-load-balancer","title":"4. Verifying the Load Balancer","text":"<p>After creating the service, run the following command to verify the provisioned load balancer:</p> <pre><code>kubectl get services\n</code></pre> <p>Output Example:</p> <pre><code>NAME            TYPE           CLUSTER-IP     EXTERNAL-IP       PORT(S)        AGE\nnginx-service   LoadBalancer   10.0.0.1       a1b2c3d4e5.elb.amazonaws.com   80:30080/TCP   5m\n</code></pre> <ul> <li>EXTERNAL-IP: Shows the address of the provisioned cloud load balancer.</li> <li>Traffic sent to this external IP will be forwarded to the service and its backend pods.</li> </ul>"},{"location":"kubernetes/open_standards/cpi_components/#supported-cloud-providers","title":"Supported Cloud Providers","text":"Cloud Provider CPI Implementation Features AWS AWS Cloud Controller Manager Load balancers, EBS storage, node management GCP GCE Cloud Controller Manager Load balancers, PD storage, node scaling Azure Azure Cloud Controller Manager Load balancers, Disk storage, scaling"},{"location":"kubernetes/open_standards/cpi_components/#conclusion","title":"Conclusion","text":"<p>The Cloud Provider Interface (CPI) allows Kubernetes to integrate seamlessly with cloud providers for provisioning cloud infrastructure like load balancers, persistent volumes, and nodes. By defining a Service with <code>type: LoadBalancer</code>, Kubernetes leverages the CPI to interact with the cloud provider\u2019s API and automatically provision a load balancer for external traffic.</p>"},{"location":"kubernetes/open_standards/csi_components/","title":"Container Storage Interface (CSI)","text":"<p>The Container Storage Interface (CSI) is a Kubernetes open standard that enables storage providers to expose their storage systems to Kubernetes in a consistent and portable way. CSI standardizes how storage volumes are provisioned, mounted, and managed, regardless of the underlying storage infrastructure.</p> <p>Here are some prominent examples of CSI-compliant implementations:</p>"},{"location":"kubernetes/open_standards/csi_components/#1-amazon-elastic-block-store-ebs-csi-driver","title":"1. Amazon Elastic Block Store (EBS) CSI Driver","text":"<ul> <li>Description:   The Amazon EBS CSI driver enables Kubernetes to manage Amazon Elastic Block Store (EBS) volumes as persistent storage. It allows dynamic provisioning and management of EBS volumes for Kubernetes workloads.</li> <li>Key Features:</li> <li>Supports dynamic provisioning of EBS volumes.</li> <li>Enables mounting and attaching EBS volumes to Kubernetes pods.</li> <li>Provides support for resizing and snapshotting volumes.</li> <li>Example Usage:   <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: ebs-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 5Gi\n  storageClassName: gp2\n</code></pre></li> <li>Use Case:   Applications running on Kubernetes that require block storage on AWS.</li> </ul>"},{"location":"kubernetes/open_standards/csi_components/#2-google-persistent-disk-csi-driver","title":"2. Google Persistent Disk CSI Driver","text":"<ul> <li>Description:   The Google Persistent Disk (PD) CSI driver allows Kubernetes to manage Google Cloud Persistent Disks as persistent volumes. It supports both standard and SSD-backed persistent disks.</li> <li>Key Features:</li> <li>Supports dynamic provisioning, resizing, and snapshots of persistent disks.</li> <li>Provides multi-read access for specific disk types.</li> <li>Example Usage:   <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: gcp-pd-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 10Gi\n  storageClassName: standard\n</code></pre></li> <li>Use Case:   Applications requiring persistent block storage on Google Cloud.</li> </ul>"},{"location":"kubernetes/open_standards/csi_components/#3-azure-disk-csi-driver","title":"3. Azure Disk CSI Driver","text":"<ul> <li>Description:   The Azure Disk CSI driver allows Kubernetes clusters to dynamically provision and manage Azure Managed Disks as persistent storage.</li> <li>Key Features:</li> <li>Supports dynamic provisioning, resizing, and snapshotting of Azure Disks.</li> <li>Integrates seamlessly with Azure Kubernetes Service (AKS).</li> <li>Example Usage:   <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: azure-disk-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 8Gi\n  storageClassName: managed-premium\n</code></pre></li> <li>Use Case:   Applications running on Kubernetes clusters deployed in Azure.</li> </ul>"},{"location":"kubernetes/open_standards/csi_components/#4-ceph-rbd-csi-driver","title":"4. Ceph RBD CSI Driver","text":"<ul> <li>Description:   Ceph RBD (RADOS Block Device) CSI driver integrates Ceph block storage with Kubernetes. It provides scalable, distributed block storage for Kubernetes clusters.</li> <li>Key Features:</li> <li>Supports dynamic provisioning and resizing of block storage.</li> <li>Integrates with on-premises and hybrid Ceph clusters.</li> <li>Example Usage:   <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: ceph-rbd-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 5Gi\n  storageClassName: ceph-rbd\n</code></pre></li> <li>Use Case:   Organizations using Ceph for on-premises distributed storage.</li> </ul>"},{"location":"kubernetes/open_standards/csi_components/#5-portworx-csi-driver","title":"5. Portworx CSI Driver","text":"<ul> <li>Description:   Portworx provides a cloud-native storage solution for Kubernetes that integrates seamlessly using the CSI standard. It supports high availability, snapshots, backups, and multi-cloud capabilities.</li> <li>Key Features:</li> <li>Supports dynamic provisioning, replication, and snapshots.</li> <li>Provides data resilience, encryption, and backup.</li> <li>Example Usage:   <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: portworx-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 10Gi\n  storageClassName: portworx-sc\n</code></pre></li> <li>Use Case:   Kubernetes workloads that require highly available and resilient storage.</li> </ul>"},{"location":"kubernetes/open_standards/csi_components/#comparison-of-csi-drivers","title":"Comparison of CSI Drivers","text":"Storage Driver Cloud Provider Features Best For Amazon EBS CSI Driver AWS Block storage, snapshots, resizing Kubernetes clusters on AWS Google PD CSI Driver Google Cloud Block storage, dynamic provisioning Kubernetes clusters on GCP Azure Disk CSI Driver Azure Managed Disks, dynamic provisioning Kubernetes clusters on Azure Ceph RBD CSI Driver On-Premises/Hybrid Distributed block storage, scalability On-premises or hybrid Kubernetes setups Portworx CSI Driver Multi-Cloud Resilience, replication, snapshots High-availability storage solutions"},{"location":"kubernetes/open_standards/csi_components/#conclusion","title":"Conclusion","text":"<p>The Container Storage Interface (CSI) provides a consistent and extensible way for Kubernetes to interact with different storage systems, both cloud-based and on-premises. Examples like Amazon EBS, Google Persistent Disk, Azure Disk, Ceph RBD, and Portworx demonstrate how CSI enables dynamic provisioning, scalability, and flexibility for Kubernetes workloads. By adhering to CSI standards, Kubernetes ensures that storage solutions are portable and interoperable across environments.</p>"},{"location":"kubernetes/open_standards/open_standards/","title":"Kubernetes \u2014 Open Standards (OCI, CRI, CNI, CSI, SMI, CPI)","text":"<p>Kubernetes embraces open standards to ensure interoperability, portability, and extensibility across platforms, tools, and environments. These standards allow Kubernetes to remain vendor-neutral, modular, and highly extensible, enabling organizations to build, deploy, and manage applications seamlessly in cloud-native ecosystems.</p>"},{"location":"kubernetes/open_standards/open_standards/#1-open-container-initiative-oci","title":"1. Open Container Initiative (OCI)","text":""},{"location":"kubernetes/open_standards/open_standards/#description","title":"Description","text":"<p>The Open Container Initiative (OCI) is a set of open standards for container runtimes, image formats, and distribution. OCI ensures consistent and interoperable container technology, allowing containers to run uniformly across platforms and tools.</p>"},{"location":"kubernetes/open_standards/open_standards/#key-components","title":"Key Components:","text":"<ul> <li> <p>OCI Runtime Specification: <code>runtime-spec</code> specifies the configuration, execution environment, and lifecycle of containers.   This outlines how to run a \u201cfilesystem bundle\u201d that is unpacked on disk. At a high-level, an OCI implementation would download an OCI Image and then unpack that image into an OCI Runtime filesystem bundle.</p> </li> <li> <p>OCI Image Specification: <code>image-spec</code> defines how to build and package container images.   The goal of this specification is to enable the creation of interoperable tools for building, transporting, and preparing a container image to run.</p> </li> <li> <p>OCI Distribution Specification: The <code>Distribution-Spec</code> provides a standard for the distribution of content in general and container images in particular. It is a most recent addition to the OCI project.   Container registries, implementing the distribution-spec, provide reliable, highly scalable, secured storage services for container images.   Customers either use a cloud provider implementation, vendor implementations, or instance the open source implementation of distribution.</p> </li> </ul>"},{"location":"kubernetes/open_standards/open_standards/#why-it-matters","title":"Why It Matters:","text":"<ul> <li>Prevents vendor lock-in for container ecosystems.</li> <li>Ensures container runtime and image portability across environments.</li> </ul>"},{"location":"kubernetes/open_standards/open_standards/#2-container-runtime-interface-cri","title":"2. Container Runtime Interface (CRI)","text":""},{"location":"kubernetes/open_standards/open_standards/#description_1","title":"Description","text":"<p>The Container Runtime Interface (CRI) is a Kubernetes API standard that allows Kubernetes to interact with different container runtimes. It abstracts the runtime layer, enabling flexibility and plug-and-play runtimes.</p>"},{"location":"kubernetes/open_standards/open_standards/#key-features","title":"Key Features:","text":"<ul> <li>Provides a gRPC API between Kubernetes kubelet and container runtimes.</li> <li>Supports various runtimes like containerd, CRI-O, and Docker (via <code>shim</code>).</li> </ul>"},{"location":"kubernetes/open_standards/open_standards/#why-it-matters_1","title":"Why It Matters:","text":"<ul> <li>Decouples Kubernetes from a specific container runtime.</li> <li>Enhances flexibility and choice in runtime solutions.</li> </ul>"},{"location":"kubernetes/open_standards/open_standards/#3-container-network-interface-cni","title":"3. Container Network Interface (CNI)","text":""},{"location":"kubernetes/open_standards/open_standards/#description_2","title":"Description","text":"<p>The Container Network Interface (CNI) standard defines how networking is configured for containers. CNI plugins allow Kubernetes to manage pod networking dynamically and flexibly.</p>"},{"location":"kubernetes/open_standards/open_standards/#key-features_1","title":"Key Features:","text":"<ul> <li>Provides a standard API to configure networking for containers.</li> <li>Supports advanced features like Network Policies for traffic control.</li> </ul>"},{"location":"kubernetes/open_standards/open_standards/#examples","title":"Examples:","text":"<ul> <li>Calico: Network security and policy enforcement.</li> <li>Flannel: Simple overlay network.</li> <li>Weave: Multi-cloud and encrypted pod networking.</li> </ul>"},{"location":"kubernetes/open_standards/open_standards/#why-it-matters_2","title":"Why It Matters:","text":"<ul> <li>Ensures interoperability across different networking plugins.</li> <li>Simplifies the configuration and management of container networking.</li> </ul>"},{"location":"kubernetes/open_standards/open_standards/#4-container-storage-interface-csi","title":"4. Container Storage Interface (CSI)","text":""},{"location":"kubernetes/open_standards/open_standards/#description_3","title":"Description","text":"<p>The Container Storage Interface (CSI) standardizes how storage providers integrate their solutions with Kubernetes. It enables dynamic provisioning and management of storage volumes.</p>"},{"location":"kubernetes/open_standards/open_standards/#key-features_2","title":"Key Features:","text":"<ul> <li>Provides APIs for creating, attaching, and mounting storage volumes.</li> <li>Works with both on-premises and cloud storage providers.</li> </ul>"},{"location":"kubernetes/open_standards/open_standards/#examples_1","title":"Examples:","text":"<ul> <li>Amazon EBS: Elastic Block Store.</li> <li>Google Persistent Disk: Cloud-native block storage.</li> <li>Ceph: Open-source storage solution.</li> </ul>"},{"location":"kubernetes/open_standards/open_standards/#why-it-matters_3","title":"Why It Matters:","text":"<ul> <li>Decouples Kubernetes from specific storage implementations.</li> <li>Enables storage portability and dynamic provisioning.</li> </ul>"},{"location":"kubernetes/open_standards/open_standards/#5-service-mesh-interface-smi","title":"5. Service Mesh Interface (SMI)","text":""},{"location":"kubernetes/open_standards/open_standards/#description_4","title":"Description","text":"<p>The Service Mesh Interface (SMI) is an open standard for service mesh interoperability in Kubernetes. It provides a set of common APIs for traffic management, security, and observability.</p>"},{"location":"kubernetes/open_standards/open_standards/#key-features_3","title":"Key Features:","text":"<ul> <li>Traffic Policies: Route, split, and retry traffic between services.</li> <li>Observability: Collect metrics, logs, and traces for service communication.</li> <li>Security: Implements mutual TLS (mTLS) for secure inter-service communication.</li> </ul>"},{"location":"kubernetes/open_standards/open_standards/#examples_2","title":"Examples:","text":"<ul> <li>Istio: Feature-rich service mesh for Kubernetes.</li> <li>Linkerd: Lightweight and simple service mesh.</li> <li>Consul: Service discovery and mesh.</li> </ul>"},{"location":"kubernetes/open_standards/open_standards/#why-it-matters_4","title":"Why It Matters:","text":"<ul> <li>Provides a unified API for service mesh implementations.</li> <li>Simplifies the adoption and management of service mesh tools.</li> </ul>"},{"location":"kubernetes/open_standards/open_standards/#6-cloud-provider-interface-cpi","title":"6. Cloud Provider Interface (CPI)","text":""},{"location":"kubernetes/open_standards/open_standards/#description_5","title":"Description","text":"<p>The Cloud Provider Interface (CPI) standardizes the integration of Kubernetes with cloud providers, enabling Kubernetes to manage cloud-specific resources like storage, load balancers, and nodes.</p>"},{"location":"kubernetes/open_standards/open_standards/#key-features_4","title":"Key Features:","text":"<ul> <li>Provides APIs for cloud infrastructure provisioning.</li> <li>Supports operations like load balancer setup, persistent volume management, and scaling.</li> </ul>"},{"location":"kubernetes/open_standards/open_standards/#examples_3","title":"Examples:","text":"<ul> <li>AWS Cloud Controller Manager: Manages AWS resources.</li> <li>Azure Cloud Controller Manager: Integrates Kubernetes with Azure.</li> <li>GCP Cloud Controller Manager: Supports Google Cloud resources.</li> </ul>"},{"location":"kubernetes/open_standards/open_standards/#why-it-matters_5","title":"Why It Matters:","text":"<ul> <li>Enables Kubernetes to operate seamlessly across multiple cloud providers.</li> <li>Ensures abstraction of cloud-specific infrastructure.</li> </ul>"},{"location":"kubernetes/open_standards/open_standards/#conclusion","title":"Conclusion","text":"<p>Kubernetes relies on open standards like OCI, CRI, CNI, CSI, SMI, and CPI to remain modular, extensible, and vendor-neutral. These standards ensure that Kubernetes can integrate with diverse runtime, networking, storage, and service mesh solutions while offering consistent behavior and flexibility across cloud-native environments. By embracing these standards, Kubernetes empowers organizations to build and scale resilient, portable, and future-proof applications.</p>"},{"location":"kubernetes/open_standards/SMI/description/","title":"Service Mesh Interface (SMI)","text":"<p>The Service Mesh Interface (SMI) is an open standard for managing service-to-service communication in Kubernetes. It provides a consistent and portable way to integrate service meshes like Linkerd, Istio, and Consul Connect into Kubernetes clusters. SMI standardizes APIs for traffic management, observability, and security.</p> <p>Below is an example demonstrating SMI Traffic Split, one of the core SMI capabilities.</p>"},{"location":"kubernetes/open_standards/SMI/description/#traffic-split-example","title":"Traffic Split Example","text":""},{"location":"kubernetes/open_standards/SMI/description/#use-case","title":"Use Case:","text":"<p>A gradual rollout (canary release) of a new version of a microservice while splitting traffic between two versions.</p>"},{"location":"kubernetes/open_standards/SMI/description/#1-prerequisites","title":"1. Prerequisites:","text":"<ul> <li>A Kubernetes cluster with a service mesh like Linkerd or Istio installed.</li> <li>Two deployments of the same service:</li> <li><code>v1</code> for the current stable version.</li> <li><code>v2</code> for the new version being tested.</li> </ul>"},{"location":"kubernetes/open_standards/SMI/description/#2-deployments-and-services","title":"2. Deployments and Services","text":""},{"location":"kubernetes/open_standards/SMI/description/#deployment-for-v1","title":"Deployment for v1:","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: backend-v1\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: backend\n      version: v1\n  template:\n    metadata:\n      labels:\n        app: backend\n        version: v1\n    spec:\n      containers:\n        - name: backend\n          image: my-backend:v1\n          ports:\n            - containerPort: 80\n</code></pre>"},{"location":"kubernetes/open_standards/SMI/description/#deployment-for-v2","title":"Deployment for v2:","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: backend-v2\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: backend\n      version: v2\n  template:\n    metadata:\n      labels:\n        app: backend\n        version: v2\n    spec:\n      containers:\n        - name: backend\n          image: my-backend:v2\n          ports:\n            - containerPort: 80\n</code></pre>"},{"location":"kubernetes/open_standards/SMI/description/#service-definition","title":"Service Definition:","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: backend-service\nspec:\n  selector:\n    app: backend\n  ports:\n    - port: 80\n      targetPort: 80\n</code></pre>"},{"location":"kubernetes/open_standards/SMI/description/#3-smi-traffic-split","title":"3. SMI Traffic Split","text":"<p>The following Traffic Split definition directs 90% of the traffic to version <code>v1</code> of the backend and 10% of the traffic to version <code>v2</code>. As confidence in <code>v2</code> grows, the weights can be adjusted gradually.</p> <pre><code>apiVersion: split.smi-spec.io/v1alpha2\nkind: TrafficSplit\nmetadata:\n  name: backend-traffic-split\nspec:\n  service: backend-service\n  backends:\n    - service: backend-v1\n      weight: 90\n    - service: backend-v2\n      weight: 10\n</code></pre>"},{"location":"kubernetes/open_standards/SMI/description/#explanation","title":"Explanation:","text":"<ul> <li>service: Refers to the Kubernetes service (<code>backend-service</code>) that acts as the main entry point.</li> <li>backends: Defines the traffic distribution between the two versions (<code>backend-v1</code> and <code>backend-v2</code>) using weights.</li> </ul>"},{"location":"kubernetes/open_standards/SMI/description/#4-observability","title":"4. Observability","text":"<p>With SMI observability tools integrated into your service mesh (like Linkerd), you can monitor:</p> <ul> <li>Traffic metrics between <code>v1</code> and <code>v2</code>.</li> <li>Response times and error rates.</li> </ul> <p>Example using Linkerd CLI:</p> <pre><code>linkerd stat traffic-split backend-traffic-split\n</code></pre>"},{"location":"kubernetes/open_standards/SMI/description/#conclusion","title":"Conclusion","text":"<p>The SMI Traffic Split API provides a standardized way to manage traffic between different versions of a service. It simplifies gradual rollouts, canary releases, and A/B testing in Kubernetes clusters, ensuring smooth service mesh interoperability regardless of the underlying implementation (Linkerd, Istio, Consul Connect).</p>"},{"location":"kubernetes/open_standards/SMI/istio/","title":"Istio: A Service Mesh for Kubernetes","text":"<p>Istio is an open-source service mesh that provides a way to control and secure service-to-service communication in modern application architectures, such as microservices. It adds observability, traffic management, and security features to applications without requiring changes to the application code.</p>"},{"location":"kubernetes/open_standards/SMI/istio/#key-features-of-istio","title":"Key Features of Istio","text":"<ol> <li> <p>Traffic Management:</p> </li> <li> <p>Provides fine-grained control over traffic routing and load balancing.</p> </li> <li> <p>Supports blue-green and canary deployments.</p> </li> <li> <p>Security:</p> </li> <li> <p>Implements strong identity-based authentication and authorization using mutual TLS (mTLS).</p> </li> <li> <p>Encrypts service-to-service communication.</p> </li> <li> <p>Observability:</p> </li> <li> <p>Offers telemetry, distributed tracing, and monitoring for all services in the mesh.</p> </li> <li> <p>Fault Tolerance:</p> </li> <li>Provides retries, timeouts, and circuit breakers to make applications more resilient.</li> </ol>"},{"location":"kubernetes/open_standards/SMI/istio/#istio-components","title":"Istio Components","text":""},{"location":"kubernetes/open_standards/SMI/istio/#1-envoy-proxy","title":"1. Envoy Proxy","text":"<ul> <li>Description: A lightweight proxy deployed as a sidecar alongside each service.</li> <li>Responsibilities:</li> <li>Intercepts and manages all inbound and outbound traffic for the service.</li> <li>Handles traffic routing, telemetry, and enforcing security policies.</li> <li>Key Features:</li> <li>Protocol support for HTTP, gRPC, WebSocket, and TCP.</li> <li>Built-in observability and traffic control.</li> </ul>"},{"location":"kubernetes/open_standards/SMI/istio/#2-istiod-control-plane","title":"2. Istiod (Control Plane)","text":"<ul> <li>Description: The central control plane component that manages the service mesh configuration.</li> <li>Responsibilities:</li> <li>Configures and manages the Envoy proxies.</li> <li>Maintains the service registry and tracks service discovery.</li> <li>Manages authentication, authorization, and telemetry configuration.</li> <li>Key Features:</li> <li>Centralized control for all traffic and security policies.</li> </ul>"},{"location":"kubernetes/open_standards/SMI/istio/#3-gateway","title":"3. Gateway","text":"<ul> <li>Description: Manages ingress and egress traffic for services inside the mesh.</li> <li>Ingress Gateway:</li> <li>Routes external traffic into the mesh.</li> <li>Acts as a reverse proxy for services in the mesh.</li> <li>Egress Gateway:</li> <li>Routes traffic from services in the mesh to external services.</li> <li>Provides control over outbound traffic policies.</li> </ul>"},{"location":"kubernetes/open_standards/SMI/istio/#4-pilot","title":"4. Pilot","text":"<ul> <li>Description: A component of the control plane that provides service discovery and traffic management.</li> <li>Responsibilities:</li> <li>Configures Envoy proxies for routing, retries, and load balancing.</li> <li>Supports advanced traffic management strategies like canary and blue-green deployments.</li> </ul>"},{"location":"kubernetes/open_standards/SMI/istio/#5-mixer-deprecated-in-istio-15","title":"5. Mixer (Deprecated in Istio 1.5+)","text":"<ul> <li>Description: Previously responsible for telemetry and policy enforcement.</li> <li>Replacement: Functionality moved to Envoy and Istiod for better performance and integration.</li> </ul>"},{"location":"kubernetes/open_standards/SMI/istio/#6-citadel","title":"6. Citadel","text":"<ul> <li>Description: Manages service identities and certificates in the mesh.</li> <li>Responsibilities:</li> <li>Issues and rotates certificates for secure mTLS communication.</li> <li>Ensures secure service-to-service authentication.</li> </ul>"},{"location":"kubernetes/open_standards/SMI/istio/#7-telemetry","title":"7. Telemetry","text":"<ul> <li>Description: Collects metrics, logs, and traces for services in the mesh.</li> <li>Responsibilities:</li> <li>Enables observability through integration with tools like Prometheus, Grafana, and Jaeger.</li> <li>Provides detailed insights into service behavior and performance.</li> </ul>"},{"location":"kubernetes/open_standards/SMI/istio/#how-istio-works","title":"How Istio Works","text":"<ol> <li> <p>Traffic Interception:</p> </li> <li> <p>Envoy sidecars intercept all traffic between services and apply traffic management and security policies.</p> </li> <li> <p>Control Plane Management:</p> </li> <li> <p>Istiod configures Envoy proxies based on the desired state defined by operators.</p> </li> <li> <p>Telemetry Collection:</p> </li> <li> <p>Envoy collects metrics and traces, sending them to monitoring systems like Prometheus or Jaeger.</p> </li> <li> <p>Authentication and Authorization:</p> </li> <li>Citadel and Envoy enforce mTLS and role-based access control (RBAC) policies.</li> </ol>"},{"location":"kubernetes/open_standards/SMI/istio/#conclusion","title":"Conclusion","text":"<p>Istio is a powerful service mesh that simplifies traffic management, enhances observability, and strengthens security for microservices. By abstracting complex networking tasks and automating policies, Istio enables developers to focus on building applications while ensuring reliability and security across the entire system.</p>"},{"location":"kubernetes/open_standards/SMI/linkerd/","title":"Linkerd: An Overview","text":"<p>Linkerd is an open-source service mesh for Kubernetes and other containerized environments. It provides a lightweight, secure, and reliable platform for managing communication between microservices in a distributed system.</p>"},{"location":"kubernetes/open_standards/SMI/linkerd/#key-features-of-linkerd","title":"Key Features of Linkerd","text":"<ol> <li> <p>Traffic Management:</p> </li> <li> <p>Handles routing, load balancing, retries, and failovers.</p> </li> <li> <p>Ensures reliable communication between microservices.</p> </li> <li> <p>Security:</p> </li> <li> <p>Provides mutual TLS (mTLS) for encrypting service-to-service communication.</p> </li> <li> <p>Automates certificate management and rotation.</p> </li> <li> <p>Observability:</p> </li> <li> <p>Offers fine-grained telemetry, including metrics, logs, and distributed tracing.</p> </li> <li> <p>Integrates with tools like Prometheus and Grafana for visualization.</p> </li> <li> <p>Lightweight Design:</p> </li> <li> <p>Designed to be minimal and performant, with a focus on operational simplicity.</p> </li> <li> <p>Uses a sidecar proxy model but maintains a small resource footprint compared to other service meshes.</p> </li> <li> <p>Kubernetes-Native:</p> </li> <li>Integrates seamlessly with Kubernetes, using native constructs like Custom Resource Definitions (CRDs).</li> <li>Automatically injects sidecars into Pods for service mesh functionality.</li> </ol>"},{"location":"kubernetes/open_standards/SMI/linkerd/#how-linkerd-works","title":"How Linkerd Works","text":"<ol> <li> <p>Sidecar Proxy:</p> </li> <li> <p>A lightweight proxy is injected as a sidecar container alongside application containers in each Pod.</p> </li> <li> <p>The proxy intercepts and manages all inbound and outbound traffic for the application.</p> </li> <li> <p>Control Plane:</p> </li> <li> <p>Manages the configuration, policy enforcement, and telemetry collection for the mesh.</p> </li> <li> <p>Components include:</p> <ul> <li>Proxy Injector: Injects the Linkerd sidecar proxy into Pods.</li> <li>Destination Controller: Manages service discovery and routing.</li> <li>Identity Service: Issues and validates mTLS certificates.</li> </ul> </li> <li> <p>Data Plane:</p> </li> <li>Comprises the sidecar proxies that handle the actual service-to-service traffic.</li> </ol>"},{"location":"kubernetes/open_standards/SMI/linkerd/#benefits-of-linkerd","title":"Benefits of Linkerd","text":"<ol> <li> <p>Improved Reliability:</p> </li> <li> <p>Automatically retries failed requests and implements failover mechanisms.</p> </li> <li> <p>Enhanced Security:</p> </li> <li> <p>Ensures all traffic between services is encrypted and authenticated using mTLS.</p> </li> <li> <p>Better Observability:</p> </li> <li> <p>Provides detailed metrics such as request success rates, latencies, and throughput.</p> </li> <li> <p>Simplicity:</p> </li> <li> <p>Easy to install and operate, with minimal configuration compared to other service meshes.</p> </li> <li> <p>Resource Efficiency:</p> </li> <li>Lightweight and performant, making it suitable for resource-constrained environments.</li> </ol>"},{"location":"kubernetes/open_standards/SMI/linkerd/#use-cases-for-linkerd","title":"Use Cases for Linkerd","text":"<ol> <li> <p>Microservices Observability:</p> </li> <li> <p>Gain visibility into service communication, performance, and failures.</p> </li> <li> <p>Zero-Trust Security:</p> </li> <li> <p>Encrypt all service-to-service communication and enforce strict authentication.</p> </li> <li> <p>Traffic Control:</p> </li> <li> <p>Implement fine-grained routing, retries, and failovers for resilient applications.</p> </li> <li> <p>Kubernetes-Native Applications:</p> </li> <li>Manage communication between microservices running in a Kubernetes cluster.</li> </ol>"},{"location":"kubernetes/open_standards/SMI/linkerd/#comparison-linkerd-vs-istio","title":"Comparison: Linkerd vs. Istio","text":"Feature Linkerd Istio Complexity Simple and lightweight Feature-rich but more complex Performance High, with minimal resource usage Moderate, requires more resources Ease of Use Quick setup and minimal configuration Requires extensive configuration Observability Focuses on metrics and simplicity Advanced telemetry and tracing Security Built-in mTLS Built-in mTLS and more policies"},{"location":"kubernetes/open_standards/SMI/linkerd/#installation-example","title":"Installation Example","text":"<p>Install Linkerd using the CLI:</p> <ol> <li>Install the CLI:</li> </ol> <pre><code>curl -sL https://run.linkerd.io/install | sh\nexport PATH=$PATH:$HOME/.linkerd2/bin\n</code></pre> <ol> <li>Validate the Cluster:</li> </ol> <pre><code>linkerd check --pre\n</code></pre> <ol> <li>Install Linkerd:</li> </ol> <pre><code>linkerd install | kubectl apply -f -\n</code></pre> <ol> <li>Inject Sidecars:    Inject Linkerd into your application Pods:</li> </ol> <pre><code>kubectl get deploy -o yaml | linkerd inject - | kubectl apply -f -\n</code></pre> <ol> <li>Access the Dashboard:    Launch the Linkerd dashboard to monitor your services:    <pre><code>linkerd dashboard\n</code></pre></li> </ol>"},{"location":"kubernetes/open_standards/SMI/linkerd/#conclusion","title":"Conclusion","text":"<p>Linkerd is a lightweight and Kubernetes-native service mesh that simplifies the management of service-to-service communication. Its focus on simplicity, security, and observability makes it an excellent choice for organizations looking to enhance their microservices architecture with minimal overhead.</p>"},{"location":"kubernetes/open_standards/SMI/what_is_a_service_mesh/","title":"What is a Service Mesh?","text":"<p>A service mesh is a dedicated infrastructure layer designed to manage service-to-service communication in modern, distributed application architectures such as microservices. It provides features like traffic management, service discovery, security, observability, and resilience without requiring changes to the application code.</p>"},{"location":"kubernetes/open_standards/SMI/what_is_a_service_mesh/#key-features-of-a-service-mesh","title":"Key Features of a Service Mesh","text":"<ol> <li> <p>Traffic Management:</p> </li> <li> <p>Provides fine-grained control over traffic between services, including load balancing, traffic shaping, retries, and timeouts.</p> </li> <li> <p>Service Discovery:</p> </li> <li> <p>Automatically detects and tracks service instances to ensure efficient routing of requests.</p> </li> <li> <p>Security:</p> </li> <li> <p>Enables secure communication between services using mutual TLS (mTLS) for authentication and encryption.</p> </li> <li> <p>Observability:</p> </li> <li> <p>Provides metrics, logs, and distributed tracing for visibility into service interactions and performance.</p> </li> <li> <p>Resilience:</p> </li> <li>Implements circuit breaking, rate limiting, and fault injection to improve system reliability.</li> </ol>"},{"location":"kubernetes/open_standards/SMI/what_is_a_service_mesh/#how-a-service-mesh-works","title":"How a Service Mesh Works","text":"<p>A service mesh typically uses a data plane and a control plane:</p> <ol> <li> <p>Data Plane:</p> </li> <li> <p>Composed of lightweight proxies (e.g., Envoy) deployed alongside application services (as sidecars) to handle service-to-service communication.</p> </li> <li> <p>Control Plane:</p> </li> <li>Centralized management layer that configures and monitors the proxies, enforcing policies and collecting telemetry data.</li> </ol>"},{"location":"kubernetes/open_standards/SMI/what_is_a_service_mesh/#benefits-of-using-a-service-mesh","title":"Benefits of Using a Service Mesh","text":"<ol> <li> <p>Simplifies Microservices Management:</p> </li> <li> <p>Decouples service communication logic from application code.</p> </li> <li> <p>Enhances Security:</p> </li> <li> <p>Automates encryption and authentication between services.</p> </li> <li> <p>Improves Reliability:</p> </li> <li> <p>Provides advanced traffic control and error-handling mechanisms.</p> </li> <li> <p>Increases Observability:</p> </li> <li>Offers deep insights into inter-service communication with metrics and tracing.</li> </ol>"},{"location":"kubernetes/open_standards/SMI/what_is_a_service_mesh/#challenges-of-a-service-mesh","title":"Challenges of a Service Mesh","text":"<ol> <li> <p>Complexity:</p> </li> <li> <p>Adds operational overhead and learning curve for implementation and management.</p> </li> <li> <p>Performance Overhead:</p> </li> <li> <p>Proxy sidecars introduce additional latency and resource consumption.</p> </li> <li> <p>Cost:</p> </li> <li>Higher infrastructure and operational costs due to additional components.</li> </ol>"},{"location":"kubernetes/open_standards/SMI/what_is_a_service_mesh/#popular-service-mesh-solutions","title":"Popular Service Mesh Solutions","text":"<ol> <li> <p>Istio:</p> </li> <li> <p>A feature-rich and widely adopted service mesh offering advanced traffic management, mTLS, and observability.</p> </li> <li> <p>Linkerd:</p> </li> <li> <p>A lightweight, simpler alternative to Istio, focusing on ease of use and minimal resource usage.</p> </li> <li> <p>Consul:</p> </li> <li> <p>A service mesh integrated with Consul\u2019s service discovery and configuration management capabilities.</p> </li> <li> <p>AWS App Mesh:</p> </li> <li>A cloud-native service mesh for managing microservices on AWS.</li> </ol>"},{"location":"kubernetes/open_standards/SMI/what_is_a_service_mesh/#when-to-use-a-service-mesh","title":"When to Use a Service Mesh","text":"<ul> <li>Large-scale microservices architectures requiring secure, reliable communication.</li> <li>Applications needing advanced observability and traffic control.</li> <li>Scenarios where managing service communication in application code becomes unmanageable.</li> </ul>"},{"location":"kubernetes/open_standards/SMI/what_is_a_service_mesh/#when-not-to-use-a-service-mesh","title":"When Not to Use a Service Mesh","text":"<ul> <li>Small-scale applications or monoliths with limited service communication.</li> <li>Environments where the added complexity outweighs the benefits.</li> </ul>"},{"location":"linux/network/bridge-bonding/","title":"Configure Bridge and Bonding Devices","text":""},{"location":"linux/network/bridge-bonding/#overview","title":"Overview","text":"<p>This guide covers network bridge configuration (for virtualization and container networking) and network bonding/teaming (for redundancy and load balancing) in Linux.</p>"},{"location":"linux/network/bridge-bonding/#network-bridges","title":"Network Bridges","text":""},{"location":"linux/network/bridge-bonding/#bridge-concepts","title":"Bridge Concepts","text":"<p>A network bridge connects two or more network segments, operating at Layer 2 (Data Link layer). Common uses: - Virtual machine networking - Container networking (Docker, LXC) - Connecting physical and virtual networks - Network segmentation</p> <p>Key Points: - Bridges forward traffic based on MAC addresses - All bridge members share the same broadcast domain - Bridge itself can have an IP address - Used extensively in virtualization (KVM, VirtualBox, etc.)</p>"},{"location":"linux/network/bridge-bonding/#creating-network-bridges","title":"Creating Network Bridges","text":""},{"location":"linux/network/bridge-bonding/#method-1-using-ip-command-temporary","title":"Method 1: Using <code>ip</code> Command (Temporary)","text":"<pre><code># Create bridge interface\nip link add name br0 type bridge\n\n# Bring bridge up\nip link set br0 up\n\n# Add interfaces to bridge\nip link set eth0 master br0\nip link set eth1 master br0\n\n# Assign IP to bridge\nip addr add 192.168.1.100/24 dev br0\n\n# View bridge\nip link show master br0    # Show bridge members\nbridge link show           # Show bridge ports\n\n# Remove interface from bridge\nip link set eth0 nomaster\n\n# Delete bridge\nip link set br0 down\nip link delete br0\n</code></pre>"},{"location":"linux/network/bridge-bonding/#method-2-using-brctl-command-legacy","title":"Method 2: Using <code>brctl</code> Command (Legacy)","text":"<pre><code># Install bridge-utils\ndnf install bridge-utils\n\n# Create bridge\nbrctl addbr br0\n\n# Add interfaces to bridge\nbrctl addif br0 eth0\nbrctl addif br0 eth1\n\n# Remove interface\nbrctl delif br0 eth0\n\n# Show bridge information\nbrctl show\nbrctl showmacs br0      # Show MAC addresses\nbrctl showstp br0       # Show STP info\n\n# Delete bridge\nbrctl delbr br0\n</code></pre>"},{"location":"linux/network/bridge-bonding/#method-3-networkmanager-persistent","title":"Method 3: NetworkManager (Persistent)","text":"<pre><code># Create bridge connection\nnmcli connection add type bridge \\\n    con-name br0 \\\n    ifname br0\n\n# Configure bridge IP\nnmcli connection modify br0 \\\n    ipv4.addresses 192.168.1.100/24 \\\n    ipv4.gateway 192.168.1.1 \\\n    ipv4.dns 8.8.8.8 \\\n    ipv4.method manual\n\n# Add slave interfaces\nnmcli connection add type bridge-slave \\\n    con-name br0-slave-eth0 \\\n    ifname eth0 \\\n    master br0\n\nnmcli connection add type bridge-slave \\\n    con-name br0-slave-eth1 \\\n    ifname eth1 \\\n    master br0\n\n# Activate bridge\nnmcli connection up br0\nnmcli connection up br0-slave-eth0\nnmcli connection up br0-slave-eth1\n\n# View configuration\nnmcli connection show br0\nnmcli device status\n\n# Modify bridge properties\nnmcli connection modify br0 bridge.stp yes\nnmcli connection modify br0 bridge.priority 32768\nnmcli connection modify br0 bridge.forward-delay 15\nnmcli connection modify br0 bridge.hello-time 2\nnmcli connection modify br0 bridge.max-age 20\n\n# Delete bridge\nnmcli connection delete br0\nnmcli connection delete br0-slave-eth0\n</code></pre>"},{"location":"linux/network/bridge-bonding/#method-4-configuration-files-rhelcentos","title":"Method 4: Configuration Files (RHEL/CentOS)","text":"<p>Bridge interface: <code>/etc/sysconfig/network-scripts/ifcfg-br0</code> <pre><code>DEVICE=br0\nTYPE=Bridge\nBOOTPROTO=none\nONBOOT=yes\nIPADDR=192.168.1.100\nPREFIX=24\nGATEWAY=192.168.1.1\nDNS1=8.8.8.8\nSTP=yes\nDELAY=0\n</code></pre></p> <p>Bridge member: <code>/etc/sysconfig/network-scripts/ifcfg-eth0</code> <pre><code>DEVICE=eth0\nTYPE=Ethernet\nBOOTPROTO=none\nONBOOT=yes\nBRIDGE=br0\n</code></pre></p> <p>Restart network: <pre><code>systemctl restart NetworkManager\n# or\nnmcli connection reload\nnmcli connection up br0\n</code></pre></p>"},{"location":"linux/network/bridge-bonding/#method-5-netplan-ubuntudebian","title":"Method 5: Netplan (Ubuntu/Debian)","text":"<p>Edit: <code>/etc/netplan/01-netcfg.yaml</code></p> <pre><code>network:\n  version: 2\n  renderer: networkd\n  ethernets:\n    eth0:\n      dhcp4: no\n    eth1:\n      dhcp4: no\n  bridges:\n    br0:\n      interfaces:\n        - eth0\n        - eth1\n      dhcp4: no\n      addresses:\n        - 192.168.1.100/24\n      gateway4: 192.168.1.1\n      nameservers:\n        addresses:\n          - 8.8.8.8\n          - 8.8.4.4\n      parameters:\n        stp: true\n        forward-delay: 4\n</code></pre> <p>Apply: <pre><code>netplan try\nnetplan apply\n</code></pre></p>"},{"location":"linux/network/bridge-bonding/#bridge-management-and-monitoring","title":"Bridge Management and Monitoring","text":""},{"location":"linux/network/bridge-bonding/#view-bridge-information","title":"View Bridge Information","text":"<pre><code># Using ip command\nip link show type bridge\nbridge link show\n\n# Show FDB (forwarding database)\nbridge fdb show\nbridge fdb show br br0\n\n# Show VLAN info\nbridge vlan show\n\n# Using brctl\nbrctl show\nbrctl showmacs br0\nbrctl showstp br0\n</code></pre>"},{"location":"linux/network/bridge-bonding/#bridge-parameters","title":"Bridge Parameters","text":""},{"location":"linux/network/bridge-bonding/#spanning-tree-protocol-stp","title":"Spanning Tree Protocol (STP)","text":"<pre><code># Enable/disable STP (prevents loops)\nip link set br0 type bridge stp_state 1  # Enable\nip link set br0 type bridge stp_state 0  # Disable\n\n# Using brctl\nbrctl stp br0 on\nbrctl stp br0 off\n\n# Set bridge priority (lower = higher priority)\nip link set br0 type bridge priority 32768\nbrctl setbridgeprio br0 32768\n\n# Set forward delay\nip link set br0 type bridge forward_delay 1500  # 15 seconds\nbrctl setfd br0 15\n\n# Set hello time\nip link set br0 type bridge hello_time 200  # 2 seconds\nbrctl sethello br0 2\n\n# Set max age\nip link set br0 type bridge max_age 2000  # 20 seconds\nbrctl setmaxage br0 20\n\n# View STP info\nbridge -d link show\n</code></pre>"},{"location":"linux/network/bridge-bonding/#port-parameters","title":"Port Parameters","text":"<pre><code># Set port priority\nbridge link set dev eth0 priority 10\n\n# Set port path cost\nbridge link set dev eth0 cost 100\n\n# Disable learning on port\nbridge link set dev eth0 learning off\n\n# Disable flooding on port\nbridge link set dev eth0 flood off\n</code></pre>"},{"location":"linux/network/bridge-bonding/#network-bonding-link-aggregation","title":"Network Bonding (Link Aggregation)","text":""},{"location":"linux/network/bridge-bonding/#bonding-concepts","title":"Bonding Concepts","text":"<p>Network bonding (also called NIC teaming) combines multiple network interfaces into a single logical interface for: - Redundancy: Failover if one link fails - Load balancing: Distribute traffic across links - Increased bandwidth: Aggregate throughput (mode-dependent)</p>"},{"location":"linux/network/bridge-bonding/#bonding-modes","title":"Bonding Modes","text":"<p>Mode 0 (balance-rr): Round-robin load balancing - Packets transmitted sequentially on each slave - Provides load balancing and fault tolerance - Requires switch support (EtherChannel/LACP)</p> <p>Mode 1 (active-backup): Active-backup - One slave active, others standby - Provides fault tolerance only - No switch configuration needed - Most compatible</p> <p>Mode 2 (balance-xor): XOR load balancing - Traffic distributed by source/destination MAC/IP - Provides load balancing and fault tolerance - May require switch configuration</p> <p>Mode 3 (broadcast): Broadcast - All traffic transmitted on all slaves - Provides fault tolerance - Rarely used</p> <p>Mode 4 (802.3ad): IEEE 802.3ad LACP - Dynamic link aggregation - Requires switch support for LACP - Provides load balancing and fault tolerance - Recommended for most use cases</p> <p>Mode 5 (balance-tlb): Adaptive transmit load balancing - Outgoing traffic balanced, incoming on one interface - No switch configuration needed - Provides load balancing and fault tolerance</p> <p>Mode 6 (balance-alb): Adaptive load balancing - Both TX and RX load balancing - No switch configuration needed - Requires ethtool support</p>"},{"location":"linux/network/bridge-bonding/#creating-network-bonds","title":"Creating Network Bonds","text":""},{"location":"linux/network/bridge-bonding/#method-1-using-ip-command-temporary_1","title":"Method 1: Using <code>ip</code> Command (Temporary)","text":"<pre><code># Load bonding module\nmodprobe bonding\n\n# Create bond interface\nip link add bond0 type bond mode active-backup\n\n# Set bonding mode (if not set during creation)\nip link set bond0 type bond mode 802.3ad\n\n# Add slaves\nip link set eth0 master bond0\nip link set eth1 master bond0\n\n# Bring interfaces up\nip link set eth0 up\nip link set eth1 up\nip link set bond0 up\n\n# Assign IP\nip addr add 192.168.1.100/24 dev bond0\n\n# View bond info\ncat /proc/net/bonding/bond0\nip link show bond0\n\n# Remove slave\nip link set eth0 nomaster\n\n# Delete bond\nip link set bond0 down\nip link delete bond0\n</code></pre>"},{"location":"linux/network/bridge-bonding/#method-2-networkmanager-persistent","title":"Method 2: NetworkManager (Persistent)","text":"<pre><code># Create bond connection\nnmcli connection add type bond \\\n    con-name bond0 \\\n    ifname bond0 \\\n    mode active-backup\n\n# Alternative modes:\n# mode balance-rr (0)\n# mode active-backup (1)\n# mode balance-xor (2)\n# mode broadcast (3)\n# mode 802.3ad (4)\n# mode balance-tlb (5)\n# mode balance-alb (6)\n\n# Configure IP\nnmcli connection modify bond0 \\\n    ipv4.addresses 192.168.1.100/24 \\\n    ipv4.gateway 192.168.1.1 \\\n    ipv4.dns 8.8.8.8 \\\n    ipv4.method manual\n\n# Add slave interfaces\nnmcli connection add type ethernet \\\n    con-name bond0-slave-eth0 \\\n    ifname eth0 \\\n    master bond0\n\nnmcli connection add type ethernet \\\n    con-name bond0-slave-eth1 \\\n    ifname eth1 \\\n    master bond0\n\n# Activate\nnmcli connection up bond0\nnmcli connection up bond0-slave-eth0\nnmcli connection up bond0-slave-eth1\n\n# View configuration\nnmcli connection show bond0\nnmcli device status\n\n# Modify bonding parameters\nnmcli connection modify bond0 \\\n    bond.options \"mode=802.3ad,miimon=100,lacp_rate=fast\"\n\n# Common bonding options:\n# miimon=100               # Link monitoring interval (ms)\n# downdelay=200            # Delay before disabling slave\n# updelay=200              # Delay before enabling slave\n# lacp_rate=fast           # LACP rate (slow/fast)\n# xmit_hash_policy=layer3+4  # Hash policy for mode 2/4\n# primary=eth0             # Primary interface for mode 1\n# arp_interval=250         # ARP monitoring interval\n# arp_ip_target=192.168.1.1  # ARP target IP\n\n# Delete bond\nnmcli connection delete bond0\nnmcli connection delete bond0-slave-eth0\nnmcli connection delete bond0-slave-eth1\n</code></pre>"},{"location":"linux/network/bridge-bonding/#method-3-configuration-files-rhelcentos","title":"Method 3: Configuration Files (RHEL/CentOS)","text":"<p>Bond interface: <code>/etc/sysconfig/network-scripts/ifcfg-bond0</code> <pre><code>DEVICE=bond0\nTYPE=Bond\nBONDING_MASTER=yes\nBOOTPROTO=none\nONBOOT=yes\nIPADDR=192.168.1.100\nPREFIX=24\nGATEWAY=192.168.1.1\nDNS1=8.8.8.8\nBONDING_OPTS=\"mode=active-backup miimon=100\"\n</code></pre></p> <p>Slave interface: <code>/etc/sysconfig/network-scripts/ifcfg-eth0</code> <pre><code>DEVICE=eth0\nTYPE=Ethernet\nBOOTPROTO=none\nONBOOT=yes\nMASTER=bond0\nSLAVE=yes\n</code></pre></p> <p>Slave interface: <code>/etc/sysconfig/network-scripts/ifcfg-eth1</code> <pre><code>DEVICE=eth1\nTYPE=Ethernet\nBOOTPROTO=none\nONBOOT=yes\nMASTER=bond0\nSLAVE=yes\n</code></pre></p> <p>Load bonding module: <code>/etc/modprobe.d/bonding.conf</code> <pre><code>alias bond0 bonding\noptions bonding mode=1 miimon=100\n</code></pre></p> <p>Restart network: <pre><code>systemctl restart NetworkManager\n</code></pre></p>"},{"location":"linux/network/bridge-bonding/#method-4-netplan-ubuntudebian","title":"Method 4: Netplan (Ubuntu/Debian)","text":"<p>Edit: <code>/etc/netplan/01-netcfg.yaml</code></p> <pre><code>network:\n  version: 2\n  renderer: networkd\n  ethernets:\n    eth0:\n      dhcp4: no\n    eth1:\n      dhcp4: no\n  bonds:\n    bond0:\n      interfaces:\n        - eth0\n        - eth1\n      addresses:\n        - 192.168.1.100/24\n      gateway4: 192.168.1.1\n      nameservers:\n        addresses:\n          - 8.8.8.8\n      parameters:\n        mode: active-backup\n        mii-monitor-interval: 100\n        # or for 802.3ad:\n        # mode: 802.3ad\n        # lacp-rate: fast\n        # transmit-hash-policy: layer3+4\n</code></pre> <p>Apply: <pre><code>netplan apply\n</code></pre></p>"},{"location":"linux/network/bridge-bonding/#bonding-configuration-options","title":"Bonding Configuration Options","text":""},{"location":"linux/network/bridge-bonding/#common-bonding-parameters","title":"Common Bonding Parameters","text":"<pre><code># Bonding modes\nmode=0              # balance-rr\nmode=1              # active-backup\nmode=2              # balance-xor\nmode=4              # 802.3ad\nmode=5              # balance-tlb\nmode=6              # balance-alb\n\n# Link monitoring\nmiimon=100          # MII link monitoring interval (ms)\narp_interval=250    # ARP monitoring interval (ms)\narp_ip_target=192.168.1.1  # Target for ARP monitoring\n\n# Failover/failback timing\ndowndelay=200       # Delay before marking slave down (ms)\nupdelay=200         # Delay before marking slave up (ms)\n\n# Mode-specific options\nprimary=eth0        # Primary interface (mode 1)\nlacp_rate=fast      # LACP heartbeat rate (slow=30s, fast=1s)\nxmit_hash_policy=layer3+4  # Hash policy (mode 2/4)\n# Options: layer2, layer3+4, layer2+3, encap2+3, encap3+4\n\n# Advanced\nfail_over_mac=none  # MAC address handling\nall_slaves_active=0 # Deliver packets to all slaves\n</code></pre>"},{"location":"linux/network/bridge-bonding/#set-bonding-options","title":"Set Bonding Options","text":"<pre><code># Using NetworkManager\nnmcli connection modify bond0 \\\n    bond.options \"mode=802.3ad,miimon=100,lacp_rate=fast,xmit_hash_policy=layer3+4\"\n\n# Using sysfs (temporary)\necho \"fast\" &gt; /sys/class/net/bond0/bonding/lacp_rate\necho \"layer3+4\" &gt; /sys/class/net/bond0/bonding/xmit_hash_policy\n</code></pre>"},{"location":"linux/network/bridge-bonding/#monitoring-bonds-and-bridges","title":"Monitoring Bonds and Bridges","text":""},{"location":"linux/network/bridge-bonding/#monitor-bonding-status","title":"Monitor Bonding Status","text":"<pre><code># View bond status\ncat /proc/net/bonding/bond0\n\n# View brief status\nip link show bond0\n\n# Using nmcli\nnmcli device show bond0\n\n# Check slave status\ncat /sys/class/net/bond0/bonding/slaves\ncat /sys/class/net/bond0/bonding/active_slave\n\n# Monitor in real-time\nwatch -n 1 'cat /proc/net/bonding/bond0'\n</code></pre>"},{"location":"linux/network/bridge-bonding/#monitor-bridge-status","title":"Monitor Bridge Status","text":"<pre><code># View bridge\nbridge link show\nip link show master br0\n\n# View MAC address table\nbridge fdb show\nbridge fdb show br br0\n\n# View statistics\nip -s link show br0\n\n# Monitor in real-time\nwatch -n 1 'bridge link show'\n</code></pre>"},{"location":"linux/network/bridge-bonding/#network-teaming-alternative-to-bonding","title":"Network Teaming (Alternative to Bonding)","text":""},{"location":"linux/network/bridge-bonding/#teaming-vs-bonding","title":"Teaming vs Bonding","text":"<ul> <li>teamd: Modern alternative to bonding</li> <li>More flexible and feature-rich</li> <li>Better performance monitoring</li> <li>JSON configuration</li> <li>Active-backup, load balancing, LACP support</li> </ul>"},{"location":"linux/network/bridge-bonding/#create-team-with-networkmanager","title":"Create Team with NetworkManager","text":"<pre><code># Create team\nnmcli connection add type team \\\n    con-name team0 \\\n    ifname team0 \\\n    config '{\"runner\": {\"name\": \"activebackup\"}}'\n\n# Alternative runners:\n# \"roundrobin\"\n# \"activebackup\"\n# \"loadbalance\"\n# \"broadcast\"\n# \"lacp\"\n\n# Configure IP\nnmcli connection modify team0 \\\n    ipv4.addresses 192.168.1.100/24 \\\n    ipv4.gateway 192.168.1.1 \\\n    ipv4.method manual\n\n# Add team ports\nnmcli connection add type team-slave \\\n    con-name team0-port1 \\\n    ifname eth0 \\\n    master team0\n\nnmcli connection add type team-slave \\\n    con-name team0-port2 \\\n    ifname eth1 \\\n    master team0\n\n# Activate\nnmcli connection up team0\n\n# View team status\nteamdctl team0 state\n</code></pre>"},{"location":"linux/network/bridge-bonding/#team-configuration-examples","title":"Team Configuration Examples","text":"<p>Active-Backup: <pre><code>{\n    \"runner\": {\n        \"name\": \"activebackup\"\n    },\n    \"link_watch\": {\n        \"name\": \"ethtool\"\n    }\n}\n</code></pre></p> <p>LACP: <pre><code>{\n    \"runner\": {\n        \"name\": \"lacp\",\n        \"active\": true,\n        \"fast_rate\": true,\n        \"tx_hash\": [\"eth\", \"ipv4\", \"ipv6\"]\n    },\n    \"link_watch\": {\n        \"name\": \"ethtool\"\n    }\n}\n</code></pre></p> <p>Load Balance: <pre><code>{\n    \"runner\": {\n        \"name\": \"loadbalance\",\n        \"tx_hash\": [\"eth\", \"ipv4\", \"ipv6\"]\n    },\n    \"link_watch\": {\n        \"name\": \"ethtool\"\n    }\n}\n</code></pre></p>"},{"location":"linux/network/bridge-bonding/#vlan-with-bridges-and-bonds","title":"VLAN with Bridges and Bonds","text":""},{"location":"linux/network/bridge-bonding/#bridge-with-vlan","title":"Bridge with VLAN","text":"<pre><code># Create VLAN interface\nip link add link eth0 name eth0.100 type vlan id 100\n\n# Add to bridge\nip link set eth0.100 master br0\n\n# Or using NetworkManager\nnmcli connection add type vlan \\\n    con-name vlan100 \\\n    ifname eth0.100 \\\n    dev eth0 \\\n    id 100 \\\n    master br0\n</code></pre>"},{"location":"linux/network/bridge-bonding/#bond-with-vlan","title":"Bond with VLAN","text":"<pre><code># Create VLAN on bond\nip link add link bond0 name bond0.100 type vlan id 100\n\n# Configure VLAN\nip addr add 192.168.100.1/24 dev bond0.100\nip link set bond0.100 up\n\n# Using NetworkManager\nnmcli connection add type vlan \\\n    con-name vlan100 \\\n    ifname bond0.100 \\\n    dev bond0 \\\n    id 100\n</code></pre>"},{"location":"linux/network/bridge-bonding/#troubleshooting","title":"Troubleshooting","text":""},{"location":"linux/network/bridge-bonding/#bridge-troubleshooting","title":"Bridge Troubleshooting","text":"<pre><code># Check bridge exists and is up\nip link show br0\nbrctl show\n\n# Check interfaces in bridge\nbridge link show\nip link show master br0\n\n# Check MAC address table\nbridge fdb show\n\n# Check STP status\nbrctl showstp br0\n\n# Verify IP configuration\nip addr show br0\n\n# Check for errors\nip -s link show br0\n\n# Test connectivity\nping -I br0 192.168.1.1\n</code></pre>"},{"location":"linux/network/bridge-bonding/#bond-troubleshooting","title":"Bond Troubleshooting","text":"<pre><code># Check bond status\ncat /proc/net/bonding/bond0\n\n# Verify mode\ncat /sys/class/net/bond0/bonding/mode\n\n# Check active slave\ncat /sys/class/net/bond0/bonding/active_slave\n\n# Check all slaves\ncat /sys/class/net/bond0/bonding/slaves\n\n# Check MII status\ncat /sys/class/net/bond0/bonding/mii_status\n\n# View statistics\nip -s link show bond0\n\n# Check slave status individually\nethtool eth0\nethtool eth1\n\n# Test failover (for testing only)\nip link set eth0 down\ncat /proc/net/bonding/bond0\nip link set eth0 up\n</code></pre>"},{"location":"linux/network/bridge-bonding/#common-issues","title":"Common Issues","text":"<p>Bridge not forwarding traffic: <pre><code># Enable forwarding\necho 1 &gt; /proc/sys/net/ipv4/ip_forward\n\n# Check iptables\niptables -L FORWARD\n\n# Disable netfilter on bridge (if needed)\necho 0 &gt; /proc/sys/net/bridge/bridge-nf-call-iptables\n</code></pre></p> <p>Bond slaves not activating: <pre><code># Check interfaces are up\nip link set eth0 up\nip link set eth1 up\n\n# Verify no IP on slaves\nip addr flush dev eth0\nip addr flush dev eth1\n\n# Check for conflicting configurations\nnmcli connection show\n</code></pre></p> <p>LACP not negotiating: <pre><code># Verify switch configuration\n# Check LACP is enabled on switch\n# Verify correct mode (802.3ad)\ncat /proc/net/bonding/bond0 | grep -A 10 \"802.3ad\"\n\n# Check LACP rate\ncat /sys/class/net/bond0/bonding/lacp_rate\n</code></pre></p>"},{"location":"linux/network/bridge-bonding/#practical-examples","title":"Practical Examples","text":""},{"location":"linux/network/bridge-bonding/#example-1-kvm-bridge","title":"Example 1: KVM Bridge","text":"<pre><code># Create bridge for KVM VMs\nnmcli connection add type bridge \\\n    con-name br0 \\\n    ifname br0 \\\n    ipv4.method disabled \\\n    ipv6.method disabled\n\nnmcli connection add type bridge-slave \\\n    con-name br0-port1 \\\n    ifname eth0 \\\n    master br0\n\nnmcli connection up br0\n</code></pre>"},{"location":"linux/network/bridge-bonding/#example-2-redundant-server-network","title":"Example 2: Redundant Server Network","text":"<pre><code># Active-backup bond with two NICs\nnmcli connection add type bond \\\n    con-name bond0 \\\n    ifname bond0 \\\n    mode active-backup \\\n    ipv4.addresses 192.168.1.100/24 \\\n    ipv4.gateway 192.168.1.1 \\\n    ipv4.method manual\n\nnmcli connection add type ethernet \\\n    slave-type bond \\\n    con-name bond0-eth0 \\\n    ifname eth0 \\\n    master bond0\n\nnmcli connection add type ethernet \\\n    slave-type bond \\\n    con-name bond0-eth1 \\\n    ifname eth1 \\\n    master bond0\n\nnmcli connection modify bond0 \\\n    bond.options \"mode=active-backup,miimon=100,primary=eth0\"\n\nnmcli connection up bond0\n</code></pre>"},{"location":"linux/network/bridge-bonding/#example-3-lacp-bond-for-high-bandwidth","title":"Example 3: LACP Bond for High Bandwidth","text":"<pre><code># 802.3ad LACP bond\nnmcli connection add type bond \\\n    con-name bond0 \\\n    ifname bond0 \\\n    mode 802.3ad \\\n    ipv4.addresses 192.168.1.100/24 \\\n    ipv4.method manual\n\nnmcli connection modify bond0 \\\n    bond.options \"mode=802.3ad,miimon=100,lacp_rate=fast,xmit_hash_policy=layer3+4\"\n\nnmcli connection add type ethernet \\\n    slave-type bond \\\n    ifname eth0 \\\n    master bond0\n\nnmcli connection add type ethernet \\\n    slave-type bond \\\n    ifname eth1 \\\n    master bond0\n\nnmcli connection up bond0\n</code></pre>"},{"location":"linux/network/bridge-bonding/#quick-reference","title":"Quick Reference","text":""},{"location":"linux/network/bridge-bonding/#bridge-commands","title":"Bridge Commands","text":"<pre><code>ip link add br0 type bridge                    # Create bridge\nip link set eth0 master br0                    # Add interface\nbridge link show                               # Show bridge ports\nbrctl show                                     # Show bridges\nip link delete br0                             # Delete bridge\n</code></pre>"},{"location":"linux/network/bridge-bonding/#bond-commands","title":"Bond Commands","text":"<pre><code>ip link add bond0 type bond mode active-backup # Create bond\nip link set eth0 master bond0                  # Add slave\ncat /proc/net/bonding/bond0                    # Show status\nip link delete bond0                           # Delete bond\n</code></pre>"},{"location":"linux/network/bridge-bonding/#networkmanager","title":"NetworkManager","text":"<pre><code>nmcli connection add type bridge con-name br0 ifname br0\nnmcli connection add type bridge-slave ifname eth0 master br0\nnmcli connection add type bond con-name bond0 mode active-backup\nnmcli connection add type ethernet master bond0 ifname eth0\n</code></pre>"},{"location":"linux/network/bridge-bonding/#exam-tips","title":"Exam Tips","text":"<ul> <li>Know how to create bridges and bonds with NetworkManager</li> <li>Understand different bonding modes and when to use each</li> <li>Be familiar with both temporary (<code>ip</code>) and persistent (nmcli/files) configuration</li> <li>Know how to troubleshoot bond and bridge issues</li> <li>Understand LACP requirements and configuration</li> <li>Practice viewing status with <code>/proc/net/bonding/</code> and <code>bridge</code> commands</li> <li>Know the difference between bonding and teaming</li> <li>Understand when bridges are needed (VMs, containers)</li> <li>Be comfortable with both RHEL and Ubuntu configurations</li> <li>Know how to verify link status and failover behavior</li> </ul>"},{"location":"linux/network/ipv4-ipv6-hostname/","title":"Configure IPv4 and IPv6 Networking and Hostname Resolution","text":""},{"location":"linux/network/ipv4-ipv6-hostname/#overview","title":"Overview","text":"<p>This guide covers essential commands and configurations for managing network interfaces, IP addressing (both IPv4 and IPv6), and hostname resolution in Linux systems.</p>"},{"location":"linux/network/ipv4-ipv6-hostname/#network-interface-management","title":"Network Interface Management","text":""},{"location":"linux/network/ipv4-ipv6-hostname/#ip-command","title":"<code>ip</code> Command","text":"<p>The modern standard for network configuration in Linux, replacing older tools like <code>ifconfig</code>.</p>"},{"location":"linux/network/ipv4-ipv6-hostname/#display-network-interfaces","title":"Display Network Interfaces","text":"<pre><code># Show all network interfaces\nip link show\n\n# Show specific interface\nip link show eth0\n\n# Show interface statistics\nip -s link show eth0\n</code></pre>"},{"location":"linux/network/ipv4-ipv6-hostname/#enabledisable-interfaces","title":"Enable/Disable Interfaces","text":"<pre><code># Bring interface up\nip link set eth0 up\n\n# Bring interface down\nip link set eth0 down\n</code></pre>"},{"location":"linux/network/ipv4-ipv6-hostname/#ipv4-configuration","title":"IPv4 Configuration","text":""},{"location":"linux/network/ipv4-ipv6-hostname/#viewing-ipv4-addresses","title":"Viewing IPv4 Addresses","text":"<pre><code># Show all IPv4 addresses\nip -4 addr show\n\n# Show IPv4 for specific interface\nip addr show eth0\n\n# Brief output\nip -br addr show\n</code></pre>"},{"location":"linux/network/ipv4-ipv6-hostname/#addingremoving-ipv4-addresses","title":"Adding/Removing IPv4 Addresses","text":"<pre><code># Add IPv4 address\nip addr add 192.168.1.100/24 dev eth0\n\n# Add with broadcast\nip addr add 192.168.1.100/24 broadcast 192.168.1.255 dev eth0\n\n# Remove IPv4 address\nip addr del 192.168.1.100/24 dev eth0\n\n# Flush all addresses from interface\nip addr flush dev eth0\n</code></pre>"},{"location":"linux/network/ipv4-ipv6-hostname/#default-gateway","title":"Default Gateway","text":"<pre><code># Show routing table\nip route show\n\n# Add default gateway\nip route add default via 192.168.1.1\n\n# Delete default gateway\nip route del default via 192.168.1.1\n\n# Add route to specific network\nip route add 10.0.0.0/8 via 192.168.1.254\n</code></pre>"},{"location":"linux/network/ipv4-ipv6-hostname/#ipv6-configuration","title":"IPv6 Configuration","text":""},{"location":"linux/network/ipv4-ipv6-hostname/#viewing-ipv6-addresses","title":"Viewing IPv6 Addresses","text":"<pre><code># Show all IPv6 addresses\nip -6 addr show\n\n# Show IPv6 only for specific interface\nip -6 addr show eth0\n\n# Show IPv6 routing table\nip -6 route show\n</code></pre>"},{"location":"linux/network/ipv4-ipv6-hostname/#addingremoving-ipv6-addresses","title":"Adding/Removing IPv6 Addresses","text":"<pre><code># Add IPv6 address\nip -6 addr add 2001:db8::1/64 dev eth0\n\n# Remove IPv6 address\nip -6 addr del 2001:db8::1/64 dev eth0\n\n# Add IPv6 default gateway\nip -6 route add default via 2001:db8::ff\n\n# Delete IPv6 default gateway\nip -6 route del default via 2001:db8::ff\n</code></pre>"},{"location":"linux/network/ipv4-ipv6-hostname/#ipv6-link-local-addresses","title":"IPv6 Link-Local Addresses","text":"<pre><code># View link-local addresses (fe80::/10)\nip -6 addr show scope link\n\n# Ping using link-local (must specify interface)\nping6 fe80::a00:27ff:fe4e:66a1%eth0\n</code></pre>"},{"location":"linux/network/ipv4-ipv6-hostname/#network-configuration-files","title":"Network Configuration Files","text":""},{"location":"linux/network/ipv4-ipv6-hostname/#networkmanager-rhelcentosfedora","title":"NetworkManager (RHEL/CentOS/Fedora)","text":""},{"location":"linux/network/ipv4-ipv6-hostname/#using-nmcli-command","title":"Using <code>nmcli</code> Command","text":"<pre><code># Show all connections\nnmcli connection show\n\n# Show active connections\nnmcli connection show --active\n\n# Show device status\nnmcli device status\n\n# Create new connection with static IPv4\nnmcli connection add \\\n    type ethernet \\\n    con-name eth0-static \\\n    ifname eth0 \\\n    ipv4.addresses 192.168.1.100/24 \\\n    ipv4.gateway 192.168.1.1 \\\n    ipv4.dns \"8.8.8.8 8.8.4.4\" \\\n    ipv4.method manual\n\n# Create DHCP connection\nnmcli connection add \\\n    type ethernet \\\n    con-name eth0-dhcp \\\n    ifname eth0 \\\n    ipv4.method auto\n\n# Modify existing connection\nnmcli connection modify eth0-static ipv4.addresses 192.168.1.101/24\n\n# Add secondary IP address\nnmcli connection modify eth0-static +ipv4.addresses 192.168.1.102/24\n\n# Configure IPv6\nnmcli connection modify eth0-static \\\n    ipv6.addresses 2001:db8::100/64 \\\n    ipv6.gateway 2001:db8::1 \\\n    ipv6.method manual\n\n# Enable/disable IPv6\nnmcli connection modify eth0-static ipv6.method auto\nnmcli connection modify eth0-static ipv6.method ignore\n\n# Activate connection\nnmcli connection up eth0-static\n\n# Deactivate connection\nnmcli connection down eth0-static\n\n# Delete connection\nnmcli connection delete eth0-static\n\n# Reload configuration\nnmcli connection reload\n</code></pre>"},{"location":"linux/network/ipv4-ipv6-hostname/#configuration-files-rhelcentos","title":"Configuration Files (RHEL/CentOS)","text":"<p>Location: <code>/etc/sysconfig/network-scripts/ifcfg-&lt;interface&gt;</code></p> <p>Example static IPv4 configuration: <pre><code># /etc/sysconfig/network-scripts/ifcfg-eth0\nTYPE=Ethernet\nBOOTPROTO=none\nNAME=eth0\nDEVICE=eth0\nONBOOT=yes\nIPADDR=192.168.1.100\nPREFIX=24\nGATEWAY=192.168.1.1\nDNS1=8.8.8.8\nDNS2=8.8.4.4\n</code></pre></p> <p>Example DHCP configuration: <pre><code>TYPE=Ethernet\nBOOTPROTO=dhcp\nNAME=eth0\nDEVICE=eth0\nONBOOT=yes\n</code></pre></p> <p>Example IPv6 configuration: <pre><code>IPV6INIT=yes\nIPV6ADDR=2001:db8::100/64\nIPV6_DEFAULTGW=2001:db8::1\n</code></pre></p>"},{"location":"linux/network/ipv4-ipv6-hostname/#netplan-ubuntudebian","title":"Netplan (Ubuntu/Debian)","text":"<p>Location: <code>/etc/netplan/*.yaml</code></p> <p>Example static configuration: <pre><code># /etc/netplan/01-netcfg.yaml\nnetwork:\n  version: 2\n  renderer: networkd\n  ethernets:\n    eth0:\n      addresses:\n        - 192.168.1.100/24\n        - 2001:db8::100/64\n      gateway4: 192.168.1.1\n      gateway6: 2001:db8::1\n      nameservers:\n        addresses:\n          - 8.8.8.8\n          - 8.8.4.4\n          - 2001:4860:4860::8888\n</code></pre></p> <p>Example DHCP configuration: <pre><code>network:\n  version: 2\n  ethernets:\n    eth0:\n      dhcp4: true\n      dhcp6: true\n</code></pre></p> <p>Apply Netplan configuration: <pre><code># Test configuration\nnetplan try\n\n# Apply configuration\nnetplan apply\n\n# Generate backend configuration\nnetplan generate\n</code></pre></p>"},{"location":"linux/network/ipv4-ipv6-hostname/#hostname-configuration","title":"Hostname Configuration","text":""},{"location":"linux/network/ipv4-ipv6-hostname/#viewing-hostname","title":"Viewing Hostname","text":"<pre><code># Show hostname\nhostname\n\n# Show FQDN (Fully Qualified Domain Name)\nhostname -f\n\n# Show short hostname\nhostname -s\n\n# Show all hostname information\nhostnamectl\n</code></pre>"},{"location":"linux/network/ipv4-ipv6-hostname/#setting-hostname","title":"Setting Hostname","text":""},{"location":"linux/network/ipv4-ipv6-hostname/#using-hostnamectl-systemd-systems","title":"Using <code>hostnamectl</code> (systemd systems)","text":"<pre><code># Set hostname\nhostnamectl set-hostname server1.example.com\n\n# Set static hostname only\nhostnamectl set-hostname server1 --static\n\n# Set pretty hostname (descriptive name)\nhostnamectl set-hostname \"Production Web Server\" --pretty\n\n# Set transient hostname (temporary)\nhostnamectl set-hostname temp-host --transient\n</code></pre>"},{"location":"linux/network/ipv4-ipv6-hostname/#manual-configuration-files","title":"Manual Configuration Files","text":"<pre><code># RHEL/CentOS/Fedora\necho \"server1.example.com\" &gt; /etc/hostname\n\n# Also update /etc/hosts\necho \"192.168.1.100 server1.example.com server1\" &gt;&gt; /etc/hosts\n</code></pre>"},{"location":"linux/network/ipv4-ipv6-hostname/#dns-and-name-resolution","title":"DNS and Name Resolution","text":""},{"location":"linux/network/ipv4-ipv6-hostname/#dns-configuration-file","title":"DNS Configuration File","text":"<p>Location: <code>/etc/resolv.conf</code></p> <pre><code># Configure DNS servers\nnameserver 8.8.8.8\nnameserver 8.8.4.4\nnameserver 2001:4860:4860::8888\n\n# Search domains\nsearch example.com local.domain\n\n# Options\noptions timeout:2\noptions attempts:3\n</code></pre>"},{"location":"linux/network/ipv4-ipv6-hostname/#etchosts-file","title":"<code>/etc/hosts</code> File","text":"<p>Static hostname to IP mapping:</p> <pre><code># IPv4\n127.0.0.1       localhost localhost.localdomain\n192.168.1.100   server1.example.com server1\n192.168.1.101   server2.example.com server2\n\n# IPv6\n::1             localhost localhost.localdomain\n2001:db8::100   server1.example.com server1\n</code></pre>"},{"location":"linux/network/ipv4-ipv6-hostname/#etcnsswitchconf","title":"<code>/etc/nsswitch.conf</code>","text":"<p>Controls the order of name resolution sources:</p> <pre><code># Important line for hostname resolution\nhosts: files dns myhostname\n\n# This means: check /etc/hosts first, then DNS, then systemd hostname\n</code></pre>"},{"location":"linux/network/ipv4-ipv6-hostname/#dns-testing-commands","title":"DNS Testing Commands","text":""},{"location":"linux/network/ipv4-ipv6-hostname/#nslookup","title":"<code>nslookup</code>","text":"<pre><code># Query DNS for domain\nnslookup example.com\n\n# Query specific DNS server\nnslookup example.com 8.8.8.8\n\n# Reverse lookup\nnslookup 8.8.8.8\n</code></pre>"},{"location":"linux/network/ipv4-ipv6-hostname/#dig","title":"<code>dig</code>","text":"<pre><code># Basic query\ndig example.com\n\n# Query specific record type\ndig example.com A      # IPv4\ndig example.com AAAA   # IPv6\ndig example.com MX     # Mail exchange\ndig example.com NS     # Name servers\n\n# Query specific DNS server\ndig @8.8.8.8 example.com\n\n# Short output\ndig +short example.com\n\n# Reverse lookup\ndig -x 8.8.8.8\n\n# Trace DNS resolution path\ndig +trace example.com\n</code></pre>"},{"location":"linux/network/ipv4-ipv6-hostname/#host","title":"<code>host</code>","text":"<pre><code># Simple lookup\nhost example.com\n\n# Reverse lookup\nhost 8.8.8.8\n\n# Query specific record type\nhost -t MX example.com\nhost -t AAAA example.com\n\n# Use specific DNS server\nhost example.com 8.8.8.8\n</code></pre>"},{"location":"linux/network/ipv4-ipv6-hostname/#getent","title":"<code>getent</code>","text":"<pre><code># Query using system resolution (respects /etc/nsswitch.conf)\ngetent hosts example.com\n\n# This uses the configured resolution order (files, DNS, etc.)\ngetent ahosts example.com  # All addresses\n</code></pre>"},{"location":"linux/network/ipv4-ipv6-hostname/#network-manager-text-ui","title":"Network Manager Text UI","text":""},{"location":"linux/network/ipv4-ipv6-hostname/#nmtui","title":"<code>nmtui</code>","text":"<p>Interactive text-based interface for NetworkManager:</p> <pre><code># Launch network configuration UI\nnmtui\n\n# Directly launch specific function\nnmtui edit       # Edit connection\nnmtui connect    # Activate connection\nnmtui hostname   # Set hostname\n</code></pre>"},{"location":"linux/network/ipv4-ipv6-hostname/#systemd-network-configuration","title":"SystemD Network Configuration","text":""},{"location":"linux/network/ipv4-ipv6-hostname/#systemd-networkd","title":"<code>systemd-networkd</code>","text":"<p>Alternative to NetworkManager on systemd systems.</p> <p>Configuration files: <code>/etc/systemd/network/*.network</code></p> <p>Example configuration: <pre><code># /etc/systemd/network/20-wired.network\n[Match]\nName=eth0\n\n[Network]\nAddress=192.168.1.100/24\nGateway=192.168.1.1\nDNS=8.8.8.8\nDNS=8.8.4.4\n</code></pre></p> <p>Enable and start: <pre><code>systemctl enable systemd-networkd\nsystemctl start systemd-networkd\nsystemctl status systemd-networkd\n</code></pre></p>"},{"location":"linux/network/ipv4-ipv6-hostname/#useful-network-information-commands","title":"Useful Network Information Commands","text":""},{"location":"linux/network/ipv4-ipv6-hostname/#show-network-configuration-summary","title":"Show Network Configuration Summary","text":"<pre><code># Modern way\nip addr show\n\n# Show only IPv4\nip -4 addr\n\n# Show only IPv6\nip -6 addr\n\n# Show with colors\nip -c addr\n\n# Brief output\nip -br addr show\n</code></pre>"},{"location":"linux/network/ipv4-ipv6-hostname/#check-connectivity","title":"Check Connectivity","text":"<pre><code># Ping IPv4\nping -c 4 192.168.1.1\n\n# Ping IPv6\nping6 -c 4 2001:db8::1\n\n# Ping with specific interface\nping -I eth0 192.168.1.1\n</code></pre>"},{"location":"linux/network/ipv4-ipv6-hostname/#persistent-vs-temporary-configuration","title":"Persistent vs Temporary Configuration","text":"<p>Temporary (runtime only): <pre><code>ip addr add 192.168.1.100/24 dev eth0\n# Lost after reboot\n</code></pre></p> <p>Persistent (survives reboot): - Use <code>nmcli</code> commands - Edit configuration files in <code>/etc/sysconfig/network-scripts/</code> (RHEL) - Edit Netplan files <code>/etc/netplan/</code> (Ubuntu) - Use <code>systemd-networkd</code> configuration files</p>"},{"location":"linux/network/ipv4-ipv6-hostname/#common-troubleshooting-steps","title":"Common Troubleshooting Steps","text":"<ol> <li> <p>Check interface status: <pre><code>ip link show\nip addr show\n</code></pre></p> </li> <li> <p>Check routing: <pre><code>ip route show\nip -6 route show\n</code></pre></p> </li> <li> <p>Test connectivity: <pre><code>ping -c 4 8.8.8.8\nping6 -c 4 2001:4860:4860::8888\n</code></pre></p> </li> <li> <p>Check DNS resolution: <pre><code>dig example.com\nnslookup example.com\ngetent hosts example.com\n</code></pre></p> </li> <li> <p>Verify configuration files: <pre><code>cat /etc/resolv.conf\ncat /etc/hosts\ncat /etc/nsswitch.conf\n</code></pre></p> </li> <li> <p>Check NetworkManager/networkd status: <pre><code>systemctl status NetworkManager\nsystemctl status systemd-networkd\n</code></pre></p> </li> </ol>"},{"location":"linux/network/ipv4-ipv6-hostname/#key-exam-tips","title":"Key Exam Tips","text":"<ul> <li>Know both temporary (<code>ip</code> commands) and persistent (config files, <code>nmcli</code>) methods</li> <li>Understand IPv4 and IPv6 addressing and configuration</li> <li>Be comfortable with CIDR notation (e.g., /24, /64)</li> <li>Practice DNS configuration and testing</li> <li>Understand the hostname resolution order in <code>/etc/nsswitch.conf</code></li> <li>Know how to configure both DHCP and static IP addresses</li> <li>Remember to restart/reload network services after configuration changes</li> </ul>"},{"location":"linux/network/monitor-troubleshoot-networking/","title":"Monitor and Troubleshoot Networking","text":""},{"location":"linux/network/monitor-troubleshoot-networking/#overview","title":"Overview","text":"<p>This guide covers essential tools and techniques for monitoring network performance, diagnosing connectivity issues, and troubleshooting network problems in Linux.</p>"},{"location":"linux/network/monitor-troubleshoot-networking/#basic-connectivity-testing","title":"Basic Connectivity Testing","text":""},{"location":"linux/network/monitor-troubleshoot-networking/#ping-command","title":"<code>ping</code> Command","text":"<p>Tests basic connectivity using ICMP Echo Request/Reply.</p> <pre><code># Basic ping\nping 8.8.8.8\n\n# Ping with count\nping -c 4 google.com\n\n# Ping with specific interval (default 1 second)\nping -i 2 192.168.1.1\n\n# Ping with specific packet size\nping -s 1000 google.com\n\n# Ping with timeout\nping -W 2 192.168.1.1\n\n# Ping IPv6\nping6 2001:4860:4860::8888\nping6 -c 4 google.com\n\n# Flood ping (requires root)\nping -f 192.168.1.1\n\n# Don't fragment packets\nping -M do -s 1472 192.168.1.1\n</code></pre> <p>Options explained: - <code>-c</code>: Count (number of packets) - <code>-i</code>: Interval between packets - <code>-s</code>: Packet size - <code>-W</code>: Timeout - <code>-f</code>: Flood mode - <code>-M</code>: MTU discovery strategy</p>"},{"location":"linux/network/monitor-troubleshoot-networking/#network-interface-monitoring","title":"Network Interface Monitoring","text":""},{"location":"linux/network/monitor-troubleshoot-networking/#ip-command","title":"<code>ip</code> Command","text":"<pre><code># Show all interfaces with statistics\nip -s link show\n\n# Show specific interface statistics\nip -s -s link show eth0\n\n# Show interface errors\nip -s link show eth0 | grep -E \"RX|TX|errors|dropped\"\n\n# Monitor in real-time\nwatch -n 1 'ip -s link show eth0'\n\n# Show ARP cache\nip neighbour show\nip neigh show\n\n# Flush ARP cache\nip neighbour flush all\n</code></pre>"},{"location":"linux/network/monitor-troubleshoot-networking/#ifconfig-legacy-but-still-useful","title":"<code>ifconfig</code> (Legacy, but still useful)","text":"<pre><code># Show all interfaces\nifconfig -a\n\n# Show specific interface\nifconfig eth0\n\n# Show statistics\nifconfig eth0 | grep -E \"RX|TX\"\n</code></pre>"},{"location":"linux/network/monitor-troubleshoot-networking/#interface-statistics-files","title":"Interface Statistics Files","text":"<pre><code># View interface statistics via /sys\ncat /sys/class/net/eth0/statistics/rx_packets\ncat /sys/class/net/eth0/statistics/tx_packets\ncat /sys/class/net/eth0/statistics/rx_errors\ncat /sys/class/net/eth0/statistics/tx_errors\ncat /sys/class/net/eth0/statistics/collisions\n\n# View all statistics for interface\nls /sys/class/net/eth0/statistics/\n</code></pre>"},{"location":"linux/network/monitor-troubleshoot-networking/#routing-and-network-path","title":"Routing and Network Path","text":""},{"location":"linux/network/monitor-troubleshoot-networking/#ip-route-command","title":"<code>ip route</code> Command","text":"<pre><code># Show routing table\nip route show\n\n# Show IPv6 routing table\nip -6 route show\n\n# Show routing table with details\nip route show table all\n\n# Show route to specific destination\nip route get 8.8.8.8\nip route get 2001:4860:4860::8888\n\n# Show routing cache (deprecated in newer kernels)\nip route show cache\n</code></pre>"},{"location":"linux/network/monitor-troubleshoot-networking/#traceroute-command","title":"<code>traceroute</code> Command","text":"<p>Shows the path packets take to reach destination.</p> <pre><code># Basic traceroute\ntraceroute google.com\n\n# Traceroute with no DNS resolution\ntraceroute -n 8.8.8.8\n\n# Traceroute using ICMP instead of UDP\ntraceroute -I google.com\n\n# Traceroute using TCP SYN\ntraceroute -T -p 80 google.com\n\n# Set maximum hops\ntraceroute -m 20 google.com\n\n# Set number of queries per hop\ntraceroute -q 3 google.com\n\n# IPv6 traceroute\ntraceroute6 google.com\n\n# MTU path discovery\ntraceroute --mtu google.com\n</code></pre>"},{"location":"linux/network/monitor-troubleshoot-networking/#tracepath-command","title":"<code>tracepath</code> Command","text":"<p>Similar to traceroute but doesn\u2019t require root privileges.</p> <pre><code># Basic tracepath\ntracepath google.com\n\n# IPv6 tracepath\ntracepath6 google.com\n\n# Set initial packet length\ntracepath -l 1400 google.com\n</code></pre>"},{"location":"linux/network/monitor-troubleshoot-networking/#mtr-command","title":"<code>mtr</code> Command","text":"<p>Combines ping and traceroute functionality with real-time updates.</p> <pre><code># Interactive mode\nmtr google.com\n\n# Report mode (non-interactive)\nmtr -r -c 10 google.com\n\n# No DNS resolution\nmtr -n google.com\n\n# Show both hostnames and IP addresses\nmtr -b google.com\n\n# Wide report mode\nmtr -w google.com\n\n# CSV output\nmtr --csv google.com\n\n# JSON output\nmtr --json google.com\n\n# Set packet size\nmtr -s 1000 google.com\n</code></pre>"},{"location":"linux/network/monitor-troubleshoot-networking/#port-and-service-connectivity","title":"Port and Service Connectivity","text":""},{"location":"linux/network/monitor-troubleshoot-networking/#telnet-command","title":"<code>telnet</code> Command","text":"<p>Test TCP connectivity to specific ports.</p> <pre><code># Test HTTP port\ntelnet google.com 80\n\n# Test HTTPS port\ntelnet google.com 443\n\n# Test SSH port\ntelnet 192.168.1.100 22\n\n# Exit telnet: Ctrl+] then type 'quit'\n</code></pre>"},{"location":"linux/network/monitor-troubleshoot-networking/#nc-netcat-command","title":"<code>nc</code> (netcat) Command","text":"<p>Swiss Army knife for network testing.</p> <pre><code># Test TCP connection\nnc -vz google.com 80\n\n# Test UDP connection\nnc -vzu 8.8.8.8 53\n\n# Scan range of ports\nnc -vz google.com 80-443\n\n# Listen on a port\nnc -l 8080\n\n# Connect and send data\necho \"GET / HTTP/1.0\" | nc google.com 80\n\n# Transfer file\n# On receiver:\nnc -l 9999 &gt; received_file\n# On sender:\nnc receiver_ip 9999 &lt; file_to_send\n\n# Port scanning\nnc -zv 192.168.1.1 20-80\n\n# Test with timeout\nnc -w 3 -vz google.com 80\n</code></pre>"},{"location":"linux/network/monitor-troubleshoot-networking/#nmap-command","title":"<code>nmap</code> Command","text":"<p>Powerful network scanning and port discovery tool.</p> <pre><code># Scan single host\nnmap 192.168.1.1\n\n# Scan with service detection\nnmap -sV 192.168.1.1\n\n# Scan specific ports\nnmap -p 22,80,443 192.168.1.1\n\n# Scan port range\nnmap -p 1-1000 192.168.1.1\n\n# Scan all ports\nnmap -p- 192.168.1.1\n\n# Fast scan (top 100 ports)\nnmap -F 192.168.1.1\n\n# Scan subnet\nnmap 192.168.1.0/24\n\n# OS detection\nnmap -O 192.168.1.1\n\n# Aggressive scan\nnmap -A 192.168.1.1\n\n# TCP SYN scan (stealth)\nnmap -sS 192.168.1.1\n\n# UDP scan\nnmap -sU 192.168.1.1\n\n# Save output\nnmap -oN output.txt 192.168.1.1\nnmap -oX output.xml 192.168.1.1\n</code></pre>"},{"location":"linux/network/monitor-troubleshoot-networking/#network-statistics-and-connections","title":"Network Statistics and Connections","text":""},{"location":"linux/network/monitor-troubleshoot-networking/#ss-command-socket-statistics","title":"<code>ss</code> Command (Socket Statistics)","text":"<p>Modern replacement for netstat, showing socket information.</p> <pre><code># Show all sockets\nss -a\n\n# Show listening TCP sockets\nss -lt\n\n# Show listening UDP sockets\nss -lu\n\n# Show all TCP connections\nss -t\n\n# Show all UDP connections\nss -u\n\n# Show process using socket\nss -p\n\n# Show summary statistics\nss -s\n\n# Show sockets with numeric ports\nss -n\n\n# Combine options\nss -tulpn\n\n# Show TCP sockets in listening state\nss -tln\n\n# Show established connections\nss -t state established\n\n# Show connections to specific port\nss -tn sport = :80\nss -tn dport = :443\n\n# Show connections to specific IP\nss dst 192.168.1.100\n\n# Show socket memory usage\nss -m\n\n# Show timer information\nss -o\n\n# Extended socket info\nss -e\n\n# Show both IPv4 and IPv6\nss -46tulpn\n</code></pre>"},{"location":"linux/network/monitor-troubleshoot-networking/#netstat-command-legacy","title":"<code>netstat</code> Command (Legacy)","text":"<pre><code># Show all listening ports\nnetstat -tuln\n\n# Show all connections with process\nnetstat -tulpn\n\n# Show routing table\nnetstat -r\n\n# Show interface statistics\nnetstat -i\n\n# Show network statistics\nnetstat -s\n\n# Continuous monitoring\nnetstat -c\n\n# Show only TCP\nnetstat -t\n\n# Show only UDP\nnetstat -u\n\n# Show listening sockets\nnetstat -l\n\n# Show all (listening and non-listening)\nnetstat -a\n</code></pre>"},{"location":"linux/network/monitor-troubleshoot-networking/#packet-capture-and-analysis","title":"Packet Capture and Analysis","text":""},{"location":"linux/network/monitor-troubleshoot-networking/#tcpdump-command","title":"<code>tcpdump</code> Command","text":"<p>Capture and analyze network packets.</p> <pre><code># Capture on specific interface\ntcpdump -i eth0\n\n# Capture specific number of packets\ntcpdump -c 100 -i eth0\n\n# Capture and save to file\ntcpdump -i eth0 -w capture.pcap\n\n# Read from file\ntcpdump -r capture.pcap\n\n# Capture with verbose output\ntcpdump -v -i eth0\ntcpdump -vv -i eth0\ntcpdump -vvv -i eth0\n\n# Show packet contents in hex and ASCII\ntcpdump -X -i eth0\n\n# Capture specific host\ntcpdump -i eth0 host 192.168.1.100\n\n# Capture specific port\ntcpdump -i eth0 port 80\n\n# Capture specific protocol\ntcpdump -i eth0 icmp\ntcpdump -i eth0 tcp\ntcpdump -i eth0 udp\n\n# Capture source or destination\ntcpdump -i eth0 src 192.168.1.100\ntcpdump -i eth0 dst 192.168.1.100\n\n# Capture network range\ntcpdump -i eth0 net 192.168.1.0/24\n\n# Complex filters\ntcpdump -i eth0 'tcp port 80 and src 192.168.1.100'\ntcpdump -i eth0 'tcp[tcpflags] &amp; tcp-syn != 0'\n\n# Capture DNS queries\ntcpdump -i eth0 -n port 53\n\n# Capture HTTP traffic\ntcpdump -i eth0 -A 'tcp port 80 and (((ip[2:2] - ((ip[0]&amp;0xf)&lt;&lt;2)) - ((tcp[12]&amp;0xf0)&gt;&gt;2)) != 0)'\n\n# Don't resolve hostnames\ntcpdump -n -i eth0\n\n# Don't resolve hostnames or ports\ntcpdump -nn -i eth0\n\n# Capture on all interfaces\ntcpdump -i any\n\n# Set snapshot length\ntcpdump -s 0 -i eth0  # Full packet\ntcpdump -s 96 -i eth0 # First 96 bytes\n\n# Rotate capture files\ntcpdump -i eth0 -w capture.pcap -C 100 -W 5\n</code></pre> <p>Common filters: - <code>host X</code>: Traffic to/from host X - <code>src X</code>: Traffic from X - <code>dst X</code>: Traffic to X - <code>net X</code>: Traffic to/from network X - <code>port X</code>: Traffic on port X - <code>portrange X-Y</code>: Traffic on port range - <code>less/greater X</code>: Packet size less/greater than X - <code>tcp/udp/icmp</code>: Specific protocol</p>"},{"location":"linux/network/monitor-troubleshoot-networking/#dns-troubleshooting","title":"DNS Troubleshooting","text":""},{"location":"linux/network/monitor-troubleshoot-networking/#dig-command","title":"<code>dig</code> Command","text":"<pre><code># Basic query\ndig example.com\n\n# Query specific record type\ndig example.com A\ndig example.com AAAA\ndig example.com MX\ndig example.com NS\ndig example.com TXT\ndig example.com SOA\n\n# Query specific DNS server\ndig @8.8.8.8 example.com\n\n# Short answer only\ndig +short example.com\n\n# Reverse DNS lookup\ndig -x 8.8.8.8\n\n# Trace DNS resolution\ndig +trace example.com\n\n# Show query time\ndig example.com +stats\n\n# Show all information\ndig example.com ANY\n\n# Disable recursion\ndig +norecurse example.com\n</code></pre>"},{"location":"linux/network/monitor-troubleshoot-networking/#nslookup-command","title":"<code>nslookup</code> Command","text":"<pre><code># Basic query\nnslookup example.com\n\n# Query specific server\nnslookup example.com 8.8.8.8\n\n# Reverse lookup\nnslookup 8.8.8.8\n\n# Interactive mode\nnslookup\n&gt; server 8.8.8.8\n&gt; set type=MX\n&gt; example.com\n&gt; exit\n</code></pre>"},{"location":"linux/network/monitor-troubleshoot-networking/#host-command","title":"<code>host</code> Command","text":"<pre><code># Basic lookup\nhost example.com\n\n# Specific record type\nhost -t A example.com\nhost -t MX example.com\nhost -t NS example.com\n\n# Reverse lookup\nhost 8.8.8.8\n\n# Verbose output\nhost -v example.com\n\n# Query specific server\nhost example.com 8.8.8.8\n</code></pre>"},{"location":"linux/network/monitor-troubleshoot-networking/#network-performance-testing","title":"Network Performance Testing","text":""},{"location":"linux/network/monitor-troubleshoot-networking/#iperf3-command","title":"<code>iperf3</code> Command","text":"<p>Network bandwidth testing tool.</p> <p>Server side: <pre><code># Start server\niperf3 -s\n\n# Start server on specific port\niperf3 -s -p 5201\n\n# Server with JSON output\niperf3 -s -J\n</code></pre></p> <p>Client side: <pre><code># Basic test\niperf3 -c server_ip\n\n# Test with specific duration\niperf3 -c server_ip -t 30\n\n# Test with specific bandwidth\niperf3 -c server_ip -b 100M\n\n# Reverse mode (server sends)\niperf3 -c server_ip -R\n\n# Bidirectional test\niperf3 -c server_ip --bidir\n\n# UDP test\niperf3 -c server_ip -u\n\n# Parallel streams\niperf3 -c server_ip -P 4\n\n# JSON output\niperf3 -c server_ip -J\n\n# Test specific port\niperf3 -c server_ip -p 5201\n</code></pre></p>"},{"location":"linux/network/monitor-troubleshoot-networking/#curl-command-for-http-testing","title":"<code>curl</code> Command for HTTP Testing","text":"<pre><code># Basic request with timing\ncurl -w \"@-\" -o /dev/null -s https://example.com &lt;&lt; 'EOF'\n    time_namelookup:  %{time_namelookup}\\n\n       time_connect:  %{time_connect}\\n\n    time_appconnect:  %{time_appconnect}\\n\n   time_pretransfer:  %{time_pretransfer}\\n\n      time_redirect:  %{time_redirect}\\n\n time_starttransfer:  %{time_starttransfer}\\n\n                    ----------\\n\n         time_total:  %{time_total}\\n\nEOF\n\n# Test connection speed\ncurl -o /dev/null https://example.com/file\n\n# Show only headers\ncurl -I https://example.com\n\n# Follow redirects with timing\ncurl -L -w \"Total time: %{time_total}s\\n\" https://example.com\n\n# Test with specific timeout\ncurl --connect-timeout 5 https://example.com\n</code></pre>"},{"location":"linux/network/monitor-troubleshoot-networking/#log-analysis-for-network-issues","title":"Log Analysis for Network Issues","text":""},{"location":"linux/network/monitor-troubleshoot-networking/#system-logs","title":"System Logs","text":"<pre><code># General system logs\njournalctl -xe\n\n# Network-related logs\njournalctl -u NetworkManager\njournalctl -u systemd-networkd\njournalctl -u chronyd\n\n# Follow logs in real-time\njournalctl -f\n\n# Show kernel messages\ndmesg | grep -i eth\ndmesg | grep -i network\ndmesg | tail -50\n\n# Traditional log files\ntail -f /var/log/messages\ntail -f /var/log/syslog\ngrep -i network /var/log/messages\n</code></pre>"},{"location":"linux/network/monitor-troubleshoot-networking/#connection-tracking","title":"Connection Tracking","text":"<pre><code># Show connection tracking table (conntrack)\nconntrack -L\n\n# Show statistics\nconntrack -S\n\n# Monitor new connections\nconntrack -E\n\n# Count connections\nconntrack -C\n</code></pre>"},{"location":"linux/network/monitor-troubleshoot-networking/#bandwidth-monitoring","title":"Bandwidth Monitoring","text":""},{"location":"linux/network/monitor-troubleshoot-networking/#iftop-command","title":"<code>iftop</code> Command","text":"<p>Real-time bandwidth monitoring per connection.</p> <pre><code># Monitor specific interface\niftop -i eth0\n\n# Don't resolve hostnames\niftop -n\n\n# Don't resolve port numbers\niftop -N\n\n# Show ports\niftop -P\n\n# Text mode (no curses)\niftop -t\n\n# Filter by network\niftop -F 192.168.1.0/24\n</code></pre>"},{"location":"linux/network/monitor-troubleshoot-networking/#nethogs-command","title":"<code>nethogs</code> Command","text":"<p>Bandwidth usage per process.</p> <pre><code># Monitor all interfaces\nnethogs\n\n# Monitor specific interface\nnethogs eth0\n\n# Don't resolve hostnames\nnethogs -v 0\n\n# Trace mode (no curses)\nnethogs -t\n</code></pre>"},{"location":"linux/network/monitor-troubleshoot-networking/#vnstat-command","title":"<code>vnstat</code> Command","text":"<p>Network traffic logger and monitor.</p> <pre><code># Show statistics for all interfaces\nvnstat\n\n# Show specific interface\nvnstat -i eth0\n\n# Live monitoring\nvnstat -l -i eth0\n\n# Show hourly stats\nvnstat -h -i eth0\n\n# Show daily stats\nvnstat -d -i eth0\n\n# Show monthly stats\nvnstat -m -i eth0\n\n# Show top days\nvnstat -t -i eth0\n\n# JSON output\nvnstat --json\n\n# Initialize database for interface\nvnstat -u -i eth0\n</code></pre>"},{"location":"linux/network/monitor-troubleshoot-networking/#bmon-command","title":"<code>bmon</code> Command","text":"<p>Bandwidth monitoring with graphical output.</p> <pre><code># Monitor all interfaces\nbmon\n\n# Monitor specific interface\nbmon -p eth0\n\n# Set update interval\nbmon -r 1\n\n# Show bits instead of bytes\nbmon -b\n</code></pre>"},{"location":"linux/network/monitor-troubleshoot-networking/#network-file-systems-monitoring","title":"Network File Systems Monitoring","text":""},{"location":"linux/network/monitor-troubleshoot-networking/#nfsstat-command","title":"<code>nfsstat</code> Command","text":"<pre><code># Show NFS statistics\nnfsstat\n\n# Show client statistics\nnfsstat -c\n\n# Show server statistics\nnfsstat -s\n\n# Show all statistics\nnfsstat -a\n\n# Show statistics with timestamps\nwatch -n 5 nfsstat\n</code></pre>"},{"location":"linux/network/monitor-troubleshoot-networking/#showmount-command","title":"<code>showmount</code> Command","text":"<pre><code># Show NFS exports\nshowmount -e nfs_server\n\n# Show mounted directories\nshowmount -d nfs_server\n\n# Show all mount points\nshowmount -a nfs_server\n</code></pre>"},{"location":"linux/network/monitor-troubleshoot-networking/#wireless-network-monitoring","title":"Wireless Network Monitoring","text":""},{"location":"linux/network/monitor-troubleshoot-networking/#iwconfig-command","title":"<code>iwconfig</code> Command","text":"<pre><code># Show wireless interfaces\niwconfig\n\n# Show specific interface\niwconfig wlan0\n\n# Set wireless parameters\niwconfig wlan0 essid \"NetworkName\"\niwconfig wlan0 key s:password\n</code></pre>"},{"location":"linux/network/monitor-troubleshoot-networking/#iw-command-modern","title":"<code>iw</code> Command (Modern)","text":"<pre><code># Show wireless devices\niw dev\n\n# Show wireless info\niw dev wlan0 info\n\n# Scan for networks\niw dev wlan0 scan\n\n# Show link status\niw dev wlan0 link\n\n# Show station info\niw dev wlan0 station dump\n</code></pre>"},{"location":"linux/network/monitor-troubleshoot-networking/#troubleshooting-workflow","title":"Troubleshooting Workflow","text":""},{"location":"linux/network/monitor-troubleshoot-networking/#step-by-step-network-troubleshooting","title":"Step-by-Step Network Troubleshooting","text":"<ol> <li> <p>Check Physical Layer <pre><code># Check if interface is up\nip link show eth0\n\n# Check cable connection\nethtool eth0 | grep \"Link detected\"\n</code></pre></p> </li> <li> <p>Check IP Configuration <pre><code># Verify IP address\nip addr show eth0\n\n# Check for DHCP lease (if using DHCP)\ndhclient -v eth0\n</code></pre></p> </li> <li> <p>Check Local Connectivity <pre><code># Ping gateway\nping -c 4 192.168.1.1\n\n# Check ARP resolution\nip neigh show\n</code></pre></p> </li> <li> <p>Check DNS Resolution <pre><code># Test DNS\ndig google.com\nnslookup google.com\n\n# Check resolv.conf\ncat /etc/resolv.conf\n</code></pre></p> </li> <li> <p>Check Routing <pre><code># Verify default gateway\nip route show\n\n# Test route to destination\nip route get 8.8.8.8\n\n# Traceroute to destination\ntraceroute 8.8.8.8\n</code></pre></p> </li> <li> <p>Check Remote Connectivity <pre><code># Ping external host\nping -c 4 8.8.8.8\n\n# Test specific service\nnc -vz google.com 443\n</code></pre></p> </li> <li> <p>Check Firewall Rules <pre><code># Check firewall status\nfirewall-cmd --list-all\niptables -L -n -v\n</code></pre></p> </li> <li> <p>Check Services and Ports <pre><code># Check listening ports\nss -tulpn\n\n# Check specific service\nsystemctl status NetworkManager\n</code></pre></p> </li> </ol>"},{"location":"linux/network/monitor-troubleshoot-networking/#common-network-issues-and-solutions","title":"Common Network Issues and Solutions","text":""},{"location":"linux/network/monitor-troubleshoot-networking/#issue-no-network-connectivity","title":"Issue: No Network Connectivity","text":"<pre><code># Check interface status\nip link show\n\n# Bring interface up\nip link set eth0 up\n\n# Restart NetworkManager\nsystemctl restart NetworkManager\n\n# Check for DHCP\ndhclient -v eth0\n</code></pre>"},{"location":"linux/network/monitor-troubleshoot-networking/#issue-dns-not-resolving","title":"Issue: DNS Not Resolving","text":"<pre><code># Check DNS servers\ncat /etc/resolv.conf\n\n# Test DNS manually\ndig @8.8.8.8 google.com\n\n# Flush DNS cache (systemd-resolved)\nresolvectl flush-caches\n\n# Restart DNS service\nsystemctl restart systemd-resolved\n</code></pre>"},{"location":"linux/network/monitor-troubleshoot-networking/#issue-slow-network-performance","title":"Issue: Slow Network Performance","text":"<pre><code># Check interface errors\nip -s link show eth0\n\n# Check bandwidth usage\niftop -i eth0\n\n# Check MTU\nip link show eth0 | grep mtu\n\n# Test bandwidth\niperf3 -c server_ip\n\n# Check for packet loss\nping -c 100 8.8.8.8 | grep loss\n</code></pre>"},{"location":"linux/network/monitor-troubleshoot-networking/#issue-intermittent-connectivity","title":"Issue: Intermittent Connectivity","text":"<pre><code># Monitor in real-time\nmtr google.com\n\n# Check for errors\ndmesg | grep -i eth\n\n# Monitor connections\nwatch -n 1 'ss -s'\n\n# Check logs\njournalctl -u NetworkManager -f\n</code></pre>"},{"location":"linux/network/monitor-troubleshoot-networking/#performance-metrics-to-monitor","title":"Performance Metrics to Monitor","text":"<ol> <li>Bandwidth: Current usage vs. available capacity</li> <li>Latency: Round-trip time (ping)</li> <li>Packet Loss: Percentage of lost packets</li> <li>Throughput: Actual data transfer rate</li> <li>Connection Count: Number of active connections</li> <li>Errors: Interface errors, collisions, drops</li> <li>DNS Resolution Time: Time to resolve hostnames</li> <li>MTU: Maximum transmission unit issues</li> </ol>"},{"location":"linux/network/monitor-troubleshoot-networking/#quick-reference-commands","title":"Quick Reference Commands","text":""},{"location":"linux/network/monitor-troubleshoot-networking/#connectivity","title":"Connectivity","text":"<pre><code>ping -c 4 8.8.8.8             # Test connectivity\ntraceroute google.com         # Trace route\nmtr google.com                # Combined ping/traceroute\n</code></pre>"},{"location":"linux/network/monitor-troubleshoot-networking/#interfaces-and-routing","title":"Interfaces and Routing","text":"<pre><code>ip addr show                  # Show IP addresses\nip link show                  # Show interfaces\nip route show                 # Show routing table\nip route get 8.8.8.8         # Show route to destination\n</code></pre>"},{"location":"linux/network/monitor-troubleshoot-networking/#connections-and-ports","title":"Connections and Ports","text":"<pre><code>ss -tulpn                     # Show listening ports\nss -t state established       # Show established TCP\nnc -vz host 80                # Test port connectivity\nnmap -p 80,443 host           # Scan specific ports\n</code></pre>"},{"location":"linux/network/monitor-troubleshoot-networking/#performance-and-monitoring","title":"Performance and Monitoring","text":"<pre><code>iftop -i eth0                 # Real-time bandwidth\nnethogs                       # Bandwidth per process\nvnstat -l -i eth0             # Live traffic stats\niperf3 -c server              # Bandwidth test\n</code></pre>"},{"location":"linux/network/monitor-troubleshoot-networking/#packet-analysis","title":"Packet Analysis","text":"<pre><code>tcpdump -i eth0 port 80       # Capture HTTP traffic\ntcpdump -nn -i eth0           # Capture without DNS lookup\ntcpdump -r file.pcap          # Read capture file\n</code></pre>"},{"location":"linux/network/monitor-troubleshoot-networking/#dns","title":"DNS","text":"<pre><code>dig google.com                # DNS query\nnslookup google.com           # Simple DNS lookup\nhost google.com               # Quick DNS lookup\n</code></pre>"},{"location":"linux/network/monitor-troubleshoot-networking/#exam-tips","title":"Exam Tips","text":"<ul> <li>Know the difference between <code>ss</code> and <code>netstat</code> (prefer <code>ss</code>)</li> <li>Practice using <code>tcpdump</code> with various filters</li> <li>Understand how to read <code>mtr</code> output</li> <li>Be comfortable with both <code>ip</code> and legacy commands</li> <li>Know how to test connectivity at each OSI layer</li> <li>Practice troubleshooting methodology systematically</li> <li>Understand common port numbers (22, 80, 443, 53, etc.)</li> <li>Know how to interpret network statistics and errors</li> <li>Be able to identify bottlenecks and performance issues</li> <li>Practice reading and analyzing packet captures</li> </ul>"},{"location":"linux/network/openssh-configuration/","title":"Configure OpenSSH Server and Client","text":""},{"location":"linux/network/openssh-configuration/#overview","title":"Overview","text":"<p>This guide covers configuration, management, and security best practices for OpenSSH server (sshd) and client (ssh) on Linux systems.</p>"},{"location":"linux/network/openssh-configuration/#ssh-basics","title":"SSH Basics","text":"<p>SSH (Secure Shell) provides encrypted network communication for: - Remote command execution - Secure file transfer (SCP, SFTP) - Port forwarding and tunneling - X11 forwarding for GUI applications</p> <p>Default port: 22/TCP</p>"},{"location":"linux/network/openssh-configuration/#installation","title":"Installation","text":""},{"location":"linux/network/openssh-configuration/#install-openssh","title":"Install OpenSSH","text":"<pre><code># RHEL/CentOS/Fedora\ndnf install openssh-server openssh-clients\n\n# Ubuntu/Debian\napt install openssh-server openssh-client\n\n# Check if installed\nrpm -qa | grep openssh          # RHEL\ndpkg -l | grep openssh          # Debian\n</code></pre>"},{"location":"linux/network/openssh-configuration/#ssh-server-sshd-configuration","title":"SSH Server (sshd) Configuration","text":""},{"location":"linux/network/openssh-configuration/#service-management","title":"Service Management","text":"<pre><code># Start SSH service\nsystemctl start sshd\n\n# Stop SSH service\nsystemctl stop sshd\n\n# Restart SSH service\nsystemctl restart sshd\n\n# Reload configuration (no connection drop)\nsystemctl reload sshd\n\n# Enable at boot\nsystemctl enable sshd\n\n# Check status\nsystemctl status sshd\n\n# Check if enabled\nsystemctl is-enabled sshd\n\n# View SSH service logs\njournalctl -u sshd\njournalctl -u sshd -f          # Follow logs\n</code></pre>"},{"location":"linux/network/openssh-configuration/#main-configuration-file","title":"Main Configuration File","text":"<p>Location: <code>/etc/ssh/sshd_config</code></p> <p>Important Configuration Directives:</p> <pre><code># Port and address binding\nPort 22                        # Change default port\n#Port 2222                     # Custom port\nListenAddress 0.0.0.0          # Listen on all IPv4\nListenAddress ::               # Listen on all IPv6\n#ListenAddress 192.168.1.100   # Specific IP only\n\n# Protocol version\nProtocol 2                     # Only use SSH protocol 2\n\n# Host keys\nHostKey /etc/ssh/ssh_host_rsa_key\nHostKey /etc/ssh/ssh_host_ecdsa_key\nHostKey /etc/ssh/ssh_host_ed25519_key\n\n# Logging\nSyslogFacility AUTH\nLogLevel INFO                  # QUIET, FATAL, ERROR, INFO, VERBOSE, DEBUG\n\n# Authentication\nPermitRootLogin no             # Disable root login (RECOMMENDED)\n#PermitRootLogin prohibit-password  # Allow only key-based root login\n#PermitRootLogin yes            # Allow all root login (NOT RECOMMENDED)\n\nPubkeyAuthentication yes       # Enable public key authentication\nPasswordAuthentication yes     # Enable password authentication\nPermitEmptyPasswords no        # Disable empty passwords\nChallengeResponseAuthentication no\n\n# PAM authentication\nUsePAM yes\n\n# Kerberos authentication\nKerberosAuthentication no\nKerberosOrLocalPasswd yes\n\n# GSSAPI authentication\nGSSAPIAuthentication no\n\n# Connection settings\nX11Forwarding yes              # Enable X11 forwarding\nX11UseLocalhost yes\nPrintMotd no                   # Don't print /etc/motd\nPrintLastLog yes\nTCPKeepAlive yes\nClientAliveInterval 300        # Send keepalive every 300 seconds\nClientAliveCountMax 3          # Disconnect after 3 failed keepalives\n\n# Login settings\nMaxAuthTries 3                 # Maximum authentication attempts\nMaxSessions 10                 # Maximum sessions per connection\nLoginGraceTime 60              # Time to authenticate (seconds)\n\n# Subsystem\nSubsystem sftp /usr/libexec/openssh/sftp-server\n\n# User/Group restrictions\nAllowUsers user1 user2         # Only these users can connect\n#DenyUsers baduser             # These users cannot connect\nAllowGroups sshusers           # Only members of these groups\n#DenyGroups restricted         # Members cannot connect\n\n# Banner\nBanner /etc/ssh/banner         # Display banner before login\n</code></pre>"},{"location":"linux/network/openssh-configuration/#example-secure-configuration","title":"Example: Secure Configuration","text":"<pre><code># /etc/ssh/sshd_config - Hardened configuration\nPort 2222\nProtocol 2\nListenAddress 0.0.0.0\n\n# Encryption\nCiphers chacha20-poly1305@openssh.com,aes256-gcm@openssh.com,aes128-gcm@openssh.com\nMACs hmac-sha2-512-etm@openssh.com,hmac-sha2-256-etm@openssh.com\nKexAlgorithms curve25519-sha256,curve25519-sha256@libssh.org,diffie-hellman-group-exchange-sha256\n\n# Authentication\nPermitRootLogin no\nPubkeyAuthentication yes\nPasswordAuthentication no\nPermitEmptyPasswords no\nChallengeResponseAuthentication no\nUsePAM yes\n\n# Access control\nAllowGroups sshusers\nMaxAuthTries 3\nMaxSessions 5\nLoginGraceTime 30\n\n# Connection security\nClientAliveInterval 300\nClientAliveCountMax 2\nX11Forwarding no\nPrintMotd no\nTCPKeepAlive yes\n\n# Logging\nLogLevel VERBOSE\nSyslogFacility AUTH\n</code></pre>"},{"location":"linux/network/openssh-configuration/#test-configuration","title":"Test Configuration","text":"<pre><code># Test configuration syntax\nsshd -t\n\n# Test with specific config file\nsshd -t -f /etc/ssh/sshd_config\n\n# Test and show configuration\nsshd -T\n\n# Show configuration for specific user\nsshd -T -C user=john\n</code></pre>"},{"location":"linux/network/openssh-configuration/#reload-configuration","title":"Reload Configuration","text":"<pre><code># Reload without dropping connections\nsystemctl reload sshd\n\n# Or send HUP signal\nkill -HUP $(cat /var/run/sshd.pid)\n</code></pre>"},{"location":"linux/network/openssh-configuration/#ssh-client-configuration","title":"SSH Client Configuration","text":""},{"location":"linux/network/openssh-configuration/#per-user-configuration","title":"Per-User Configuration","text":"<p>Location: <code>~/.ssh/config</code></p> <pre><code># Global defaults\nHost *\n    ServerAliveInterval 60\n    ServerAliveCountMax 3\n    Compression yes\n    ForwardAgent no\n\n# Specific host\nHost webserver\n    HostName web.example.com\n    User admin\n    Port 2222\n    IdentityFile ~/.ssh/web_rsa\n\nHost db\n    HostName 192.168.1.100\n    User dbadmin\n    Port 22\n    IdentityFile ~/.ssh/db_key\n    ForwardAgent yes\n\n# Pattern matching\nHost *.example.com\n    User john\n    IdentityFile ~/.ssh/example_rsa\n\n# Jump host (bastion)\nHost internal-server\n    HostName 10.0.1.100\n    User admin\n    ProxyJump bastion.example.com\n\n# Multiple jump hosts\nHost deep-server\n    HostName 10.0.2.100\n    ProxyJump bastion1.example.com,bastion2.example.com\n</code></pre>"},{"location":"linux/network/openssh-configuration/#system-wide-configuration","title":"System-Wide Configuration","text":"<p>Location: <code>/etc/ssh/ssh_config</code></p>"},{"location":"linux/network/openssh-configuration/#ssh-key-management","title":"SSH Key Management","text":""},{"location":"linux/network/openssh-configuration/#generate-ssh-key-pairs","title":"Generate SSH Key Pairs","text":"<pre><code># Generate RSA key (default, 3072 bits)\nssh-keygen\n\n# Generate RSA with specific bits\nssh-keygen -t rsa -b 4096 -C \"user@email.com\"\n\n# Generate Ed25519 key (recommended, more secure)\nssh-keygen -t ed25519 -C \"user@email.com\"\n\n# Generate ECDSA key\nssh-keygen -t ecdsa -b 521\n\n# Specify file location\nssh-keygen -t ed25519 -f ~/.ssh/custom_key\n\n# Generate without passphrase (not recommended)\nssh-keygen -t ed25519 -N \"\"\n\n# Change passphrase of existing key\nssh-keygen -p -f ~/.ssh/id_ed25519\n</code></pre>"},{"location":"linux/network/openssh-configuration/#key-files","title":"Key Files","text":"<pre><code>~/.ssh/id_rsa          # Private key (RSA)\n~/.ssh/id_rsa.pub      # Public key (RSA)\n~/.ssh/id_ed25519      # Private key (Ed25519)\n~/.ssh/id_ed25519.pub  # Public key (Ed25519)\n~/.ssh/authorized_keys # Authorized public keys\n~/.ssh/known_hosts     # Known host fingerprints\n~/.ssh/config          # Client configuration\n</code></pre>"},{"location":"linux/network/openssh-configuration/#deploy-public-key","title":"Deploy Public Key","text":"<pre><code># Method 1: Using ssh-copy-id (recommended)\nssh-copy-id user@remote-host\nssh-copy-id -i ~/.ssh/id_ed25519.pub user@remote-host\nssh-copy-id -p 2222 user@remote-host\n\n# Method 2: Manual copy\ncat ~/.ssh/id_ed25519.pub | ssh user@remote-host \"mkdir -p ~/.ssh &amp;&amp; cat &gt;&gt; ~/.ssh/authorized_keys\"\n\n# Method 3: Direct append\nssh user@remote-host \"echo '$(cat ~/.ssh/id_ed25519.pub)' &gt;&gt; ~/.ssh/authorized_keys\"\n\n# Method 4: SCP\nscp ~/.ssh/id_ed25519.pub user@remote-host:~/key.pub\nssh user@remote-host \"mkdir -p ~/.ssh &amp;&amp; cat ~/key.pub &gt;&gt; ~/.ssh/authorized_keys &amp;&amp; rm ~/key.pub\"\n</code></pre>"},{"location":"linux/network/openssh-configuration/#correct-permissions","title":"Correct Permissions","text":"<pre><code># On remote server\nchmod 700 ~/.ssh\nchmod 600 ~/.ssh/authorized_keys\nchmod 600 ~/.ssh/id_*\nchmod 644 ~/.ssh/id_*.pub\nchmod 644 ~/.ssh/known_hosts\nchmod 600 ~/.ssh/config\n\n# Fix all at once\nchmod 700 ~/.ssh &amp;&amp; chmod 600 ~/.ssh/* &amp;&amp; chmod 644 ~/.ssh/*.pub\n</code></pre>"},{"location":"linux/network/openssh-configuration/#manage-authorized-keys","title":"Manage Authorized Keys","text":"<pre><code># Add key to authorized_keys\necho \"ssh-ed25519 AAAA...\" &gt;&gt; ~/.ssh/authorized_keys\n\n# View authorized keys\ncat ~/.ssh/authorized_keys\n\n# Remove specific key (edit file)\nvim ~/.ssh/authorized_keys\n\n# Limit key usage (prepend to key in authorized_keys)\nfrom=\"192.168.1.0/24\" ssh-ed25519 AAAA...\ncommand=\"/usr/local/bin/backup.sh\" ssh-ed25519 AAAA...\nno-port-forwarding,no-X11-forwarding ssh-ed25519 AAAA...\n</code></pre>"},{"location":"linux/network/openssh-configuration/#ssh-agent","title":"SSH Agent","text":"<pre><code># Start SSH agent\neval $(ssh-agent)\n\n# Add key to agent\nssh-add ~/.ssh/id_ed25519\n\n# Add with timeout (seconds)\nssh-add -t 3600 ~/.ssh/id_ed25519\n\n# List loaded keys\nssh-add -l\n\n# List loaded keys with full public key\nssh-add -L\n\n# Remove specific key\nssh-add -d ~/.ssh/id_ed25519\n\n# Remove all keys\nssh-add -D\n\n# Kill agent\nssh-agent -k\n</code></pre>"},{"location":"linux/network/openssh-configuration/#connecting-with-ssh","title":"Connecting with SSH","text":""},{"location":"linux/network/openssh-configuration/#basic-connection","title":"Basic Connection","text":"<pre><code># Connect to host\nssh user@hostname\n\n# Connect to specific port\nssh -p 2222 user@hostname\n\n# Connect using specific key\nssh -i ~/.ssh/custom_key user@hostname\n\n# Connect with verbose output\nssh -v user@hostname          # Level 1\nssh -vv user@hostname         # Level 2\nssh -vvv user@hostname        # Level 3 (most verbose)\n\n# Connect and execute command\nssh user@hostname \"ls -la /tmp\"\n\n# Connect and execute multiple commands\nssh user@hostname \"uptime; df -h; free -m\"\n\n# Using config alias\nssh webserver                 # Uses ~/.ssh/config\n</code></pre>"},{"location":"linux/network/openssh-configuration/#advanced-connection-options","title":"Advanced Connection Options","text":"<pre><code># Disable strict host key checking (use carefully)\nssh -o StrictHostKeyChecking=no user@hostname\n\n# Specify cipher\nssh -c aes256-gcm@openssh.com user@hostname\n\n# Enable compression\nssh -C user@hostname\n\n# Allocate pseudo-TTY\nssh -t user@hostname\n\n# No pseudo-TTY\nssh -T user@hostname\n\n# Run in background\nssh -f user@hostname \"sleep 10; command\"\n\n# Keep connection alive\nssh -o ServerAliveInterval=60 user@hostname\n\n# X11 forwarding\nssh -X user@hostname\nssh -Y user@hostname          # Trusted X11 forwarding\n</code></pre>"},{"location":"linux/network/openssh-configuration/#jump-hosts-bastion","title":"Jump Hosts (Bastion)","text":"<pre><code># Connect through jump host\nssh -J jump-host user@target-host\n\n# Multiple jump hosts\nssh -J jump1,jump2 user@target\n\n# Alternative syntax\nssh -o ProxyJump=jump-host user@target\n\n# With different users\nssh -J jumpuser@jump-host targetuser@target\n</code></pre>"},{"location":"linux/network/openssh-configuration/#ssh-tunneling-and-port-forwarding","title":"SSH Tunneling and Port Forwarding","text":""},{"location":"linux/network/openssh-configuration/#local-port-forwarding","title":"Local Port Forwarding","text":"<p>Forward local port to remote destination.</p> <pre><code># Basic syntax: ssh -L local_port:destination:destination_port user@ssh_server\n\n# Forward local port 8080 to remote localhost:80\nssh -L 8080:localhost:80 user@remote-host\n\n# Access remote database locally\nssh -L 3306:localhost:3306 user@database-server\n# Now connect to localhost:3306 locally\n\n# Forward to third host through SSH server\nssh -L 8080:internal-web:80 user@gateway\n\n# Multiple port forwards\nssh -L 8080:web:80 -L 3306:db:3306 user@gateway\n\n# Bind to specific interface\nssh -L 192.168.1.100:8080:localhost:80 user@remote\n</code></pre>"},{"location":"linux/network/openssh-configuration/#remote-port-forwarding","title":"Remote Port Forwarding","text":"<p>Forward remote port to local destination.</p> <pre><code># Basic syntax: ssh -R remote_port:destination:destination_port user@ssh_server\n\n# Expose local web server to remote\nssh -R 8080:localhost:80 user@remote-host\n# Remote host can access your local web server on port 8080\n\n# Allow remote server to forward to internal network\nssh -R 3306:database:3306 user@remote\n\n# Bind to all remote interfaces (requires GatewayPorts yes in sshd_config)\nssh -R 0.0.0.0:8080:localhost:80 user@remote\n</code></pre>"},{"location":"linux/network/openssh-configuration/#dynamic-port-forwarding-socks-proxy","title":"Dynamic Port Forwarding (SOCKS Proxy)","text":"<p>Create a SOCKS proxy for all traffic.</p> <pre><code># Create SOCKS proxy on local port 1080\nssh -D 1080 user@remote-host\n\n# Use specific bind address\nssh -D 127.0.0.1:1080 user@remote\n\n# Configure browser or application to use SOCKS proxy:\n# Host: localhost, Port: 1080, SOCKS v5\n\n# Test SOCKS proxy\ncurl --socks5 localhost:1080 http://example.com\n</code></pre>"},{"location":"linux/network/openssh-configuration/#combined-tunneling","title":"Combined Tunneling","text":"<pre><code># Local + Dynamic forwarding\nssh -L 8080:web:80 -D 1080 user@gateway\n\n# Background with no shell\nssh -fNL 8080:localhost:80 user@remote\n\n# Options explained:\n# -f: Background\n# -N: No command execution\n# -L: Local forward\n# -R: Remote forward\n# -D: Dynamic forward\n</code></pre>"},{"location":"linux/network/openssh-configuration/#ssh-tunnel-as-daemon","title":"SSH Tunnel as Daemon","text":"<pre><code># Create persistent tunnel\nssh -fNL 8080:localhost:80 user@remote\n\n# With auto-reconnect in cron\n*/5 * * * * pgrep -f \"ssh -fNL\" || ssh -fNL 8080:localhost:80 user@remote\n\n# Using autossh (better for persistent tunnels)\nautossh -M 0 -fNL 8080:localhost:80 user@remote\n</code></pre>"},{"location":"linux/network/openssh-configuration/#file-transfer-with-ssh","title":"File Transfer with SSH","text":""},{"location":"linux/network/openssh-configuration/#scp-secure-copy","title":"SCP (Secure Copy)","text":"<pre><code># Copy file to remote\nscp file.txt user@remote:/path/to/destination\n\n# Copy file from remote\nscp user@remote:/path/to/file.txt /local/destination\n\n# Copy directory recursively\nscp -r /local/dir user@remote:/remote/dir\n\n# Copy with specific port\nscp -P 2222 file.txt user@remote:/path\n\n# Copy with compression\nscp -C large_file.txt user@remote:/path\n\n# Preserve permissions and timestamps\nscp -p file.txt user@remote:/path\n\n# Copy between two remote hosts\nscp user1@remote1:/file user2@remote2:/path\n\n# Limit bandwidth (in Kbps)\nscp -l 1000 large_file user@remote:/path\n\n# Verbose output\nscp -v file.txt user@remote:/path\n</code></pre>"},{"location":"linux/network/openssh-configuration/#sftp-ssh-file-transfer-protocol","title":"SFTP (SSH File Transfer Protocol)","text":"<pre><code># Connect to SFTP server\nsftp user@remote-host\n\n# Connect with specific port\nsftp -P 2222 user@remote\n\n# SFTP interactive commands\nsftp&gt; ls                      # List remote directory\nsftp&gt; lls                     # List local directory\nsftp&gt; pwd                     # Print remote working directory\nsftp&gt; lpwd                    # Print local working directory\nsftp&gt; cd /remote/path         # Change remote directory\nsftp&gt; lcd /local/path         # Change local directory\nsftp&gt; get file.txt            # Download file\nsftp&gt; get -r directory/       # Download directory\nsftp&gt; put file.txt            # Upload file\nsftp&gt; put -r directory/       # Upload directory\nsftp&gt; mkdir newdir            # Create remote directory\nsftp&gt; rmdir directory         # Remove remote directory\nsftp&gt; rm file.txt             # Delete remote file\nsftp&gt; rename old.txt new.txt  # Rename remote file\nsftp&gt; chmod 755 script.sh     # Change permissions\nsftp&gt; exit                    # Quit\n\n# Batch mode with command file\necho \"get file.txt\" | sftp user@remote\n\n# Execute single command\nsftp user@remote &lt;&lt;&lt; \"get /remote/file.txt\"\n</code></pre>"},{"location":"linux/network/openssh-configuration/#rsync-over-ssh","title":"rsync over SSH","text":"<pre><code># Basic sync\nrsync -avz /local/path/ user@remote:/remote/path/\n\n# Sync with specific SSH port\nrsync -avz -e \"ssh -p 2222\" /local/ user@remote:/remote/\n\n# Sync with progress\nrsync -avz --progress /local/ user@remote:/remote/\n\n# Dry run (test without changes)\nrsync -avz --dry-run /local/ user@remote:/remote/\n\n# Delete files in destination not in source\nrsync -avz --delete /local/ user@remote:/remote/\n\n# Exclude files\nrsync -avz --exclude='*.log' /local/ user@remote:/remote/\n\n# Options explained:\n# -a: Archive mode (preserves permissions, times, etc.)\n# -v: Verbose\n# -z: Compress\n# -e: Specify SSH command\n</code></pre>"},{"location":"linux/network/openssh-configuration/#ssh-security-best-practices","title":"SSH Security Best Practices","text":""},{"location":"linux/network/openssh-configuration/#1-disable-root-login","title":"1. Disable Root Login","text":"<pre><code># In /etc/ssh/sshd_config\nPermitRootLogin no\n</code></pre>"},{"location":"linux/network/openssh-configuration/#2-use-key-based-authentication","title":"2. Use Key-Based Authentication","text":"<pre><code># Generate strong key\nssh-keygen -t ed25519 -C \"user@email.com\"\n\n# Disable password authentication after deploying keys\nPasswordAuthentication no\n</code></pre>"},{"location":"linux/network/openssh-configuration/#3-change-default-port","title":"3. Change Default Port","text":"<pre><code># In /etc/ssh/sshd_config\nPort 2222\n\n# Update firewall\nfirewall-cmd --permanent --add-port=2222/tcp\nfirewall-cmd --reload\n</code></pre>"},{"location":"linux/network/openssh-configuration/#4-limit-user-access","title":"4. Limit User Access","text":"<pre><code># In /etc/ssh/sshd_config\nAllowUsers john jane admin\n# Or use groups\nAllowGroups sshusers\n</code></pre>"},{"location":"linux/network/openssh-configuration/#5-use-fail2ban","title":"5. Use fail2ban","text":"<pre><code># Install fail2ban\ndnf install fail2ban\n\n# Configure for SSH\n# /etc/fail2ban/jail.local\n[sshd]\nenabled = true\nport = ssh\nfilter = sshd\nlogpath = /var/log/secure\nmaxretry = 3\nbantime = 3600\n</code></pre>"},{"location":"linux/network/openssh-configuration/#6-two-factor-authentication","title":"6. Two-Factor Authentication","text":"<pre><code># Install Google Authenticator PAM module\ndnf install google-authenticator\n\n# Configure PAM\n# Add to /etc/pam.d/sshd\nauth required pam_google_authenticator.so\n\n# Configure sshd_config\nChallengeResponseAuthentication yes\nAuthenticationMethods publickey,keyboard-interactive\n</code></pre>"},{"location":"linux/network/openssh-configuration/#7-restrict-ssh-protocol","title":"7. Restrict SSH Protocol","text":"<pre><code># Only use protocol 2\nProtocol 2\n</code></pre>"},{"location":"linux/network/openssh-configuration/#8-set-login-grace-time","title":"8. Set Login Grace Time","text":"<pre><code># Limit time to authenticate\nLoginGraceTime 30\n</code></pre>"},{"location":"linux/network/openssh-configuration/#9-use-tcp-wrappers","title":"9. Use TCP Wrappers","text":"<pre><code># /etc/hosts.allow\nsshd: 192.168.1.0/24\n\n# /etc/hosts.deny\nsshd: ALL\n</code></pre>"},{"location":"linux/network/openssh-configuration/#10-configure-idle-timeout","title":"10. Configure Idle Timeout","text":"<pre><code># In /etc/ssh/sshd_config\nClientAliveInterval 300\nClientAliveCountMax 2\n</code></pre>"},{"location":"linux/network/openssh-configuration/#troubleshooting-ssh","title":"Troubleshooting SSH","text":""},{"location":"linux/network/openssh-configuration/#check-ssh-service","title":"Check SSH Service","text":"<pre><code># Service status\nsystemctl status sshd\n\n# Check if listening\nss -tlnp | grep ssh\nnetstat -tlnp | grep ssh\n\n# Test configuration\nsshd -t\n\n# View logs\njournalctl -u sshd -f\ntail -f /var/log/secure    # RHEL\ntail -f /var/log/auth.log  # Ubuntu\n</code></pre>"},{"location":"linux/network/openssh-configuration/#verbose-connection-testing","title":"Verbose Connection Testing","text":"<pre><code># Client-side debugging\nssh -vvv user@host\n\n# Look for:\n# - Key exchange\n# - Authentication attempts\n# - Cipher negotiation\n# - Connection errors\n</code></pre>"},{"location":"linux/network/openssh-configuration/#common-issues","title":"Common Issues","text":""},{"location":"linux/network/openssh-configuration/#connection-refused","title":"Connection Refused","text":"<pre><code># Check if service running\nsystemctl status sshd\n\n# Check firewall\nfirewall-cmd --list-all\niptables -L -n\n\n# Check if listening on correct interface\nss -tlnp | grep :22\n</code></pre>"},{"location":"linux/network/openssh-configuration/#permission-denied-publickey","title":"Permission Denied (publickey)","text":"<pre><code># Check key permissions\nls -la ~/.ssh/\n\n# Should be:\n# 700 for ~/.ssh/\n# 600 for private keys\n# 644 for public keys\n\n# Check server logs\ntail -f /var/log/secure\n\n# Verify key is in authorized_keys\ncat ~/.ssh/authorized_keys\n\n# Test with password (if enabled)\nssh -o PubkeyAuthentication=no user@host\n</code></pre>"},{"location":"linux/network/openssh-configuration/#host-key-verification-failed","title":"Host Key Verification Failed","text":"<pre><code># Remove old key\nssh-keygen -R hostname\n\n# Or edit known_hosts\nvim ~/.ssh/known_hosts\n\n# Accept new key\nssh -o StrictHostKeyChecking=no user@host\n</code></pre>"},{"location":"linux/network/openssh-configuration/#too-many-authentication-failures","title":"Too Many Authentication Failures","text":"<pre><code># Limit keys offered\nssh -o IdentitiesOnly=yes -i ~/.ssh/specific_key user@host\n\n# Or configure in ~/.ssh/config\nHost problem-host\n    IdentitiesOnly yes\n    IdentityFile ~/.ssh/specific_key\n</code></pre>"},{"location":"linux/network/openssh-configuration/#firewall-configuration","title":"Firewall Configuration","text":""},{"location":"linux/network/openssh-configuration/#firewalld","title":"firewalld","text":"<pre><code># Allow SSH\nfirewall-cmd --permanent --add-service=ssh\nfirewall-cmd --reload\n\n# Allow custom SSH port\nfirewall-cmd --permanent --add-port=2222/tcp\nfirewall-cmd --reload\n\n# Allow from specific source\nfirewall-cmd --permanent --add-rich-rule='rule family=\"ipv4\" source address=\"192.168.1.0/24\" service name=\"ssh\" accept'\nfirewall-cmd --reload\n</code></pre>"},{"location":"linux/network/openssh-configuration/#iptables","title":"iptables","text":"<pre><code># Allow SSH\niptables -A INPUT -p tcp --dport 22 -j ACCEPT\n\n# Allow from specific source\niptables -A INPUT -p tcp -s 192.168.1.0/24 --dport 22 -j ACCEPT\n\n# Save rules\niptables-save &gt; /etc/sysconfig/iptables\n</code></pre>"},{"location":"linux/network/openssh-configuration/#quick-reference-commands","title":"Quick Reference Commands","text":""},{"location":"linux/network/openssh-configuration/#service-management_1","title":"Service Management","text":"<pre><code>systemctl start/stop/restart sshd    # Manage service\nsystemctl enable sshd                 # Enable at boot\nsshd -t                               # Test configuration\n</code></pre>"},{"location":"linux/network/openssh-configuration/#key-management","title":"Key Management","text":"<pre><code>ssh-keygen -t ed25519                 # Generate key\nssh-copy-id user@host                 # Deploy key\nssh-add                               # Add to agent\n</code></pre>"},{"location":"linux/network/openssh-configuration/#connecting","title":"Connecting","text":"<pre><code>ssh user@host                         # Basic connection\nssh -p 2222 user@host                 # Custom port\nssh -i key user@host                  # Specific key\nssh -J jump user@target               # Jump host\n</code></pre>"},{"location":"linux/network/openssh-configuration/#port-forwarding","title":"Port Forwarding","text":"<pre><code>ssh -L 8080:dest:80 user@host        # Local forward\nssh -R 8080:dest:80 user@host        # Remote forward\nssh -D 1080 user@host                 # SOCKS proxy\n</code></pre>"},{"location":"linux/network/openssh-configuration/#file-transfer","title":"File Transfer","text":"<pre><code>scp file user@host:/path              # Copy file\nsftp user@host                        # Interactive transfer\nrsync -avz /src/ user@host:/dst/     # Sync directories\n</code></pre>"},{"location":"linux/network/openssh-configuration/#exam-tips","title":"Exam Tips","text":"<ul> <li>Know both server (<code>sshd_config</code>) and client (<code>ssh_config</code>) configuration</li> <li>Understand key-based authentication setup and troubleshooting</li> <li>Practice port forwarding scenarios (local, remote, dynamic)</li> <li>Know how to secure SSH (disable root, change port, keys only)</li> <li>Understand file permissions for SSH directories and files</li> <li>Be comfortable with <code>ssh-keygen</code>, <code>ssh-copy-id</code>, and <code>ssh-agent</code></li> <li>Know how to test SSH configuration without breaking access</li> <li>Practice troubleshooting with verbose output (<code>-vvv</code>)</li> <li>Understand TCP wrappers and firewall configuration for SSH</li> <li>Know the difference between SCP, SFTP, and rsync</li> </ul>"},{"location":"linux/network/packet-filtering-nat/","title":"Configure Packet Filtering, Port Redirection, and NAT","text":""},{"location":"linux/network/packet-filtering-nat/#overview","title":"Overview","text":"<p>This guide covers firewall configuration using firewalld and iptables, including packet filtering, port forwarding (redirection), and Network Address Translation (NAT) in Linux.</p>"},{"location":"linux/network/packet-filtering-nat/#firewall-concepts","title":"Firewall Concepts","text":""},{"location":"linux/network/packet-filtering-nat/#key-terms","title":"Key Terms","text":"<ul> <li>Packet Filtering: Controlling network traffic based on rules</li> <li>Stateful Firewall: Tracks connection state</li> <li>Stateless Firewall: Examines packets individually</li> <li>NAT: Network Address Translation (translates IP addresses)</li> <li>Port Forwarding: Redirects traffic from one port to another</li> <li>Masquerading: Dynamic source NAT (SNAT) for outbound traffic</li> <li>DNAT: Destination NAT (port forwarding, load balancing)</li> <li>SNAT: Source NAT (masquerading, explicit source translation)</li> </ul>"},{"location":"linux/network/packet-filtering-nat/#netfilteriptables-tables","title":"Netfilter/iptables Tables","text":"<ul> <li>filter: Default table for packet filtering (INPUT, OUTPUT, FORWARD)</li> <li>nat: Network address translation (PREROUTING, POSTROUTING, OUTPUT)</li> <li>mangle: Packet alteration (all chains)</li> <li>raw: Connection tracking exemption (PREROUTING, OUTPUT)</li> </ul>"},{"location":"linux/network/packet-filtering-nat/#iptables-chains","title":"iptables Chains","text":"<ul> <li>INPUT: Incoming packets destined for local system</li> <li>OUTPUT: Outgoing packets from local system</li> <li>FORWARD: Packets being routed through the system</li> <li>PREROUTING: Packets before routing decision (DNAT)</li> <li>POSTROUTING: Packets after routing decision (SNAT/Masquerading)</li> </ul>"},{"location":"linux/network/packet-filtering-nat/#firewalld-modern-firewall-management","title":"firewalld (Modern Firewall Management)","text":""},{"location":"linux/network/packet-filtering-nat/#overview_1","title":"Overview","text":"<p>firewalld is a dynamic firewall daemon that uses zones and services for easier management.</p>"},{"location":"linux/network/packet-filtering-nat/#installation-and-service-management","title":"Installation and Service Management","text":"<pre><code># Install firewalld\ndnf install firewalld\n\n# Start service\nsystemctl start firewalld\n\n# Enable at boot\nsystemctl enable firewalld\n\n# Check status\nsystemctl status firewalld\nfirewall-cmd --state\n\n# Stop firewall\nsystemctl stop firewalld\n\n# Disable at boot\nsystemctl disable firewalld\n</code></pre>"},{"location":"linux/network/packet-filtering-nat/#zones-concept","title":"Zones Concept","text":"<p>Zones define trust levels for network connections.</p> <pre><code># List all zones\nfirewall-cmd --get-zones\n\n# List active zones\nfirewall-cmd --get-active-zones\n\n# Get default zone\nfirewall-cmd --get-default-zone\n\n# Set default zone\nfirewall-cmd --set-default-zone=public\n\n# List configuration for zone\nfirewall-cmd --zone=public --list-all\n\n# List configuration for all zones\nfirewall-cmd --list-all-zones\n</code></pre> <p>Common zones: - drop: Drop all incoming, allow outgoing - block: Reject all incoming, allow outgoing - public: Public networks (default) - external: External networks with masquerading - dmz: DMZ (limited access) - work: Work networks - home: Home networks - internal: Internal networks - trusted: Trust all connections</p>"},{"location":"linux/network/packet-filtering-nat/#basic-firewall-cmd-operations","title":"Basic firewall-cmd Operations","text":""},{"location":"linux/network/packet-filtering-nat/#view-current-configuration","title":"View Current Configuration","text":"<pre><code># Show all settings\nfirewall-cmd --list-all\n\n# Show settings for specific zone\nfirewall-cmd --zone=public --list-all\n\n# List services\nfirewall-cmd --list-services\n\n# List ports\nfirewall-cmd --list-ports\n\n# List rich rules\nfirewall-cmd --list-rich-rules\n\n# List all\nfirewall-cmd --list-all-zones\n</code></pre>"},{"location":"linux/network/packet-filtering-nat/#runtime-vs-permanent-changes","title":"Runtime vs Permanent Changes","text":"<pre><code># Runtime change (lost on reload/reboot)\nfirewall-cmd --add-service=http\n\n# Permanent change (survives reload/reboot)\nfirewall-cmd --permanent --add-service=http\n\n# Apply both runtime and permanent\nfirewall-cmd --add-service=http\nfirewall-cmd --permanent --add-service=http\n\n# Reload to apply permanent changes\nfirewall-cmd --reload\n\n# Complete restart\nfirewall-cmd --complete-reload\n</code></pre>"},{"location":"linux/network/packet-filtering-nat/#managing-services","title":"Managing Services","text":""},{"location":"linux/network/packet-filtering-nat/#predefined-services","title":"Predefined Services","text":"<pre><code># List available services\nfirewall-cmd --get-services\n\n# Add service\nfirewall-cmd --permanent --add-service=http\nfirewall-cmd --permanent --add-service=https\nfirewall-cmd --permanent --add-service=ssh\n\n# Add multiple services\nfirewall-cmd --permanent --add-service={http,https,ssh}\n\n# Remove service\nfirewall-cmd --permanent --remove-service=http\n\n# Query service\nfirewall-cmd --query-service=http\n</code></pre>"},{"location":"linux/network/packet-filtering-nat/#custom-services","title":"Custom Services","text":"<p>Service definitions: <code>/etc/firewalld/services/</code></p> <pre><code># Create custom service file\n# /etc/firewalld/services/myapp.xml\n&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\n&lt;service&gt;\n  &lt;short&gt;MyApp&lt;/short&gt;\n  &lt;description&gt;My Application Service&lt;/description&gt;\n  &lt;port protocol=\"tcp\" port=\"8080\"/&gt;\n  &lt;port protocol=\"udp\" port=\"8080\"/&gt;\n&lt;/service&gt;\n\n# Reload firewalld\nfirewall-cmd --reload\n\n# Add custom service\nfirewall-cmd --permanent --add-service=myapp\nfirewall-cmd --reload\n</code></pre>"},{"location":"linux/network/packet-filtering-nat/#managing-ports","title":"Managing Ports","text":"<pre><code># Add single port\nfirewall-cmd --permanent --add-port=8080/tcp\n\n# Add port range\nfirewall-cmd --permanent --add-port=8000-8100/tcp\n\n# Add multiple ports\nfirewall-cmd --permanent --add-port={80/tcp,443/tcp,8080/tcp}\n\n# Remove port\nfirewall-cmd --permanent --remove-port=8080/tcp\n\n# Query port\nfirewall-cmd --query-port=8080/tcp\n\n# Add UDP port\nfirewall-cmd --permanent --add-port=53/udp\n</code></pre>"},{"location":"linux/network/packet-filtering-nat/#source-based-filtering","title":"Source-Based Filtering","text":"<pre><code># Allow traffic from specific source\nfirewall-cmd --permanent --add-source=192.168.1.0/24\n\n# Add source to specific zone\nfirewall-cmd --permanent --zone=trusted --add-source=10.0.0.0/8\n\n# Remove source\nfirewall-cmd --permanent --remove-source=192.168.1.100\n\n# Query source\nfirewall-cmd --query-source=192.168.1.0/24\n\n# Change zone for interface\nfirewall-cmd --zone=public --change-interface=eth0\n</code></pre>"},{"location":"linux/network/packet-filtering-nat/#rich-rules","title":"Rich Rules","text":"<p>More complex firewall rules with additional options.</p> <pre><code># Allow SSH from specific IP\nfirewall-cmd --permanent --add-rich-rule='rule family=\"ipv4\" source address=\"192.168.1.100\" service name=\"ssh\" accept'\n\n# Allow HTTP from subnet\nfirewall-cmd --permanent --add-rich-rule='rule family=\"ipv4\" source address=\"192.168.1.0/24\" service name=\"http\" accept'\n\n# Reject traffic from IP\nfirewall-cmd --permanent --add-rich-rule='rule family=\"ipv4\" source address=\"10.0.0.5\" reject'\n\n# Drop traffic from IP\nfirewall-cmd --permanent --add-rich-rule='rule family=\"ipv4\" source address=\"10.0.0.5\" drop'\n\n# Log accepted packets\nfirewall-cmd --permanent --add-rich-rule='rule family=\"ipv4\" source address=\"192.168.1.0/24\" service name=\"ssh\" log prefix=\"SSH-ACCESS\" level=\"info\" accept'\n\n# Rate limiting (prevent DoS)\nfirewall-cmd --permanent --add-rich-rule='rule service name=\"ssh\" accept limit value=\"3/m\"'\n\n# Port-based rule\nfirewall-cmd --permanent --add-rich-rule='rule family=\"ipv4\" source address=\"192.168.1.0/24\" port port=\"8080\" protocol=\"tcp\" accept'\n\n# Time-based rule (not commonly supported)\nfirewall-cmd --permanent --add-rich-rule='rule family=\"ipv4\" source address=\"192.168.1.0/24\" service name=\"http\" accept'\n\n# Remove rich rule\nfirewall-cmd --permanent --remove-rich-rule='rule family=\"ipv4\" source address=\"192.168.1.100\" service name=\"ssh\" accept'\n\n# List rich rules\nfirewall-cmd --list-rich-rules\n</code></pre>"},{"location":"linux/network/packet-filtering-nat/#port-forwarding-port-redirection","title":"Port Forwarding (Port Redirection)","text":""},{"location":"linux/network/packet-filtering-nat/#local-port-forwarding","title":"Local Port Forwarding","text":"<pre><code># Forward port 80 to 8080 on same system\nfirewall-cmd --permanent --add-forward-port=port=80:proto=tcp:toport=8080\n\n# Forward to different host\nfirewall-cmd --permanent --add-forward-port=port=80:proto=tcp:toaddr=192.168.1.100:toport=8080\n\n# Forward with source filtering\nfirewall-cmd --permanent --zone=public --add-rich-rule='rule family=\"ipv4\" source address=\"192.168.1.0/24\" forward-port port=\"80\" protocol=\"tcp\" to-port=\"8080\"'\n\n# Remove port forward\nfirewall-cmd --permanent --remove-forward-port=port=80:proto=tcp:toport=8080\n</code></pre>"},{"location":"linux/network/packet-filtering-nat/#masquerading-nat","title":"Masquerading (NAT)","text":"<pre><code># Enable masquerading (SNAT for outbound traffic)\nfirewall-cmd --permanent --zone=public --add-masquerade\n\n# Disable masquerading\nfirewall-cmd --permanent --zone=public --remove-masquerade\n\n# Query masquerading\nfirewall-cmd --zone=public --query-masquerade\n\n# Common use case: Internet gateway\n# External interface (public zone) with masquerading enabled\nfirewall-cmd --permanent --zone=public --add-interface=eth0\nfirewall-cmd --permanent --zone=public --add-masquerade\n\n# Internal interface (internal zone)\nfirewall-cmd --permanent --zone=internal --add-interface=eth1\n</code></pre>"},{"location":"linux/network/packet-filtering-nat/#direct-rules","title":"Direct Rules","text":"<p>Raw iptables rules within firewalld.</p> <pre><code># Add direct rule\nfirewall-cmd --permanent --direct --add-rule ipv4 filter INPUT 0 -p tcp --dport 9000 -j ACCEPT\n\n# Remove direct rule\nfirewall-cmd --permanent --direct --remove-rule ipv4 filter INPUT 0 -p tcp --dport 9000 -j ACCEPT\n\n# List direct rules\nfirewall-cmd --direct --get-all-rules\n</code></pre>"},{"location":"linux/network/packet-filtering-nat/#interface-management","title":"Interface Management","text":"<pre><code># Add interface to zone\nfirewall-cmd --permanent --zone=public --add-interface=eth0\n\n# Change interface zone\nfirewall-cmd --zone=internal --change-interface=eth1\n\n# Remove interface from zone\nfirewall-cmd --permanent --zone=public --remove-interface=eth0\n\n# Query interface\nfirewall-cmd --get-zone-of-interface=eth0\n</code></pre>"},{"location":"linux/network/packet-filtering-nat/#panic-mode","title":"Panic Mode","text":"<p>Emergency mode that blocks all traffic.</p> <pre><code># Enable panic mode\nfirewall-cmd --panic-on\n\n# Disable panic mode\nfirewall-cmd --panic-off\n\n# Query panic mode\nfirewall-cmd --query-panic\n</code></pre>"},{"location":"linux/network/packet-filtering-nat/#iptables-traditional-firewall","title":"iptables (Traditional Firewall)","text":""},{"location":"linux/network/packet-filtering-nat/#installation","title":"Installation","text":"<pre><code># Install iptables\ndnf install iptables iptables-services\n\n# Stop firewalld (conflicts with iptables)\nsystemctl stop firewalld\nsystemctl disable firewalld\nsystemctl mask firewalld\n\n# Enable iptables\nsystemctl start iptables\nsystemctl enable iptables\n</code></pre>"},{"location":"linux/network/packet-filtering-nat/#basic-iptables-syntax","title":"Basic iptables Syntax","text":"<pre><code>iptables [-t table] COMMAND CHAIN PARAMETERS -j TARGET\n</code></pre>"},{"location":"linux/network/packet-filtering-nat/#viewing-rules","title":"Viewing Rules","text":"<pre><code># List all rules\niptables -L\n\n# List with line numbers\niptables -L --line-numbers\n\n# List with verbose output\niptables -L -v\n\n# List with numeric output (no DNS)\niptables -L -n\n\n# List specific chain\niptables -L INPUT\n\n# List specific table\niptables -t nat -L\n\n# List all with packet counts\niptables -L -v -n\n\n# Show rules as commands\niptables-save\n</code></pre>"},{"location":"linux/network/packet-filtering-nat/#basic-packet-filtering","title":"Basic Packet Filtering","text":""},{"location":"linux/network/packet-filtering-nat/#allowdeny-traffic","title":"Allow/Deny Traffic","text":"<pre><code># Accept all incoming HTTP\niptables -A INPUT -p tcp --dport 80 -j ACCEPT\n\n# Accept all incoming HTTPS\niptables -A INPUT -p tcp --dport 443 -j ACCEPT\n\n# Accept SSH\niptables -A INPUT -p tcp --dport 22 -j ACCEPT\n\n# Accept from specific IP\niptables -A INPUT -s 192.168.1.100 -j ACCEPT\n\n# Accept from subnet\niptables -A INPUT -s 192.168.1.0/24 -j ACCEPT\n\n# Drop from specific IP\niptables -A INPUT -s 10.0.0.5 -j DROP\n\n# Reject from specific IP\niptables -A INPUT -s 10.0.0.5 -j REJECT\n\n# Accept on specific interface\niptables -A INPUT -i eth0 -p tcp --dport 80 -j ACCEPT\n\n# Accept established connections\niptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT\n\n# Accept loopback\niptables -A INPUT -i lo -j ACCEPT\n</code></pre>"},{"location":"linux/network/packet-filtering-nat/#default-policies","title":"Default Policies","text":"<pre><code># Set default policies\niptables -P INPUT DROP\niptables -P FORWARD DROP\niptables -P OUTPUT ACCEPT\n\n# View policies\niptables -L | grep policy\n</code></pre>"},{"location":"linux/network/packet-filtering-nat/#common-rule-patterns","title":"Common Rule Patterns","text":"<pre><code># Allow SSH with rate limiting\niptables -A INPUT -p tcp --dport 22 -m state --state NEW -m recent --set\niptables -A INPUT -p tcp --dport 22 -m state --state NEW -m recent --update --seconds 60 --hitcount 4 -j DROP\niptables -A INPUT -p tcp --dport 22 -j ACCEPT\n\n# Allow ICMP (ping)\niptables -A INPUT -p icmp --icmp-type echo-request -j ACCEPT\n\n# Allow DNS\niptables -A INPUT -p udp --dport 53 -j ACCEPT\niptables -A INPUT -p tcp --dport 53 -j ACCEPT\n\n# Allow established and related\niptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT\n\n# Log dropped packets\niptables -A INPUT -j LOG --log-prefix \"IPTABLES-DROPPED: \" --log-level 4\niptables -A INPUT -j DROP\n</code></pre>"},{"location":"linux/network/packet-filtering-nat/#deleting-rules","title":"Deleting Rules","text":"<pre><code># Delete by specification\niptables -D INPUT -p tcp --dport 80 -j ACCEPT\n\n# Delete by line number\niptables -D INPUT 5\n\n# Delete all rules in chain\niptables -F INPUT\n\n# Delete all rules in all chains\niptables -F\n\n# Delete all rules in specific table\niptables -t nat -F\n</code></pre>"},{"location":"linux/network/packet-filtering-nat/#inserting-rules","title":"Inserting Rules","text":"<pre><code># Insert at specific position\niptables -I INPUT 1 -p tcp --dport 22 -j ACCEPT\n\n# Replace rule at position\niptables -R INPUT 1 -p tcp --dport 2222 -j ACCEPT\n</code></pre>"},{"location":"linux/network/packet-filtering-nat/#nat-with-iptables","title":"NAT with iptables","text":""},{"location":"linux/network/packet-filtering-nat/#masquerading-snat","title":"Masquerading (SNAT)","text":"<pre><code># Enable IP forwarding (required for NAT)\necho 1 &gt; /proc/sys/net/ipv4/ip_forward\n\n# Make permanent\necho \"net.ipv4.ip_forward = 1\" &gt;&gt; /etc/sysctl.conf\nsysctl -p\n\n# Masquerade outgoing traffic\niptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE\n\n# Masquerade specific subnet\niptables -t nat -A POSTROUTING -s 192.168.1.0/24 -o eth0 -j MASQUERADE\n\n# SNAT (explicit source translation)\niptables -t nat -A POSTROUTING -o eth0 -j SNAT --to-source 203.0.113.5\n</code></pre>"},{"location":"linux/network/packet-filtering-nat/#dnat-port-forwarding","title":"DNAT (Port Forwarding)","text":"<pre><code># Forward external port to internal server\niptables -t nat -A PREROUTING -p tcp --dport 80 -j DNAT --to-destination 192.168.1.100:80\n\n# Forward to different port\niptables -t nat -A PREROUTING -p tcp --dport 8080 -j DNAT --to-destination 192.168.1.100:80\n\n# Forward with source restriction\niptables -t nat -A PREROUTING -s 10.0.0.0/8 -p tcp --dport 80 -j DNAT --to-destination 192.168.1.100:80\n\n# Port range forwarding\niptables -t nat -A PREROUTING -p tcp --dport 8000:8100 -j DNAT --to-destination 192.168.1.100\n\n# Allow forwarded traffic\niptables -A FORWARD -p tcp -d 192.168.1.100 --dport 80 -j ACCEPT\niptables -A FORWARD -m state --state ESTABLISHED,RELATED -j ACCEPT\n</code></pre>"},{"location":"linux/network/packet-filtering-nat/#complete-nat-router-example","title":"Complete NAT Router Example","text":"<pre><code># Enable IP forwarding\nsysctl -w net.ipv4.ip_forward=1\n\n# INPUT chain (for router itself)\niptables -A INPUT -i lo -j ACCEPT\niptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT\niptables -A INPUT -p tcp --dport 22 -j ACCEPT\niptables -A INPUT -j DROP\n\n# FORWARD chain (for routed traffic)\niptables -A FORWARD -i eth1 -o eth0 -j ACCEPT\niptables -A FORWARD -m state --state ESTABLISHED,RELATED -j ACCEPT\niptables -A FORWARD -j DROP\n\n# NAT (masquerade internal network)\niptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE\n\n# Port forwarding (external 80 to internal 192.168.1.100:80)\niptables -t nat -A PREROUTING -i eth0 -p tcp --dport 80 -j DNAT --to-destination 192.168.1.100:80\niptables -A FORWARD -p tcp -d 192.168.1.100 --dport 80 -j ACCEPT\n</code></pre>"},{"location":"linux/network/packet-filtering-nat/#advanced-iptables-features","title":"Advanced iptables Features","text":""},{"location":"linux/network/packet-filtering-nat/#connection-tracking","title":"Connection Tracking","text":"<pre><code># Match connection states\niptables -A INPUT -m state --state NEW,ESTABLISHED,RELATED -j ACCEPT\niptables -A INPUT -m state --state INVALID -j DROP\n\n# Match conntrack states\niptables -A INPUT -m conntrack --ctstate ESTABLISHED,RELATED -j ACCEPT\n</code></pre>"},{"location":"linux/network/packet-filtering-nat/#rate-limiting","title":"Rate Limiting","text":"<pre><code># Limit new connections\niptables -A INPUT -p tcp --dport 80 -m limit --limit 25/minute --limit-burst 100 -j ACCEPT\n\n# Limit ICMP\niptables -A INPUT -p icmp --icmp-type echo-request -m limit --limit 1/second -j ACCEPT\n\n# Recent module for rate limiting\niptables -A INPUT -p tcp --dport 22 -m state --state NEW -m recent --set --name SSH\niptables -A INPUT -p tcp --dport 22 -m state --state NEW -m recent --update --seconds 60 --hitcount 4 --rttl --name SSH -j DROP\n</code></pre>"},{"location":"linux/network/packet-filtering-nat/#multiple-ports","title":"Multiple Ports","text":"<pre><code># Match multiple ports\niptables -A INPUT -p tcp -m multiport --dports 80,443,8080 -j ACCEPT\n\n# Match port ranges\niptables -A INPUT -p tcp --dport 8000:8100 -j ACCEPT\n</code></pre>"},{"location":"linux/network/packet-filtering-nat/#mac-address-filtering","title":"MAC Address Filtering","text":"<pre><code># Allow specific MAC address\niptables -A INPUT -m mac --mac-source 00:11:22:33:44:55 -j ACCEPT\n</code></pre>"},{"location":"linux/network/packet-filtering-nat/#time-based-rules","title":"Time-Based Rules","text":"<pre><code># Allow during specific hours\niptables -A INPUT -p tcp --dport 80 -m time --timestart 09:00 --timestop 18:00 -j ACCEPT\n\n# Allow on specific days\niptables -A INPUT -p tcp --dport 80 -m time --weekdays Mon,Tue,Wed,Thu,Fri -j ACCEPT\n</code></pre>"},{"location":"linux/network/packet-filtering-nat/#string-matching","title":"String Matching","text":"<pre><code># Block packets containing string\niptables -A FORWARD -m string --string \"badstring\" --algo bm -j DROP\n</code></pre>"},{"location":"linux/network/packet-filtering-nat/#saving-and-restoring-rules","title":"Saving and Restoring Rules","text":"<pre><code># Save rules\niptables-save &gt; /etc/sysconfig/iptables\n\n# Restore rules\niptables-restore &lt; /etc/sysconfig/iptables\n\n# Save via service (RHEL/CentOS)\nservice iptables save\n\n# Persistent rules across reboots\nsystemctl enable iptables\n</code></pre>"},{"location":"linux/network/packet-filtering-nat/#nftables-modern-replacement-for-iptables","title":"nftables (Modern Replacement for iptables)","text":""},{"location":"linux/network/packet-filtering-nat/#overview_2","title":"Overview","text":"<p>nftables is the modern successor to iptables, offering better performance and syntax.</p>"},{"location":"linux/network/packet-filtering-nat/#installation_1","title":"Installation","text":"<pre><code># Install nftables\ndnf install nftables\n\n# Enable service\nsystemctl enable nftables\nsystemctl start nftables\n</code></pre>"},{"location":"linux/network/packet-filtering-nat/#basic-commands","title":"Basic Commands","text":"<pre><code># List ruleset\nnft list ruleset\n\n# Add table\nnft add table inet filter\n\n# Add chain\nnft add chain inet filter input { type filter hook input priority 0 \\; }\n\n# Add rule\nnft add rule inet filter input tcp dport 22 accept\n\n# Delete rule by handle\nnft -a list ruleset  # Show handles\nnft delete rule inet filter input handle 5\n\n# Flush rules\nnft flush ruleset\n\n# Save configuration\nnft list ruleset &gt; /etc/nftables.conf\n\n# Load configuration\nnft -f /etc/nftables.conf\n</code></pre>"},{"location":"linux/network/packet-filtering-nat/#ip-forwarding-and-routing","title":"IP Forwarding and Routing","text":""},{"location":"linux/network/packet-filtering-nat/#enable-ip-forwarding","title":"Enable IP Forwarding","text":"<pre><code># Temporary\necho 1 &gt; /proc/sys/net/ipv4/ip_forward\n\n# Check status\ncat /proc/sys/net/ipv4/ip_forward\n\n# Permanent\necho \"net.ipv4.ip_forward = 1\" &gt;&gt; /etc/sysctl.conf\nsysctl -p\n\n# For IPv6\necho \"net.ipv6.conf.all.forwarding = 1\" &gt;&gt; /etc/sysctl.conf\nsysctl -p\n</code></pre>"},{"location":"linux/network/packet-filtering-nat/#connection-tracking_1","title":"Connection Tracking","text":"<pre><code># View connection tracking table\nconntrack -L\n\n# Count connections\nconntrack -C\n\n# Delete specific connection\nconntrack -D -p tcp --dport 80\n\n# View statistics\ncat /proc/net/nf_conntrack\n</code></pre>"},{"location":"linux/network/packet-filtering-nat/#common-firewall-scenarios","title":"Common Firewall Scenarios","text":""},{"location":"linux/network/packet-filtering-nat/#scenario-1-basic-web-server","title":"Scenario 1: Basic Web Server","text":"<pre><code># Using firewalld\nfirewall-cmd --permanent --add-service=http\nfirewall-cmd --permanent --add-service=https\nfirewall-cmd --permanent --add-service=ssh\nfirewall-cmd --reload\n\n# Using iptables\niptables -A INPUT -i lo -j ACCEPT\niptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT\niptables -A INPUT -p tcp --dport 22 -j ACCEPT\niptables -A INPUT -p tcp --dport 80 -j ACCEPT\niptables -A INPUT -p tcp --dport 443 -j ACCEPT\niptables -A INPUT -j DROP\niptables-save &gt; /etc/sysconfig/iptables\n</code></pre>"},{"location":"linux/network/packet-filtering-nat/#scenario-2-nat-gateway","title":"Scenario 2: NAT Gateway","text":"<pre><code># Enable forwarding\nsysctl -w net.ipv4.ip_forward=1\n\n# Using firewalld\nfirewall-cmd --permanent --zone=external --add-interface=eth0\nfirewall-cmd --permanent --zone=external --add-masquerade\nfirewall-cmd --permanent --zone=internal --add-interface=eth1\nfirewall-cmd --reload\n\n# Using iptables\niptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE\niptables -A FORWARD -i eth1 -o eth0 -j ACCEPT\niptables -A FORWARD -m state --state ESTABLISHED,RELATED -j ACCEPT\n</code></pre>"},{"location":"linux/network/packet-filtering-nat/#scenario-3-port-forwarding-dmz","title":"Scenario 3: Port Forwarding (DMZ)","text":"<pre><code># Forward external 80 to DMZ server\n# Using firewalld\nfirewall-cmd --permanent --zone=public --add-forward-port=port=80:proto=tcp:toaddr=192.168.1.100:toport=80\nfirewall-cmd --reload\n\n# Using iptables\niptables -t nat -A PREROUTING -i eth0 -p tcp --dport 80 -j DNAT --to-destination 192.168.1.100:80\niptables -A FORWARD -p tcp -d 192.168.1.100 --dport 80 -m state --state NEW,ESTABLISHED,RELATED -j ACCEPT\n</code></pre>"},{"location":"linux/network/packet-filtering-nat/#scenario-4-restrict-ssh-access","title":"Scenario 4: Restrict SSH Access","text":"<pre><code># Allow SSH only from specific network\n# Using firewalld\nfirewall-cmd --permanent --remove-service=ssh\nfirewall-cmd --permanent --add-rich-rule='rule family=\"ipv4\" source address=\"10.0.0.0/8\" service name=\"ssh\" accept'\nfirewall-cmd --reload\n\n# Using iptables\niptables -A INPUT -p tcp -s 10.0.0.0/8 --dport 22 -j ACCEPT\niptables -A INPUT -p tcp --dport 22 -j DROP\n</code></pre>"},{"location":"linux/network/packet-filtering-nat/#troubleshooting","title":"Troubleshooting","text":""},{"location":"linux/network/packet-filtering-nat/#debug-firewalld","title":"Debug firewalld","text":"<pre><code># Check service status\nfirewall-cmd --state\nsystemctl status firewalld\n\n# View logs\njournalctl -u firewalld -f\n\n# Test rules\nfirewall-cmd --direct --get-all-rules\nfirewall-cmd --list-all\n\n# Reload\nfirewall-cmd --reload\n</code></pre>"},{"location":"linux/network/packet-filtering-nat/#debug-iptables","title":"Debug iptables","text":"<pre><code># View packet counts\niptables -L -v -n\n\n# Watch packets in real-time\nwatch -n 1 'iptables -L -v -n'\n\n# Enable logging\niptables -A INPUT -j LOG --log-prefix \"IPTABLES: \"\ntail -f /var/log/messages\n\n# Trace packets\niptables -t raw -A PREROUTING -p tcp --dport 80 -j TRACE\n</code></pre>"},{"location":"linux/network/packet-filtering-nat/#test-connectivity","title":"Test Connectivity","text":"<pre><code># Test from external\ntelnet server_ip 80\nnc -vz server_ip 80\n\n# Test locally\ncurl localhost:80\nss -tlnp | grep :80\n</code></pre>"},{"location":"linux/network/packet-filtering-nat/#quick-reference","title":"Quick Reference","text":""},{"location":"linux/network/packet-filtering-nat/#firewalld","title":"firewalld","text":"<pre><code>firewall-cmd --list-all                                    # View configuration\nfirewall-cmd --permanent --add-service=http                # Add service\nfirewall-cmd --permanent --add-port=8080/tcp               # Add port\nfirewall-cmd --permanent --add-source=192.168.1.0/24       # Add source\nfirewall-cmd --permanent --add-masquerade                  # Enable NAT\nfirewall-cmd --permanent --add-forward-port=port=80:proto=tcp:toport=8080  # Port forward\nfirewall-cmd --reload                                      # Apply changes\n</code></pre>"},{"location":"linux/network/packet-filtering-nat/#iptables","title":"iptables","text":"<pre><code>iptables -L -n -v                                         # List rules\niptables -A INPUT -p tcp --dport 80 -j ACCEPT            # Add rule\niptables -D INPUT 5                                       # Delete rule\niptables -F                                               # Flush all\niptables-save &gt; /etc/sysconfig/iptables                  # Save rules\niptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE     # Masquerade\niptables -t nat -A PREROUTING -p tcp --dport 80 -j DNAT --to 192.168.1.100  # Port forward\n</code></pre>"},{"location":"linux/network/packet-filtering-nat/#exam-tips","title":"Exam Tips","text":"<ul> <li>Know both firewalld and iptables basics</li> <li>Understand the difference between runtime and permanent changes</li> <li>Practice NAT and port forwarding scenarios</li> <li>Remember to enable IP forwarding for routing/NAT</li> <li>Know how to troubleshoot with logs and packet counts</li> <li>Understand firewalld zones concept</li> <li>Be comfortable with rich rules in firewalld</li> <li>Know the iptables table/chain structure</li> <li>Practice saving and restoring firewall rules</li> <li>Understand connection tracking and stateful filtering</li> </ul>"},{"location":"linux/network/reverse-proxy-load-balancer/","title":"Implement Reverse Proxies and Load Balancers","text":""},{"location":"linux/network/reverse-proxy-load-balancer/#overview","title":"Overview","text":"<p>This guide covers implementation of reverse proxies and load balancers using HAProxy, Nginx, and Apache in Linux systems.</p>"},{"location":"linux/network/reverse-proxy-load-balancer/#concepts","title":"Concepts","text":""},{"location":"linux/network/reverse-proxy-load-balancer/#reverse-proxy","title":"Reverse Proxy","text":"<p>A reverse proxy sits in front of web servers and forwards client requests to those servers. Benefits: - SSL/TLS termination - Caching - Compression - Security (hide backend servers) - Single entry point</p>"},{"location":"linux/network/reverse-proxy-load-balancer/#load-balancer","title":"Load Balancer","text":"<p>A load balancer distributes incoming traffic across multiple servers. Benefits: - High availability - Scalability - Redundancy - Performance optimization</p>"},{"location":"linux/network/reverse-proxy-load-balancer/#key-terms","title":"Key Terms","text":"<ul> <li>Frontend: Entry point that receives client requests</li> <li>Backend: Pool of servers that handle requests</li> <li>Health Check: Monitoring to verify server availability</li> <li>Session Persistence: Maintaining user session on same server (sticky sessions)</li> <li>Load Balancing Algorithm: Method to distribute traffic</li> </ul>"},{"location":"linux/network/reverse-proxy-load-balancer/#load-balancing-algorithms","title":"Load Balancing Algorithms","text":"<ul> <li>Round Robin: Sequential distribution</li> <li>Least Connections: Server with fewest active connections</li> <li>IP Hash: Based on client IP address</li> <li>Weighted: Servers have different capacities</li> <li>Least Response Time: Fastest responding server</li> </ul>"},{"location":"linux/network/reverse-proxy-load-balancer/#haproxy-high-availability-proxy","title":"HAProxy (High Availability Proxy)","text":""},{"location":"linux/network/reverse-proxy-load-balancer/#overview_1","title":"Overview","text":"<p>HAProxy is a popular open-source load balancer and proxy server for TCP and HTTP-based applications.</p>"},{"location":"linux/network/reverse-proxy-load-balancer/#installation","title":"Installation","text":"<pre><code># RHEL/CentOS/Fedora\ndnf install haproxy\n\n# Ubuntu/Debian\napt install haproxy\n\n# Start service\nsystemctl start haproxy\nsystemctl enable haproxy\n\n# Check status\nsystemctl status haproxy\n</code></pre>"},{"location":"linux/network/reverse-proxy-load-balancer/#configuration-file","title":"Configuration File","text":"<p>Location: <code>/etc/haproxy/haproxy.cfg</code></p>"},{"location":"linux/network/reverse-proxy-load-balancer/#basic-haproxy-configuration-structure","title":"Basic HAProxy Configuration Structure","text":"<pre><code># Global settings\nglobal\n    # Process management\n    # Logging\n    # Performance tuning\n\n# Default settings for all sections\ndefaults\n    # Timeouts\n    # Mode (tcp/http)\n    # Options\n\n# Frontend - entry point\nfrontend &lt;name&gt;\n    # Bind address and port\n    # ACLs\n    # Backend selection\n\n# Backend - server pool\nbackend &lt;name&gt;\n    # Load balancing algorithm\n    # Health checks\n    # Server definitions\n</code></pre>"},{"location":"linux/network/reverse-proxy-load-balancer/#simple-http-load-balancer-example","title":"Simple HTTP Load Balancer Example","text":"<pre><code># /etc/haproxy/haproxy.cfg\n\nglobal\n    log /dev/log local0\n    log /dev/log local1 notice\n    chroot /var/lib/haproxy\n    stats socket /run/haproxy/admin.sock mode 660 level admin\n    stats timeout 30s\n    user haproxy\n    group haproxy\n    daemon\n\ndefaults\n    log     global\n    mode    http\n    option  httplog\n    option  dontlognull\n    timeout connect 5000\n    timeout client  50000\n    timeout server  50000\n\nfrontend http_front\n    bind *:80\n    stats uri /haproxy?stats\n    default_backend http_back\n\nbackend http_back\n    balance roundrobin\n    server web1 192.168.1.10:80 check\n    server web2 192.168.1.11:80 check\n    server web3 192.168.1.12:80 check\n</code></pre> <p>Test and reload: <pre><code># Test configuration\nhaproxy -c -f /etc/haproxy/haproxy.cfg\n\n# Reload without dropping connections\nsystemctl reload haproxy\n\n# Restart\nsystemctl restart haproxy\n</code></pre></p>"},{"location":"linux/network/reverse-proxy-load-balancer/#advanced-haproxy-configuration","title":"Advanced HAProxy Configuration","text":""},{"location":"linux/network/reverse-proxy-load-balancer/#ssltls-termination","title":"SSL/TLS Termination","text":"<pre><code>frontend https_front\n    bind *:443 ssl crt /etc/haproxy/certs/cert.pem\n    bind *:80\n    redirect scheme https code 301 if !{ ssl_fc }\n    default_backend web_back\n\nbackend web_back\n    balance roundrobin\n    server web1 192.168.1.10:80 check\n    server web2 192.168.1.11:80 check\n</code></pre>"},{"location":"linux/network/reverse-proxy-load-balancer/#health-checks","title":"Health Checks","text":"<pre><code>backend web_back\n    balance roundrobin\n    option httpchk GET /health\n    http-check expect status 200\n\n    server web1 192.168.1.10:80 check inter 2000 rise 2 fall 3\n    server web2 192.168.1.11:80 check inter 2000 rise 2 fall 3\n\n# Parameters:\n# check      - Enable health checks\n# inter 2000 - Check interval (2 seconds)\n# rise 2     - Consider server up after 2 successful checks\n# fall 3     - Consider server down after 3 failed checks\n</code></pre>"},{"location":"linux/network/reverse-proxy-load-balancer/#load-balancing-algorithms_1","title":"Load Balancing Algorithms","text":"<pre><code>backend web_back\n    # Round robin (default)\n    balance roundrobin\n\n    # Or least connections\n    # balance leastconn\n\n    # Or source IP hash\n    # balance source\n\n    # Or URI hash\n    # balance uri\n\n    # Or URL parameter hash\n    # balance url_param userid\n\n    server web1 192.168.1.10:80 check\n    server web2 192.168.1.11:80 check\n</code></pre>"},{"location":"linux/network/reverse-proxy-load-balancer/#weighted-load-balancing","title":"Weighted Load Balancing","text":"<pre><code>backend web_back\n    balance roundrobin\n    server web1 192.168.1.10:80 check weight 100\n    server web2 192.168.1.11:80 check weight 50\n    server web3 192.168.1.12:80 check weight 150\n    # web3 gets 1.5x more traffic than web1\n</code></pre>"},{"location":"linux/network/reverse-proxy-load-balancer/#session-persistence-sticky-sessions","title":"Session Persistence (Sticky Sessions)","text":"<pre><code>backend web_back\n    balance roundrobin\n    # Cookie-based persistence\n    cookie SERVERID insert indirect nocache\n    server web1 192.168.1.10:80 check cookie web1\n    server web2 192.168.1.11:80 check cookie web2\n\n    # Or source IP based\n    # stick-table type ip size 200k expire 30m\n    # stick on src\n</code></pre>"},{"location":"linux/network/reverse-proxy-load-balancer/#acls-access-control-lists","title":"ACLs (Access Control Lists)","text":"<pre><code>frontend http_front\n    bind *:80\n\n    # Define ACLs\n    acl is_api path_beg /api\n    acl is_admin path_beg /admin\n    acl is_static path_end .jpg .png .css .js\n    acl allowed_ips src 192.168.1.0/24 10.0.0.0/8\n\n    # Use ACLs\n    use_backend api_back if is_api\n    use_backend admin_back if is_admin allowed_ips\n    use_backend static_back if is_static\n    default_backend web_back\n\nbackend api_back\n    server api1 192.168.1.20:8080 check\n\nbackend admin_back\n    server admin1 192.168.1.30:8080 check\n\nbackend static_back\n    server static1 192.168.1.40:80 check\n\nbackend web_back\n    server web1 192.168.1.10:80 check\n</code></pre>"},{"location":"linux/network/reverse-proxy-load-balancer/#http-headers-manipulation","title":"HTTP Headers Manipulation","text":"<pre><code>frontend http_front\n    bind *:80\n    # Add headers\n    http-request add-header X-Forwarded-Proto https\n    http-request set-header X-Client-IP %[src]\n    default_backend web_back\n\nbackend web_back\n    # Add backend headers\n    http-request add-header X-Backend-Server %[srv_name]\n    # Remove headers\n    http-response del-header Server\n    server web1 192.168.1.10:80 check\n</code></pre>"},{"location":"linux/network/reverse-proxy-load-balancer/#rate-limiting","title":"Rate Limiting","text":"<pre><code>frontend http_front\n    bind *:80\n    # Track client requests\n    stick-table type ip size 100k expire 30s store http_req_rate(10s)\n    http-request track-sc0 src\n    # Deny if more than 100 requests in 10 seconds\n    http-request deny if { sc_http_req_rate(0) gt 100 }\n    default_backend web_back\n</code></pre>"},{"location":"linux/network/reverse-proxy-load-balancer/#haproxy-statistics-page","title":"HAProxy Statistics Page","text":"<pre><code>frontend http_front\n    bind *:80\n    # Enable stats\n    stats enable\n    stats uri /haproxy-stats\n    stats auth admin:password\n    stats refresh 30s\n    default_backend web_back\n</code></pre> <p>Access: http://your-server/haproxy-stats</p>"},{"location":"linux/network/reverse-proxy-load-balancer/#haproxy-runtime-api","title":"HAProxy Runtime API","text":"<pre><code># Connect to admin socket\necho \"show stat\" | socat stdio /run/haproxy/admin.sock\n\n# Show servers\necho \"show servers state\" | socat stdio /run/haproxy/admin.sock\n\n# Disable server\necho \"disable server web_back/web1\" | socat stdio /run/haproxy/admin.sock\n\n# Enable server\necho \"enable server web_back/web1\" | socat stdio /run/haproxy/admin.sock\n\n# Set server weight\necho \"set weight web_back/web1 50\" | socat stdio /run/haproxy/admin.sock\n</code></pre>"},{"location":"linux/network/reverse-proxy-load-balancer/#nginx-as-reverse-proxy-and-load-balancer","title":"Nginx as Reverse Proxy and Load Balancer","text":""},{"location":"linux/network/reverse-proxy-load-balancer/#installation_1","title":"Installation","text":"<pre><code># RHEL/CentOS/Fedora\ndnf install nginx\n\n# Ubuntu/Debian\napt install nginx\n\n# Start service\nsystemctl start nginx\nsystemctl enable nginx\n</code></pre>"},{"location":"linux/network/reverse-proxy-load-balancer/#basic-reverse-proxy-configuration","title":"Basic Reverse Proxy Configuration","text":"<pre><code># /etc/nginx/nginx.conf or /etc/nginx/conf.d/proxy.conf\n\nhttp {\n    upstream backend {\n        server 192.168.1.10:80;\n        server 192.168.1.11:80;\n        server 192.168.1.12:80;\n    }\n\n    server {\n        listen 80;\n        server_name example.com;\n\n        location / {\n            proxy_pass http://backend;\n            proxy_set_header Host $host;\n            proxy_set_header X-Real-IP $remote_addr;\n            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n            proxy_set_header X-Forwarded-Proto $scheme;\n        }\n    }\n}\n</code></pre> <p>Test and reload: <pre><code># Test configuration\nnginx -t\n\n# Reload\nsystemctl reload nginx\n\n# Restart\nsystemctl restart nginx\n</code></pre></p>"},{"location":"linux/network/reverse-proxy-load-balancer/#load-balancing-methods","title":"Load Balancing Methods","text":"<pre><code># Round robin (default)\nupstream backend {\n    server 192.168.1.10:80;\n    server 192.168.1.11:80;\n}\n\n# Least connections\nupstream backend {\n    least_conn;\n    server 192.168.1.10:80;\n    server 192.168.1.11:80;\n}\n\n# IP hash (session persistence)\nupstream backend {\n    ip_hash;\n    server 192.168.1.10:80;\n    server 192.168.1.11:80;\n}\n\n# Weighted load balancing\nupstream backend {\n    server 192.168.1.10:80 weight=3;\n    server 192.168.1.11:80 weight=1;\n    server 192.168.1.12:80 weight=2;\n}\n\n# Hash-based (generic)\nupstream backend {\n    hash $request_uri consistent;\n    server 192.168.1.10:80;\n    server 192.168.1.11:80;\n}\n</code></pre>"},{"location":"linux/network/reverse-proxy-load-balancer/#health-checks_1","title":"Health Checks","text":"<pre><code>upstream backend {\n    server 192.168.1.10:80 max_fails=3 fail_timeout=30s;\n    server 192.168.1.11:80 max_fails=3 fail_timeout=30s;\n    server 192.168.1.12:80 backup;  # Backup server\n}\n\n# max_fails: Number of failed attempts before marking down\n# fail_timeout: Time to consider server down\n# backup: Only used when other servers are down\n</code></pre>"},{"location":"linux/network/reverse-proxy-load-balancer/#ssltls-termination_1","title":"SSL/TLS Termination","text":"<pre><code>server {\n    listen 443 ssl http2;\n    server_name example.com;\n\n    ssl_certificate /etc/nginx/ssl/cert.pem;\n    ssl_certificate_key /etc/nginx/ssl/key.pem;\n    ssl_protocols TLSv1.2 TLSv1.3;\n    ssl_ciphers HIGH:!aNULL:!MD5;\n\n    location / {\n        proxy_pass http://backend;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n    }\n}\n\n# Redirect HTTP to HTTPS\nserver {\n    listen 80;\n    server_name example.com;\n    return 301 https://$server_name$request_uri;\n}\n</code></pre>"},{"location":"linux/network/reverse-proxy-load-balancer/#caching","title":"Caching","text":"<pre><code># Define cache path\nproxy_cache_path /var/cache/nginx levels=1:2 keys_zone=my_cache:10m max_size=1g inactive=60m;\n\nserver {\n    listen 80;\n\n    location / {\n        proxy_cache my_cache;\n        proxy_cache_valid 200 60m;\n        proxy_cache_valid 404 10m;\n        proxy_cache_key \"$scheme$request_method$host$request_uri\";\n        add_header X-Cache-Status $upstream_cache_status;\n\n        proxy_pass http://backend;\n    }\n}\n</code></pre>"},{"location":"linux/network/reverse-proxy-load-balancer/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code>http {\n    # Rate limiting\n    limit_req_zone $binary_remote_addr zone=mylimit:10m rate=10r/s;\n\n    # Connection limiting\n    limit_conn_zone $binary_remote_addr zone=addr:10m;\n\n    upstream backend {\n        least_conn;\n        server 192.168.1.10:80 weight=3 max_fails=2 fail_timeout=30s;\n        server 192.168.1.11:80 weight=2 max_fails=2 fail_timeout=30s;\n        server 192.168.1.12:80 backup;\n\n        # Keepalive connections\n        keepalive 32;\n    }\n\n    server {\n        listen 80;\n        server_name example.com;\n\n        # Apply rate limit\n        location / {\n            limit_req zone=mylimit burst=20 nodelay;\n            limit_conn addr 10;\n\n            proxy_pass http://backend;\n            proxy_http_version 1.1;\n            proxy_set_header Connection \"\";\n            proxy_set_header Host $host;\n            proxy_set_header X-Real-IP $remote_addr;\n            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n            proxy_set_header X-Forwarded-Proto $scheme;\n\n            # Timeouts\n            proxy_connect_timeout 60s;\n            proxy_send_timeout 60s;\n            proxy_read_timeout 60s;\n        }\n\n        # Static content (bypass backend)\n        location ~* \\.(jpg|jpeg|png|gif|ico|css|js)$ {\n            root /var/www/static;\n            expires 30d;\n            access_log off;\n        }\n\n        # Health check endpoint\n        location /health {\n            access_log off;\n            return 200 \"healthy\\n\";\n        }\n    }\n}\n</code></pre>"},{"location":"linux/network/reverse-proxy-load-balancer/#apache-as-reverse-proxy","title":"Apache as Reverse Proxy","text":""},{"location":"linux/network/reverse-proxy-load-balancer/#installation-and-modules","title":"Installation and Modules","text":"<pre><code># RHEL/CentOS/Fedora\ndnf install httpd\n\n# Enable proxy modules\n# Modules are typically enabled by default or in:\n# /etc/httpd/conf.modules.d/00-proxy.conf\n\n# Required modules:\n# mod_proxy\n# mod_proxy_http\n# mod_proxy_balancer\n# mod_lbmethod_byrequests\n\n# Ubuntu/Debian\napt install apache2\na2enmod proxy\na2enmod proxy_http\na2enmod proxy_balancer\na2enmod lbmethod_byrequests\n\nsystemctl restart httpd  # or apache2\n</code></pre>"},{"location":"linux/network/reverse-proxy-load-balancer/#basic-reverse-proxy","title":"Basic Reverse Proxy","text":"<pre><code># /etc/httpd/conf.d/proxy.conf\n\n&lt;VirtualHost *:80&gt;\n    ServerName example.com\n\n    ProxyPreserveHost On\n    ProxyPass / http://192.168.1.10/\n    ProxyPassReverse / http://192.168.1.10/\n\n    # Logging\n    ErrorLog /var/log/httpd/proxy_error.log\n    CustomLog /var/log/httpd/proxy_access.log combined\n&lt;/VirtualHost&gt;\n</code></pre>"},{"location":"linux/network/reverse-proxy-load-balancer/#load-balancer-configuration","title":"Load Balancer Configuration","text":"<pre><code>&lt;VirtualHost *:80&gt;\n    ServerName example.com\n\n    # Load balancer\n    &lt;Proxy balancer://mycluster&gt;\n        BalancerMember http://192.168.1.10:80\n        BalancerMember http://192.168.1.11:80\n        BalancerMember http://192.168.1.12:80\n\n        # Load balancing method\n        ProxySet lbmethod=byrequests\n        # Options: byrequests, bytraffic, bybusyness\n    &lt;/Proxy&gt;\n\n    ProxyPreserveHost On\n    ProxyPass / balancer://mycluster/\n    ProxyPassReverse / balancer://mycluster/\n&lt;/VirtualHost&gt;\n</code></pre>"},{"location":"linux/network/reverse-proxy-load-balancer/#advanced-apache-load-balancer","title":"Advanced Apache Load Balancer","text":"<pre><code>&lt;VirtualHost *:80&gt;\n    ServerName example.com\n\n    &lt;Proxy balancer://mycluster&gt;\n        # Server definitions with parameters\n        BalancerMember http://192.168.1.10:80 loadfactor=3 route=node1\n        BalancerMember http://192.168.1.11:80 loadfactor=2 route=node2\n        BalancerMember http://192.168.1.12:80 loadfactor=1 status=+H\n        # status=+H means hot standby (backup)\n\n        # Health check (requires mod_proxy_hcheck)\n        # ProxySet lbmethod=bybusyness\n\n        # Session stickiness\n        ProxySet stickysession=ROUTEID\n    &lt;/Proxy&gt;\n\n    # Headers\n    RequestHeader set X-Forwarded-Proto \"http\"\n    RequestHeader set X-Forwarded-Port \"80\"\n\n    ProxyPass / balancer://mycluster/\n    ProxyPassReverse / balancer://mycluster/\n\n    # Manager interface\n    &lt;Location /balancer-manager&gt;\n        SetHandler balancer-manager\n        Require ip 192.168.1.0/24\n    &lt;/Location&gt;\n&lt;/VirtualHost&gt;\n</code></pre>"},{"location":"linux/network/reverse-proxy-load-balancer/#ssl-termination","title":"SSL Termination","text":"<pre><code>&lt;VirtualHost *:443&gt;\n    ServerName example.com\n\n    SSLEngine on\n    SSLCertificateFile /etc/pki/tls/certs/cert.pem\n    SSLCertificateKeyFile /etc/pki/tls/private/key.pem\n\n    &lt;Proxy balancer://mycluster&gt;\n        BalancerMember http://192.168.1.10:80\n        BalancerMember http://192.168.1.11:80\n    &lt;/Proxy&gt;\n\n    RequestHeader set X-Forwarded-Proto \"https\"\n    ProxyPreserveHost On\n    ProxyPass / balancer://mycluster/\n    ProxyPassReverse / balancer://mycluster/\n&lt;/VirtualHost&gt;\n\n# HTTP to HTTPS redirect\n&lt;VirtualHost *:80&gt;\n    ServerName example.com\n    Redirect permanent / https://example.com/\n&lt;/VirtualHost&gt;\n</code></pre>"},{"location":"linux/network/reverse-proxy-load-balancer/#comparison-haproxy-vs-nginx-vs-apache","title":"Comparison: HAProxy vs Nginx vs Apache","text":"Feature HAProxy Nginx Apache Primary Use Load balancing Web server + proxy Web server + proxy Performance Excellent Excellent Good Configuration Moderate complexity Moderate Complex Layer 4 LB Yes Yes (stream) Limited Layer 7 LB Yes Yes Yes Health Checks Advanced Basic+ Basic SSL Termination Yes Yes Yes Statistics Built-in Third-party mod_status Hot Reload Yes Yes Limited Ease of Use Moderate Good Moderate"},{"location":"linux/network/reverse-proxy-load-balancer/#common-scenarios","title":"Common Scenarios","text":""},{"location":"linux/network/reverse-proxy-load-balancer/#scenario-1-web-application-load-balancer","title":"Scenario 1: Web Application Load Balancer","text":"<pre><code># Nginx configuration\nhttp {\n    upstream webapp {\n        least_conn;\n        server 192.168.1.10:8080 weight=2;\n        server 192.168.1.11:8080 weight=2;\n        server 192.168.1.12:8080 weight=1;\n\n        keepalive 32;\n    }\n\n    server {\n        listen 80;\n        server_name app.example.com;\n\n        location / {\n            proxy_pass http://webapp;\n            proxy_http_version 1.1;\n            proxy_set_header Connection \"\";\n            proxy_set_header Host $host;\n            proxy_set_header X-Real-IP $remote_addr;\n            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        }\n    }\n}\n</code></pre>"},{"location":"linux/network/reverse-proxy-load-balancer/#scenario-2-api-gateway-with-rate-limiting","title":"Scenario 2: API Gateway with Rate Limiting","text":"<pre><code>http {\n    limit_req_zone $binary_remote_addr zone=api_limit:10m rate=100r/s;\n\n    upstream api_backend {\n        server 192.168.1.20:3000;\n        server 192.168.1.21:3000;\n    }\n\n    server {\n        listen 80;\n        server_name api.example.com;\n\n        location /api/ {\n            limit_req zone=api_limit burst=50 nodelay;\n\n            proxy_pass http://api_backend;\n            proxy_set_header Host $host;\n            proxy_set_header X-Real-IP $remote_addr;\n        }\n    }\n}\n</code></pre>"},{"location":"linux/network/reverse-proxy-load-balancer/#scenario-3-high-availability-setup-with-health-checks","title":"Scenario 3: High Availability Setup with Health Checks","text":"<pre><code># HAProxy configuration\nfrontend http_front\n    bind *:80\n    default_backend web_servers\n\nbackend web_servers\n    balance leastconn\n    option httpchk GET /health HTTP/1.1\\r\\nHost:\\ example.com\n    http-check expect status 200\n\n    server web1 192.168.1.10:80 check inter 5s fall 3 rise 2\n    server web2 192.168.1.11:80 check inter 5s fall 3 rise 2\n    server web3 192.168.1.12:80 check inter 5s fall 3 rise 2 backup\n</code></pre>"},{"location":"linux/network/reverse-proxy-load-balancer/#scenario-4-ssl-offloading","title":"Scenario 4: SSL Offloading","text":"<pre><code>server {\n    listen 443 ssl http2;\n    server_name secure.example.com;\n\n    ssl_certificate /etc/nginx/ssl/cert.pem;\n    ssl_certificate_key /etc/nginx/ssl/key.pem;\n    ssl_protocols TLSv1.2 TLSv1.3;\n\n    location / {\n        proxy_pass http://backend;\n        proxy_set_header X-Forwarded-Proto https;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n    }\n}\n</code></pre>"},{"location":"linux/network/reverse-proxy-load-balancer/#monitoring-and-logging","title":"Monitoring and Logging","text":""},{"location":"linux/network/reverse-proxy-load-balancer/#haproxy-stats","title":"HAProxy Stats","text":"<pre><code># Via stats page\nfrontend http_front\n    stats enable\n    stats uri /stats\n    stats auth admin:password\n\n# Via socket\necho \"show stat\" | socat stdio /run/haproxy/admin.sock\n\n# Show info\necho \"show info\" | socat stdio /run/haproxy/admin.sock\n</code></pre>"},{"location":"linux/network/reverse-proxy-load-balancer/#nginx-status","title":"Nginx Status","text":"<pre><code>server {\n    listen 80;\n\n    location /nginx_status {\n        stub_status on;\n        access_log off;\n        allow 127.0.0.1;\n        deny all;\n    }\n}\n</code></pre>"},{"location":"linux/network/reverse-proxy-load-balancer/#logs","title":"Logs","text":"<pre><code># HAProxy logs\ntail -f /var/log/haproxy.log\n\n# Nginx logs\ntail -f /var/log/nginx/access.log\ntail -f /var/log/nginx/error.log\n\n# Apache logs\ntail -f /var/log/httpd/access_log\ntail -f /var/log/httpd/error_log\n</code></pre>"},{"location":"linux/network/reverse-proxy-load-balancer/#troubleshooting","title":"Troubleshooting","text":""},{"location":"linux/network/reverse-proxy-load-balancer/#check-configuration","title":"Check Configuration","text":"<pre><code># HAProxy\nhaproxy -c -f /etc/haproxy/haproxy.cfg\n\n# Nginx\nnginx -t\n\n# Apache\napachectl configtest\nhttpd -t\n</code></pre>"},{"location":"linux/network/reverse-proxy-load-balancer/#test-backend-connectivity","title":"Test Backend Connectivity","text":"<pre><code># From load balancer\ncurl -I http://192.168.1.10/\ntelnet 192.168.1.10 80\nnc -vz 192.168.1.10 80\n</code></pre>"},{"location":"linux/network/reverse-proxy-load-balancer/#debug-issues","title":"Debug Issues","text":"<pre><code># Check if service is running\nsystemctl status haproxy\nsystemctl status nginx\nsystemctl status httpd\n\n# Check listening ports\nss -tlnp | grep haproxy\nss -tlnp | grep nginx\n\n# View connections\nss -tn | grep :80\n\n# Check logs\njournalctl -u haproxy -f\njournalctl -u nginx -f\n</code></pre>"},{"location":"linux/network/reverse-proxy-load-balancer/#common-issues","title":"Common Issues","text":"<p>Backend servers not responding: <pre><code># Check health\ncurl http://backend-ip/health\n\n# Verify firewall\nfirewall-cmd --list-all\n\n# Check backend logs\n</code></pre></p> <p>SSL certificate errors: <pre><code># Verify certificate\nopenssl x509 -in /path/to/cert.pem -text -noout\n\n# Test SSL\nopenssl s_client -connect example.com:443\n</code></pre></p>"},{"location":"linux/network/reverse-proxy-load-balancer/#quick-reference","title":"Quick Reference","text":""},{"location":"linux/network/reverse-proxy-load-balancer/#haproxy","title":"HAProxy","text":"<pre><code>haproxy -c -f /etc/haproxy/haproxy.cfg    # Test config\nsystemctl reload haproxy                   # Reload\necho \"show stat\" | socat stdio /run/haproxy/admin.sock  # Stats\n</code></pre>"},{"location":"linux/network/reverse-proxy-load-balancer/#nginx","title":"Nginx","text":"<pre><code>nginx -t                                   # Test config\nsystemctl reload nginx                     # Reload\ncurl http://localhost/nginx_status         # Status\n</code></pre>"},{"location":"linux/network/reverse-proxy-load-balancer/#apache","title":"Apache","text":"<pre><code>apachectl configtest                       # Test config\nsystemctl reload httpd                     # Reload\ncurl http://localhost/server-status        # Status\n</code></pre>"},{"location":"linux/network/reverse-proxy-load-balancer/#exam-tips","title":"Exam Tips","text":"<ul> <li>Know how to configure basic reverse proxy with Nginx or HAProxy</li> <li>Understand load balancing algorithms (round-robin, least-conn, etc.)</li> <li>Be familiar with health checks configuration</li> <li>Know how to implement SSL termination</li> <li>Understand session persistence mechanisms</li> <li>Practice testing configurations before applying</li> <li>Know how to view statistics and monitor</li> <li>Be comfortable with ACLs and routing rules</li> <li>Understand the difference between Layer 4 and Layer 7 load balancing</li> <li>Know how to troubleshoot backend connectivity issues</li> <li>Practice configuring multiple backends with different weights</li> <li>Understand when to use each solution (HAProxy vs Nginx vs Apache)</li> </ul>"},{"location":"linux/network/static-routing/","title":"Configure Static Routing","text":""},{"location":"linux/network/static-routing/#overview","title":"Overview","text":"<p>This guide covers static routing configuration, route management, policy-based routing, and advanced routing concepts in Linux.</p>"},{"location":"linux/network/static-routing/#routing-concepts","title":"Routing Concepts","text":""},{"location":"linux/network/static-routing/#key-terms","title":"Key Terms","text":"<ul> <li>Routing: Process of forwarding packets between networks</li> <li>Static Route: Manually configured route (doesn\u2019t change automatically)</li> <li>Dynamic Route: Automatically learned route (via routing protocols)</li> <li>Default Gateway: Route used when no specific route matches</li> <li>Metric: Cost or priority of a route (lower is preferred)</li> <li>Next Hop: Next router/gateway to forward packets</li> <li>Administrative Distance: Route preference (lower is preferred)</li> <li>Policy Routing: Routing based on source, not just destination</li> </ul>"},{"location":"linux/network/static-routing/#routing-table-components","title":"Routing Table Components","text":"<ul> <li>Destination: Target network</li> <li>Gateway: Next hop IP address</li> <li>Genmask/Prefix: Network mask</li> <li>Interface: Outgoing network interface</li> <li>Metric: Route priority</li> </ul>"},{"location":"linux/network/static-routing/#viewing-routing-tables","title":"Viewing Routing Tables","text":""},{"location":"linux/network/static-routing/#using-ip-route-command","title":"Using <code>ip route</code> Command","text":"<pre><code># Show routing table\nip route show\nip route list\nip r\n\n# Show with details\nip route show table all\n\n# Show specific table\nip route show table main\nip route show table local\n\n# Show IPv6 routes\nip -6 route show\n\n# Show route to specific destination\nip route get 8.8.8.8\nip route get 192.168.1.100\n\n# Show cached routes (deprecated in newer kernels)\nip route show cache\n</code></pre>"},{"location":"linux/network/static-routing/#using-route-command-legacy","title":"Using <code>route</code> Command (Legacy)","text":"<pre><code># Show routing table\nroute -n\n\n# Show with hostname resolution\nroute\n\n# Show IPv6 routes\nroute -A inet6 -n\n</code></pre>"},{"location":"linux/network/static-routing/#using-netstat-command","title":"Using <code>netstat</code> Command","text":"<pre><code># Show routing table\nnetstat -r\nnetstat -rn\n\n# Show IPv6 routes\nnetstat -rn -A inet6\n</code></pre>"},{"location":"linux/network/static-routing/#routing-table-files","title":"Routing Table Files","text":"<pre><code># View kernel routing table\ncat /proc/net/route\n\n# View IPv6 routing table\ncat /proc/net/ipv6_route\n\n# Format: destination, gateway, netmask, flags, metric, ref, use, interface\n</code></pre>"},{"location":"linux/network/static-routing/#managing-static-routes","title":"Managing Static Routes","text":""},{"location":"linux/network/static-routing/#using-ip-route-command_1","title":"Using <code>ip route</code> Command","text":""},{"location":"linux/network/static-routing/#add-routes","title":"Add Routes","text":"<pre><code># Add route to network via gateway\nip route add 10.0.0.0/8 via 192.168.1.1\n\n# Add route via specific interface\nip route add 10.0.0.0/8 dev eth1\n\n# Add route with both gateway and interface\nip route add 10.0.0.0/8 via 192.168.1.1 dev eth0\n\n# Add default gateway\nip route add default via 192.168.1.1\nip route add 0.0.0.0/0 via 192.168.1.1\n\n# Add route with metric\nip route add 10.0.0.0/8 via 192.168.1.1 metric 100\n\n# Add host route (single IP)\nip route add 10.0.0.5/32 via 192.168.1.1\n\n# Add IPv6 route\nip -6 route add 2001:db8::/32 via 2001:db8::1\n\n# Add IPv6 default gateway\nip -6 route add default via 2001:db8::1\n</code></pre>"},{"location":"linux/network/static-routing/#delete-routes","title":"Delete Routes","text":"<pre><code># Delete specific route\nip route del 10.0.0.0/8 via 192.168.1.1\n\n# Delete default gateway\nip route del default\n\n# Delete IPv6 route\nip -6 route del 2001:db8::/32\n</code></pre>"},{"location":"linux/network/static-routing/#replace-routes","title":"Replace Routes","text":"<pre><code># Replace existing route\nip route replace 10.0.0.0/8 via 192.168.1.2\n\n# Replace default gateway\nip route replace default via 192.168.1.254\n</code></pre>"},{"location":"linux/network/static-routing/#flush-routes","title":"Flush Routes","text":"<pre><code># Flush all routes\nip route flush table main\n\n# Flush routes to specific network\nip route flush 10.0.0.0/8\n\n# Flush cached routes\nip route flush cache\n</code></pre>"},{"location":"linux/network/static-routing/#using-route-command-legacy_1","title":"Using <code>route</code> Command (Legacy)","text":"<pre><code># Add route\nroute add -net 10.0.0.0/8 gw 192.168.1.1\n\n# Add default gateway\nroute add default gw 192.168.1.1\n\n# Add host route\nroute add -host 10.0.0.5 gw 192.168.1.1\n\n# Delete route\nroute del -net 10.0.0.0/8\n\n# Delete default gateway\nroute del default\n\n# Add route via interface\nroute add -net 10.0.0.0/8 dev eth1\n</code></pre>"},{"location":"linux/network/static-routing/#persistent-static-routes","title":"Persistent Static Routes","text":""},{"location":"linux/network/static-routing/#rhelcentosfedora","title":"RHEL/CentOS/Fedora","text":""},{"location":"linux/network/static-routing/#method-1-network-scripts","title":"Method 1: Network Scripts","text":"<p>Create route files: <code>/etc/sysconfig/network-scripts/route-&lt;interface&gt;</code></p> <pre><code># /etc/sysconfig/network-scripts/route-eth0\n10.0.0.0/8 via 192.168.1.1\n172.16.0.0/12 via 192.168.1.1 dev eth0\ndefault via 192.168.1.254\n\n# Format:\n# network/prefix via gateway [dev interface] [metric N]\n</code></pre> <p>Alternative format: <pre><code># /etc/sysconfig/network-scripts/route-eth0\nADDRESS0=10.0.0.0\nNETMASK0=255.0.0.0\nGATEWAY0=192.168.1.1\n\nADDRESS1=172.16.0.0\nNETMASK1=255.240.0.0\nGATEWAY1=192.168.1.1\n</code></pre></p>"},{"location":"linux/network/static-routing/#method-2-networkmanager","title":"Method 2: NetworkManager","text":"<p>Using <code>nmcli</code>: <pre><code># Add static route to connection\nnmcli connection modify eth0 +ipv4.routes \"10.0.0.0/8 192.168.1.1\"\n\n# Add multiple routes\nnmcli connection modify eth0 +ipv4.routes \"10.0.0.0/8 192.168.1.1, 172.16.0.0/12 192.168.1.1\"\n\n# Add route with metric\nnmcli connection modify eth0 +ipv4.routes \"10.0.0.0/8 192.168.1.1 100\"\n\n# Remove route\nnmcli connection modify eth0 -ipv4.routes \"10.0.0.0/8 192.168.1.1\"\n\n# View routes\nnmcli connection show eth0 | grep ipv4.routes\n\n# Apply changes\nnmcli connection up eth0\n\n# IPv6 routes\nnmcli connection modify eth0 +ipv6.routes \"2001:db8::/32 2001:db8::1\"\n</code></pre></p>"},{"location":"linux/network/static-routing/#method-3-networkmanager-configuration-files","title":"Method 3: NetworkManager Configuration Files","text":"<p>Edit: <code>/etc/NetworkManager/system-connections/&lt;connection&gt;.nmconnection</code></p> <pre><code>[ipv4]\nmethod=manual\naddress1=192.168.1.100/24,192.168.1.1\nroute1=10.0.0.0/8,192.168.1.1\nroute2=172.16.0.0/12,192.168.1.1,100\ndns=8.8.8.8;8.8.4.4;\n\n[ipv6]\nmethod=manual\naddress1=2001:db8::100/64,2001:db8::1\nroute1=2001:db8:1::/48,2001:db8::1\n</code></pre> <p>Reload NetworkManager: <pre><code>nmcli connection reload\nnmcli connection up eth0\n</code></pre></p>"},{"location":"linux/network/static-routing/#ubuntudebian-netplan","title":"Ubuntu/Debian (Netplan)","text":"<p>Edit: <code>/etc/netplan/01-netcfg.yaml</code></p> <pre><code>network:\n  version: 2\n  renderer: networkd\n  ethernets:\n    eth0:\n      addresses:\n        - 192.168.1.100/24\n      gateway4: 192.168.1.1\n      routes:\n        - to: 10.0.0.0/8\n          via: 192.168.1.1\n          metric: 100\n        - to: 172.16.0.0/12\n          via: 192.168.1.1\n        - to: 0.0.0.0/0\n          via: 192.168.1.254\n          metric: 200\n      nameservers:\n        addresses:\n          - 8.8.8.8\n          - 8.8.4.4\n</code></pre> <p>IPv6 example: <pre><code>network:\n  version: 2\n  ethernets:\n    eth0:\n      addresses:\n        - 2001:db8::100/64\n      gateway6: 2001:db8::1\n      routes:\n        - to: 2001:db8:1::/48\n          via: 2001:db8::1\n</code></pre></p> <p>Apply configuration: <pre><code>netplan try      # Test configuration\nnetplan apply    # Apply configuration\n</code></pre></p>"},{"location":"linux/network/static-routing/#systemd-networkd","title":"systemd-networkd","text":"<p>Configuration: <code>/etc/systemd/network/</code></p> <pre><code># /etc/systemd/network/20-wired.network\n[Match]\nName=eth0\n\n[Network]\nAddress=192.168.1.100/24\nGateway=192.168.1.1\n\n[Route]\nDestination=10.0.0.0/8\nGateway=192.168.1.1\nMetric=100\n\n[Route]\nDestination=172.16.0.0/12\nGateway=192.168.1.1\n\n[Route]\nDestination=0.0.0.0/0\nGateway=192.168.1.254\nMetric=200\n</code></pre> <p>Restart service: <pre><code>systemctl restart systemd-networkd\n</code></pre></p>"},{"location":"linux/network/static-routing/#multiple-default-gateways","title":"Multiple Default Gateways","text":""},{"location":"linux/network/static-routing/#metric-based-selection","title":"Metric-Based Selection","text":"<pre><code># Primary default gateway (lower metric)\nip route add default via 192.168.1.1 metric 100\n\n# Backup default gateway (higher metric)\nip route add default via 192.168.2.1 metric 200\n\n# View\nip route show\n</code></pre>"},{"location":"linux/network/static-routing/#multiple-gateways-load-balancing","title":"Multiple Gateways (Load Balancing)","text":"<pre><code># Equal-cost multi-path routing\nip route add default \\\n    nexthop via 192.168.1.1 dev eth0 weight 1 \\\n    nexthop via 192.168.2.1 dev eth1 weight 1\n</code></pre>"},{"location":"linux/network/static-routing/#policy-based-routing","title":"Policy-Based Routing","text":""},{"location":"linux/network/static-routing/#routing-tables","title":"Routing Tables","text":""},{"location":"linux/network/static-routing/#view-routing-tables","title":"View Routing Tables","text":"<pre><code># List all tables\ncat /etc/iproute2/rt_tables\n\n# Default tables:\n# 0     unspec\n# 253   default\n# 254   main\n# 255   local\n\n# Add custom table\necho \"100 custom\" &gt;&gt; /etc/iproute2/rt_tables\n</code></pre>"},{"location":"linux/network/static-routing/#manage-routes-in-custom-tables","title":"Manage Routes in Custom Tables","text":"<pre><code># Add route to custom table\nip route add 10.0.0.0/8 via 192.168.1.1 table custom\n\n# Add default gateway to custom table\nip route add default via 192.168.1.1 table custom\n\n# Show routes in custom table\nip route show table custom\n\n# Delete route from table\nip route del 10.0.0.0/8 table custom\n</code></pre>"},{"location":"linux/network/static-routing/#routing-rules-policy-routing","title":"Routing Rules (Policy Routing)","text":""},{"location":"linux/network/static-routing/#view-rules","title":"View Rules","text":"<pre><code># Show routing rules\nip rule show\nip rule list\n\n# Default rules:\n# 0: from all lookup local\n# 32766: from all lookup main\n# 32767: from all lookup default\n</code></pre>"},{"location":"linux/network/static-routing/#add-rules","title":"Add Rules","text":"<p>Source-based routing: <pre><code># Route traffic from specific source via custom table\nip rule add from 192.168.1.100 table custom priority 100\n\n# Route traffic from network\nip rule add from 192.168.1.0/24 table custom priority 100\n</code></pre></p> <p>Destination-based routing: <pre><code># Route traffic to specific destination via custom table\nip rule add to 10.0.0.0/8 table custom priority 100\n</code></pre></p> <p>Interface-based routing: <pre><code># Route traffic arriving on interface\nip rule add iif eth0 table custom priority 100\n\n# Route traffic leaving on interface\nip rule add oif eth1 table custom priority 100\n</code></pre></p> <p>TOS-based routing: <pre><code># Route based on Type of Service\nip rule add tos 0x10 table custom priority 100\n</code></pre></p> <p>Fwmark-based routing: <pre><code># Route based on firewall mark\nip rule add fwmark 1 table custom priority 100\n</code></pre></p> <p>Combined rules: <pre><code># Complex rule\nip rule add from 192.168.1.0/24 to 10.0.0.0/8 table custom priority 100\n</code></pre></p>"},{"location":"linux/network/static-routing/#delete-rules","title":"Delete Rules","text":"<pre><code># Delete by specification\nip rule del from 192.168.1.100 table custom\n\n# Delete by priority\nip rule del priority 100\n\n# Flush all rules (dangerous!)\nip rule flush\n</code></pre>"},{"location":"linux/network/static-routing/#complete-policy-routing-example","title":"Complete Policy Routing Example","text":"<p>Scenario: Route traffic from different networks through different gateways</p> <pre><code># Create custom routing tables\necho \"100 isp1\" &gt;&gt; /etc/iproute2/rt_tables\necho \"200 isp2\" &gt;&gt; /etc/iproute2/rt_tables\n\n# Add routes to tables\nip route add default via 10.0.1.1 table isp1\nip route add default via 10.0.2.1 table isp2\n\n# Add routing rules\nip rule add from 192.168.1.0/24 table isp1 priority 100\nip rule add from 192.168.2.0/24 table isp2 priority 200\n\n# Add routes for local networks in both tables\nip route add 192.168.1.0/24 dev eth1 table isp1\nip route add 192.168.2.0/24 dev eth2 table isp2\n\n# Flush routing cache\nip route flush cache\n</code></pre>"},{"location":"linux/network/static-routing/#source-based-routing-example","title":"Source-Based Routing Example","text":"<p>Route different users through different gateways:</p> <pre><code># Setup\necho \"100 admin_table\" &gt;&gt; /etc/iproute2/rt_tables\n\n# Add default gateway for admin table\nip route add default via 192.168.1.1 table admin_table\n\n# Add local network routes\nip route add 192.168.1.0/24 dev eth0 table admin_table\n\n# Route admin user (192.168.1.100) through specific gateway\nip rule add from 192.168.1.100 table admin_table priority 100\n\n# Verify\nip rule show\nip route show table admin_table\n</code></pre>"},{"location":"linux/network/static-routing/#equal-cost-multi-path-ecmp-routing","title":"Equal-Cost Multi-Path (ECMP) Routing","text":""},{"location":"linux/network/static-routing/#load-balancing-between-gateways","title":"Load Balancing Between Gateways","text":"<pre><code># Add multi-path default route\nip route add default \\\n    nexthop via 192.168.1.1 dev eth0 weight 1 \\\n    nexthop via 192.168.2.1 dev eth1 weight 1\n\n# Unequal weight distribution (2:1 ratio)\nip route add default \\\n    nexthop via 192.168.1.1 dev eth0 weight 2 \\\n    nexthop via 192.168.2.1 dev eth1 weight 1\n\n# View\nip route show\n</code></pre>"},{"location":"linux/network/static-routing/#reverse-path-filtering","title":"Reverse Path Filtering","text":""},{"location":"linux/network/static-routing/#configure-rp_filter","title":"Configure rp_filter","text":"<pre><code># Check current settings\ncat /proc/sys/net/ipv4/conf/all/rp_filter\ncat /proc/sys/net/ipv4/conf/eth0/rp_filter\n\n# Values:\n# 0 = No source validation\n# 1 = Strict mode (recommended)\n# 2 = Loose mode\n\n# Set temporarily\necho 1 &gt; /proc/sys/net/ipv4/conf/all/rp_filter\n\n# Set permanently\necho \"net.ipv4.conf.all.rp_filter = 1\" &gt;&gt; /etc/sysctl.conf\necho \"net.ipv4.conf.default.rp_filter = 1\" &gt;&gt; /etc/sysctl.conf\nsysctl -p\n</code></pre>"},{"location":"linux/network/static-routing/#advanced-routing-features","title":"Advanced Routing Features","text":""},{"location":"linux/network/static-routing/#nexthop-objects-newer-kernels","title":"Nexthop Objects (Newer Kernels)","text":"<pre><code># Create nexthop object\nip nexthop add id 1 via 192.168.1.1 dev eth0\nip nexthop add id 2 via 192.168.2.1 dev eth1\n\n# Create nexthop group\nip nexthop add id 10 group 1/2\n\n# Use nexthop in route\nip route add 10.0.0.0/8 nhid 10\n\n# View nexthops\nip nexthop show\n</code></pre>"},{"location":"linux/network/static-routing/#route-metrics-and-preferences","title":"Route Metrics and Preferences","text":"<pre><code># Lower metric is preferred\nip route add 10.0.0.0/8 via 192.168.1.1 metric 10\nip route add 10.0.0.0/8 via 192.168.2.1 metric 20\n\n# With both routes present, 192.168.1.1 is preferred\n</code></pre>"},{"location":"linux/network/static-routing/#administrative-distance","title":"Administrative Distance","text":"<p>Not directly configurable in Linux, but protocols have default preferences: - Connected: 0 - Static: 1 - OSPF: 110 - RIP: 120</p>"},{"location":"linux/network/static-routing/#route-attributes","title":"Route Attributes","text":"<pre><code># Add route with specific attributes\nip route add 10.0.0.0/8 via 192.168.1.1 \\\n    metric 100 \\\n    mtu 1400 \\\n    advmss 1360\n\n# Show route with all attributes\nip route show 10.0.0.0/8\n</code></pre>"},{"location":"linux/network/static-routing/#troubleshooting-routing","title":"Troubleshooting Routing","text":""},{"location":"linux/network/static-routing/#verify-routes","title":"Verify Routes","text":"<pre><code># Check routing table\nip route show\n\n# Test route to destination\nip route get 8.8.8.8\nip route get 10.0.0.5\n\n# Check specific table\nip route show table custom\n\n# Check rules\nip rule show\n</code></pre>"},{"location":"linux/network/static-routing/#trace-route-path","title":"Trace Route Path","text":"<pre><code># Traceroute\ntraceroute 8.8.8.8\ntraceroute -n 8.8.8.8  # No DNS resolution\n\n# MTR (better)\nmtr 8.8.8.8\nmtr -n 8.8.8.8\n</code></pre>"},{"location":"linux/network/static-routing/#check-ip-forwarding","title":"Check IP Forwarding","text":"<pre><code># Check if enabled\ncat /proc/sys/net/ipv4/ip_forward\n\n# Enable temporarily\necho 1 &gt; /proc/sys/net/ipv4/ip_forward\n\n# Enable permanently\necho \"net.ipv4.ip_forward = 1\" &gt;&gt; /etc/sysctl.conf\nsysctl -p\n</code></pre>"},{"location":"linux/network/static-routing/#debug-routing-issues","title":"Debug Routing Issues","text":"<pre><code># Check interface status\nip link show\nip addr show\n\n# Check if gateway is reachable\nping -c 4 192.168.1.1\n\n# Check ARP table\nip neigh show\narp -n\n\n# Monitor routing changes\nip monitor route\n\n# Check routing cache (older kernels)\nip route show cache\n</code></pre>"},{"location":"linux/network/static-routing/#common-issues","title":"Common Issues","text":"<p>Issue: No route to host <pre><code># Add missing route\nip route add 10.0.0.0/8 via 192.168.1.1\n\n# Or add default gateway\nip route add default via 192.168.1.1\n</code></pre></p> <p>Issue: Asymmetric routing <pre><code># May need to disable rp_filter\necho 0 &gt; /proc/sys/net/ipv4/conf/all/rp_filter\n\n# Or use policy routing\n</code></pre></p> <p>Issue: Routes not persisting <pre><code># Add to configuration files\n# RHEL: /etc/sysconfig/network-scripts/route-*\n# Ubuntu: /etc/netplan/*.yaml\n# Or use NetworkManager\n</code></pre></p>"},{"location":"linux/network/static-routing/#routing-with-multiple-interfaces","title":"Routing with Multiple Interfaces","text":""},{"location":"linux/network/static-routing/#setup-routing-between-interfaces","title":"Setup Routing Between Interfaces","text":"<pre><code># Enable IP forwarding\nsysctl -w net.ipv4.ip_forward=1\n\n# Add routes\nip route add 10.0.0.0/8 via 192.168.1.1 dev eth0\nip route add 172.16.0.0/12 via 192.168.2.1 dev eth1\n\n# Default route\nip route add default via 192.168.1.1 dev eth0\n</code></pre>"},{"location":"linux/network/static-routing/#interface-specific-routing","title":"Interface-Specific Routing","text":"<pre><code># Force traffic out specific interface\nip route add 10.0.0.0/8 dev eth1\nip route add 172.16.0.0/12 dev eth0\n\n# Source-based interface selection\nip rule add from 192.168.1.0/24 oif eth0\nip rule add from 192.168.2.0/24 oif eth1\n</code></pre>"},{"location":"linux/network/static-routing/#monitoring-routing","title":"Monitoring Routing","text":""},{"location":"linux/network/static-routing/#real-time-monitoring","title":"Real-Time Monitoring","text":"<pre><code># Monitor route changes\nip monitor route\n\n# Monitor all IP events\nip monitor\n\n# Monitor specific table\nip monitor route table custom\n\n# Watch routing table\nwatch -n 1 'ip route show'\n</code></pre>"},{"location":"linux/network/static-routing/#routing-statistics","title":"Routing Statistics","text":"<pre><code># View route cache statistics\nip -s route show cache\n\n# Interface statistics\nip -s link show eth0\n\n# Routing protocol statistics (if running dynamic routing)\nvtysh -c \"show ip route\"\n</code></pre>"},{"location":"linux/network/static-routing/#quick-reference-commands","title":"Quick Reference Commands","text":""},{"location":"linux/network/static-routing/#view-routes","title":"View Routes","text":"<pre><code>ip route show                          # Show routing table\nip route show table all                # All tables\nip route get 8.8.8.8                  # Route to destination\nip rule show                          # Show routing rules\n</code></pre>"},{"location":"linux/network/static-routing/#add-routes_1","title":"Add Routes","text":"<pre><code>ip route add 10.0.0.0/8 via 192.168.1.1                    # Basic route\nip route add default via 192.168.1.1                       # Default gateway\nip route add 10.0.0.0/8 via 192.168.1.1 metric 100        # With metric\nip route add 10.0.0.0/8 via 192.168.1.1 table custom      # Custom table\n</code></pre>"},{"location":"linux/network/static-routing/#policy-routing","title":"Policy Routing","text":"<pre><code>echo \"100 custom\" &gt;&gt; /etc/iproute2/rt_tables              # Add table\nip route add default via 192.168.1.1 table custom         # Add route to table\nip rule add from 192.168.1.0/24 table custom priority 100 # Add rule\n</code></pre>"},{"location":"linux/network/static-routing/#delete-routes_1","title":"Delete Routes","text":"<pre><code>ip route del 10.0.0.0/8               # Delete route\nip route del default                   # Delete default\nip rule del priority 100               # Delete rule\n</code></pre>"},{"location":"linux/network/static-routing/#persistent-configuration","title":"Persistent Configuration","text":"<pre><code># RHEL/CentOS\nvim /etc/sysconfig/network-scripts/route-eth0\nnmcli connection modify eth0 +ipv4.routes \"10.0.0.0/8 192.168.1.1\"\n\n# Ubuntu\nvim /etc/netplan/01-netcfg.yaml\nnetplan apply\n</code></pre>"},{"location":"linux/network/static-routing/#practical-examples","title":"Practical Examples","text":""},{"location":"linux/network/static-routing/#example-1-multi-homed-host","title":"Example 1: Multi-Homed Host","text":"<pre><code># Host with two network connections\n# eth0: 192.168.1.100/24 (ISP1 - gateway 192.168.1.1)\n# eth1: 192.168.2.100/24 (ISP2 - gateway 192.168.2.1)\n\n# Setup tables\necho \"100 isp1\" &gt;&gt; /etc/iproute2/rt_tables\necho \"200 isp2\" &gt;&gt; /etc/iproute2/rt_tables\n\n# Add routes\nip route add default via 192.168.1.1 table isp1\nip route add default via 192.168.2.1 table isp2\nip route add 192.168.1.0/24 dev eth0 table isp1\nip route add 192.168.2.0/24 dev eth1 table isp2\n\n# Add rules\nip rule add from 192.168.1.100 table isp1\nip rule add from 192.168.2.100 table isp2\n\n# Main table default (for locally generated traffic)\nip route add default via 192.168.1.1 metric 100\nip route add default via 192.168.2.1 metric 200\n</code></pre>"},{"location":"linux/network/static-routing/#example-2-vpn-routing","title":"Example 2: VPN Routing","text":"<pre><code># Route specific traffic through VPN\n# VPN interface: tun0, VPN gateway: 10.8.0.1\n\n# Add route for specific network through VPN\nip route add 10.0.0.0/8 via 10.8.0.1 dev tun0\n\n# Or use policy routing\necho \"100 vpn\" &gt;&gt; /etc/iproute2/rt_tables\nip route add default via 10.8.0.1 dev tun0 table vpn\nip rule add from 192.168.1.100 table vpn\n</code></pre>"},{"location":"linux/network/static-routing/#example-3-dmz-routing","title":"Example 3: DMZ Routing","text":"<pre><code># Router with three interfaces\n# eth0: WAN (Internet) - 203.0.113.5\n# eth1: LAN - 192.168.1.1/24\n# eth2: DMZ - 10.0.0.1/24\n\n# Enable forwarding\necho 1 &gt; /proc/sys/net/ipv4/ip_forward\n\n# Routes\nip route add default via 203.0.113.1 dev eth0\nip route add 192.168.1.0/24 dev eth1\nip route add 10.0.0.0/24 dev eth2\n\n# Allow LAN to DMZ\n# (firewall rules needed too)\n</code></pre>"},{"location":"linux/network/static-routing/#exam-tips","title":"Exam Tips","text":"<ul> <li>Know how to add/delete/modify routes with <code>ip route</code></li> <li>Understand routing tables and policy-based routing</li> <li>Be familiar with persistent route configuration</li> <li>Know how to troubleshoot with <code>ip route get</code></li> <li>Understand metrics and route selection</li> <li>Practice multi-path routing scenarios</li> <li>Know the difference between runtime and persistent routes</li> <li>Be comfortable with both RHEL and Debian-based configurations</li> <li>Understand source-based routing concepts</li> <li>Know how to verify routing with traceroute/mtr</li> <li>Remember to enable IP forwarding for routing between interfaces</li> <li>Practice reading and understanding routing table output</li> </ul>"},{"location":"linux/network/time-synchronization/","title":"Set and Synchronize System Time Using Time Servers","text":""},{"location":"linux/network/time-synchronization/#overview","title":"Overview","text":"<p>This guide covers time synchronization, NTP (Network Time Protocol), Chrony, and time zone management in Linux systems.</p>"},{"location":"linux/network/time-synchronization/#time-concepts","title":"Time Concepts","text":"<ul> <li>System Time: Current time maintained by the kernel</li> <li>Hardware Clock (RTC): Real-Time Clock, battery-powered clock in the hardware</li> <li>UTC: Coordinated Universal Time (standard reference)</li> <li>Local Time: Time adjusted for timezone</li> <li>NTP: Network Time Protocol for time synchronization</li> <li>Stratum: Distance from reference clock (lower is better, 0 is atomic clock)</li> </ul>"},{"location":"linux/network/time-synchronization/#view-current-time","title":"View Current Time","text":""},{"location":"linux/network/time-synchronization/#date-command","title":"<code>date</code> Command","text":"<pre><code># Show current date and time\ndate\n\n# Show in UTC\ndate -u\n\n# Show in specific format\ndate \"+%Y-%m-%d %H:%M:%S\"\ndate \"+%A, %B %d, %Y\"\n\n# Show time in seconds since epoch (1970-01-01)\ndate +%s\n\n# Show date from timestamp\ndate -d @1609459200\n</code></pre>"},{"location":"linux/network/time-synchronization/#timedatectl-command-systemd","title":"<code>timedatectl</code> Command (systemd)","text":"<pre><code># Show time and date settings\ntimedatectl\n\n# Output includes:\n# - Local time\n# - Universal time (UTC)\n# - RTC time\n# - Time zone\n# - NTP enabled status\n# - NTP synchronized status\n\n# Show available timezones\ntimedatectl list-timezones\n\n# Show time status in detail\ntimedatectl status\n</code></pre>"},{"location":"linux/network/time-synchronization/#set-system-time","title":"Set System Time","text":""},{"location":"linux/network/time-synchronization/#manual-time-setting","title":"Manual Time Setting","text":""},{"location":"linux/network/time-synchronization/#using-date","title":"Using <code>date</code>","text":"<pre><code># Set time manually (format: MMDDhhmmYYYY.ss)\ndate 123123592024.00  # December 31, 23:59:00, 2024\n\n# Set from string\ndate -s \"2024-12-31 23:59:00\"\ndate -s \"31 DEC 2024 23:59:00\"\n\n# Set date only\ndate --set=\"2024-12-31\"\n\n# Set time only\ndate --set=\"23:59:00\"\n</code></pre>"},{"location":"linux/network/time-synchronization/#using-timedatectl","title":"Using <code>timedatectl</code>","text":"<pre><code># Set date and time\ntimedatectl set-time \"2024-12-31 23:59:00\"\n\n# Set date only\ntimedatectl set-time \"2024-12-31\"\n\n# Note: NTP must be disabled to set time manually\ntimedatectl set-ntp false\n</code></pre>"},{"location":"linux/network/time-synchronization/#time-zone-management","title":"Time Zone Management","text":""},{"location":"linux/network/time-synchronization/#view-time-zone","title":"View Time Zone","text":"<pre><code># Using timedatectl\ntimedatectl\n\n# Check timezone file\nls -l /etc/localtime\n\n# View timezone name\ncat /etc/timezone  # Debian/Ubuntu\n</code></pre>"},{"location":"linux/network/time-synchronization/#set-time-zone","title":"Set Time Zone","text":""},{"location":"linux/network/time-synchronization/#using-timedatectl_1","title":"Using <code>timedatectl</code>","text":"<pre><code># List all available timezones\ntimedatectl list-timezones\n\n# Filter timezones\ntimedatectl list-timezones | grep America\ntimedatectl list-timezones | grep Europe/\n\n# Set timezone\ntimedatectl set-timezone America/New_York\ntimedatectl set-timezone Europe/London\ntimedatectl set-timezone Asia/Tokyo\ntimedatectl set-timezone UTC\n\n# Verify\ntimedatectl\n</code></pre>"},{"location":"linux/network/time-synchronization/#manual-method","title":"Manual Method","text":"<pre><code># Create symbolic link to timezone file\nln -sf /usr/share/zoneinfo/America/New_York /etc/localtime\n\n# Update /etc/timezone (Debian/Ubuntu)\necho \"America/New_York\" &gt; /etc/timezone\n</code></pre>"},{"location":"linux/network/time-synchronization/#hardware-clock-rtc","title":"Hardware Clock (RTC)","text":""},{"location":"linux/network/time-synchronization/#hwclock-command","title":"<code>hwclock</code> Command","text":"<pre><code># Show hardware clock time\nhwclock --show\nhwclock -r\n\n# Show in UTC\nhwclock --show --utc\n\n# Show in local time\nhwclock --show --localtime\n\n# Set hardware clock from system time\nhwclock --systohc\n\n# Set system time from hardware clock\nhwclock --hctosys\n\n# Set hardware clock manually\nhwclock --set --date=\"2024-12-31 23:59:00\"\n</code></pre>"},{"location":"linux/network/time-synchronization/#hardware-clock-mode","title":"Hardware Clock Mode","text":"<pre><code># Check if RTC is in UTC or local time\ntimedatectl | grep \"RTC in local TZ\"\n\n# Set RTC to UTC (recommended)\ntimedatectl set-local-rtc 0\n\n# Set RTC to local time (not recommended, can cause issues)\ntimedatectl set-local-rtc 1\n</code></pre>"},{"location":"linux/network/time-synchronization/#ntp-network-time-protocol","title":"NTP - Network Time Protocol","text":""},{"location":"linux/network/time-synchronization/#check-ntp-status","title":"Check NTP Status","text":"<pre><code># Using timedatectl\ntimedatectl status\ntimedatectl timesync-status\n\n# Show NTP service status\ntimedatectl show-timesync --all\n</code></pre>"},{"location":"linux/network/time-synchronization/#enabledisable-ntp","title":"Enable/Disable NTP","text":"<pre><code># Enable NTP synchronization\ntimedatectl set-ntp true\n\n# Disable NTP synchronization\ntimedatectl set-ntp false\n\n# Check if enabled\ntimedatectl | grep \"NTP service\"\n</code></pre>"},{"location":"linux/network/time-synchronization/#chrony-modern-ntp-implementation","title":"Chrony (Modern NTP Implementation)","text":""},{"location":"linux/network/time-synchronization/#chrony-overview","title":"Chrony Overview","text":"<p>Chrony is the modern replacement for ntpd, designed for: - Better performance on systems that are intermittently connected - Faster synchronization - Better handling of network congestion - Works well with virtual machines</p>"},{"location":"linux/network/time-synchronization/#installation","title":"Installation","text":"<pre><code># RHEL/CentOS/Fedora\ndnf install chrony\n\n# Ubuntu/Debian\napt install chrony\n</code></pre>"},{"location":"linux/network/time-synchronization/#chrony-service-management","title":"Chrony Service Management","text":"<pre><code># Start chronyd service\nsystemctl start chronyd\n\n# Stop chronyd service\nsystemctl stop chronyd\n\n# Restart chronyd service\nsystemctl restart chronyd\n\n# Enable at boot\nsystemctl enable chronyd\n\n# Check status\nsystemctl status chronyd\n\n# Check if running\nsystemctl is-active chronyd\n</code></pre>"},{"location":"linux/network/time-synchronization/#chrony-configuration-file","title":"Chrony Configuration File","text":"<p>Location: <code>/etc/chrony.conf</code> (RHEL) or <code>/etc/chrony/chrony.conf</code> (Ubuntu)</p> <p>Example configuration: <pre><code># Use public NTP servers from pool.ntp.org\npool pool.ntp.org iburst\npool 0.pool.ntp.org iburst\npool 1.pool.ntp.org iburst\n\n# Or use specific servers\nserver 0.rhel.pool.ntp.org iburst\nserver 1.rhel.pool.ntp.org iburst\nserver time.google.com iburst\n\n# Record rate of clock gain/loss\ndriftfile /var/lib/chrony/drift\n\n# Allow system clock to be stepped in the first three updates\nmakestep 1.0 3\n\n# Enable RTC synchronization\nrtcsync\n\n# Serve time to local network (optional)\nallow 192.168.1.0/24\n\n# Log directory\nlogdir /var/log/chrony\n</code></pre></p> <p>Configuration options explained: - <code>pool</code>: Use pool of NTP servers - <code>server</code>: Use specific NTP server - <code>iburst</code>: Send burst of packets at startup for faster sync - <code>makestep</code>: Allow clock to be stepped if offset is large - <code>driftfile</code>: File to record clock drift rate - <code>rtcsync</code>: Enable kernel RTC synchronization - <code>allow</code>: Allow clients from this network</p>"},{"location":"linux/network/time-synchronization/#chronyc-command-chrony-client","title":"<code>chronyc</code> Command (Chrony Client)","text":""},{"location":"linux/network/time-synchronization/#view-time-sources","title":"View Time Sources","text":"<pre><code># Show NTP sources\nchronyc sources\n\n# Show sources with verbose output\nchronyc sources -v\n\n# Columns explained:\n# M: Mode (^ = server, = = peer)\n# S: State (* = current sync source, + = acceptable, - = not acceptable)\n# Stratum: Distance from reference clock\n# Reach: Reachability (octal, 377 = all 8 recent attempts successful)\n# LastRx: Time since last sample\n# Poll: Polling interval\n\n# Show detailed source information\nchronyc sourcestats\n\n# Show source information with verbose output\nchronyc sourcestats -v\n</code></pre>"},{"location":"linux/network/time-synchronization/#tracking-information","title":"Tracking Information","text":"<pre><code># Show system clock performance\nchronyc tracking\n\n# Output includes:\n# - Reference ID and stratum\n# - System time offset\n# - Root delay and dispersion\n# - Update interval\n# - Leap status\n</code></pre>"},{"location":"linux/network/time-synchronization/#manual-time-synchronization","title":"Manual Time Synchronization","text":"<pre><code># Force immediate synchronization\nchronyc makestep\n\n# Add new server temporarily\nchronyc add server time.google.com\n\n# Delete server\nchronyc delete time.google.com\n</code></pre>"},{"location":"linux/network/time-synchronization/#activity-and-statistics","title":"Activity and Statistics","text":"<pre><code># Show how many servers/peers are online\nchronyc activity\n\n# Show NTP authentication status\nchronyc authdata\n\n# Show client access\nchronyc clients\n</code></pre>"},{"location":"linux/network/time-synchronization/#testing-and-diagnostics","title":"Testing and Diagnostics","text":"<pre><code># Perform burst of measurements\nchronyc burst 3/5\n\n# Check server reachability\nchronyc reachability\n\n# Dump all measurements\nchronyc ntpdata\n</code></pre>"},{"location":"linux/network/time-synchronization/#legacy-ntp-daemon-ntpd","title":"Legacy NTP Daemon (ntpd)","text":""},{"location":"linux/network/time-synchronization/#installation_1","title":"Installation","text":"<pre><code># RHEL/CentOS/Fedora\ndnf install ntp\n\n# Ubuntu/Debian\napt install ntp\n</code></pre>"},{"location":"linux/network/time-synchronization/#configuration","title":"Configuration","text":"<p>Location: <code>/etc/ntp.conf</code></p> <p>Example configuration: <pre><code># Use public servers\nserver 0.pool.ntp.org iburst\nserver 1.pool.ntp.org iburst\nserver 2.pool.ntp.org iburst\n\n# Drift file\ndriftfile /var/lib/ntp/drift\n\n# Access restrictions\nrestrict default nomodify notrap nopeer noquery\nrestrict 127.0.0.1\nrestrict ::1\n\n# Allow local network to query\nrestrict 192.168.1.0 mask 255.255.255.0 nomodify notrap\n</code></pre></p>"},{"location":"linux/network/time-synchronization/#service-management","title":"Service Management","text":"<pre><code># Start service\nsystemctl start ntpd\n\n# Enable at boot\nsystemctl enable ntpd\n\n# Check status\nsystemctl status ntpd\n</code></pre>"},{"location":"linux/network/time-synchronization/#ntpq-command","title":"<code>ntpq</code> Command","text":"<pre><code># Show peers\nntpq -p\n\n# Show more detailed peer information\nntpq -pn\n\n# Interactive mode\nntpq\nntpq&gt; peers\nntpq&gt; associations\nntpq&gt; quit\n</code></pre>"},{"location":"linux/network/time-synchronization/#ntpdate-command-deprecated","title":"<code>ntpdate</code> Command (deprecated)","text":"<pre><code># Synchronize time once (deprecated, use chronyc or ntpd)\nntpdate pool.ntp.org\n\n# Note: Cannot run while ntpd is running\nsystemctl stop ntpd\nntpdate pool.ntp.org\nsystemctl start ntpd\n</code></pre>"},{"location":"linux/network/time-synchronization/#systemd-timesyncd-lightweight-ntp","title":"Systemd-timesyncd (Lightweight NTP)","text":""},{"location":"linux/network/time-synchronization/#overview_1","title":"Overview","text":"<p>Simple SNTP client included with systemd, used when neither Chrony nor ntpd is installed.</p>"},{"location":"linux/network/time-synchronization/#configuration_1","title":"Configuration","text":"<p>Location: <code>/etc/systemd/timesyncd.conf</code></p> <pre><code>[Time]\nNTP=0.pool.ntp.org 1.pool.ntp.org\nFallbackNTP=time.google.com\n</code></pre>"},{"location":"linux/network/time-synchronization/#service-management_1","title":"Service Management","text":"<pre><code># Start service\nsystemctl start systemd-timesyncd\n\n# Enable at boot\nsystemctl enable systemd-timesyncd\n\n# Check status\nsystemctl status systemd-timesyncd\n\n# View sync status\ntimedatectl timesync-status\n</code></pre>"},{"location":"linux/network/time-synchronization/#public-ntp-server-pools","title":"Public NTP Server Pools","text":""},{"location":"linux/network/time-synchronization/#common-ntp-servers","title":"Common NTP Servers","text":"<pre><code># Generic pool\npool.ntp.org\n0.pool.ntp.org\n1.pool.ntp.org\n2.pool.ntp.org\n3.pool.ntp.org\n\n# Regional pools\nus.pool.ntp.org\neurope.pool.ntp.org\nasia.pool.ntp.org\n\n# Vendor-specific pools\n0.rhel.pool.ntp.org\n0.ubuntu.pool.ntp.org\n0.centos.pool.ntp.org\n\n# Public servers\ntime.google.com\ntime.cloudflare.com\ntime.nist.gov\n</code></pre>"},{"location":"linux/network/time-synchronization/#firewall-configuration-for-ntp","title":"Firewall Configuration for NTP","text":"<pre><code># Allow NTP traffic (UDP port 123)\nfirewall-cmd --permanent --add-service=ntp\nfirewall-cmd --reload\n\n# Or manually with port\nfirewall-cmd --permanent --add-port=123/udp\nfirewall-cmd --reload\n\n# For iptables\niptables -A INPUT -p udp --dport 123 -j ACCEPT\niptables -A OUTPUT -p udp --sport 123 -j ACCEPT\n</code></pre>"},{"location":"linux/network/time-synchronization/#troubleshooting-time-synchronization","title":"Troubleshooting Time Synchronization","text":""},{"location":"linux/network/time-synchronization/#check-time-status","title":"Check Time Status","text":"<pre><code># Overall status\ntimedatectl status\n\n# Chrony tracking\nchronyc tracking\nchronyc sources -v\n\n# Check service\nsystemctl status chronyd\njournalctl -u chronyd -f\n</code></pre>"},{"location":"linux/network/time-synchronization/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"linux/network/time-synchronization/#1-time-not-synchronizing","title":"1. Time Not Synchronizing","text":"<pre><code># Check if NTP is enabled\ntimedatectl | grep \"NTP service\"\n\n# Enable if disabled\ntimedatectl set-ntp true\n\n# Restart chronyd\nsystemctl restart chronyd\n\n# Check sources\nchronyc sources -v\n\n# Force synchronization\nchronyc makestep\n</code></pre>"},{"location":"linux/network/time-synchronization/#2-large-time-offset","title":"2. Large Time Offset","text":"<pre><code># Check current offset\nchronyc tracking\n\n# If offset is very large, may need to step\nchronyc makestep\n\n# Or restart service\nsystemctl restart chronyd\n</code></pre>"},{"location":"linux/network/time-synchronization/#3-no-sources-available","title":"3. No Sources Available","text":"<pre><code># Check network connectivity\nping pool.ntp.org\n\n# Check DNS resolution\nnslookup pool.ntp.org\n\n# Check firewall\nfirewall-cmd --list-all | grep ntp\n\n# Test NTP port\nnc -u pool.ntp.org 123\n</code></pre>"},{"location":"linux/network/time-synchronization/#4-sources-not-reachable","title":"4. Sources Not Reachable","text":"<pre><code># Check sources\nchronyc sources\n\n# Look at Reach column (should be 377 for good connectivity)\n# If showing 0, check:\n# - Firewall rules\n# - Network connectivity\n# - Server configuration\n\n# Try different servers\nchronyc add server time.google.com\n</code></pre>"},{"location":"linux/network/time-synchronization/#log-files","title":"Log Files","text":"<pre><code># Chrony logs\njournalctl -u chronyd\ntail -f /var/log/chrony/*.log\n\n# System logs\njournalctl -xe\n\n# Check for time-related errors\njournalctl | grep -i \"time\\|clock\\|ntp\\|chrony\"\n</code></pre>"},{"location":"linux/network/time-synchronization/#best-practices","title":"Best Practices","text":"<ol> <li>Use Chrony instead of ntpd for most modern systems</li> <li>Use pool servers (pool.ntp.org) for redundancy</li> <li>Use iburst option for faster initial synchronization</li> <li>Set RTC to UTC to avoid timezone issues</li> <li>Monitor synchronization regularly with chronyc tracking</li> <li>Allow firewall access for NTP (UDP 123)</li> <li>Use regional pools for better latency</li> <li>Keep chronyd running continuously for best accuracy</li> <li>Check logs regularly for synchronization issues</li> <li>Enable at boot to ensure time is correct after restart</li> </ol>"},{"location":"linux/network/time-synchronization/#quick-reference-commands","title":"Quick Reference Commands","text":""},{"location":"linux/network/time-synchronization/#view-time","title":"View Time","text":"<pre><code>date                          # Current date/time\ntimedatectl                   # Full time status\ntimedatectl status            # Detailed status\nhwclock --show                # Hardware clock\n</code></pre>"},{"location":"linux/network/time-synchronization/#set-time","title":"Set Time","text":"<pre><code>timedatectl set-time \"YYYY-MM-DD HH:MM:SS\"\ntimedatectl set-timezone America/New_York\ntimedatectl set-ntp true\n</code></pre>"},{"location":"linux/network/time-synchronization/#chrony","title":"Chrony","text":"<pre><code>systemctl start chronyd       # Start service\nchronyc sources               # Show sources\nchronyc tracking              # Show sync status\nchronyc makestep              # Force sync\n</code></pre>"},{"location":"linux/network/time-synchronization/#troubleshooting","title":"Troubleshooting","text":"<pre><code>timedatectl timesync-status   # Sync status\njournalctl -u chronyd         # View logs\nchronyc sources -v            # Detailed sources\nchronyc activity              # Source activity\n</code></pre>"},{"location":"linux/network/time-synchronization/#exam-tips","title":"Exam Tips","text":"<ul> <li>Know how to use both <code>timedatectl</code> and <code>chronyc</code></li> <li>Understand the difference between system time and hardware clock</li> <li>Be able to configure timezone correctly</li> <li>Know how to enable/disable NTP synchronization</li> <li>Understand chrony.conf configuration options</li> <li>Be able to troubleshoot synchronization issues</li> <li>Remember the <code>iburst</code> option for faster sync</li> <li>Know how to check synchronization status</li> <li>Understand the importance of UTC for RTC</li> <li>Practice manual time setting (with NTP disabled)</li> </ul>"},{"location":"linux/storage/automounters/","title":"Configure Filesystem Automounters","text":""},{"location":"linux/storage/automounters/#what-are-automounters","title":"What are Automounters?","text":"<p>Automounters automatically mount filesystems when you access them, and automatically unmount them when they\u2019re not being used. Think of them as smart mounting - the system handles the mounting/unmounting for you.</p>"},{"location":"linux/storage/automounters/#why-use-automounters","title":"Why Use Automounters?","text":"<p>Without automount:</p> <ul> <li>Mount 20 network shares at boot</li> <li>Uses resources even if not needed</li> <li>Boot takes longer</li> <li>More points of failure</li> </ul> <p>With automount:</p> <ul> <li>Mount on-demand (only when accessed)</li> <li>Unmount when idle (free resources)</li> <li>Faster boot</li> <li>More resilient to network issues</li> </ul> <p>Common uses:</p> <ul> <li>User home directories on networks</li> <li>Shared company data</li> <li>USB/CD-ROM auto-mounting</li> <li>NFS shares that aren\u2019t always needed</li> <li>Mounting many filesystems efficiently</li> </ul>"},{"location":"linux/storage/automounters/#how-automount-works","title":"How Automount Works","text":"<p>Traditional mounting:</p> <pre><code># You manually mount\nmount server:/home/john /home/john\n\n# Must manually unmount\numount /home/john\n</code></pre> <p>Automounting:</p> <pre><code># You just access the directory\ncd /home/john\n\n# System automatically:\n# 1. Detects access\n# 2. Mounts server:/home/john\n# 3. Gives you access\n\n# After 5 minutes of inactivity:\n# - System automatically unmounts\n</code></pre>"},{"location":"linux/storage/automounters/#components","title":"Components","text":"<p>Master Map (<code>/etc/auto.master</code>):</p> <ul> <li>Main configuration</li> <li>Points to other maps</li> <li>Sets global options</li> </ul> <p>Map Files:</p> <ul> <li>Define what to mount and where</li> <li>Can be files, LDAP, NIS, etc.</li> </ul> <p>autofs Service:</p> <ul> <li>The daemon that does the work</li> <li>Monitors access to mount points</li> <li>Handles mounting/unmounting</li> </ul>"},{"location":"linux/storage/automounters/#installing-autofs","title":"Installing autofs","text":"<pre><code># RHEL/CentOS/Rocky\ndnf install autofs\nsystemctl enable --now autofs\n\n# Debian/Ubuntu\napt install autofs\nsystemctl enable --now autofs\n\n# Check status\nsystemctl status autofs\n</code></pre>"},{"location":"linux/storage/automounters/#master-map-etcautomaster","title":"Master Map - /etc/auto.master","text":"<p>What it is: The main configuration file that tells autofs where to look for mount definitions.</p> <p>Format:</p> <pre><code>mount-point  map-file  [options]\n</code></pre> <p>Examples:</p> <pre><code># Simple indirect map\n/misc /etc/auto.misc\n\n# Direct map (special mount point /-)\n/- /etc/auto.direct\n\n# With timeout (unmount after 60 seconds)\n/home /etc/auto.home --timeout=60\n\n# Multiple maps\n/misc /etc/auto.misc\n/net /etc/auto.net\n/- /etc/auto.direct\n</code></pre> <p>Common options:</p> <ul> <li><code>--timeout=N</code> - Unmount after N seconds idle (default 300)</li> <li><code>--ghost</code> - Create empty directories for mount points</li> <li><code>--browse</code> - Show all available mounts (uses more resources)</li> </ul>"},{"location":"linux/storage/automounters/#indirect-maps-most-common-type","title":"Indirect Maps - Most Common Type","text":""},{"location":"linux/storage/automounters/#what-are-indirect-maps","title":"What are Indirect Maps?","text":"<p>With indirect maps, all mounts appear under a parent directory. The parent is defined in the master map, and the map file defines subdirectories.</p> <p>Example:</p> <ul> <li>Master map says: <code>/data</code> uses <code>/etc/auto.data</code></li> <li>Map file says: <code>projects</code> mounts from <code>server:/projects</code></li> <li>Result: Accessing <code>/data/projects</code> automatically mounts <code>server:/projects</code></li> </ul>"},{"location":"linux/storage/automounters/#creating-indirect-maps","title":"Creating Indirect Maps","text":"<p>Real-world scenario - Company file shares:</p> <pre><code># Step 1: Edit master map\nvi /etc/auto.master\n\n# Add line:\n/company /etc/auto.company --timeout=300\n\n# Step 2: Create map file\nvi /etc/auto.company\n\n# Add shares:\nprojects    -rw,soft,intr    nfs-server:/export/projects\ndocs        -ro,soft         nfs-server:/export/documentation  \nshared      -rw,soft,intr    nfs-server:/export/shared\n\n# Step 3: Reload autofs\nsystemctl reload autofs\n\n# Step 4: Test\nls /company/projects\n# Automatically mounts nfs-server:/export/projects!\n\n# Step 5: Check mount\ndf -h | grep projects\nmount | grep projects\n\n# After 5 minutes of inactivity, it auto-unmounts\n</code></pre>"},{"location":"linux/storage/automounters/#map-file-format","title":"Map File Format","text":"<pre><code>key  [options]  location\n</code></pre> <p>Components:</p> <ul> <li>key: Subdirectory name (becomes /mount-point/key)</li> <li>options: Mount options (optional)</li> <li>location: What to mount</li> </ul> <p>Examples:</p> <pre><code># NFS mount\ndata    -rw,soft,intr    server:/export/data\n\n# Multiple NFS servers (failover)\nbackup  -ro    server1:/backup  server2:/backup  server3:/backup\n\n# Local device\ncdrom   -fstype=iso9660,ro    :/dev/cdrom\n\n# Bind mount (mount local directory)\narchive -fstype=bind    :/backup/archive\n</code></pre>"},{"location":"linux/storage/automounters/#direct-maps-specific-paths","title":"Direct Maps - Specific Paths","text":""},{"location":"linux/storage/automounters/#what-are-direct-maps","title":"What are Direct Maps?","text":"<p>Direct maps let you mount to any specific path, not just under a parent directory.</p> <p>Example:</p> <ul> <li>Want to mount at <code>/data/warehouse</code> (not <code>/something/warehouse</code>)</li> <li>Use direct map with <code>/data/warehouse</code> as the path</li> </ul>"},{"location":"linux/storage/automounters/#creating-direct-maps","title":"Creating Direct Maps","text":"<p>Real-world scenario - Specific locations:</p> <pre><code># Step 1: Edit master map\nvi /etc/auto.master\n\n# Add direct map line:\n/- /etc/auto.direct\n\n# Step 2: Create direct map\nvi /etc/auto.direct\n\n# Add mounts with full paths:\n/data/warehouse    -rw,soft,intr    storage:/export/warehouse\n/data/analytics    -rw,soft,intr    storage:/export/analytics\n/opt/shared        -ro              fileserver:/export/tools\n/mnt/archive       -ro              backup:/export/archive\n\n# Step 3: Reload\nsystemctl reload autofs\n\n# Step 4: Test\nls /data/warehouse\n# Mounts automatically!\n</code></pre>"},{"location":"linux/storage/automounters/#common-configurations","title":"Common Configurations","text":""},{"location":"linux/storage/automounters/#user-home-directories","title":"User Home Directories","text":"<p>Scenario: Network home directories. When john logs in, their home mounts automatically.</p> <pre><code># Master map\n/home /etc/auto.home\n\n# Map file (/etc/auto.home)\n*    -rw,soft,intr    nfs-server:/home/&amp;\n\n# How it works:\n# User \"john\" accesses /home/john\n# * matches \"john\"\n# &amp; is replaced with \"john\"\n# Result: mounts nfs-server:/home/john to /home/john\n</code></pre> <p>Full example:</p> <pre><code># Step 1: Master map\necho \"/home /etc/auto.home --timeout=600\" &gt;&gt; /etc/auto.master\n\n# Step 2: Map file\ncat &gt; /etc/auto.home &lt;&lt; 'EOF'\n*    -rw,soft,intr,rsize=8192,wsize=8192    homeserver:/home/&amp;\nEOF\n\n# Step 3: Reload\nsystemctl reload autofs\n\n# Now when any user logs in, their home auto-mounts!\n</code></pre>"},{"location":"linux/storage/automounters/#multiple-data-shares","title":"Multiple Data Shares","text":"<p>Scenario: Company has several shared drives.</p> <pre><code># Master map\n/shares /etc/auto.shares\n\n# Map file\nvi /etc/auto.shares\n\nprojects    -rw,soft    proj-server:/projects\nfinance     -rw,soft    fin-server:/finance\nhr          -ro,soft    hr-server:/human-resources\nmarketing   -rw,soft    mkt-server:/marketing\n\n# Result:\n# /shares/projects \u2192 proj-server:/projects\n# /shares/finance  \u2192 fin-server:/finance\n# /shares/hr       \u2192 hr-server:/human-resources\n# /shares/marketing \u2192 mkt-server:/marketing\n</code></pre>"},{"location":"linux/storage/automounters/#windows-cifs-shares","title":"Windows (CIFS) Shares","text":"<p>Scenario: Mount Windows file servers.</p> <pre><code># Master map\n/windows /etc/auto.windows\n\n# Map file\nvi /etc/auto.windows\n\nshare1  -fstype=cifs,credentials=/root/.smbcreds,uid=1000,gid=1000  ://fileserver/share1\nshare2  -fstype=cifs,credentials=/root/.smbcreds,uid=1000,gid=1000  ://fileserver/share2\n\n# Credentials file\ncat &gt; /root/.smbcreds &lt;&lt; EOF\nusername=domain\\\\user\npassword=SecurePass123\ndomain=COMPANY\nEOF\nchmod 600 /root/.smbcreds\n\n# Reload\nsystemctl reload autofs\n</code></pre>"},{"location":"linux/storage/automounters/#removable-media","title":"Removable Media","text":"<p>Scenario: Auto-mount USB drives and CDs.</p> <pre><code># Master map\n/media /etc/auto.media\n\n# Map file\nvi /etc/auto.media\n\ncdrom   -fstype=iso9660,ro,nosuid,nodev    :/dev/cdrom\nusb     -fstype=auto,nodev,nosuid          :/dev/sdb1\n</code></pre>"},{"location":"linux/storage/automounters/#advanced-features","title":"Advanced Features","text":""},{"location":"linux/storage/automounters/#wildcards-and-variables","title":"Wildcards and Variables","text":"<p>Using wildcards:</p> <pre><code># In auto.home\n*    -rw    server:/home/&amp;\n\n# Matches any username\n# &amp; is replaced with matched value\n</code></pre> <p>Variables available:</p> <ul> <li><code>${key}</code> - The matched key</li> <li><code>${*}</code> - Everything matched</li> <li><code>$USER</code> - Current user (in some contexts)</li> </ul> <p>Example with subdirectories:</p> <pre><code># Map file\n*/*  -rw,soft  server:/dept/${0}\n\n# Accessing /data/sales/reports\n# Mounts: server:/dept/sales/reports\n</code></pre>"},{"location":"linux/storage/automounters/#multiple-server-failover","title":"Multiple Server Failover","text":"<p>Scenario: Use backup servers if primary fails.</p> <pre><code># Try servers in order\ndata  -ro  server1:/data  server2:/data  server3:/data\n\n# First available server is used\n# Automatic failover if one fails\n</code></pre>"},{"location":"linux/storage/automounters/#nested-automounts","title":"Nested Automounts","text":"<p>Scenario: Organize mounts in hierarchies.</p> <pre><code># Master map\n/company /etc/auto.company\n\n# /etc/auto.company\ndata    /etc/auto.company.data\nprojects /etc/auto.company.projects\n\n# /etc/auto.company.data\ncurrent  -rw,soft  server:/data/current\narchive  -ro,soft  server:/data/archive\n\n# Result:\n# /company/data/current\n# /company/data/archive\n# /company/projects/...\n</code></pre>"},{"location":"linux/storage/automounters/#managing-autofs","title":"Managing autofs","text":""},{"location":"linux/storage/automounters/#service-control","title":"Service Control","text":"<pre><code># Start autofs\nsystemctl start autofs\n\n# Stop autofs (unmounts all autofs mounts)\nsystemctl stop autofs\n\n# Reload configuration (safer, doesn't unmount)\nsystemctl reload autofs\n\n# Restart (unmounts everything, reloads)\nsystemctl restart autofs\n\n# Check status\nsystemctl status autofs\n\n# Enable at boot\nsystemctl enable autofs\n</code></pre> <p>Always use <code>reload</code> instead of <code>restart</code> when possible!</p>"},{"location":"linux/storage/automounters/#checking-configuration","title":"Checking Configuration","text":"<pre><code># Test configuration without starting\nautomount -f -v\n\n# Run in debug mode (foreground)\nsystemctl stop autofs\nautomount -f -d\n\n# View logs\njournalctl -u autofs -f\ntail -f /var/log/messages | grep automount\n</code></pre>"},{"location":"linux/storage/automounters/#troubleshooting","title":"Troubleshooting","text":""},{"location":"linux/storage/automounters/#problem-mount-not-working","title":"Problem: Mount Not Working","text":"<p>Steps to diagnose:</p> <pre><code># Step 1: Check service\nsystemctl status autofs\n\n# Step 2: Check configuration syntax\ncat /etc/auto.master\ncat /etc/auto.misc\n\n# Step 3: Test access\nls /misc/share\ncd /misc/share\n\n# Step 4: Check logs\njournalctl -u autofs --since \"5 minutes ago\"\ndmesg | tail -20\n\n# Step 5: Verify server\nshowmount -e nfs-server\nping nfs-server\n</code></pre> <p>Common mistakes:</p> <pre><code># Wrong: Missing the colon for local device\ncdrom   -fstype=iso9660,ro    /dev/cdrom\n\n# Right: Add colon before local device\ncdrom   -fstype=iso9660,ro    :/dev/cdrom\n\n# Wrong: Full path in indirect map\n/data/projects  -rw,soft  server:/projects\n\n# Right: Just the key\nprojects  -rw,soft  server:/projects\n</code></pre>"},{"location":"linux/storage/automounters/#problem-permission-denied","title":"Problem: Permission Denied","text":"<p>For NFS:</p> <pre><code># Check server exports\nshowmount -e server\n\n# Verify client IP is allowed\nexportfs -v\n\n# Test manual mount\nmount -t nfs server:/export /mnt/test\n</code></pre> <p>For CIFS:</p> <pre><code># Check credentials file\ncat /root/.smbcreds\nls -l /root/.smbcreds  # Should be 600\n\n# Test with smbclient\nsmbclient //server/share -U username\n</code></pre>"},{"location":"linux/storage/automounters/#problem-mount-not-unmounting","title":"Problem: Mount Not Unmounting","text":"<p>Check timeout:</p> <pre><code># View current timeout\ngrep timeout /etc/auto.master\n\n# Increase timeout\n/data /etc/auto.data --timeout=600\n</code></pre> <p>Force unmount if needed:</p> <pre><code># Find what's using it\nlsof /data/projects\nfuser -m /data/projects\n\n# Kill processes\nfuser -k /data/projects\n\n# Manual unmount\numount -l /data/projects\n</code></pre>"},{"location":"linux/storage/automounters/#problem-slow-performance","title":"Problem: Slow Performance","text":"<p>Check network:</p> <pre><code># Test NFS performance\ndd if=/dev/zero of=/autofs/mount/test bs=1M count=100\n\n# Check mount options\nmount | grep autofs\n</code></pre> <p>Optimize options:</p> <pre><code># In map file, add performance options\ndata  -rw,soft,intr,rsize=32768,wsize=32768,noatime  server:/data\n</code></pre>"},{"location":"linux/storage/automounters/#debug-mode","title":"Debug Mode","text":"<p>When nothing works, use debug mode:</p> <pre><code># Stop service\nsystemctl stop autofs\n\n# Run in foreground with debug\nautomount -f -d\n\n# In another terminal, try to access mount\nls /autofs/path\n\n# Watch debug output in first terminal\n# Shows exactly what autofs is doing\n</code></pre> <p>Example output:</p> <pre><code>attempting to mount entry /misc/data\nlookup_mount: lookup(file): looking up data\nparse_mount: parse(sun): expanded entry: -rw,soft server:/data\nmount_mount: mounting /misc/data from server:/data with options rw,soft\nmounted /misc/data successfully\n</code></pre>"},{"location":"linux/storage/automounters/#best-practices","title":"Best Practices","text":"<p>1. Always use <code>_netdev</code> equivalent options:</p> <pre><code># Don't rely on network at boot\n# autofs handles network wait automatically\n</code></pre> <p>2. Set reasonable timeouts:</p> <pre><code># Too short: Constant mounting/unmounting\n# Too long: Resources held unnecessarily\n# Good default: 300 seconds (5 minutes)\n/data /etc/auto.data --timeout=300\n</code></pre> <p>3. Use indirect maps when possible:</p> <pre><code># Cleaner, more organized\n/shares /etc/auto.shares\n</code></pre> <p>4. Secure credentials:</p> <pre><code>chmod 600 /root/.smbcreds\nchown root:root /root/.smbcreds\n</code></pre> <p>5. Test before making permanent:</p> <pre><code># Test mount manually first\nmount -t nfs server:/data /mnt/test\n\n# Then add to autofs\n</code></pre> <p>6. Use <code>reload</code> not <code>restart</code>:</p> <pre><code># After config changes\nsystemctl reload autofs\n# Not: systemctl restart autofs\n</code></pre> <p>7. Monitor autofs:</p> <pre><code># Regular checks\nsystemctl status autofs\njournalctl -u autofs --since today\n</code></pre>"},{"location":"linux/storage/automounters/#real-world-complete-example","title":"Real-World Complete Example","text":"<p>Scenario: Company with multiple shares and user homes.</p> <pre><code># === Step 1: Master Map ===\ncat &gt; /etc/auto.master &lt;&lt; 'EOF'\n/home     /etc/auto.home --timeout=600\n/company  /etc/auto.company --timeout=300\n/-        /etc/auto.direct\nEOF\n\n# === Step 2: Home Directories ===\ncat &gt; /etc/auto.home &lt;&lt; 'EOF'\n*    -rw,soft,intr,rsize=8192,wsize=8192    homeserver.company.local:/home/&amp;\nEOF\n\n# === Step 3: Company Shares ===\ncat &gt; /etc/auto.company &lt;&lt; 'EOF'\nprojects    -rw,soft,intr    proj-server:/projects\nfinance     -rw,soft,intr    fin-server:/finance\ndocs        -ro,soft         doc-server:/documentation\nshared      -rw,soft,intr    file-server:/shared\nEOF\n\n# === Step 4: Direct Mounts ===\ncat &gt; /etc/auto.direct &lt;&lt; 'EOF'\n/data/warehouse    -rw,soft,intr    storage:/warehouse\n/opt/tools         -ro,soft         tools:/export/tools\nEOF\n\n# === Step 5: Reload ===\nsystemctl reload autofs\n\n# === Step 6: Test ===\n# User homes\nls /home/john           # Auto-mounts homeserver:/home/john\n\n# Company shares\nls /company/projects    # Auto-mounts proj-server:/projects\ncd /company/finance     # Auto-mounts fin-server:/finance\n\n# Direct mounts\ncd /data/warehouse      # Auto-mounts storage:/warehouse\n\n# === Step 7: Verify ===\nmount | grep autofs\ndf -h | grep autofs\n\n# === Step 8: Monitor ===\njournalctl -u autofs -f\n</code></pre>"},{"location":"linux/storage/automounters/#quick-reference","title":"Quick Reference","text":""},{"location":"linux/storage/automounters/#installation","title":"Installation","text":"<pre><code># Install\ndnf install autofs\nsystemctl enable --now autofs\n</code></pre>"},{"location":"linux/storage/automounters/#configuration-files","title":"Configuration Files","text":"<pre><code># Master map (main config)\n/etc/auto.master\n\n# Map files (mount definitions)\n/etc/auto.misc\n/etc/auto.home\n</code></pre>"},{"location":"linux/storage/automounters/#basic-indirect-map","title":"Basic Indirect Map","text":"<pre><code># Master map\n/mnt /etc/auto.mnt\n\n# Map file\ndata  -rw,soft,intr  server:/export/data\n\n# Result: /mnt/data\n</code></pre>"},{"location":"linux/storage/automounters/#basic-direct-map","title":"Basic Direct Map","text":"<pre><code># Master map\n/- /etc/auto.direct\n\n# Map file\n/data/warehouse  -rw,soft  server:/warehouse\n\n# Result: /data/warehouse\n</code></pre>"},{"location":"linux/storage/automounters/#service-management","title":"Service Management","text":"<pre><code>systemctl reload autofs     # Apply changes (safe)\nsystemctl status autofs     # Check status\njournalctl -u autofs -f     # View logs\nautomount -f -v             # Test config\n</code></pre>"},{"location":"linux/storage/automounters/#testing","title":"Testing","text":"<pre><code># Test by accessing\nls /mount/point\n\n# Check if mounted\nmount | grep autofs\ndf -h | grep autofs\n\n# Debug mode\nsystemctl stop autofs\nautomount -f -d\n</code></pre>"},{"location":"linux/storage/filesystems/","title":"Create, Manage, and Troubleshoot Filesystems","text":""},{"location":"linux/storage/filesystems/#what-is-a-filesystem","title":"What is a Filesystem?","text":"<p>A filesystem is the method and structure used to organize and store files on a storage device. Think of it as the filing system for your hard drive - it determines how data is stored, how files are named, what metadata is kept, and how space is managed.</p> <p>Without a filesystem, a hard drive is just a blank slate. The filesystem gives it structure and makes it usable.</p>"},{"location":"linux/storage/filesystems/#why-different-filesystems","title":"Why Different Filesystems?","text":"<p>Different filesystems are optimized for different uses:</p> <ul> <li>ext4: General purpose, very stable, great for Linux systems</li> <li>XFS: Excellent for large files, high performance, used in enterprise</li> <li>Btrfs: Modern features like snapshots and compression</li> <li>VFAT/FAT32: Cross-platform compatibility (Windows, Mac, Linux)</li> <li>NTFS: Windows native filesystem</li> <li>exFAT: Modern cross-platform for large files</li> </ul>"},{"location":"linux/storage/filesystems/#common-linux-filesystems","title":"Common Linux Filesystems","text":""},{"location":"linux/storage/filesystems/#ext4-fourth-extended-filesystem","title":"ext4 - Fourth Extended Filesystem","text":"<p>What it is: The most common Linux filesystem. Reliable, mature, and the default on most Linux distributions.</p> <p>Best for:</p> <ul> <li>Linux system partitions</li> <li>General purpose storage</li> <li>Root and /home partitions</li> <li>When you need stability and maturity</li> </ul> <p>Key features:</p> <ul> <li>Maximum file size: 16 TB</li> <li>Maximum partition size: 1 EB (Exabyte)</li> <li>Journaling (protects against corruption)</li> <li>Backwards compatible with ext3 and ext2</li> </ul> <p>When to use: Default choice for most Linux installations. If unsure, use ext4.</p>"},{"location":"linux/storage/filesystems/#xfs-high-performance-filesystem","title":"XFS - High Performance Filesystem","text":"<p>What it is: Originally from SGI, designed for high-performance computing.</p> <p>Best for:</p> <ul> <li>Large files (videos, databases, disk images)</li> <li>High throughput workloads</li> <li>Enterprise storage systems</li> <li>NAS devices</li> </ul> <p>Key features:</p> <ul> <li>Excellent performance with large files</li> <li>Online defragmentation</li> <li>Can grow online (but cannot shrink!)</li> <li>Maximum file size: 8 EB</li> </ul> <p>When to use: Database servers, video editing, large file storage.</p> <p>Important limitation: You CANNOT shrink XFS filesystems!</p>"},{"location":"linux/storage/filesystems/#btrfs-b-tree-filesystem","title":"Btrfs - B-Tree Filesystem","text":"<p>What it is: Modern filesystem with advanced features like built-in snapshots and compression.</p> <p>Best for:</p> <ul> <li>When you need snapshots</li> <li>When you want compression</li> <li>Desktop systems</li> <li>Development environments</li> </ul> <p>Key features:</p> <ul> <li>Built-in snapshots (no LVM needed)</li> <li>Online compression</li> <li>Self-healing (detects and repairs corruption)</li> <li>Subvolumes (like lightweight partitions)</li> </ul> <p>When to use: When you want modern features and don\u2019t mind some complexity.</p>"},{"location":"linux/storage/filesystems/#fat32-and-exfat","title":"FAT32 and exFAT","text":"<p>What they are: Simple filesystems that work on Windows, Mac, and Linux.</p> <p>Best for:</p> <ul> <li>USB flash drives</li> <li>External hard drives</li> <li>Sharing files between different operating systems</li> </ul> <p>Limitations:</p> <ul> <li>FAT32: Cannot handle files larger than 4GB</li> <li>exFAT: No file size limit, but not as widely supported as FAT32</li> </ul>"},{"location":"linux/storage/filesystems/#creating-filesystems","title":"Creating Filesystems","text":""},{"location":"linux/storage/filesystems/#mkfsext4-create-ext4-filesystem","title":"mkfs.ext4 - Create ext4 Filesystem","text":"<p>What it does: Formats a partition or disk with the ext4 filesystem.</p> <p>Why use it: To make a disk usable for storing files on Linux.</p> <p>\u26a0\ufe0f WARNING: This ERASES all data on the partition!</p> <p>Examples:</p> <pre><code># Basic ext4 creation\nmkfs.ext4 /dev/sdb1\n\n# With a label (helps identify the filesystem)\nmkfs.ext4 -L \"DATA\" /dev/sdb1\n\n# With less reserved space (default is 5%, which wastes space)\nmkfs.ext4 -m 1 /dev/sdb1\n\n# Force creation (even if filesystem already exists)\nmkfs.ext4 -F /dev/sdb1\n</code></pre> <p>Real-world scenario - New data drive:</p> <pre><code># You added a new 1TB drive to your server\n# Step 1: Create partition\nfdisk /dev/sdb\n# Press: n, p, 1, Enter, Enter, w\n\n# Step 2: Create ext4 filesystem with label\nmkfs.ext4 -L \"ServerData\" /dev/sdb1\n\n# Output:\n# Creating filesystem with 262144000 4k blocks and 65536000 inodes\n# Filesystem UUID: 1234abcd-5678-efgh...\n# Allocating group tables: done\n# Writing inode tables: done\n# Creating journal (32768 blocks): done\n# Writing superblocks and filesystem accounting information: done\n\n# Step 3: Create mount point and mount\nmkdir /data\nmount /dev/sdb1 /data\n\n# Step 4: Make it permanent\necho \"LABEL=ServerData /data ext4 defaults 0 2\" &gt;&gt; /etc/fstab\n</code></pre> <p>Understanding the output:</p> <ul> <li>UUID: Unique identifier for this filesystem</li> <li>Inode tables: Where file metadata is stored</li> <li>Journal: Protects against corruption during crashes</li> </ul>"},{"location":"linux/storage/filesystems/#mkfsxfs-create-xfs-filesystem","title":"mkfs.xfs - Create XFS Filesystem","text":"<p>What it does: Formats a partition with the XFS filesystem.</p> <p>Why use it: When you need high performance for large files or databases.</p> <p>Examples:</p> <pre><code># Basic XFS creation\nmkfs.xfs /dev/sdb1\n\n# With a label\nmkfs.xfs -L \"Database\" /dev/sdb1\n\n# Force creation (overwrite existing)\nmkfs.xfs -f /dev/sdb1\n</code></pre> <p>Real-world scenario - Database server:</p> <pre><code># Creating XFS for PostgreSQL database\nmkfs.xfs -L \"PostgreSQL\" /dev/sdb1\n\n# Mount it\nmkdir /var/lib/postgresql\nmount /dev/sdb1 /var/lib/postgresql\n\n# Permanent mount\necho \"LABEL=PostgreSQL /var/lib/postgresql xfs defaults 0 2\" &gt;&gt; /etc/fstab\n\n# Change ownership for PostgreSQL\nchown -R postgres:postgres /var/lib/postgresql\n</code></pre>"},{"location":"linux/storage/filesystems/#mkfsbtrfs-create-btrfs-filesystem","title":"mkfs.btrfs - Create Btrfs Filesystem","text":"<p>What it does: Creates a Btrfs filesystem with advanced features.</p> <p>Why use it: When you want snapshots, compression, or other modern features.</p> <p>Examples:</p> <pre><code># Basic Btrfs creation\nmkfs.btrfs /dev/sdb1\n\n# With a label\nmkfs.btrfs -L \"BtrfsData\" /dev/sdb1\n\n# Create with multiple devices (RAID-like)\nmkfs.btrfs -d raid1 -m raid1 /dev/sdb1 /dev/sdc1\n\n# Force creation\nmkfs.btrfs -f /dev/sdb1\n</code></pre> <p>Real-world scenario - Development workstation:</p> <pre><code># Create Btrfs for /home with snapshot capability\nmkfs.btrfs -L \"Home\" /dev/sdb1\n\n# Mount with compression\nmount -o compress=zstd /dev/sdb1 /home\n\n# Make permanent with compression\necho \"LABEL=Home /home btrfs compress=zstd 0 0\" &gt;&gt; /etc/fstab\n</code></pre>"},{"location":"linux/storage/filesystems/#mkfsvfat-create-fat32-filesystem","title":"mkfs.vfat - Create FAT32 Filesystem","text":"<p>What it does: Creates FAT32 filesystem for cross-platform compatibility.</p> <p>Why use it: USB drives, EFI boot partitions, sharing between operating systems.</p> <p>Examples:</p> <pre><code># Create FAT32\nmkfs.vfat -F 32 /dev/sdb1\n\n# With volume name\nmkfs.vfat -F 32 -n \"USB_DRIVE\" /dev/sdb1\n</code></pre> <p>Real-world scenario - USB flash drive:</p> <pre><code># Format USB drive for Windows/Mac/Linux compatibility\n# Step 1: Identify the drive\nlsblk\n# sdb is your USB drive\n\n# Step 2: Unmount if mounted\numount /dev/sdb1\n\n# Step 3: Create new partition (if needed)\nfdisk /dev/sdb\n# d (delete), n (new), p (primary), 1, Enter, Enter, t (type), c (W95 FAT32), w (write)\n\n# Step 4: Format as FAT32\nmkfs.vfat -F 32 -n \"MYUSB\" /dev/sdb1\n\n# Now it works on any computer\n</code></pre>"},{"location":"linux/storage/filesystems/#mounting-filesystems","title":"Mounting Filesystems","text":""},{"location":"linux/storage/filesystems/#understanding-mounting","title":"Understanding Mounting","text":"<p>In Linux, you don\u2019t access drives by letters like Windows (C:, D:). Instead, you \u201cmount\u201d them to directories. The mount point is where the filesystem appears in your directory tree.</p> <p>Example: Mount <code>/dev/sdb1</code> to <code>/data</code>, and the files on that disk appear at <code>/data</code>.</p>"},{"location":"linux/storage/filesystems/#mount-attach-filesystems","title":"mount - Attach Filesystems","text":"<p>What it does: Makes a filesystem accessible at a specific directory.</p> <p>Why use it: You can\u2019t access files on a disk until it\u2019s mounted!</p> <p>Examples:</p> <pre><code># Basic mount\nmount /dev/sdb1 /mnt\n\n# Mount with filesystem type specified\nmount -t ext4 /dev/sdb1 /mnt/data\n\n# Mount read-only\nmount -o ro /dev/sdb1 /mnt/data\n\n# Mount with multiple options\nmount -o rw,noatime /dev/sdb1 /mnt/data\n\n# Mount all entries in /etc/fstab\nmount -a\n\n# Remount with different options (no unmount needed)\nmount -o remount,rw /mnt/data\n\n# Mount an ISO file\nmount -o loop image.iso /mnt/iso\n\n# Bind mount (mount directory to another location)\nmount --bind /source/dir /dest/dir\n</code></pre> <p>Common mount options:</p> <ul> <li><code>ro</code> - Read-only (cannot modify files)</li> <li><code>rw</code> - Read-write (can modify files) </li> <li><code>noatime</code> - Don\u2019t update access times (faster)</li> <li><code>noexec</code> - Cannot execute binaries (security)</li> <li><code>nosuid</code> - Ignore setuid bits (security)</li> <li><code>nodev</code> - No device files (security)</li> </ul> <p>Real-world scenario - Mounting a new drive:</p> <pre><code># You've created filesystem on /dev/sdb1\n# Create mount point\nmkdir /data\n\n# Test mount first\nmount /dev/sdb1 /data\n\n# Check if it worked\ndf -h /data\nls /data\n\n# If good, make permanent\necho \"/dev/sdb1 /data ext4 defaults 0 2\" &gt;&gt; /etc/fstab\n\n# Unmount\numount /data\n\n# Test fstab entry\nmount -a\n\n# Verify\ndf -h /data\n</code></pre> <p>Real-world scenario - Mount with specific options:</p> <pre><code># Mount external drive with better performance\nmount -o noatime,nodiratime /dev/sdb1 /media/external\n\n# Mount with specific user ownership (for FAT32/exFAT)\nmount -o uid=1000,gid=1000 /dev/sdb1 /media/usb\n</code></pre>"},{"location":"linux/storage/filesystems/#umount-detach-filesystems","title":"umount - Detach Filesystems","text":"<p>What it does: Unmounts a filesystem, making it safe to remove.</p> <p>Why use it: Before unplugging USB drives or doing maintenance, always unmount!</p> <p>Examples:</p> <pre><code># Unmount by mount point\numount /mnt/data\n\n# Unmount by device\numount /dev/sdb1\n\n# Force unmount (if regular umount fails)\numount -f /mnt/data\n\n# Lazy unmount (detach now, cleanup when no longer busy)\numount -l /mnt/data\n\n# Unmount all\numount -a\n</code></pre> <p>Real-world scenario - \u201cDevice is busy\u201d error:</p> <pre><code># Try to unmount\numount /mnt/data\n# Error: target is busy\n\n# Find what's using it\nlsof /mnt/data\n# Shows: some-process is using a file\n\n# or\nfuser -m /mnt/data\n# Shows: 1234 (PID of process)\n\n# Option 1: Close the application\nkill 1234\n\n# Option 2: Force unmount\numount -f /mnt/data\n\n# Option 3: Lazy unmount (not recommended unless necessary)\numount -l /mnt/data\n</code></pre>"},{"location":"linux/storage/filesystems/#etcfstab-permanent-mounts","title":"/etc/fstab - Permanent Mounts","text":"<p>What it is: Configuration file that tells Linux what to mount automatically at boot.</p> <p>Why use it: So you don\u2019t have to manually mount filesystems every time you reboot.</p> <p>Format:</p> <pre><code>device  mount_point  filesystem_type  options  dump  pass\n</code></pre> <p>Examples:</p> <pre><code># Basic entry for data partition\n/dev/sdb1  /data  ext4  defaults  0  2\n\n# Using UUID (more reliable than /dev/sdX)\nUUID=1234-5678  /data  ext4  defaults  0  2\n\n# Using label\nLABEL=DATA  /data  ext4  defaults  0  2\n\n# Network filesystem (wait for network)\n192.168.1.100:/share  /mnt/nfs  nfs  defaults,_netdev  0  0\n\n# Swap partition\n/dev/sdb2  none  swap  sw  0  0\n\n# With performance options\nUUID=abcd-1234  /data  ext4  noatime,nodiratime  0  2\n\n# USB drive that might not always be present\nUUID=1234-5678  /media/usb  vfat  noauto,user  0  0\n</code></pre> <p>Understanding the fields:</p> <ol> <li>Device: <code>/dev/sdb1</code>, <code>UUID=...</code>, or <code>LABEL=...</code></li> <li>Mount point: Where it appears (<code>/data</code>, <code>/mnt/backup</code>)</li> <li>Filesystem: <code>ext4</code>, <code>xfs</code>, <code>ntfs</code>, <code>vfat</code>, etc.</li> <li>Options: <code>defaults</code>, <code>noatime</code>, <code>ro</code>, etc.</li> <li>Dump: Backup with dump (0=no, 1=yes) - rarely used</li> <li>Pass: fsck check order (0=no check, 1=first, 2=later)</li> </ol> <p>Real-world scenario - Adding entry:</p> <pre><code># Get UUID of partition\nblkid /dev/sdb1\n# Output: /dev/sdb1: UUID=\"1234abcd-...\" TYPE=\"ext4\" LABEL=\"DATA\"\n\n# Edit fstab\nvi /etc/fstab\n\n# Add entry (using UUID is more reliable)\nUUID=1234abcd-... /data ext4 defaults 0 2\n\n# Save and test (doesn't actually mount, just checks syntax)\nmount -a\n\n# If no errors, reboot to verify it mounts automatically\nreboot\n</code></pre>"},{"location":"linux/storage/filesystems/#managing-filesystems","title":"Managing Filesystems","text":""},{"location":"linux/storage/filesystems/#tune2fs-tune-ext4-filesystems","title":"tune2fs - Tune ext4 Filesystems","text":"<p>What it does: Adjusts parameters of ext2/ext3/ext4 filesystems.</p> <p>Why use it: Change labels, reserved space, mount counts, and other settings without reformatting.</p> <p>Examples:</p> <pre><code># View filesystem information\ntune2fs -l /dev/sdb1\n\n# Change filesystem label\ntune2fs -L \"NewLabel\" /dev/sdb1\n\n# Set reserved space to 1% (default is 5%)\ntune2fs -m 1 /dev/sdb1\n\n# Disable fsck on mount count\ntune2fs -c 0 /dev/sdb1\n\n# Disable fsck on time interval\ntune2fs -i 0 /dev/sdb1\n\n# Set error behavior to remount read-only\ntune2fs -e remount-ro /dev/sdb1\n</code></pre> <p>Real-world scenario - Free up reserved space:</p> <pre><code># ext4 reserves 5% for root by default\n# On large data drives, this wastes space\n\n# Check current settings\ntune2fs -l /dev/sdb1 | grep -i \"block count\\|reserved\"\n\n# Reserved block count:     5120000 (5% of 100GB = 5GB wasted!)\n# Change to 1%\ntune2fs -m 1 /dev/sdb1\n\n# Now you have 4GB more usable space\n</code></pre>"},{"location":"linux/storage/filesystems/#xfs_admin-manage-xfs-filesystems","title":"xfs_admin - Manage XFS Filesystems","text":"<p>What it does: Changes parameters of XFS filesystems.</p> <p>Why use it: Change labels or UUIDs on XFS.</p> <p>Examples:</p> <pre><code># Show current label\nxfs_admin -l /dev/sdb1\n\n# Set new label\nxfs_admin -L \"NewLabel\" /dev/sdb1\n\n# Show UUID\nxfs_admin -u /dev/sdb1\n\n# Generate new UUID\nxfs_admin -U generate /dev/sdb1\n</code></pre>"},{"location":"linux/storage/filesystems/#btrfs-manage-btrfs-filesystems","title":"btrfs - Manage Btrfs Filesystems","text":"<p>What it does: Comprehensive tool for managing Btrfs features.</p> <p>Why use it: Work with subvolumes, snapshots, and other Btrfs features.</p> <p>Examples:</p> <pre><code># Show Btrfs filesystems\nbtrfs filesystem show\n\n# Show space usage\nbtrfs filesystem df /mnt/btrfs\n\n# Create subvolume\nbtrfs subvolume create /mnt/btrfs/mysubvol\n\n# List subvolumes\nbtrfs subvolume list /mnt/btrfs\n\n# Create snapshot\nbtrfs subvolume snapshot /mnt/btrfs/mysubvol /mnt/btrfs/snapshot_2024\n\n# Resize filesystem (can grow or shrink)\nbtrfs filesystem resize +10G /mnt/btrfs\nbtrfs filesystem resize max /mnt/btrfs\n</code></pre> <p>Real-world scenario - Snapshots before update:</p> <pre><code># Take snapshot before system update\nbtrfs subvolume snapshot / /.snapshots/before-update\n\n# Perform update\ndnf update -y\n\n# If something breaks, boot from snapshot\n# (requires bootloader configuration)\n\n# If everything works, delete snapshot\nbtrfs subvolume delete /.snapshots/before-update\n</code></pre>"},{"location":"linux/storage/filesystems/#checking-and-repairing-filesystems","title":"Checking and Repairing Filesystems","text":""},{"location":"linux/storage/filesystems/#fsck-filesystem-check","title":"fsck - Filesystem Check","text":"<p>What it does: Checks and repairs filesystem errors.</p> <p>Why use it: Fix corruption, prepare for resize operations, routine maintenance.</p> <p>\u26a0\ufe0f CRITICAL: Always unmount before running fsck!</p> <p>Examples:</p> <pre><code># Check filesystem (unmount first!)\nfsck /dev/sdb1\n\n# Automatic repair\nfsck -y /dev/sdb1\n\n# Check even if marked clean\nfsck -f /dev/sdb1\n\n# Check all filesystems in /etc/fstab\nfsck -A\n</code></pre> <p>Real-world scenario - Fixing corruption:</p> <pre><code># System won't boot, dropped to emergency shell\n# Root filesystem has errors\n\n# Try to mount, fails with errors\nmount /dev/sda1 /mnt\n# Error: filesystem has errors\n\n# Check and repair\nfsck -y /dev/sda1\n# Pass 1: Checking inodes...\n# Pass 2: Checking directory structure...\n# Pass 3: Checking directory connectivity...\n# Pass 4: Checking reference counts...\n# Pass 5: Checking group summary information...\n# Fixed 15 errors\n\n# Now mount works\nmount /dev/sda1 /mnt\n</code></pre>"},{"location":"linux/storage/filesystems/#e2fsck-ext4-filesystem-check","title":"e2fsck - ext4 Filesystem Check","text":"<p>What it does: Specialized tool for ext2/ext3/ext4 filesystems.</p> <p>Why use it: More control than generic fsck for ext filesystems.</p> <p>Examples:</p> <pre><code># Automatic repair\ne2fsck -y /dev/sdb1\n\n# Force check even if clean\ne2fsck -f /dev/sdb1\n\n# Check for bad blocks\ne2fsck -c /dev/sdb1\n\n# Read-only check (no modifications)\ne2fsck -n /dev/sdb1\n</code></pre>"},{"location":"linux/storage/filesystems/#xfs_repair-xfs-filesystem-repair","title":"xfs_repair - XFS Filesystem Repair","text":"<p>What it does: Repairs XFS filesystems.</p> <p>Why use it: XFS doesn\u2019t use fsck; use this instead.</p> <p>Examples:</p> <pre><code># Dry run (check only)\nxfs_repair -n /dev/sdb1\n\n# Repair filesystem (unmount first!)\nxfs_repair /dev/sdb1\n\n# Force log zeroing (last resort, may lose data)\nxfs_repair -L /dev/sdb1\n</code></pre>"},{"location":"linux/storage/filesystems/#resizing-filesystems","title":"Resizing Filesystems","text":""},{"location":"linux/storage/filesystems/#resize2fs-resize-ext4","title":"resize2fs - Resize ext4","text":"<p>What it does: Grows or shrinks ext2/ext3/ext4 filesystems.</p> <p>Why use it: Adjust filesystem size to match partition size.</p> <p>Examples:</p> <pre><code># Grow to fill partition (after extending partition)\nresize2fs /dev/sdb1\n\n# Shrink to specific size (unmount first!)\nresize2fs /dev/sdb1 50G\n\n# Shrink to minimum possible size\nresize2fs -M /dev/sdb1\n</code></pre> <p>Real-world scenario - Growing filesystem:</p> <pre><code># You extended LVM volume from 50GB to 100GB\n# Filesystem is still 50GB\n\n# Step 1: Check current size\ndf -h /data\n# 50GB\n\n# Step 2: Resize filesystem\nresize2fs /dev/vg0/lv_data\n\n# Step 3: Verify\ndf -h /data\n# 100GB - done!\n</code></pre> <p>Real-world scenario - Shrinking filesystem:</p> <pre><code># You want to shrink partition from 100GB to 50GB\n# ALWAYS shrink filesystem BEFORE shrinking partition!\n\n# Step 1: Unmount\numount /data\n\n# Step 2: Check filesystem\ne2fsck -f /dev/sdb1\n\n# Step 3: Shrink filesystem to 50GB\nresize2fs /dev/sdb1 50G\n\n# Step 4: Shrink partition to 50GB (using fdisk, parted, or LVM)\nlvreduce -L 50G /dev/vg0/lv_data\n\n# Step 5: Mount\nmount /dev/vg0/lv_data /data\n</code></pre>"},{"location":"linux/storage/filesystems/#xfs_growfs-grow-xfs","title":"xfs_growfs - Grow XFS","text":"<p>What it does: Grows XFS filesystems (online - while mounted).</p> <p>Why use it: Expand XFS to use additional space.</p> <p>\u26a0\ufe0f Note: XFS can ONLY grow, never shrink!</p> <p>Examples:</p> <pre><code># Grow to fill device\nxfs_growfs /mnt/data\n\n# Grow by specific amount\nxfs_growfs -D 100g /mnt/data\n</code></pre> <p>Real-world scenario:</p> <pre><code># Extended partition, need to grow XFS\n# Step 1: Extend partition (already done)\n\n# Step 2: Grow filesystem (while mounted!)\nxfs_growfs /data\n\n# Step 3: Verify\ndf -h /data\n</code></pre>"},{"location":"linux/storage/filesystems/#checking-filesystem-information","title":"Checking Filesystem Information","text":""},{"location":"linux/storage/filesystems/#blkid-block-device-information","title":"blkid - Block Device Information","text":"<p>What it does: Shows UUID, label, and filesystem type of block devices.</p> <p>Why use it: Get information needed for /etc/fstab entries.</p> <p>Examples:</p> <pre><code># Show all block devices\nblkid\n\n# Show specific device\nblkid /dev/sdb1\n\n# Show only UUID\nblkid -s UUID -o value /dev/sdb1\n\n# Show only label\nblkid -s LABEL -o value /dev/sdb1\n</code></pre> <p>Output example:</p> <pre><code>blkid /dev/sdb1\n/dev/sdb1: UUID=\"1234abcd-...\" TYPE=\"ext4\" LABEL=\"DATA\" PARTLABEL=\"primary\" PARTUUID=\"...\"\n</code></pre>"},{"location":"linux/storage/filesystems/#lsblk-list-block-devices","title":"lsblk - List Block Devices","text":"<p>What it does: Shows tree view of all block devices with mount points.</p> <p>Why use it: Quick overview of all disks and partitions.</p> <p>Examples:</p> <pre><code># Basic tree view\nlsblk\n\n# With filesystem information\nlsblk -f\n\n# With full device paths\nlsblk -p\n\n# Custom columns\nlsblk -o NAME,SIZE,TYPE,MOUNTPOINT\n</code></pre> <p>Output example:</p> <pre><code>lsblk\nNAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT\nsda      8:0    0  100G  0 disk\n\u251c\u2500sda1   8:1    0   50G  0 part /\n\u2514\u2500sda2   8:2    0   50G  0 part /home\nsdb      8:16   0  500G  0 disk\n\u2514\u2500sdb1   8:17   0  500G  0 part /data\n</code></pre>"},{"location":"linux/storage/filesystems/#troubleshooting","title":"Troubleshooting","text":""},{"location":"linux/storage/filesystems/#problem-filesystem-wont-mount","title":"Problem: Filesystem Won\u2019t Mount","text":"<p>Symptoms: Mount command fails with errors.</p> <p>Solutions:</p> <pre><code># Check if device exists\nls -l /dev/sdb1\n\n# Check filesystem type\nblkid /dev/sdb1\n\n# Try specifying filesystem type\nmount -t ext4 /dev/sdb1 /mnt\n\n# Check for errors\nfsck -y /dev/sdb1\n\n# Check system logs\ndmesg | tail -20\njournalctl -xe\n</code></pre>"},{"location":"linux/storage/filesystems/#problem-device-is-busy","title":"Problem: Device is Busy","text":"<p>Symptoms: Cannot unmount because device is in use.</p> <p>Solutions:</p> <pre><code># Find what's using it\nlsof /mnt/data\nfuser -vm /mnt/data\n\n# Change to different directory\ncd ~\n\n# Kill processes (careful!)\nfuser -k /mnt/data\n\n# Lazy unmount (last resort)\numount -l /mnt/data\n</code></pre>"},{"location":"linux/storage/filesystems/#problem-read-only-filesystem","title":"Problem: Read-only Filesystem","text":"<p>Symptoms: Cannot write to filesystem.</p> <p>Solutions:</p> <pre><code># Remount as read-write\nmount -o remount,rw /mount/point\n\n# If that fails, check for errors\numount /mount/point\nfsck -y /dev/sdb1\nmount /dev/sdb1 /mount/point\n</code></pre>"},{"location":"linux/storage/filesystems/#problem-no-space-left-on-device-but-df-shows-space","title":"Problem: No Space Left on Device (But df Shows Space)","text":"<p>Symptoms: Cannot create files even though df shows free space.</p> <p>Cause: Out of inodes (file metadata structures).</p> <p>Solution:</p> <pre><code># Check inodes\ndf -i\n\n# If inodes are at 100%, delete many small files\n# Or recreate filesystem with more inodes\n</code></pre>"},{"location":"linux/storage/filesystems/#quick-reference","title":"Quick Reference","text":""},{"location":"linux/storage/filesystems/#creating-filesystems_1","title":"Creating Filesystems","text":"<pre><code>mkfs.ext4 /dev/sdb1                # ext4\nmkfs.xfs /dev/sdb1                 # XFS\nmkfs.btrfs /dev/sdb1               # Btrfs\nmkfs.vfat -F 32 /dev/sdb1          # FAT32\n</code></pre>"},{"location":"linux/storage/filesystems/#mounting","title":"Mounting","text":"<pre><code>mount /dev/sdb1 /mnt               # Mount\numount /mnt                        # Unmount\nmount -a                           # Mount all (fstab)\n</code></pre>"},{"location":"linux/storage/filesystems/#checking","title":"Checking","text":"<pre><code>fsck -y /dev/sdb1                  # Check and repair\ne2fsck -f /dev/sdb1                # ext4 check\nxfs_repair /dev/sdb1               # XFS repair\n</code></pre>"},{"location":"linux/storage/filesystems/#resizing","title":"Resizing","text":"<pre><code>resize2fs /dev/sdb1                # Grow ext4\nxfs_growfs /mnt/data               # Grow XFS\n</code></pre>"},{"location":"linux/storage/filesystems/#information","title":"Information","text":"<pre><code>blkid /dev/sdb1                    # UUID, label, type\nlsblk -f                           # Tree with fs info\ndf -h                              # Space usage\ntune2fs -l /dev/sdb1               # ext4 details\n</code></pre>"},{"location":"linux/storage/lvm-storage/","title":"Configure and Manage LVM Storage","text":""},{"location":"linux/storage/lvm-storage/#what-is-lvm","title":"What is LVM?","text":"<p>LVM (Logical Volume Manager) is a flexible disk management system that sits between your physical hard drives and your filesystems. Think of it as a layer of abstraction that makes managing storage much easier and more flexible than traditional partitioning.</p>"},{"location":"linux/storage/lvm-storage/#why-use-lvm","title":"Why Use LVM?","text":"<p>Traditional Partitioning Problems: - Once you create a partition, resizing it is difficult and risky - You\u2019re limited by physical disk boundaries - Moving data between disks requires backup and restore - You can\u2019t easily combine multiple disks into one large volume</p> <p>LVM Solutions: - Resize volumes easily (grow or shrink) while the system is running - Combine multiple physical disks into one large pool of storage - Move data between disks without downtime - Create snapshots for backups - Add more storage space without reformatting</p>"},{"location":"linux/storage/lvm-storage/#lvm-architecture-the-three-layers","title":"LVM Architecture - The Three Layers","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Filesystems (ext4, xfs, etc.)          \u2502  \u2190 What users see\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Logical Volumes (LV)                   \u2502  \u2190 Virtual partitions\n\u2502  /dev/vg01/lv_data                      \u2502\n\u2502  /dev/vg01/lv_web                       \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Volume Groups (VG)                     \u2502  \u2190 Storage pool\n\u2502  vg01: combines all PVs                 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Physical Volumes (PV)                  \u2502  \u2190 Physical disks/partitions\n\u2502  /dev/sdb, /dev/sdc, /dev/sdd          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>1. Physical Volumes (PV) - Your actual hard drives or partitions    - Example: /dev/sdb, /dev/sdc1    - These are the raw storage devices you\u2019ll use</p> <p>2. Volume Groups (VG) - A pool of storage made from one or more PVs    - Example: vg_data (combining /dev/sdb and /dev/sdc)    - Think of it as a big bucket of storage space</p> <p>3. Logical Volumes (LV) - Virtual partitions created from VG space    - Example: lv_database, lv_webserver    - These are what you format and mount, just like regular partitions</p>"},{"location":"linux/storage/lvm-storage/#physical-volume-management","title":"Physical Volume Management","text":""},{"location":"linux/storage/lvm-storage/#what-are-physical-volumes","title":"What Are Physical Volumes?","text":"<p>Physical Volumes are your actual hard drives or partitions that have been prepared for use with LVM. Before you can use a disk with LVM, you must initialize it as a Physical Volume.</p>"},{"location":"linux/storage/lvm-storage/#pvcreate-initialize-a-disk-for-lvm","title":"pvcreate - Initialize a Disk for LVM","text":"<p>What it does: Prepares a disk or partition to be used with LVM by writing special metadata.</p> <p>Why use it: This is always your first step when adding a new disk to LVM.</p> <p>Examples:</p> <pre><code># Initialize an entire disk for LVM\npvcreate /dev/sdb\n# Output: Physical volume \"/dev/sdb\" successfully created.\n\n# Initialize a partition (you must create the partition first with fdisk/parted)\npvcreate /dev/sdc1\n\n# Initialize multiple disks at once\npvcreate /dev/sdb /dev/sdc /dev/sdd\n</code></pre> <p>Real-world scenario: You\u2019ve just added a new 500GB disk (/dev/sdb) to your server. Before you can use it with LVM: <pre><code>pvcreate /dev/sdb\n</code></pre> Now /dev/sdb is ready to be added to a Volume Group.</p>"},{"location":"linux/storage/lvm-storage/#pvs-quick-overview-of-physical-volumes","title":"pvs - Quick Overview of Physical Volumes","text":"<p>What it does: Shows a simple summary of all Physical Volumes on your system.</p> <p>Why use it: Quick check to see what PVs exist, their size, and which VG they belong to.</p> <p>Example:</p> <pre><code>pvs\n</code></pre> <p>Output: <pre><code>PV         VG     Fmt  Attr PSize   PFree\n/dev/sdb   vg01   lvm2 a--  100.00g 50.00g\n/dev/sdc   vg01   lvm2 a--  200.00g 200.00g\n/dev/sdd   vg02   lvm2 a--  500.00g 100.00g\n</code></pre></p> <p>What this tells you: - /dev/sdb and /dev/sdc are both part of vg01 - /dev/sdb has 50GB free out of 100GB total - /dev/sdc is completely unused (200GB free out of 200GB)</p>"},{"location":"linux/storage/lvm-storage/#pvdisplay-detailed-information-about-physical-volumes","title":"pvdisplay - Detailed Information About Physical Volumes","text":"<p>What it does: Shows detailed information about one or all Physical Volumes.</p> <p>Why use it: When you need detailed information like UUID, exact sizes, or troubleshooting.</p> <p>Examples:</p> <pre><code># Show details for all PVs\npvdisplay\n\n# Show details for a specific PV\npvdisplay /dev/sdb\n</code></pre> <p>Output example: <pre><code>  --- Physical volume ---\n  PV Name               /dev/sdb\n  VG Name               vg01\n  PV Size               100.00 GiB\n  Allocatable           yes\n  PE Size               4.00 MiB\n  Total PE              25599\n  Free PE               12799\n  Allocated PE          12800\n  PV UUID               abc123-def456-...\n</code></pre></p> <p>What this means: - This PV is part of vg01 - Total size is 100GB - About half is used (12800 PE allocated out of 25599 total) - PE (Physical Extent) is the smallest unit of space LVM uses (4MB blocks)</p>"},{"location":"linux/storage/lvm-storage/#pvmove-move-data-off-a-physical-volume","title":"pvmove - Move Data Off a Physical Volume","text":"<p>What it does: Moves all data from one Physical Volume to other PVs in the same Volume Group.</p> <p>Why use it: You want to remove a disk from your system (maybe it\u2019s failing, or you\u2019re upgrading).</p> <p>Example:</p> <pre><code># Move all data from /dev/sdb to other disks in the VG\npvmove /dev/sdb\n\n# Move data from /dev/sdb to a specific disk\npvmove /dev/sdb /dev/sdc\n</code></pre> <p>Real-world scenario: Your /dev/sdb disk is showing SMART errors and needs to be replaced:</p> <pre><code># Step 1: Move all data off the failing disk\n# (This can take hours for large disks!)\npvmove /dev/sdb\n\n# Step 2: Remove it from the VG\nvgreduce vg01 /dev/sdb\n\n# Step 3: Remove PV label\npvremove /dev/sdb\n\n# Step 4: Physically replace the disk\n# Step 5: Add the new disk\npvcreate /dev/sdb\nvgextend vg01 /dev/sdb\n</code></pre>"},{"location":"linux/storage/lvm-storage/#pvremove-remove-physical-volume","title":"pvremove - Remove Physical Volume","text":"<p>What it does: Removes the LVM label from a disk, making it no longer a Physical Volume.</p> <p>Why use it: Clean up after removing a disk from LVM, or reusing a disk for something else.</p> <p>Example:</p> <pre><code># Remove PV label (must not be in use!)\npvremove /dev/sdb\n</code></pre> <p>Important: The PV must not be part of any VG. Remove it from the VG first with <code>vgreduce</code>.</p>"},{"location":"linux/storage/lvm-storage/#volume-group-management","title":"Volume Group Management","text":""},{"location":"linux/storage/lvm-storage/#what-are-volume-groups","title":"What Are Volume Groups?","text":"<p>A Volume Group is a storage pool created from one or more Physical Volumes. Think of it as combining multiple hard drives into one big pool of storage that you can then divide up however you want.</p>"},{"location":"linux/storage/lvm-storage/#vgcreate-create-a-new-volume-group","title":"vgcreate - Create a New Volume Group","text":"<p>What it does: Creates a new storage pool from one or more Physical Volumes.</p> <p>Why use it: This is how you combine disks into a unified storage pool.</p> <p>Examples:</p> <pre><code># Create a VG from a single PV\nvgcreate vg_data /dev/sdb\n\n# Create a VG from multiple PVs (combining 3 disks into one pool)\nvgcreate vg_data /dev/sdb /dev/sdc /dev/sdd\n\n# Create VG with a specific extent size (16MB instead of default 4MB)\n# Larger extents = slightly less overhead for very large volumes\nvgcreate -s 16M vg_bigdata /dev/sdb /dev/sdc\n</code></pre> <p>Real-world scenario: You have three 1TB disks and want to create one large storage pool:</p> <pre><code># Step 1: Initialize all disks\npvcreate /dev/sdb /dev/sdc /dev/sdd\n\n# Step 2: Create VG combining all three (total ~3TB)\nvgcreate vg_storage /dev/sdb /dev/sdc /dev/sdd\n\n# Step 3: Verify\nvgs vg_storage\n# Shows: vg_storage with ~3TB total size\n</code></pre>"},{"location":"linux/storage/lvm-storage/#vgs-quick-overview-of-volume-groups","title":"vgs - Quick Overview of Volume Groups","text":"<p>What it does: Shows a summary of all Volume Groups.</p> <p>Why use it: Quick check of VG size, free space, and how many PVs/LVs they contain.</p> <p>Example:</p> <pre><code>vgs\n</code></pre> <p>Output: <pre><code>VG          #PV #LV #SN Attr   VSize   VFree\nvg_data       2   3   0 wz--n- 300.00g  50.00g\nvg_backup     1   1   0 wz--n- 500.00g 400.00g\n</code></pre></p> <p>What this tells you: - vg_data has 2 PVs, 3 LVs, 300GB total, 50GB free - vg_backup has 1 PV, 1 LV, 500GB total, 400GB free</p>"},{"location":"linux/storage/lvm-storage/#vgdisplay-detailed-volume-group-information","title":"vgdisplay - Detailed Volume Group Information","text":"<p>What it does: Shows detailed information about Volume Groups.</p> <p>Why use it: Get complete details including UUIDs, extent information, and which LVs exist.</p> <p>Example:</p> <pre><code># Show all VGs in detail\nvgdisplay\n\n# Show specific VG\nvgdisplay vg_data\n</code></pre> <p>Output example: <pre><code>  --- Volume group ---\n  VG Name               vg_data\n  System ID\n  Format                lvm2\n  Metadata Areas        2\n  Metadata Sequence No  15\n  VG Access             read/write\n  VG Status             resizable\n  MAX LV                0\n  Cur LV                3\n  Open LV               2\n  Max PV                0\n  Cur PV                2\n  Act PV                2\n  VG Size               299.99 GiB\n  PE Size               4.00 MiB\n  Total PE              76798\n  Alloc PE / Size       64000 / 250.00 GiB\n  Free  PE / Size       12798 / 49.99 GiB\n  VG UUID               xyz789-abc123-...\n</code></pre></p>"},{"location":"linux/storage/lvm-storage/#vgextend-add-storage-to-a-volume-group","title":"vgextend - Add Storage to a Volume Group","text":"<p>What it does: Adds a Physical Volume to an existing Volume Group, increasing available space.</p> <p>Why use it: When you need more space and add a new disk to the system.</p> <p>Example:</p> <pre><code># Add a new disk to existing VG\npvcreate /dev/sdd\nvgextend vg_data /dev/sdd\n</code></pre> <p>Real-world scenario: Your vg_data is running low on space. You add a new 500GB disk:</p> <pre><code># Step 1: Initialize new disk\npvcreate /dev/sdd\n# Output: Physical volume \"/dev/sdd\" successfully created.\n\n# Step 2: Add to existing VG\nvgextend vg_data /dev/sdd\n# Output: Volume group \"vg_data\" successfully extended\n\n# Step 3: Verify new space available\nvgs vg_data\n# VSize will now show 500GB more space\n</code></pre>"},{"location":"linux/storage/lvm-storage/#vgreduce-remove-a-disk-from-volume-group","title":"vgreduce - Remove a Disk from Volume Group","text":"<p>What it does: Removes a Physical Volume from a Volume Group.</p> <p>Why use it: Before removing a disk or decommissioning storage.</p> <p>Example:</p> <pre><code># First, move data off the disk\npvmove /dev/sdd\n\n# Then remove it from the VG\nvgreduce vg_data /dev/sdd\n\n# Finally, remove PV label\npvremove /dev/sdd\n</code></pre> <p>Real-world scenario: You want to remove /dev/sdd from vg_data:</p> <pre><code># Check what's on it first\npvs /dev/sdd\n\n# Move any data to other disks in the VG\npvmove /dev/sdd\n# This may take a while...\n\n# Remove from VG\nvgreduce vg_data /dev/sdd\n# Output: Volume group \"vg_data\" successfully reduced\n\n# Clean up\npvremove /dev/sdd\n</code></pre>"},{"location":"linux/storage/lvm-storage/#vgremove-delete-a-volume-group","title":"vgremove - Delete a Volume Group","text":"<p>What it does: Completely removes a Volume Group.</p> <p>Why use it: Cleaning up or starting over with storage configuration.</p> <p>Example:</p> <pre><code># Remove all LVs first\nlvremove /dev/vg_data/lv_web\nlvremove /dev/vg_data/lv_db\n\n# Then remove the VG\nvgremove vg_data\n</code></pre> <p>Warning: This is destructive! All Logical Volumes must be removed first.</p>"},{"location":"linux/storage/lvm-storage/#logical-volume-management","title":"Logical Volume Management","text":""},{"location":"linux/storage/lvm-storage/#what-are-logical-volumes","title":"What Are Logical Volumes?","text":"<p>Logical Volumes are virtual partitions created from Volume Group space. They\u2019re what you actually format with filesystems and mount. Unlike traditional partitions, they can be easily resized, moved, and snapshotted.</p>"},{"location":"linux/storage/lvm-storage/#lvcreate-create-a-logical-volume","title":"lvcreate - Create a Logical Volume","text":"<p>What it does: Creates a new Logical Volume from available space in a Volume Group.</p> <p>Why use it: This creates the \u201cpartition\u201d you\u2019ll format and use for storing data.</p> <p>Examples:</p> <pre><code># Create a 20GB logical volume named \"lv_web\"\nlvcreate -L 20G -n lv_web vg_data\n\n# Create an LV using 50% of the VG\nlvcreate -l 50%VG -n lv_database vg_data\n\n# Create an LV using ALL free space\nlvcreate -l 100%FREE -n lv_backup vg_data\n\n# Create an LV with exactly 5000 extents (size depends on PE size)\nlvcreate -l 5000 -n lv_app vg_data\n</code></pre> <p>Real-world scenario - Setting up a web server:</p> <pre><code># You have vg_data with 500GB free space\n# Create volumes for different purposes:\n\n# 50GB for web files\nlvcreate -L 50G -n lv_web vg_data\n\n# 100GB for database\nlvcreate -L 100G -n lv_database vg_data\n\n# 30GB for logs\nlvcreate -L 30G -n lv_logs vg_data\n\n# Use remaining space for backups\nlvcreate -l 100%FREE -n lv_backup vg_data\n\n# Verify\nlvs vg_data\n</code></pre> <p>Complete workflow - From disk to mounted filesystem:</p> <pre><code># 1. Create the LV\nlvcreate -L 50G -n lv_webapp vg_data\n\n# 2. Create a filesystem on it\nmkfs.ext4 /dev/vg_data/lv_webapp\n\n# 3. Create mount point\nmkdir /var/www\n\n# 4. Mount it\nmount /dev/vg_data/lv_webapp /var/www\n\n# 5. Make it permanent in /etc/fstab\necho \"/dev/vg_data/lv_webapp /var/www ext4 defaults 0 2\" &gt;&gt; /etc/fstab\n\n# 6. Verify\ndf -h /var/www\n</code></pre>"},{"location":"linux/storage/lvm-storage/#lvs-quick-overview-of-logical-volumes","title":"lvs - Quick Overview of Logical Volumes","text":"<p>What it does: Shows a summary of all Logical Volumes.</p> <p>Why use it: Quick check of LV sizes, which VG they\u2019re in, and their status.</p> <p>Example:</p> <pre><code>lvs\n</code></pre> <p>Output: <pre><code>LV          VG       Attr       LSize   Pool Origin Data%\nlv_database vg_data  -wi-ao---- 100.00g\nlv_web      vg_data  -wi-ao----  50.00g\nlv_backup   vg_data  -wi-a----- 200.00g\n</code></pre></p> <p>What the attributes mean: - <code>w</code> = writable - <code>i</code> = inherited allocation policy - <code>a</code> = active (usable) - <code>o</code> = open (currently mounted)</p>"},{"location":"linux/storage/lvm-storage/#lvdisplay-detailed-logical-volume-information","title":"lvdisplay - Detailed Logical Volume Information","text":"<p>What it does: Shows detailed information about Logical Volumes.</p> <p>Why use it: Get full details including device paths, segments, and exact sizes.</p> <p>Example:</p> <pre><code># Show all LVs\nlvdisplay\n\n# Show specific LV\nlvdisplay /dev/vg_data/lv_web\n</code></pre> <p>Output example: <pre><code>  --- Logical volume ---\n  LV Path                /dev/vg_data/lv_web\n  LV Name                lv_web\n  VG Name                vg_data\n  LV UUID                mno789-pqr012-...\n  LV Write Access        read/write\n  LV Creation host, time server01, 2024-10-28 10:30:15\n  LV Status              available\n  # open                 1\n  LV Size                50.00 GiB\n  Current LE             12800\n  Segments               1\n  Allocation             inherit\n  Read ahead sectors     auto\n  Block device           253:0\n</code></pre></p>"},{"location":"linux/storage/lvm-storage/#lvextend-increase-logical-volume-size","title":"lvextend - Increase Logical Volume Size","text":"<p>What it does: Makes a Logical Volume larger by allocating more space from the Volume Group.</p> <p>Why use it: When you\u2019re running out of space on a filesystem and need more room.</p> <p>Examples:</p> <pre><code># Add 10GB to an LV\nlvextend -L +10G /dev/vg_data/lv_web\n\n# Extend to a total of 100GB\nlvextend -L 100G /dev/vg_data/lv_web\n\n# Use all remaining free space in VG\nlvextend -l +100%FREE /dev/vg_data/lv_web\n\n# Extend AND resize the filesystem in one command (convenient!)\nlvextend -L +10G -r /dev/vg_data/lv_web\n</code></pre> <p>Real-world scenario - Running out of space:</p> <p>You\u2019re getting warnings that /var/www is 90% full:</p> <pre><code># Check current situation\ndf -h /var/www\n# Shows: 45GB used out of 50GB (90% full)\n\nlvs /dev/vg_data/lv_web\n# Shows: 50GB LV\n\nvgs vg_data\n# Shows: 200GB free in VG\n\n# Solution: Add 30GB more\nlvextend -L +30G /dev/vg_data/lv_web\n# Output: Size of logical volume vg_data/lv_web changed from 50.00 GiB to 80.00 GiB\n\n# Resize the filesystem to use new space\nresize2fs /dev/vg_data/lv_web\n# Output: The filesystem is now 20971520 blocks long\n\n# Verify\ndf -h /var/www\n# Shows: 45GB used out of 80GB (56% full)\n</code></pre> <p>Important: After extending the LV, you must resize the filesystem: - ext4/ext3/ext2: <code>resize2fs /dev/vg_data/lv_name</code> - xfs: <code>xfs_growfs /mount/point</code> - Or use <code>-r</code> flag with lvextend to do both automatically</p>"},{"location":"linux/storage/lvm-storage/#lvreduce-decrease-logical-volume-size","title":"lvreduce - Decrease Logical Volume Size","text":"<p>What it does: Makes a Logical Volume smaller, freeing space back to the Volume Group.</p> <p>Why use it: When you over-allocated space and want to reclaim it for other uses.</p> <p>WARNING: This is dangerous! Always backup data first!</p> <p>Example:</p> <pre><code># CRITICAL: Shrink filesystem FIRST, then LV\n# If you shrink the LV first, you'll lose data!\n\n# For ext4:\n# Step 1: Unmount (required for shrinking)\numount /mnt/data\n\n# Step 2: Check filesystem\ne2fsck -f /dev/vg_data/lv_data\n\n# Step 3: Shrink filesystem to 30GB\nresize2fs /dev/vg_data/lv_data 30G\n\n# Step 4: Shrink LV to match\nlvreduce -L 30G /dev/vg_data/lv_data\n# It will ask for confirmation - type 'y'\n\n# Step 5: Remount\nmount /dev/vg_data/lv_data /mnt/data\n</code></pre> <p>Real-world scenario: You allocated 100GB for /opt/app but only use 25GB:</p> <pre><code># Check usage\ndf -h /opt/app\n# Shows: 25GB used out of 100GB\n\n# Back up data first! (Just in case)\ntar czf /backup/app-backup.tar.gz /opt/app\n\n# Unmount\numount /opt/app\n\n# Check and repair filesystem\ne2fsck -f /dev/vg_data/lv_app\n\n# Shrink filesystem to 40GB (leaving room for growth)\nresize2fs /dev/vg_data/lv_app 40G\n\n# Shrink LV to match\nlvreduce -L 40G /dev/vg_data/lv_app\n\n# Remount\nmount /dev/vg_data/lv_app /opt/app\n\n# Verify\ndf -h /opt/app\n# Now shows: 25GB used out of 40GB\n\n# The freed 60GB is now available in vg_data for other uses\nvgs vg_data\n</code></pre> <p>Note: XFS filesystems cannot be shrunk - only grown! Plan sizes carefully.</p>"},{"location":"linux/storage/lvm-storage/#lvremove-delete-a-logical-volume","title":"lvremove - Delete a Logical Volume","text":"<p>What it does: Completely removes a Logical Volume and frees its space back to the VG.</p> <p>Why use it: Cleaning up unused volumes or removing test environments.</p> <p>Example:</p> <pre><code># Unmount first\numount /mnt/old\n\n# Remove the LV\nlvremove /dev/vg_data/lv_old\n# Asks: \"Do you really want to remove...?\" type 'y'\n\n# Or force removal without confirmation (dangerous!)\nlvremove -f /dev/vg_data/lv_old\n</code></pre>"},{"location":"linux/storage/lvm-storage/#lvm-snapshots","title":"LVM Snapshots","text":""},{"location":"linux/storage/lvm-storage/#what-are-snapshots","title":"What Are Snapshots?","text":"<p>A snapshot is a point-in-time copy of a Logical Volume. It doesn\u2019t copy all the data immediately; instead, it uses copy-on-write technology: - When you create a snapshot, it initially uses very little space - As the original LV changes, the snapshot stores the old data - The snapshot shows how the LV looked at the moment you created it</p>"},{"location":"linux/storage/lvm-storage/#why-use-snapshots","title":"Why Use Snapshots?","text":"<p>Perfect for: - Backing up a live database without stopping it - Testing changes safely (take snapshot, test, revert if needed) - Creating consistent backups of busy filesystems</p> <p>Example Use Case: You need to backup a database that runs 24/7. You can\u2019t stop it, but you need a consistent backup: 1. Take a snapshot (takes seconds) 2. The snapshot is a frozen point-in-time view 3. Back up from the snapshot (take hours if needed) 4. Original database keeps running and changing 5. Delete snapshot when backup complete</p>"},{"location":"linux/storage/lvm-storage/#creating-snapshots","title":"Creating Snapshots","text":"<pre><code># Create a 10GB snapshot of lv_database\nlvcreate -L 10G -s -n lv_database_snap /dev/vg_data/lv_database\n\n# The size (10GB) is for storing changes, not the whole database\n# Size needed depends on how much changes during snapshot lifetime\n</code></pre> <p>Real-world scenario - Database backup:</p> <pre><code># Your database LV is 100GB, actively being used\n# Create snapshot (only needs space for changes during backup)\nlvcreate -L 20G -s -n lv_db_backup /dev/vg_data/lv_database\n# Output: Logical volume \"lv_db_backup\" created\n\n# Mount the snapshot read-only\nmkdir /mnt/snap\nmount -o ro /dev/vg_data/lv_db_backup /mnt/snap\n\n# Backup from snapshot (original database keeps running!)\ntar czf /backup/database_$(date +%Y%m%d).tar.gz -C /mnt/snap .\n\n# Cleanup\numount /mnt/snap\nlvremove /dev/vg_data/lv_db_backup\n</code></pre>"},{"location":"linux/storage/lvm-storage/#monitoring-snapshot-usage","title":"Monitoring Snapshot Usage","text":"<p>What to watch: Snapshots need space to store changes. If they fill up, they become invalid!</p> <pre><code># Check snapshot usage\nlvs -a -o +snap_percent\n\n# Output shows percentage used:\n# LV                VG      ...  Snap%\n# lv_database_snap  vg_data ...  15.23\n</code></pre> <p>If it reaches 100%, the snapshot becomes invalid and useless. Solution: make snapshot size larger.</p>"},{"location":"linux/storage/lvm-storage/#extending-a-snapshot","title":"Extending a Snapshot","text":"<pre><code># If snapshot is getting full, extend it\nlvextend -L +5G /dev/vg_data/lv_database_snap\n</code></pre>"},{"location":"linux/storage/lvm-storage/#reverting-with-snapshots-rollback","title":"Reverting with Snapshots (Rollback)","text":"<p>What it does: Merges the snapshot back to the original, reverting all changes.</p> <p>When to use: You tested something, it broke, and you want to undo everything.</p> <p>Example:</p> <pre><code># Before major system update:\n# Take snapshot\nlvcreate -L 20G -s -n lv_root_snap /dev/vg_system/lv_root\n\n# Perform risky operation (install updates, etc.)\ndnf update -y\n\n# If something breaks, revert:\numount /\nlvconvert --merge /dev/vg_system/lv_root_snap\n# Reboot required to complete merge\nreboot\n\n# System comes back to pre-update state!\n</code></pre> <p>Warning: Merging destroys the snapshot and reverts the original LV. Make sure this is what you want!</p>"},{"location":"linux/storage/lvm-storage/#complete-real-world-examples","title":"Complete Real-World Examples","text":""},{"location":"linux/storage/lvm-storage/#example-1-setting-up-lvm-from-scratch","title":"Example 1: Setting Up LVM from Scratch","text":"<p>Scenario: New server with three empty disks. Set up LVM for flexible storage.</p> <pre><code># Step 1: Initialize physical disks\npvcreate /dev/sdb /dev/sdc /dev/sdd\n# Output: Physical volume \"/dev/sdb\" successfully created.\n#         Physical volume \"/dev/sdc\" successfully created.\n#         Physical volume \"/dev/sdd\" successfully created.\n\n# Step 2: Create volume group combining all disks\nvgcreate vg_data /dev/sdb /dev/sdc /dev/sdd\n# Output: Volume group \"vg_data\" successfully created\n\n# Step 3: Check available space\nvgs vg_data\n# Shows total combined space (e.g., 1.5TB)\n\n# Step 4: Create logical volumes for different purposes\nlvcreate -L 200G -n lv_database vg_data\nlvcreate -L 100G -n lv_webapp vg_data\nlvcreate -L 50G -n lv_logs vg_data\nlvcreate -l 100%FREE -n lv_backup vg_data\n\n# Step 5: Create filesystems\nmkfs.ext4 /dev/vg_data/lv_database\nmkfs.xfs /dev/vg_data/lv_webapp\nmkfs.ext4 /dev/vg_data/lv_logs\nmkfs.ext4 /dev/vg_data/lv_backup\n\n# Step 6: Create mount points\nmkdir -p /data/database\nmkdir -p /var/www\nmkdir -p /var/log/apps\nmkdir -p /backup\n\n# Step 7: Mount filesystems\nmount /dev/vg_data/lv_database /data/database\nmount /dev/vg_data/lv_webapp /var/www\nmount /dev/vg_data/lv_logs /var/log/apps\nmount /dev/vg_data/lv_backup /backup\n\n# Step 8: Make permanent in /etc/fstab\ncat &gt;&gt; /etc/fstab &lt;&lt; EOF\n/dev/vg_data/lv_database /data/database ext4 defaults 0 2\n/dev/vg_data/lv_webapp /var/www xfs defaults 0 2\n/dev/vg_data/lv_logs /var/log/apps ext4 defaults 0 2\n/dev/vg_data/lv_backup /backup ext4 defaults 0 2\nEOF\n\n# Step 9: Verify everything\ndf -h\nlvs\n</code></pre>"},{"location":"linux/storage/lvm-storage/#example-2-expanding-storage-when-running-out-of-space","title":"Example 2: Expanding Storage When Running Out of Space","text":"<p>Scenario: /var/www is 95% full and you need more space immediately.</p> <pre><code># Step 1: Check current situation\ndf -h /var/www\n# Filesystem                Size  Used Avail Use% Mounted on\n# /dev/vg_data/lv_webapp    50G   48G   2G  96% /var/www\n\n# Step 2: Check if VG has free space\nvgs vg_data\n# VG       #PV #LV #SN Attr   VSize  VFree\n# vg_data    3   4   0 wz--n- 1.50t  500.00g\n\n# Good! We have 500GB free\n\n# Step 3: Extend the LV by 50GB\nlvextend -L +50G /dev/vg_data/lv_webapp\n# Output: Size of logical volume vg_data/lv_webapp changed from 50.00 GiB to 100.00 GiB\n\n# Step 4: Resize the filesystem (xfs in this case)\nxfs_growfs /var/www\n# Output: data blocks changed from 13107200 to 26214400\n\n# OR for ext4, use:\n# resize2fs /dev/vg_data/lv_webapp\n\n# Step 5: Verify\ndf -h /var/www\n# Filesystem                Size  Used Avail Use% Mounted on\n# /dev/vg_data/lv_webapp   100G   48G   52G  48% /var/www\n\n# Problem solved! No downtime needed.\n</code></pre>"},{"location":"linux/storage/lvm-storage/#example-3-adding-a-new-disk-to-existing-setup","title":"Example 3: Adding a New Disk to Existing Setup","text":"<p>Scenario: Server is running low on storage across all volumes. Add new 1TB disk.</p> <pre><code># Step 1: Initialize new disk\npvcreate /dev/sde\n# Output: Physical volume \"/dev/sde\" successfully created.\n\n# Step 2: Add to existing volume group\nvgextend vg_data /dev/sde\n# Output: Volume group \"vg_data\" successfully extended\n\n# Step 3: Verify new space available\nvgs vg_data\n# Now shows 1TB more space in VFree\n\n# Step 4: Extend volumes that need space\n# Database needs more space\nlvextend -L +200G /dev/vg_data/lv_database\nresize2fs /dev/vg_data/lv_database\n\n# Backup needs more space\nlvextend -L +300G /dev/vg_data/lv_backup\nresize2fs /dev/vg_data/lv_backup\n\n# Step 5: Verify\ndf -h\nlvs vg_data\n</code></pre>"},{"location":"linux/storage/lvm-storage/#example-4-safe-database-maintenance-with-snapshots","title":"Example 4: Safe Database Maintenance with Snapshots","text":"<p>Scenario: Need to perform risky database migrations. Want ability to rollback.</p> <pre><code># Step 1: Stop database (if possible) or at least sync\nsystemctl stop postgresql\n\n# Step 2: Create snapshot (20GB for changes)\nlvcreate -L 20G -s -n lv_db_premigration /dev/vg_data/lv_database\n# Output: Logical volume \"lv_db_premigration\" created\n\n# Step 3: Start database and perform migrations\nsystemctl start postgresql\n./run-database-migration.sh\n\n# Step 4a: If migration successful, remove snapshot\nlvremove /dev/vg_data/lv_db_premigration\n\n# Step 4b: If migration failed, rollback\nsystemctl stop postgresql\numount /data/database\nlvconvert --merge /dev/vg_data/lv_db_premigration\n# Merge will happen on next mount/reboot\nmount /dev/vg_data/lv_database /data/database\nsystemctl start postgresql\n# Database is now back to pre-migration state!\n</code></pre>"},{"location":"linux/storage/lvm-storage/#example-5-replacing-a-failing-disk","title":"Example 5: Replacing a Failing Disk","text":"<p>Scenario: SMART errors on /dev/sdc. Need to replace without downtime.</p> <pre><code># Step 1: Add new replacement disk\npvcreate /dev/sdf\nvgextend vg_data /dev/sdf\n\n# Step 2: Move data from failing disk to new disk\n# This can take hours! System stays online.\npvmove /dev/sdc /dev/sdf\n# Shows progress: /dev/sdc: Moved: 45.2%\n\n# Step 3: After pvmove completes, remove old disk from VG\nvgreduce vg_data /dev/sdc\n\n# Step 4: Remove PV label\npvremove /dev/sdc\n\n# Step 5: Physically remove /dev/sdc\n# No data loss, no downtime!\n</code></pre>"},{"location":"linux/storage/lvm-storage/#troubleshooting-common-issues","title":"Troubleshooting Common Issues","text":""},{"location":"linux/storage/lvm-storage/#issue-1-cannot-create-lv-insufficient-space","title":"Issue 1: \u201cCannot create LV - insufficient space\u201d","text":"<p>Problem: Trying to create LV but get error about insufficient space.</p> <pre><code># Check VG free space\nvgs vg_data\n\n# If VFree is 0, you need to either:\n# a) Add more disks\npvcreate /dev/sde\nvgextend vg_data /dev/sde\n\n# b) Remove or shrink other LVs to free space\nlvremove /dev/vg_data/lv_unused\n# or\nlvreduce -L 50G /dev/vg_data/lv_oversized\n</code></pre>"},{"location":"linux/storage/lvm-storage/#issue-2-device-or-resource-busy-when-removing-lv","title":"Issue 2: \u201cDevice or resource busy\u201d when removing LV","text":"<p>Problem: Can\u2019t remove an LV because it\u2019s in use.</p> <pre><code># Find what's using it\nlsof /dev/vg_data/lv_name\n# or\nfuser -m /mount/point\n\n# Unmount it\numount /mount/point\n\n# Try again\nlvremove /dev/vg_data/lv_name\n</code></pre>"},{"location":"linux/storage/lvm-storage/#issue-3-snapshot-became-invalid","title":"Issue 3: Snapshot became invalid","text":"<p>Problem: Snapshot shows \u201cSnapshot status: INVALID\u201d</p> <p>Cause: Snapshot ran out of space to store changes.</p> <pre><code># Check snapshot usage\nlvs -a -o +snap_percent\n\n# If near 100%, extend it quickly\nlvextend -L +10G /dev/vg_data/lv_snap\n\n# For future: create larger snapshots initially\n</code></pre>"},{"location":"linux/storage/lvm-storage/#issue-4-cannot-activate-lv","title":"Issue 4: Cannot activate LV","text":"<p>Problem: LV won\u2019t activate after reboot.</p> <pre><code># Scan for LVM volumes\npvscan\nvgscan\nlvscan\n\n# Activate all\nvgchange -ay\n\n# Activate specific VG\nvgchange -ay vg_data\n</code></pre>"},{"location":"linux/storage/lvm-storage/#best-practices","title":"Best Practices","text":"<p>1. Plan Before Creating - Think about future growth - Leave free space in VG (20-30%) for flexibility - Use logical naming (lv_database, not lv1)</p> <p>2. Regular Monitoring - Check space regularly: <code>df -h</code> and <code>lvs</code> - Monitor VG free space: <code>vgs</code> - Set up alerts at 80% full</p> <p>3. Backup Before Changes - Always backup before shrinking - Take snapshots before risky operations - Document your LVM layout</p> <p>4. Extent Size - Default 4MB is fine for most cases - Use larger (16MB+) for very large volumes (multiple TB)</p> <p>5. Snapshot Management - Size snapshots appropriately (10-30% of original) - Don\u2019t keep snapshots long-term (performance impact) - Monitor snapshot usage</p> <p>6. Documentation - Keep diagrams of your LVM setup - Document which LVs contain what data - Note any special configurations</p>"},{"location":"linux/storage/lvm-storage/#quick-command-reference","title":"Quick Command Reference","text":"<pre><code># === Creating LVM from scratch ===\npvcreate /dev/sdb                    # Initialize disk\nvgcreate vg_name /dev/sdb           # Create volume group\nlvcreate -L 50G -n lv_name vg_name  # Create logical volume\nmkfs.ext4 /dev/vg_name/lv_name      # Format with filesystem\nmount /dev/vg_name/lv_name /mnt     # Mount it\n\n# === Viewing status ===\npvs                                  # List physical volumes\nvgs                                  # List volume groups\nlvs                                  # List logical volumes\n\n# === Expanding storage ===\nlvextend -L +10G /dev/vg/lv         # Add 10GB to LV\nresize2fs /dev/vg/lv                # Grow ext4 filesystem\nxfs_growfs /mount/point             # Grow XFS filesystem\n\n# === Snapshots ===\nlvcreate -L 10G -s -n snap /dev/vg/lv  # Create snapshot\nlvremove /dev/vg/snap                   # Remove snapshot\nlvconvert --merge /dev/vg/snap          # Revert to snapshot\n\n# === Adding storage ===\npvcreate /dev/sdc                    # Initialize new disk\nvgextend vg_name /dev/sdc           # Add to volume group\n\n# === Removing storage ===\npvmove /dev/sdc                      # Move data off disk\nvgreduce vg_name /dev/sdc           # Remove from VG\npvremove /dev/sdc                    # Clean up\n</code></pre>"},{"location":"linux/storage/remote-filesystems/","title":"Use Remote Filesystems and Network Block Devices","text":""},{"location":"linux/storage/remote-filesystems/#what-are-remote-filesystems","title":"What Are Remote Filesystems?","text":"<p>Remote filesystems let you access files stored on another computer over the network as if they were local. Instead of copying files back and forth, you mount the remote location and work with files directly.</p> <p>Think of it like network drives in Windows, but more powerful and with different protocols for different needs.</p>"},{"location":"linux/storage/remote-filesystems/#why-use-remote-filesystems","title":"Why Use Remote Filesystems?","text":"<p>Central Storage:</p> <ul> <li>One server stores data, many clients access it</li> <li>Easy backups (backup one location, not many computers)</li> <li>Consistent data across multiple machines</li> </ul> <p>Resource Sharing:</p> <ul> <li>Share files between Linux, Windows, and Mac</li> <li>Access powerful storage from lightweight clients</li> <li>Cost effective (one big storage server vs many small disks)</li> </ul>"},{"location":"linux/storage/remote-filesystems/#nfs-network-file-system","title":"NFS - Network File System","text":""},{"location":"linux/storage/remote-filesystems/#what-is-nfs","title":"What is NFS?","text":"<p>NFS is the native file-sharing protocol for Unix/Linux. It\u2019s fast, efficient, and designed for Linux systems to share files with each other.</p> <p>Think of it as: Linux-to-Linux file sharing (though other OS can use it too).</p> <p>Common uses:</p> <ul> <li>Shared home directories on office networks</li> <li>Central storage for web servers</li> <li>Shared data between multiple Linux servers</li> <li>Development teams sharing code</li> </ul>"},{"location":"linux/storage/remote-filesystems/#nfs-versions","title":"NFS Versions","text":"<p>NFSv3:</p> <ul> <li>Older, widely supported</li> <li>Can use UDP or TCP</li> <li>Simpler but less features</li> </ul> <p>NFSv4: (Recommended)</p> <ul> <li>Modern standard</li> <li>Better security (includes Kerberos support)</li> <li>Better performance</li> <li>TCP only</li> <li>Stateful (tracks connections)</li> </ul>"},{"location":"linux/storage/remote-filesystems/#setting-up-nfs-server","title":"Setting Up NFS Server","text":""},{"location":"linux/storage/remote-filesystems/#installing-nfs-server","title":"Installing NFS Server","text":"<pre><code># RHEL/CentOS/Rocky/Alma\ndnf install nfs-utils\nsystemctl enable --now nfs-server\n\n# Debian/Ubuntu\napt install nfs-kernel-server\nsystemctl enable --now nfs-server\n</code></pre>"},{"location":"linux/storage/remote-filesystems/#configuring-exports-etcexports","title":"Configuring Exports - /etc/exports","text":"<p>What it is: This file tells NFS what directories to share and who can access them.</p> <p>Format:</p> <pre><code>/path/to/share    client(options)\n</code></pre> <p>Examples:</p> <pre><code># Share /data with one specific computer\n/data 192.168.1.100(rw,sync,no_subtree_check)\n\n# Share with entire network\n/data 192.168.1.0/24(rw,sync,no_subtree_check)\n\n# Share read-only with everyone\n/public *(ro,sync,no_subtree_check)\n\n# Share /home with specific computers (root on client stays root on server)\n/home 192.168.1.100(rw,sync,no_subtree_check,no_root_squash)\n\n# Multiple clients, different permissions\n/data 192.168.1.100(rw) 192.168.1.0/24(ro)\n</code></pre> <p>Common options:</p> <ul> <li><code>rw</code> - Read-write access</li> <li><code>ro</code> - Read-only access</li> <li><code>sync</code> - Sync writes immediately (safer, slower)</li> <li><code>async</code> - Async writes (faster, less safe)</li> <li><code>no_subtree_check</code> - Don\u2019t check subdirectories (recommended, faster)</li> <li><code>no_root_squash</code> - Don\u2019t map root user to nobody (needed for some setups)</li> <li><code>root_squash</code> - Map root to nobody (default, safer)</li> </ul> <p>Real-world scenario - Company file share:</p> <pre><code># Edit /etc/exports\nvi /etc/exports\n\n# Add share for company data\n# Office computers (192.168.1.0/24) get read-write\n# Remote workers (192.168.2.0/24) get read-only\n/company/data 192.168.1.0/24(rw,sync,no_subtree_check) 192.168.2.0/24(ro,sync,no_subtree_check)\n\n# Apply changes\nexportfs -ra\n\n# Verify\nexportfs -v\n</code></pre>"},{"location":"linux/storage/remote-filesystems/#exportfs-manage-exports","title":"exportfs - Manage Exports","text":"<p>What it does: Applies changes to /etc/exports without restarting NFS server.</p> <p>Why use it: Fast way to add/remove shares or change settings.</p> <p>Examples:</p> <pre><code># Show current exports\nexportfs -v\n\n# Reload /etc/exports (apply changes)\nexportfs -ra\n\n# Export all in /etc/exports\nexportfs -a\n\n# Unexport all\nexportfs -ua\n\n# Unexport specific directory\nexportfs -u 192.168.1.100:/data\n\n# Temporarily export (not in /etc/exports)\nexportfs -o rw,sync 192.168.1.100:/tmp/test\n</code></pre> <p>Real-world scenario:</p> <pre><code># Add new share to /etc/exports\necho \"/backups 192.168.1.0/24(ro,sync,no_subtree_check)\" &gt;&gt; /etc/exports\n\n# Apply without restarting\nexportfs -ra\n\n# Verify it's exported\nexportfs -v | grep backups\n</code></pre>"},{"location":"linux/storage/remote-filesystems/#showmount-check-nfs-exports","title":"showmount - Check NFS Exports","text":"<p>What it does: Shows what a server is exporting.</p> <p>Why use it: Verify your exports or see what another server shares.</p> <p>Examples:</p> <pre><code># Show exports on local server\nshowmount -e localhost\n\n# Show exports on remote server\nshowmount -e nfs-server.example.com\n\n# Show all current mounts\nshowmount -a\n\n# Show directories being exported\nshowmount -d\n</code></pre> <p>Output example:</p> <pre><code>showmount -e nfs-server\nExport list for nfs-server:\n/data      192.168.1.0/24\n/home      192.168.1.100,192.168.1.101\n/public    *\n</code></pre>"},{"location":"linux/storage/remote-filesystems/#firewall-configuration","title":"Firewall Configuration","text":"<pre><code># RHEL/CentOS\nfirewall-cmd --permanent --add-service=nfs\nfirewall-cmd --permanent --add-service=rpc-bind\nfirewall-cmd --permanent --add-service=mountd\nfirewall-cmd --reload\n</code></pre>"},{"location":"linux/storage/remote-filesystems/#using-nfs-as-client","title":"Using NFS as Client","text":""},{"location":"linux/storage/remote-filesystems/#installing-nfs-client","title":"Installing NFS Client","text":"<pre><code># RHEL/CentOS\ndnf install nfs-utils\n\n# Debian/Ubuntu\napt install nfs-common\n</code></pre>"},{"location":"linux/storage/remote-filesystems/#mounting-nfs-shares","title":"Mounting NFS Shares","text":"<p>What it does: Connects to remote share and makes it accessible locally.</p> <p>Why use it: Access files stored on NFS server.</p> <p>Examples:</p> <pre><code># Basic mount\nmount nfs-server:/data /mnt/data\n\n# Specify NFSv4\nmount -t nfs4 nfs-server:/data /mnt/data\n\n# With options\nmount -t nfs -o rw,hard,intr nfs-server:/data /mnt/data\n\n# Mount read-only\nmount -t nfs -o ro nfs-server:/data /mnt/data\n</code></pre> <p>Common mount options:</p> <ul> <li><code>hard</code> - Keep trying if server unavailable (recommended for important data)</li> <li><code>soft</code> - Give up after timeout (can cause data loss)</li> <li><code>intr</code> - Allow interruption if mount hangs</li> <li><code>rsize=8192</code> - Read buffer size</li> <li><code>wsize=8192</code> - Write buffer size</li> <li><code>_netdev</code> - Wait for network before mounting (important for /etc/fstab)</li> </ul> <p>Real-world scenario - Mount company share:</p> <pre><code># Create mount point\nmkdir -p /mnt/company-data\n\n# Test mount first\nmount -t nfs nfs-server.company.com:/data /mnt/company-data\n\n# Check if it works\nls /mnt/company-data\ndf -h /mnt/company-data\n\n# If good, make permanent in /etc/fstab\necho \"nfs-server.company.com:/data /mnt/company-data nfs defaults,_netdev 0 0\" &gt;&gt; /etc/fstab\n\n# Test fstab\numount /mnt/company-data\nmount -a\n</code></pre>"},{"location":"linux/storage/remote-filesystems/#permanent-nfs-mounts-etcfstab","title":"Permanent NFS Mounts - /etc/fstab","text":"<p>Format:</p> <pre><code>server:/export  /mount/point  nfs  options  0  0\n</code></pre> <p>Examples:</p> <pre><code># Basic NFS mount\nnfs-server:/data /mnt/data nfs defaults,_netdev 0 0\n\n# NFSv4 with specific options\nnfs-server:/data /mnt/data nfs4 rw,hard,intr,_netdev 0 0\n\n# With performance tuning\nnfs-server:/data /mnt/data nfs rw,hard,intr,rsize=8192,wsize=8192,_netdev 0 0\n\n# Read-only mount\nnfs-server:/public /mnt/public nfs ro,_netdev 0 0\n</code></pre> <p>Always use <code>_netdev</code>! This tells Linux to wait for the network before mounting, preventing boot failures.</p>"},{"location":"linux/storage/remote-filesystems/#cifssmb-windows-file-sharing","title":"CIFS/SMB - Windows File Sharing","text":""},{"location":"linux/storage/remote-filesystems/#what-is-cifssmb","title":"What is CIFS/SMB?","text":"<p>SMB (Server Message Block) is the file-sharing protocol used by Windows. CIFS is an older dialect of SMB. Linux can both connect to Windows shares and serve files to Windows computers.</p> <p>Think of it as: Linux-to-Windows file sharing (and vice versa).</p> <p>Common uses:</p> <ul> <li>Access Windows file servers from Linux</li> <li>Connect to NAS devices</li> <li>Share files between mixed Windows/Linux environments</li> <li>Access Samba shares</li> </ul>"},{"location":"linux/storage/remote-filesystems/#installing-cifs-client","title":"Installing CIFS Client","text":"<pre><code># RHEL/CentOS\ndnf install cifs-utils\n\n# Debian/Ubuntu\napt install cifs-utils\n</code></pre>"},{"location":"linux/storage/remote-filesystems/#mounting-cifssmb-shares","title":"Mounting CIFS/SMB Shares","text":"<p>What it does: Connects to Windows/Samba shares.</p> <p>Why use it: Access files on Windows servers or NAS devices.</p> <p>Examples:</p> <pre><code># Basic mount with username/password (not secure!)\nmount -t cifs //server/share /mnt/share -o username=myuser,password=mypass\n\n# Using credentials file (recommended)\nmount -t cifs //server/share /mnt/share -o credentials=/root/.smbcreds\n\n# With domain\nmount -t cifs //server/share /mnt/share -o credentials=/root/.smbcreds,domain=COMPANY\n\n# Specify permissions\nmount -t cifs //server/share /mnt/share -o credentials=/root/.smbcreds,uid=1000,gid=1000\n\n# Guest access (no password)\nmount -t cifs //server/share /mnt/share -o guest\n</code></pre> <p>Common options:</p> <ul> <li><code>username=user</code> - Username for authentication</li> <li><code>password=pass</code> - Password (avoid, use credentials file!)</li> <li><code>credentials=file</code> - File with username/password</li> <li><code>domain=DOMAIN</code> - Windows domain</li> <li><code>uid=1000</code> - Set owner of files</li> <li><code>gid=1000</code> - Set group of files</li> <li><code>file_mode=0755</code> - Permissions for files</li> <li><code>dir_mode=0755</code> - Permissions for directories</li> </ul>"},{"location":"linux/storage/remote-filesystems/#credentials-file","title":"Credentials File","text":"<p>Why use it: Never put passwords directly in mount commands or /etc/fstab!</p> <p>Create credentials file:</p> <pre><code># Create file\nvi /root/.smbcreds\n\n# Add credentials\nusername=myuser\npassword=mypassword\ndomain=COMPANY\n\n# Secure it (CRITICAL!)\nchmod 600 /root/.smbcreds\nchown root:root /root/.smbcreds\n</code></pre> <p>Real-world scenario - Mount NAS share:</p> <pre><code># Step 1: Create credentials file\ncat &gt; /root/.nascreds &lt;&lt; EOF\nusername=admin\npassword=SecurePassword123\nEOF\nchmod 600 /root/.nascreds\n\n# Step 2: Create mount point\nmkdir /mnt/nas\n\n# Step 3: Test mount\nmount -t cifs //nas.local/backup /mnt/nas -o credentials=/root/.nascreds,uid=1000,gid=1000\n\n# Step 4: Make permanent\necho \"//nas.local/backup /mnt/nas cifs credentials=/root/.nascreds,uid=1000,gid=1000,_netdev 0 0\" &gt;&gt; /etc/fstab\n</code></pre>"},{"location":"linux/storage/remote-filesystems/#etcfstab-for-cifs","title":"/etc/fstab for CIFS","text":"<pre><code># Using credentials file\n//server/share /mnt/share cifs credentials=/root/.smbcreds,_netdev 0 0\n\n# With specific permissions\n//server/share /mnt/share cifs credentials=/root/.smbcreds,uid=1000,gid=1000,file_mode=0755,dir_mode=0755,_netdev 0 0\n\n# Guest access\n//server/public /mnt/public cifs guest,_netdev 0 0\n</code></pre>"},{"location":"linux/storage/remote-filesystems/#iscsi-network-block-devices","title":"iSCSI - Network Block Devices","text":""},{"location":"linux/storage/remote-filesystems/#what-is-iscsi","title":"What is iSCSI?","text":"<p>iSCSI (Internet SCSI) provides block-level access to storage over a network. Unlike NFS or SMB which share files, iSCSI makes a remote disk appear as if it\u2019s locally attached.</p> <p>Think of it as: A hard drive that\u2019s physically in another computer but appears local.</p> <p>Key Difference:</p> <ul> <li>NFS/CIFS: Share files (filesystem-level)</li> <li>iSCSI: Share entire disks (block-level)</li> </ul> <p>Common uses:</p> <ul> <li>SAN (Storage Area Network) connections</li> <li>Virtual machine storage</li> <li>Database servers needing fast storage</li> <li>Clustered filesystems</li> </ul> <p>Components:</p> <ul> <li>Target: The iSCSI server (provides storage)</li> <li>Initiator: The iSCSI client (uses storage)</li> <li>IQN: Unique identifier (like iqn.2024-01.com.example:server)</li> <li>LUN: Logical Unit Number (the actual storage unit)</li> </ul>"},{"location":"linux/storage/remote-filesystems/#iscsi-target-server","title":"iSCSI Target (Server)","text":""},{"location":"linux/storage/remote-filesystems/#installing-iscsi-target","title":"Installing iSCSI Target","text":"<pre><code># RHEL/CentOS\ndnf install targetcli\nsystemctl enable --now target\n\n# Debian/Ubuntu\napt install targetcli-fb\nsystemctl enable --now target\n</code></pre>"},{"location":"linux/storage/remote-filesystems/#configuring-iscsi-target-with-targetcli","title":"Configuring iSCSI Target with targetcli","text":"<p>What it does: Interactive tool to configure iSCSI storage.</p> <p>Why use it: Easy way to share disks over network.</p> <p>Real-world scenario - Share a disk:</p> <pre><code># Enter interactive mode\ntargetcli\n\n# Create backing storage (file-based, 100GB)\n/backstores/fileio create disk01 /storage/disk01.img 100G\n\n# OR use a real disk\n/backstores/block create disk01 /dev/sdb\n\n# Create iSCSI target\n/iscsi create iqn.2024-01.com.example:storage01\n\n# Create LUN (connects storage to target)\n/iscsi/iqn.2024-01.com.example:storage01/tpg1/luns create /backstores/fileio/disk01\n\n# Allow specific initiator to connect\n/iscsi/iqn.2024-01.com.example:storage01/tpg1/acls create iqn.2024-01.com.example:client01\n\n# Configure network portal\n/iscsi/iqn.2024-01.com.example:storage01/tpg1/portals create 192.168.1.100\n\n# Save and exit\nsaveconfig\nexit\n</code></pre> <p>View configuration:</p> <pre><code>targetcli ls\n</code></pre>"},{"location":"linux/storage/remote-filesystems/#firewall-for-iscsi","title":"Firewall for iSCSI","text":"<pre><code>firewall-cmd --permanent --add-port=3260/tcp\nfirewall-cmd --reload\n</code></pre>"},{"location":"linux/storage/remote-filesystems/#iscsi-initiator-client","title":"iSCSI Initiator (Client)","text":""},{"location":"linux/storage/remote-filesystems/#installing-iscsi-initiator","title":"Installing iSCSI Initiator","text":"<pre><code># RHEL/CentOS\ndnf install iscsi-initiator-utils\nsystemctl enable --now iscsid\n\n# Debian/Ubuntu\napt install open-iscsi\nsystemctl enable --now iscsid\n</code></pre>"},{"location":"linux/storage/remote-filesystems/#setting-initiator-name","title":"Setting Initiator Name","text":"<pre><code># Edit initiator name\nvi /etc/iscsi/initiatorname.iscsi\n\n# Set to match what target allows\nInitiatorName=iqn.2024-01.com.example:client01\n\n# Restart service\nsystemctl restart iscsid\n</code></pre>"},{"location":"linux/storage/remote-filesystems/#connecting-to-iscsi-target","title":"Connecting to iSCSI Target","text":"<p>What it does: Discovers and connects to iSCSI storage.</p> <p>Why use it: Makes remote storage available as local disk.</p> <p>Real-world scenario:</p> <pre><code># Step 1: Discover available targets\niscsiadm -m discovery -t st -p 192.168.1.100\n\n# Output shows available targets:\n# 192.168.1.100:3260,1 iqn.2024-01.com.example:storage01\n\n# Step 2: Login to target (connect)\niscsiadm -m node --login\n\n# Step 3: Check for new disk\nlsblk\n# New disk appears (probably /dev/sdc)\n\n# Step 4: Use it like any disk\n# Create partition\nfdisk /dev/sdc\n\n# Create filesystem\nmkfs.ext4 /dev/sdc1\n\n# Mount\nmkdir /mnt/iscsi\nmount /dev/sdc1 /mnt/iscsi\n\n# Step 5: Make permanent\necho \"/dev/sdc1 /mnt/iscsi ext4 _netdev 0 0\" &gt;&gt; /etc/fstab\n</code></pre>"},{"location":"linux/storage/remote-filesystems/#managing-iscsi-sessions","title":"Managing iSCSI Sessions","text":"<pre><code># Show active sessions\niscsiadm -m session\n\n# Show detailed session info\niscsiadm -m session -P 3\n\n# Logout (disconnect)\niscsiadm -m node -T iqn.2024-01.com.example:storage01 -p 192.168.1.100 --logout\n\n# Login\niscsiadm -m node -T iqn.2024-01.com.example:storage01 -p 192.168.1.100 --login\n\n# Automatic login at boot\niscsiadm -m node -T iqn.2024-01.com.example:storage01 -p 192.168.1.100 --op update -n node.startup -v automatic\n\n# Rescan for new LUNs\niscsiadm -m session --rescan\n</code></pre>"},{"location":"linux/storage/remote-filesystems/#troubleshooting-remote-filesystems","title":"Troubleshooting Remote Filesystems","text":""},{"location":"linux/storage/remote-filesystems/#nfs-troubleshooting","title":"NFS Troubleshooting","text":"<p>Problem: Cannot mount NFS share</p> <pre><code># Step 1: Check if server is exporting\nshowmount -e nfs-server\n\n# Step 2: Test network connectivity\nping nfs-server\ntelnet nfs-server 2049\n\n# Step 3: Check firewall\nfirewall-cmd --list-services\n\n# Step 4: Try mounting with verbose\nmount -v -t nfs nfs-server:/data /mnt/data\n\n# Step 5: Check logs\njournalctl -u nfs-server\ndmesg | grep nfs\n</code></pre> <p>Problem: NFS mount hangs</p> <pre><code># Use soft mount for testing\nmount -t nfs -o soft,timeo=10 nfs-server:/data /mnt/data\n\n# Check if server is responsive\nrpcinfo -p nfs-server\n</code></pre>"},{"location":"linux/storage/remote-filesystems/#cifs-troubleshooting","title":"CIFS Troubleshooting","text":"<p>Problem: Cannot mount Windows share</p> <pre><code># Step 1: List available shares\nsmbclient -L //server -U username\n\n# Step 2: Test connection\nsmbclient //server/share -U username\n\n# Step 3: Check credentials file\ncat /root/.smbcreds\nls -l /root/.smbcreds  # Should be 600\n\n# Step 4: Try with verbose\nmount -v -t cifs //server/share /mnt -o credentials=/root/.smbcreds\n\n# Step 5: Specify SMB version\nmount -t cifs //server/share /mnt -o credentials=/root/.smbcreds,vers=3.0\n</code></pre> <p>Problem: Permission denied on CIFS</p> <pre><code># Mount with specific UID/GID\nmount -t cifs //server/share /mnt -o credentials=/root/.smbcreds,uid=1000,gid=1000,file_mode=0755,dir_mode=0755\n</code></pre>"},{"location":"linux/storage/remote-filesystems/#iscsi-troubleshooting","title":"iSCSI Troubleshooting","text":"<p>Problem: Cannot discover target</p> <pre><code># Check network\nping target-server\nnc -zv target-server 3260\n\n# Check firewall\nfirewall-cmd --list-ports\n\n# Check initiator name matches\ncat /etc/iscsi/initiatorname.iscsi\n\n# Restart service\nsystemctl restart iscsid\n</code></pre> <p>Problem: Lost connection</p> <pre><code># Check session status\niscsiadm -m session -P 3\n\n# Restart session\niscsiadm -m node --logout\niscsiadm -m node --login\n\n# Check logs\njournalctl -u iscsid\n</code></pre>"},{"location":"linux/storage/remote-filesystems/#performance-tips","title":"Performance Tips","text":""},{"location":"linux/storage/remote-filesystems/#nfs-performance","title":"NFS Performance","text":"<pre><code># Increase buffer sizes\nmount -o rsize=32768,wsize=32768 server:/data /mnt/data\n\n# Use async for better performance (less safe)\nmount -o async server:/data /mnt/data\n\n# Combine options\nmount -o rsize=32768,wsize=32768,async,noatime server:/data /mnt/data\n</code></pre>"},{"location":"linux/storage/remote-filesystems/#cifs-performance","title":"CIFS Performance","text":"<pre><code># Use SMB3 with multichannel\nmount -t cifs //server/share /mnt -o vers=3.0,multichannel\n\n# Increase cache\nmount -t cifs //server/share /mnt -o cache=strict\n</code></pre>"},{"location":"linux/storage/remote-filesystems/#quick-reference","title":"Quick Reference","text":""},{"location":"linux/storage/remote-filesystems/#nfs","title":"NFS","text":"<pre><code># Server\nexportfs -ra                      # Reload exports\nshowmount -e                      # Show exports\n\n# Client\nmount nfs-server:/data /mnt       # Mount NFS share\numount /mnt                       # Unmount\n</code></pre>"},{"location":"linux/storage/remote-filesystems/#cifs","title":"CIFS","text":"<pre><code># Mount\nmount -t cifs //server/share /mnt -o credentials=/root/.creds\n\n# List shares\nsmbclient -L //server -U user\n</code></pre>"},{"location":"linux/storage/remote-filesystems/#iscsi","title":"iSCSI","text":"<pre><code># Discover targets\niscsiadm -m discovery -t st -p server\n\n# Connect\niscsiadm -m node --login\n\n# Show sessions\niscsiadm -m session\n\n# Disconnect\niscsiadm -m node --logout\n</code></pre>"},{"location":"linux/storage/remote-filesystems/#credentials-file_1","title":"Credentials File","text":"<pre><code># Create CIFS credentials\ncat &gt; /root/.smbcreds &lt;&lt; EOF\nusername=myuser\npassword=mypass\ndomain=COMPANY\nEOF\nchmod 600 /root/.smbcreds\n</code></pre>"},{"location":"linux/storage/storage-performance/","title":"Monitor Storage Performance","text":""},{"location":"linux/storage/storage-performance/#what-is-storage-performance-monitoring","title":"What is Storage Performance Monitoring?","text":"<p>Storage performance monitoring is about watching how your disks and filesystems are performing. Are they fast enough? Are they bottlenecking your system? Are they about to fail?</p> <p>Think of it like checking your car\u2019s dashboard - you want to know if everything\u2019s running smoothly before problems occur.</p>"},{"location":"linux/storage/storage-performance/#why-monitor-storage","title":"Why Monitor Storage?","text":"<p>Prevent problems:</p> <ul> <li>Catch failing disks before they fail completely</li> <li>Identify bottlenecks before users complain</li> <li>Plan capacity before running out of space</li> </ul> <p>Optimize performance:</p> <ul> <li>Find slow I/O operations</li> <li>Identify what\u2019s causing disk activity</li> <li>Tune system for better performance</li> </ul> <p>Troubleshoot issues:</p> <ul> <li>System is slow - is it the disk?</li> <li>Application timing out - disk bottleneck?</li> <li>High load - what\u2019s accessing the disk?</li> </ul>"},{"location":"linux/storage/storage-performance/#key-performance-metrics","title":"Key Performance Metrics","text":""},{"location":"linux/storage/storage-performance/#understanding-the-numbers","title":"Understanding the Numbers","text":"<p>IOPS (Input/Output Operations Per Second):</p> <ul> <li>How many read/write operations per second</li> <li>Like \u201chow many files can I access per second\u201d</li> <li>Higher = better (especially for many small files)</li> <li>Example: Database servers need high IOPS</li> </ul> <p>Throughput (MB/s):</p> <ul> <li>How much data transferred per second</li> <li>Like \u201chow fast can I copy a large file\u201d</li> <li>Higher = better (especially for large files)</li> <li>Example: Video editing needs high throughput</li> </ul> <p>Latency (milliseconds):</p> <ul> <li>How long each operation takes</li> <li>Like \u201chow long do I wait for disk response\u201d</li> <li>Lower = better</li> <li>Good: &lt;10ms, Acceptable: 10-20ms, Bad: &gt;20ms</li> </ul> <p>Utilization (%):</p> <ul> <li>How busy the disk is</li> <li>100% means fully busy</li> <li> <p>80% sustained = potential bottleneck</p> </li> <li>Example: 95% util = disk can\u2019t keep up</li> </ul>"},{"location":"linux/storage/storage-performance/#iostat-io-statistics","title":"iostat - I/O Statistics","text":""},{"location":"linux/storage/storage-performance/#what-is-iostat","title":"What is iostat?","text":"<p>iostat shows how busy your disks are, how much data they\u2019re reading/writing, and if there are performance issues.</p> <p>Think of it as: A real-time dashboard for your disks.</p>"},{"location":"linux/storage/storage-performance/#installing-iostat","title":"Installing iostat","text":"<pre><code># RHEL/CentOS/Rocky\ndnf install sysstat\nsystemctl enable --now sysstat\n\n# Debian/Ubuntu\napt install sysstat\nsystemctl enable --now sysstat\n</code></pre>"},{"location":"linux/storage/storage-performance/#using-iostat","title":"Using iostat","text":"<p>Examples:</p> <pre><code># Basic snapshot\niostat\n\n# Extended statistics (most useful!)\niostat -x\n\n# Human-readable sizes\niostat -xh\n\n# Update every 2 seconds\niostat -x 2\n\n# 10 samples, 2 seconds apart\niostat -x 2 10\n\n# Show in megabytes\niostat -xm 2\n\n# With timestamps\niostat -xt 2\n</code></pre> <p>Real-world scenario - Check disk performance:</p> <pre><code># System feels slow, check disks\niostat -x 2\n\nDevice   r/s   w/s   rkB/s   wkB/s  await  %util\nsda     10.5  45.2   512     2048   8.3    45.2\nsdb    150.3  89.7  3072     4096   85.5   98.7\nsdc      2.1   1.3    64       32   2.1     5.3\n</code></pre> <p>Reading the output:</p> <ul> <li>sda: Normal activity, 45% busy, 8ms wait - Good!</li> <li>sdb: Very busy (98.7%), high wait (85ms) - BOTTLENECK!</li> <li>sdc: Barely used, 5% busy - Fine</li> </ul> <p>Solution: sdb is your problem. It\u2019s nearly 100% busy and operations are waiting 85ms.</p>"},{"location":"linux/storage/storage-performance/#key-iostat-columns","title":"Key iostat Columns","text":"<pre><code>Device:   Drive name (sda, sdb, nvme0n1)\nr/s:      Reads per second\nw/s:      Writes per second  \nrkB/s:    Kilobytes read per second\nwkB/s:    Kilobytes written per second\nawait:    Average wait time in milliseconds\n%util:    How busy the drive is (percentage)\n</code></pre> <p>What\u2019s good vs bad:</p> <pre><code>await    Interpretation\n&lt;10ms    Excellent (SSD territory)\n10-20ms  Good (normal HDD)\n20-50ms  Slow (loaded system)\n&gt;50ms    Problem! (bottleneck)\n\n%util    Interpretation\n&lt;50%     Plenty of capacity\n50-80%   Moderate load\n80-95%   Getting busy\n&gt;95%     Saturated (bottleneck!)\n</code></pre>"},{"location":"linux/storage/storage-performance/#iotop-io-by-process","title":"iotop - I/O by Process","text":""},{"location":"linux/storage/storage-performance/#what-is-iotop","title":"What is iotop?","text":"<p>iotop shows which programs are using disk I/O. Like <code>top</code> but for disk activity.</p> <p>Think of it as: \u201cWho\u2019s hogging my disk?\u201d</p>"},{"location":"linux/storage/storage-performance/#installing-iotop","title":"Installing iotop","text":"<pre><code># RHEL/CentOS\ndnf install iotop\n\n# Debian/Ubuntu\napt install iotop\n</code></pre>"},{"location":"linux/storage/storage-performance/#using-iotop","title":"Using iotop","text":"<p>Examples:</p> <pre><code># Basic view\niotop\n\n# Only show processes doing I/O (most useful!)\niotop -o\n\n# Accumulated I/O (total since start)\niotop -oa\n\n# With timestamps\niotop -oat\n\n# Batch mode (for logging)\niotop -ob -n 10\n\n# Monitor specific process\niotop -p 1234\n\n# Monitor specific user\niotop -u mysql\n</code></pre> <p>Interactive keys:</p> <ul> <li><code>o</code> - Toggle showing only active processes</li> <li><code>a</code> - Toggle accumulated mode</li> <li><code>r</code> - Reverse sort</li> <li><code>q</code> - Quit</li> </ul> <p>Real-world scenario - System slow, find culprit:</p> <pre><code>iotop -o\n\nTotal DISK READ: 85.3 MB/s | Total DISK WRITE: 120.5 MB/s\nTID  USER     DISK READ  DISK WRITE  COMMAND\n2341 mysql    65.3 MB/s   95.2 MB/s  mysqld\n3422 root     15.2 MB/s   20.1 MB/s  tar\n4551 www-data  4.8 MB/s    5.2 MB/s  apache2\n</code></pre> <p>Reading the output:</p> <ul> <li>MySQL is hammering the disk (65MB/s read, 95MB/s write)</li> <li>tar backup is also using disk</li> <li>Apache is minimal</li> </ul> <p>Solution: MySQL query or backup causing high I/O.</p>"},{"location":"linux/storage/storage-performance/#vmstat-system-statistics","title":"vmstat - System Statistics","text":""},{"location":"linux/storage/storage-performance/#what-is-vmstat","title":"What is vmstat?","text":"<p>vmstat shows overall system performance including memory, swap, and I/O wait. Good for seeing if disk is causing system slowness.</p> <p>Think of it as: Overall health monitor with disk focus.</p>"},{"location":"linux/storage/storage-performance/#using-vmstat","title":"Using vmstat","text":"<p>Examples:</p> <pre><code># Single snapshot\nvmstat\n\n# Update every 2 seconds\nvmstat 2\n\n# 10 updates\nvmstat 2 10\n\n# Show active/inactive memory\nvmstat -a 2\n\n# Disk statistics\nvmstat -d\n\n# Memory statistics\nvmstat -s\n</code></pre> <p>Real-world scenario - Is disk slowing system?</p> <pre><code>vmstat 2\n\nprocs -----------memory---------- ---swap-- -----io---- --cpu----\n r  b   swpd   free   buff  cache   si   so    bi    bo   wa\n 2  3  10240  2048  4096  8192     0    0   500  2000   35\n 1  4  10240  2048  4096  8192     0    0   800  3500   45\n 3  5  10240  2048  4096  8192     0    0  1200  5000   52\n</code></pre> <p>Reading the output:</p> <ul> <li>r: Processes waiting for CPU</li> <li>b: Processes blocked waiting for I/O (3-5 = high!)</li> <li>si/so: Swap in/out (0 = good, no swapping)</li> <li>bi/bo: Blocks in/out (high numbers = heavy I/O)</li> <li>wa: I/O wait (35-52% = PROBLEM!)</li> </ul> <p>Interpretation:</p> <p>When wa (I/O wait) is consistently above 20%, your system is waiting for disk operations. This is a bottleneck!</p>"},{"location":"linux/storage/storage-performance/#df-and-du-space-usage","title":"df and du - Space Usage","text":""},{"location":"linux/storage/storage-performance/#df-disk-free","title":"df - Disk Free","text":"<p>What it does: Shows how much space is used/available on filesystems.</p> <p>Think of it as: \u201cHow full is my disk?\u201d</p> <p>Examples:</p> <pre><code># Human-readable\ndf -h\n\n# With filesystem types\ndf -hT\n\n# Inode usage (number of files)\ndf -hi\n\n# Specific filesystem\ndf -h /var\n</code></pre> <p>Output example:</p> <pre><code>df -h\nFilesystem      Size  Used Avail Use% Mounted on\n/dev/sda1        50G   35G   13G  73% /\n/dev/sdb1       500G  475G   25G  95% /data\ntmpfs           8.0G  1.0M  8.0G   1% /tmp\n</code></pre> <p>Critical thresholds:</p> <ul> <li>&lt;80%: Healthy</li> <li>80-90%: Monitor closely</li> <li>90-95%: Take action soon</li> <li> <p>95%: Critical! Free space immediately</p> </li> </ul> <p>Real-world scenario - Disk filling up:</p> <pre><code>df -h /\n# 95% full - need to find what's using space!\n\n# Find large directories\ndu -sh /* | sort -rh | head -10\n# Output:\n# 30G  /var\n# 10G  /usr\n# 5G   /home\n\n# Drill down\ndu -sh /var/* | sort -rh | head -5\n# 25G  /var/log\n# 3G   /var/cache\n# 2G   /var/lib\n\n# Found it! /var/log\nls -lh /var/log\n# old-logs.tar.gz is 20GB!\n</code></pre>"},{"location":"linux/storage/storage-performance/#du-disk-usage","title":"du - Disk Usage","text":"<p>What it does: Shows size of directories and files.</p> <p>Think of it as: \u201cWhat\u2019s taking up space?\u201d</p> <p>Examples:</p> <pre><code># Current directory total\ndu -sh .\n\n# Each subdirectory\ndu -sh *\n\n# Top level only\ndu -h --max-depth=1 /var\n\n# Sort by size\ndu -sh /var/* | sort -rh\n\n# Top 10 largest\ndu -ah /home | sort -rh | head -10\n\n# Exclude patterns\ndu -sh --exclude='*.log' /var\n</code></pre> <p>Real-world scenario - Find space hogs:</p> <pre><code># Root is 95% full\ndf -h /\n\n# Start at root\ndu -sh /* 2&gt;/dev/null | sort -rh\n# Output shows /var is huge\n\n# Go deeper\ndu -sh /var/* | sort -rh\n# /var/log is 50GB!\n\n# Find biggest logs\ndu -sh /var/log/* | sort -rh | head -5\n# application.log is 45GB!\n\n# Clean up\ngzip /var/log/application.log\n# or delete old logs\n</code></pre>"},{"location":"linux/storage/storage-performance/#lsof-and-fuser-find-open-files","title":"lsof and fuser - Find Open Files","text":""},{"location":"linux/storage/storage-performance/#lsof-list-open-files","title":"lsof - List Open Files","text":"<p>What it does: Shows which processes have which files open.</p> <p>Think of it as: \u201cWho\u2019s using this file/directory?\u201d</p> <p>Examples:</p> <pre><code># All open files in directory\nlsof +D /var/log\n\n# Files opened by user\nlsof -u mysql\n\n# Files opened by process\nlsof -p 1234\n\n# What's using a mount point\nlsof /mnt/data\n\n# Network connections\nlsof -i\n\n# What's using port 80\nlsof -i :80\n\n# Find deleted but open files (wasting space!)\nlsof | grep deleted\n</code></pre> <p>Real-world scenario - Can\u2019t unmount:</p> <pre><code># Try to unmount\numount /data\n# Error: device is busy\n\n# Find what's using it\nlsof +D /data\n# Output:\n# COMMAND  PID USER   FD   TYPE DEVICE SIZE/OFF NODE NAME\n# bash    1234 john  cwd    DIR    8,1     4096   12 /data/work\n# mysql   5678 mysql  4r    REG    8,1  5242880   45 /data/db/file.db\n\n# Kill the processes or close their files\n</code></pre>"},{"location":"linux/storage/storage-performance/#fuser-find-process-using-file","title":"fuser - Find Process Using File","text":"<p>What it does: Simpler than lsof, shows PIDs using a file.</p> <p>Think of it as: Quick \u201cwho\u2019s using this?\u201d</p> <p>Examples:</p> <pre><code># Show processes using file\nfuser /var/log/syslog\n\n# Verbose (show user and type)\nfuser -v /mnt/data\n\n# What's using the filesystem\nfuser -m /mnt/data\n\n# Kill all processes using it (dangerous!)\nfuser -k /mnt/data\n\n# Interactive kill (asks first)\nfuser -ki /mnt/data\n</code></pre>"},{"location":"linux/storage/storage-performance/#smartctl-disk-health","title":"smartctl - Disk Health","text":""},{"location":"linux/storage/storage-performance/#what-is-smartctl","title":"What is smartctl?","text":"<p>smartctl reads SMART (Self-Monitoring, Analysis and Reporting Technology) data from drives. This predicts disk failures before they happen!</p> <p>Think of it as: Your disk\u2019s health checkup.</p>"},{"location":"linux/storage/storage-performance/#installing-smartctl","title":"Installing smartctl","text":"<pre><code># RHEL/CentOS\ndnf install smartmontools\nsystemctl enable --now smartd\n\n# Debian/Ubuntu\napt install smartmontools\nsystemctl enable --now smartd\n</code></pre>"},{"location":"linux/storage/storage-performance/#using-smartctl","title":"Using smartctl","text":"<p>Examples:</p> <pre><code># Health status (most important!)\nsmartctl -H /dev/sda\n\n# All SMART data\nsmartctl -a /dev/sda\n\n# Attributes only\nsmartctl -A /dev/sda\n\n# Self-test log\nsmartctl -l selftest /dev/sda\n\n# Error log\nsmartctl -l error /dev/sda\n\n# Run short self-test\nsmartctl -t short /dev/sda\n\n# Run long self-test (takes hours)\nsmartctl -t long /dev/sda\n\n# For NVMe drives\nsmartctl -a /dev/nvme0\n</code></pre> <p>Real-world scenario - Check disk health:</p> <pre><code># Monthly health check\nsmartctl -H /dev/sda\n\n# Output:\n# === START OF READ SMART DATA SECTION ===\n# SMART overall-health self-assessment test result: PASSED\n\n# Good! Now check details\nsmartctl -A /dev/sda\n</code></pre>"},{"location":"linux/storage/storage-performance/#important-smart-attributes","title":"Important SMART Attributes","text":"<pre><code>ID  ATTRIBUTE_NAME          VALUE\n5   Reallocated_Sector_Ct   100    \u2190 Should be 0 or very low\n10  Spin_Retry_Count         100    \u2190 Should be 0\n196 Reallocated_Event_Count  100    \u2190 Should be 0\n197 Current_Pending_Sector   100    \u2190 Should be 0 (failing!)\n198 Offline_Uncorrectable    100    \u2190 Should be 0 (failing!)\n</code></pre> <p>Warning signs (REPLACE DISK!):</p> <ul> <li>Health status: FAILING</li> <li>Reallocated sectors increasing</li> <li>Current pending sectors &gt; 0</li> <li>High error count</li> <li>Multiple failed self-tests</li> </ul> <p>Example - Failing disk:</p> <pre><code>smartctl -A /dev/sdb\n\nID  ATTRIBUTE_NAME          VALUE\n5   Reallocated_Sector_Ct    85    \u2190 BAD! Was 100, now 85\n197 Current_Pending_Sector    1    \u2190 VERY BAD! Sectors failing\n198 Offline_Uncorrectable     2    \u2190 VERY BAD! Unreadable data\n\nsmartctl -H /dev/sdb\n# Result: FAILING!\n\n# ACTION REQUIRED:\n# 1. Backup immediately!\n# 2. Replace disk\n# 3. Do NOT wait!\n</code></pre>"},{"location":"linux/storage/storage-performance/#simple-performance-test","title":"Simple Performance Test","text":""},{"location":"linux/storage/storage-performance/#dd-basic-disk-speed-test","title":"dd - Basic Disk Speed Test","text":"<p>What it does: Tests raw disk read/write speed.</p> <p>Think of it as: Simple benchmark.</p> <p>Examples:</p> <pre><code># Write test (1GB file)\ndd if=/dev/zero of=/tmp/testfile bs=1M count=1000 oflag=direct\n\n# Output:\n# 1000+0 records in\n# 1000+0 records out\n# 1048576000 bytes (1.0 GB) copied, 5.2 s, 202 MB/s\n\n# Read test\ndd if=/tmp/testfile of=/dev/null bs=1M\n\n# Cleanup\nrm /tmp/testfile\n</code></pre> <p>Interpreting results:</p> <ul> <li>HDD: 100-200 MB/s typical</li> <li>SATA SSD: 500-600 MB/s</li> <li>NVMe SSD: 2000-7000 MB/s</li> </ul>"},{"location":"linux/storage/storage-performance/#troubleshooting-scenarios","title":"Troubleshooting Scenarios","text":""},{"location":"linux/storage/storage-performance/#scenario-1-system-very-slow","title":"Scenario 1: System Very Slow","text":"<p>Steps:</p> <pre><code># 1. Check I/O wait\nvmstat 2 5\n# If wa &gt; 20%, disk is the problem\n\n# 2. Find busy disk\niostat -x 2 5\n# Look for %util &gt; 90%\n\n# 3. Find culprit process\niotop -o\n# See what's hammering disk\n\n# 4. Fix\n# Kill process, optimize query, add cache, etc.\n</code></pre>"},{"location":"linux/storage/storage-performance/#scenario-2-disk-almost-full","title":"Scenario 2: Disk Almost Full","text":"<p>Steps:</p> <pre><code># 1. Confirm full\ndf -h\n\n# 2. Find large directories\ndu -sh /* | sort -rh | head -10\n\n# 3. Drill down\ndu -sh /var/* | sort -rh | head -5\n\n# 4. Find specific files\nfind /var/log -type f -size +100M\n\n# 5. Clean up\n# Delete, archive, or move files\n</code></pre>"},{"location":"linux/storage/storage-performance/#scenario-3-cannot-unmount","title":"Scenario 3: Cannot Unmount","text":"<p>Steps:</p> <pre><code># 1. Find what's using it\nlsof +D /mount/point\nfuser -vm /mount/point\n\n# 2. Kill processes\nkill PID\n\n# 3. If still busy, force\numount -l /mount/point\n</code></pre>"},{"location":"linux/storage/storage-performance/#monitoring-best-practices","title":"Monitoring Best Practices","text":"<p>1. Regular health checks:</p> <pre><code># Weekly disk health\nsmartctl -H /dev/sda\n\n# Daily space check\ndf -h | grep -v tmpfs\n</code></pre> <p>2. Set up alerts:</p> <pre><code># Alert when &gt;90% full\ndf -h | awk '$5 &gt; 90 {print $0}'\n\n# Alert on SMART warnings\nsmartctl -H /dev/sda | grep -i fail\n</code></pre> <p>3. Keep historical data:</p> <pre><code># Log iostat daily\niostat -x 60 1440 &gt; /var/log/iostat-$(date +%Y%m%d).log\n</code></pre> <p>4. Monitor trends:</p> <pre><code># Space usage over time\ndu -sh /var/log &gt;&gt; /var/log/space-usage.log\n</code></pre>"},{"location":"linux/storage/storage-performance/#quick-reference","title":"Quick Reference","text":""},{"location":"linux/storage/storage-performance/#check-performance","title":"Check Performance","text":"<pre><code>iostat -x 2              # Disk busy?\niotop -o                 # Who's using disk?\nvmstat 2                 # I/O wait high?\ndf -h                    # Space available?\n</code></pre>"},{"location":"linux/storage/storage-performance/#find-space-hogs","title":"Find Space Hogs","text":"<pre><code>du -sh /* | sort -rh     # Largest directories\ndu -sh * | sort -rh      # Current dir\nfind / -type f -size +1G # Files &gt; 1GB\n</code></pre>"},{"location":"linux/storage/storage-performance/#check-health","title":"Check Health","text":"<pre><code>smartctl -H /dev/sda     # Overall health\nsmartctl -A /dev/sda     # Detailed attributes\ndmesg | grep -i error    # System errors\n</code></pre>"},{"location":"linux/storage/storage-performance/#troubleshoot-issues","title":"Troubleshoot Issues","text":"<pre><code>lsof +D /path            # What's using path\nfuser -vm /path          # PIDs using path\nmount | grep /path       # Is it mounted?\n</code></pre>"},{"location":"linux/storage/storage-performance/#speed-test","title":"Speed Test","text":"<pre><code># Write speed\ndd if=/dev/zero of=/tmp/test bs=1M count=1000 oflag=direct\n\n# Read speed  \ndd if=/tmp/test of=/dev/null bs=1M\n</code></pre>"},{"location":"linux/storage/swap-space/","title":"Configure and Manage Swap Space","text":""},{"location":"linux/storage/swap-space/#what-is-swap-space","title":"What is Swap Space?","text":"<p>Swap is space on your hard drive that acts as extra (but slower) RAM. When your physical RAM fills up, Linux moves less-used data to swap to free up RAM for active programs.</p> <p>Think of it like this:</p> <ul> <li>RAM: Your desk - fast and convenient for work you\u2019re doing now</li> <li>Swap: A filing cabinet - slower to access, but expands your working space</li> </ul>"},{"location":"linux/storage/swap-space/#when-is-swap-used","title":"When is Swap Used?","text":"<p>Scenario 1: Running out of RAM</p> <p>You\u2019re running many programs and RAM fills up. Linux moves inactive pages to swap, freeing RAM for active processes.</p> <p>Scenario 2: Hibernation</p> <p>When you hibernate, Linux copies all RAM to swap, then powers off. On resume, it reads swap back into RAM.</p> <p>Scenario 3: Memory Management</p> <p>Even with free RAM, Linux might swap out rarely-used pages to have more free RAM for file caching (speeds up the system).</p>"},{"location":"linux/storage/swap-space/#swap-size-recommendations","title":"Swap Size Recommendations","text":"Physical RAM Swap Size (No Hibernation) With Hibernation &lt; 2 GB 2x RAM 3x RAM 2-8 GB = RAM 2x RAM 8-64 GB 4-8 GB (or 0.5x RAM) 1.5x RAM &gt; 64 GB 4-8 GB minimum Not practical <p>Note: These are guidelines. Your needs depend on your workload.</p>"},{"location":"linux/storage/swap-space/#creating-swap-space","title":"Creating Swap Space","text":""},{"location":"linux/storage/swap-space/#two-types-of-swap","title":"Two Types of Swap","text":"<p>1. Swap Partition:</p> <ul> <li>Dedicated disk partition for swap</li> <li>Slightly faster</li> <li>Fixed size (harder to change)</li> <li>Traditional approach</li> </ul> <p>2. Swap File:</p> <ul> <li>Regular file used as swap</li> <li>Easier to create/resize</li> <li>More flexible</li> <li>Modern approach</li> </ul> <p>Both work equally well. Swap files are more convenient.</p>"},{"location":"linux/storage/swap-space/#creating-swap-files","title":"Creating Swap Files","text":""},{"location":"linux/storage/swap-space/#fallocate-quick-file-creation","title":"fallocate - Quick File Creation","text":"<p>What it does: Instantly creates a file of specific size.</p> <p>Why use it: Fastest way to create swap file.</p> <p>Example - Creating 2GB swap file:</p> <pre><code># Step 1: Create the file (instant!)\nfallocate -l 2G /swapfile\n\n# Step 2: Set correct permissions (CRITICAL for security!)\nchmod 600 /swapfile\n\n# Step 3: Format as swap\nmkswap /swapfile\n\n# Step 4: Enable it\nswapon /swapfile\n\n# Step 5: Verify\nswapon --show\nfree -h\n\n# Step 6: Make permanent\necho '/swapfile none swap sw 0 0' &gt;&gt; /etc/fstab\n</code></pre> <p>Real-world scenario - VPS with no swap:</p> <pre><code># Many VPS providers don't include swap\n# Check current swap\nfree -h\n# Shows: Swap: 0B\n\n# Create 4GB swap file\nsudo fallocate -l 4G /swapfile\nsudo chmod 600 /swapfile\nsudo mkswap /swapfile\nsudo swapon /swapfile\n\n# Verify\nfree -h\n# Now shows: Swap: 4.0Gi\n\n# Make permanent\necho '/swapfile none swap sw 0 0' | sudo tee -a /etc/fstab\n</code></pre>"},{"location":"linux/storage/swap-space/#dd-alternative-file-creation","title":"dd - Alternative File Creation","text":"<p>What it does: Copies data to create file (slower but more compatible).</p> <p>Why use it: Works on all systems, shows progress.</p> <p>Example:</p> <pre><code># Create 2GB swap file with progress\ndd if=/dev/zero of=/swapfile bs=1M count=2048 status=progress\n\n# Then follow same steps as fallocate\nchmod 600 /swapfile\nmkswap /swapfile\nswapon /swapfile\n</code></pre>"},{"location":"linux/storage/swap-space/#creating-swap-partitions","title":"Creating Swap Partitions","text":""},{"location":"linux/storage/swap-space/#using-fdisk","title":"Using fdisk","text":"<p>Real-world scenario - New disk for swap:</p> <pre><code># Step 1: Identify disk\nlsblk\n# /dev/sdb is your new disk\n\n# Step 2: Create partition\nfdisk /dev/sdb\n\n# In fdisk:\n# Press: n (new partition)\n# Press: p (primary)\n# Press: 1 (partition number)\n# Press: Enter (default start)\n# Press: +8G (8GB size)\n# Press: t (change type)\n# Press: 82 (Linux swap)\n# Press: w (write changes)\n\n# Step 3: Format as swap\nmkswap /dev/sdb1\n\n# Step 4: Enable\nswapon /dev/sdb1\n\n# Step 5: Make permanent\necho '/dev/sdb1 none swap sw 0 0' &gt;&gt; /etc/fstab\n</code></pre>"},{"location":"linux/storage/swap-space/#swap-commands","title":"Swap Commands","text":""},{"location":"linux/storage/swap-space/#mkswap-format-as-swap","title":"mkswap - Format as Swap","text":"<p>What it does: Prepares a partition or file to be used as swap space.</p> <p>Why use it: Required before using any space as swap.</p> <p>Examples:</p> <pre><code># Format partition\nmkswap /dev/sdb1\n\n# Format file\nmkswap /swapfile\n\n# With label (helpful for identification)\nmkswap -L \"swap1\" /dev/sdb1\n</code></pre> <p>Output example:</p> <pre><code>Setting up swapspace version 1, size = 2 GiB (2147479552 bytes)\nno label, UUID=1234abcd-5678-...\n</code></pre>"},{"location":"linux/storage/swap-space/#swapon-enable-swap","title":"swapon - Enable Swap","text":"<p>What it does: Activates swap space so Linux can use it.</p> <p>Why use it: Swap isn\u2019t usable until you enable it.</p> <p>Examples:</p> <pre><code># Enable specific swap\nswapon /swapfile\nswapon /dev/sdb1\n\n# Enable all swap in /etc/fstab\nswapon -a\n\n# Enable with priority (higher number = used first)\nswapon -p 10 /dev/sdb1\nswapon -p 5 /swapfile\n\n# Show what's active\nswapon --show\n\n# Verbose mode\nswapon -v /swapfile\n</code></pre> <p>Understanding priority:</p> <pre><code># Fast SSD swap - high priority (used first)\nswapon -p 100 /dev/nvme0n1p3\n\n# Slow HDD swap - low priority (used last)\nswapon -p 10 /dev/sdb1\n</code></pre> <p>Output of swapon \u2013show:</p> <pre><code>NAME      TYPE SIZE USED PRIO\n/swapfile file   2G   0B   -2\n/dev/sdb1 partition 4G 512M 5\n</code></pre>"},{"location":"linux/storage/swap-space/#swapoff-disable-swap","title":"swapoff - Disable Swap","text":"<p>What it does: Deactivates swap space.</p> <p>Why use it: Before removing swap or making changes.</p> <p>Examples:</p> <pre><code># Disable specific swap\nswapoff /swapfile\n\n# Disable all swap\nswapoff -a\n\n# Verbose\nswapoff -v /swapfile\n</code></pre> <p>Real-world scenario - Resizing swap file:</p> <pre><code># Current 2GB swap is too small, need 4GB\n\n# Step 1: Disable swap\nswapoff /swapfile\n\n# Step 2: Delete old file\nrm /swapfile\n\n# Step 3: Create new 4GB file\nfallocate -l 4G /swapfile\nchmod 600 /swapfile\n\n# Step 4: Format and enable\nmkswap /swapfile\nswapon /swapfile\n\n# Step 5: Verify\nswapon --show\n</code></pre>"},{"location":"linux/storage/swap-space/#free-memory-and-swap-status","title":"free - Memory and Swap Status","text":"<p>What it does: Shows RAM and swap usage.</p> <p>Why use it: Quick check of memory situation.</p> <p>Examples:</p> <pre><code># Human-readable\nfree -h\n\n# In megabytes\nfree -m\n\n# In gigabytes  \nfree -g\n\n# Continuous updates (every 2 seconds)\nfree -h -s 2\n\n# With totals\nfree -ht\n\n# Wide mode (more detailed)\nfree -hw\n</code></pre> <p>Output example:</p> <pre><code>free -h\n              total        used        free      shared  buff/cache   available\nMem:           16Gi       8.0Gi       2.0Gi       100Mi       6.0Gi       7.5Gi\nSwap:          4.0Gi       512Mi       3.5Gi\n</code></pre> <p>What it means:</p> <ul> <li>total: Total RAM/swap installed</li> <li>used: Currently in use by programs</li> <li>free: Completely unused</li> <li>shared: Used by tmpfs/shared memory</li> <li>buff/cache: Used for caching (can be freed if needed)</li> <li>available: How much can be used by programs (free + reclaimable cache)</li> </ul>"},{"location":"linux/storage/swap-space/#vmstat-virtual-memory-statistics","title":"vmstat - Virtual Memory Statistics","text":"<p>What it does: Shows system activity including swap in/out.</p> <p>Why use it: Monitor if system is swapping heavily (performance problem).</p> <p>Examples:</p> <pre><code># Single snapshot\nvmstat\n\n# Update every 2 seconds\nvmstat 2\n\n# 10 updates, 2 seconds apart\nvmstat 2 10\n\n# Memory statistics\nvmstat -s\n\n# Active/inactive memory\nvmstat -a\n\n# Disk statistics\nvmstat -d\n</code></pre> <p>Output example:</p> <pre><code>vmstat 2\nprocs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----\n r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st\n 1  0  10240 204800 102400 512000    0    0    10    50  100  200  5  2 90  3  0\n 0  1  10240 204800 102400 512000    5   10    20   100  150  250  8  4 85  3  0\n</code></pre> <p>Critical columns:</p> <ul> <li>si (swap in): KB/s reading from swap (from disk to RAM)</li> <li>so (swap out): KB/s writing to swap (from RAM to disk)</li> <li>wa: % CPU waiting for I/O</li> </ul> <p>What to watch for:</p> <pre><code>si  so  \u2190 Both zero: No swapping (good!)\n0   0\n\nsi  so  \u2190 Occasional swapping (normal)\n5   10\n\nsi   so  \u2190 Heavy swapping (performance problem!)\n100  200\n</code></pre> <p>If you see high si/so consistently, you need more RAM!</p>"},{"location":"linux/storage/swap-space/#persistent-swap-configuration","title":"Persistent Swap Configuration","text":""},{"location":"linux/storage/swap-space/#etcfstab-entries","title":"/etc/fstab Entries","text":"<p>Format:</p> <pre><code>device/file  none  swap  options  0  0\n</code></pre> <p>Examples:</p> <pre><code># Swap file\n/swapfile none swap sw 0 0\n\n# Swap partition\n/dev/sdb1 none swap sw 0 0\n\n# With priority\n/dev/sdb1 none swap pri=10 0 0\n/swapfile none swap pri=5 0 0\n\n# Using UUID (more reliable)\nUUID=1234abcd-... none swap sw 0 0\n\n# Using label\nLABEL=swap1 none swap sw 0 0\n</code></pre> <p>Multiple swap spaces:</p> <pre><code># Fast SSD swap (priority 100)\n/dev/nvme0n1p3 none swap pri=100 0 0\n\n# Slower SSD swap (priority 50)\n/dev/sda2 none swap pri=50 0 0\n\n# Emergency HDD swap (priority 10)\n/swapfile none swap pri=10 0 0\n</code></pre> <p>Priority rules:</p> <ul> <li>Higher number = used first</li> <li>Same priority = used in parallel (striped for performance)</li> <li>Default priority = -2</li> </ul>"},{"location":"linux/storage/swap-space/#tuning-swap-behavior","title":"Tuning Swap Behavior","text":""},{"location":"linux/storage/swap-space/#vmswappiness-control-swap-tendency","title":"vm.swappiness - Control Swap Tendency","text":"<p>What it is: Kernel parameter controlling how aggressively Linux swaps.</p> <p>Why tune it: Balance between RAM usage and swap usage.</p> <p>Value range: 0-100</p> <ul> <li>0: Avoid swapping except emergency</li> <li>1: Minimum swapping (recommended for servers)</li> <li>10: Very little swapping (good for servers)</li> <li>60: Default (balanced)</li> <li>100: Swap aggressively</li> </ul> <p>Examples:</p> <pre><code># Check current value\ncat /proc/sys/vm/swappiness\nsysctl vm.swappiness\n\n# Temporary change (until reboot)\nsysctl -w vm.swappiness=10\necho 10 &gt; /proc/sys/vm/swappiness\n\n# Permanent change\necho \"vm.swappiness=10\" &gt;&gt; /etc/sysctl.d/99-swappiness.conf\nsysctl -p /etc/sysctl.d/99-swappiness.conf\n\n# Or edit /etc/sysctl.conf\nvi /etc/sysctl.conf\n# Add: vm.swappiness=10\nsysctl -p\n</code></pre> <p>Recommendations:</p> <pre><code># Desktop/Laptop (more responsive)\nvm.swappiness=60\n\n# Server with plenty of RAM\nvm.swappiness=10\n\n# Server with limited RAM\nvm.swappiness=30\n\n# Database server\nvm.swappiness=1\n</code></pre> <p>Real-world scenario - Web server optimization:</p> <pre><code># Server has 32GB RAM, barely uses swap\n# Lower swappiness to keep more in RAM\n\n# Check current setting\ncat /proc/sys/vm/swappiness\n# 60 (default)\n\n# Set to minimal swapping\necho \"vm.swappiness=10\" &gt;&gt; /etc/sysctl.d/99-swappiness.conf\nsysctl -p /etc/sysctl.d/99-swappiness.conf\n\n# Monitor results\nvmstat 2\n# Watch si/so values (should be mostly 0)\n</code></pre>"},{"location":"linux/storage/swap-space/#swap-on-lvm","title":"Swap on LVM","text":""},{"location":"linux/storage/swap-space/#why-use-lvm-for-swap","title":"Why Use LVM for Swap?","text":"<p>Benefits:</p> <ul> <li>Easy to resize</li> <li>Can be on multiple disks</li> <li>Snapshots possible (though not common for swap)</li> </ul> <p>Example - Create LVM swap:</p> <pre><code># Step 1: Create logical volume\nlvcreate -L 8G -n lv_swap vg01\n\n# Step 2: Format as swap\nmkswap /dev/vg01/lv_swap\n\n# Step 3: Enable\nswapon /dev/vg01/lv_swap\n\n# Step 4: Make permanent\necho '/dev/vg01/lv_swap none swap sw 0 0' &gt;&gt; /etc/fstab\n</code></pre> <p>Resizing LVM swap:</p> <pre><code># Need more swap space\n\n# Step 1: Disable swap\nswapoff /dev/vg01/lv_swap\n\n# Step 2: Extend logical volume\nlvextend -L +4G /dev/vg01/lv_swap\n\n# Step 3: Reformat (required for swap!)\nmkswap /dev/vg01/lv_swap\n\n# Step 4: Re-enable\nswapon /dev/vg01/lv_swap\n\n# Step 5: Verify\nswapon --show\nfree -h\n</code></pre>"},{"location":"linux/storage/swap-space/#monitoring-swap-usage","title":"Monitoring Swap Usage","text":""},{"location":"linux/storage/swap-space/#check-whats-using-swap","title":"Check What\u2019s Using Swap","text":"<p>Find swap usage by process:</p> <pre><code># Quick check\nfor file in /proc/*/status; do\n    awk '/VmSwap|Name/{printf $2 \" \" $3}END{print \"\"}' $file\ndone | sort -k 2 -n -r | head\n\n# Output shows:\n# firefox 512000 (512MB)\n# chrome 256000 (256MB)\n# mysql 128000 (128MB)\n</code></pre> <p>Detailed swap analysis:</p> <pre><code># For each process, show swap usage\nfor pid in $(ls /proc | grep -E '^[0-9]+$'); do\n    if [ -f /proc/$pid/smaps ]; then\n        swap=$(grep Swap /proc/$pid/smaps 2&gt;/dev/null | awk '{sum+=$2} END {print sum}')\n        if [ ! -z \"$swap\" ] &amp;&amp; [ \"$swap\" -gt 0 ] 2&gt;/dev/null; then\n            name=$(cat /proc/$pid/comm 2&gt;/dev/null)\n            echo \"$swap KB - $name (PID: $pid)\"\n        fi\n    fi\ndone | sort -n -r | head -20\n</code></pre>"},{"location":"linux/storage/swap-space/#troubleshooting","title":"Troubleshooting","text":""},{"location":"linux/storage/swap-space/#problem-system-running-out-of-memory","title":"Problem: System Running Out of Memory","text":"<p>Symptoms: System very slow, heavy swapping.</p> <p>Solutions:</p> <pre><code># Check situation\nfree -h\nvmstat 2 5\n\n# If swap is full or nearly full:\n# Option 1: Add more swap (temporary fix)\nfallocate -l 4G /emergency-swap\nchmod 600 /emergency-swap\nmkswap /emergency-swap\nswapon /emergency-swap\n\n# Option 2: Find memory hogs\nps aux --sort=-%mem | head\ntop -o %MEM\n\n# Option 3: Kill memory-hungry processes (carefully!)\npkill firefox\npkill chrome\n\n# Long-term solution: Add more RAM!\n</code></pre>"},{"location":"linux/storage/swap-space/#problem-heavy-swapping-system-slow","title":"Problem: Heavy Swapping (System Slow)","text":"<p>Symptoms: High si/so in vmstat, system sluggish.</p> <p>Solutions:</p> <pre><code># Monitor swapping\nvmstat 2\n\n# If heavy swapping (si/so &gt; 100):\n# Solution 1: Lower swappiness\nsysctl -w vm.swappiness=10\n\n# Solution 2: Find what's swapped out\n# (See \"Check What's Using Swap\" above)\n\n# Solution 3: Clear swap and reload to RAM\n# WARNING: Only if you have enough free RAM!\nswapoff -a\nswapon -a\n</code></pre>"},{"location":"linux/storage/swap-space/#problem-cannot-enable-swap","title":"Problem: Cannot Enable Swap","text":"<p>Symptoms: swapon fails with error.</p> <p>Solutions:</p> <pre><code># Check if formatted as swap\nfile -s /swapfile\n# Should show: swap file\n\n# If not formatted:\nmkswap /swapfile\n\n# Check permissions (swap files must be 600)\nls -l /swapfile\nchmod 600 /swapfile\n\n# Check logs\ndmesg | grep swap\njournalctl | grep swap\n</code></pre>"},{"location":"linux/storage/swap-space/#problem-swap-not-activating-at-boot","title":"Problem: Swap Not Activating at Boot","text":"<p>Symptoms: After reboot, swap isn\u2019t active.</p> <p>Solutions:</p> <pre><code># Check fstab entry\ncat /etc/fstab | grep swap\n\n# Test manual activation\nswapon -a\n\n# Check for errors\nsystemctl status swap.target\nsystemctl list-units | grep swap\n\n# Verify file/partition exists\nls -l /swapfile\n</code></pre>"},{"location":"linux/storage/swap-space/#best-practices","title":"Best Practices","text":"<p>1. Secure swap files:</p> <pre><code># Always set 600 permissions\nchmod 600 /swapfile\n</code></pre> <p>2. Use appropriate size:</p> <pre><code># Server with 16GB RAM\n# 8GB swap is plenty\n</code></pre> <p>3. Tune swappiness:</p> <pre><code># Servers: 10\n# Desktops: 60 (default)\n</code></pre> <p>4. Monitor regularly:</p> <pre><code># Check weekly\nfree -h\nvmstat 2 5\n</code></pre> <p>5. Multiple swap for performance:</p> <pre><code># Equal priority for striping\n/dev/sda2 none swap pri=10 0 0\n/dev/sdb2 none swap pri=10 0 0\n</code></pre>"},{"location":"linux/storage/swap-space/#quick-reference","title":"Quick Reference","text":""},{"location":"linux/storage/swap-space/#creating-swap","title":"Creating Swap","text":"<pre><code># Swap file (recommended)\nfallocate -l 4G /swapfile\nchmod 600 /swapfile\nmkswap /swapfile\nswapon /swapfile\necho '/swapfile none swap sw 0 0' &gt;&gt; /etc/fstab\n</code></pre>"},{"location":"linux/storage/swap-space/#managing-swap","title":"Managing Swap","text":"<pre><code>swapon --show              # Show active swap\nswapon -a                  # Enable all\nswapoff -a                 # Disable all\nfree -h                    # Check usage\nvmstat 2                   # Monitor activity\n</code></pre>"},{"location":"linux/storage/swap-space/#tuning","title":"Tuning","text":"<pre><code># Check swappiness\ncat /proc/sys/vm/swappiness\n\n# Set permanently\necho \"vm.swappiness=10\" &gt;&gt; /etc/sysctl.d/99-swap.conf\nsysctl -p\n</code></pre>"},{"location":"linux/storage/swap-space/#monitoring","title":"Monitoring","text":"<pre><code># Current status\nfree -h\nswapon --show\n\n# Watch for swapping\nvmstat 2 10\n\n# Find what's using swap\ngrep VmSwap /proc/*/status | grep -v \"0 kB\"\n</code></pre>"},{"location":"linux/storage/virtual-file-system/","title":"Manage and Configure the Virtual File System","text":""},{"location":"linux/storage/virtual-file-system/#what-is-the-virtual-file-system","title":"What is the Virtual File System?","text":"<p>The Virtual File System (VFS) is Linux\u2019s way of providing a uniform interface to access different types of filesystems. It\u2019s like a translator that lets applications work with files the same way, whether they\u2019re on ext4, XFS, NFS, or any other filesystem.</p> <p>Think of VFS as the middleman between your applications and the actual storage. When you run <code>cat file.txt</code>, you don\u2019t need to know if that file is on a local disk, a network share, or even in memory - VFS handles all the complexity.</p>"},{"location":"linux/storage/virtual-file-system/#understanding-the-linux-directory-structure","title":"Understanding the Linux Directory Structure","text":""},{"location":"linux/storage/virtual-file-system/#why-this-matters","title":"Why This Matters","text":"<p>Unlike Windows with its C:, D:, E: drives, Linux has ONE unified directory tree. Everything starts from <code>/</code> (root), and all storage devices, network shares, and even system information appear as directories within this tree.</p>"},{"location":"linux/storage/virtual-file-system/#the-root-directory-","title":"The Root Directory - /","text":"<p>Everything in Linux starts here. This is not the home directory of the root user (that\u2019s <code>/root</code>), but the very top of the entire filesystem hierarchy.</p> <pre><code># View the root directory\nls /\n</code></pre> <p>Output: <pre><code>bin  boot  dev  etc  home  lib  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var\n</code></pre></p>"},{"location":"linux/storage/virtual-file-system/#critical-system-directories","title":"Critical System Directories","text":""},{"location":"linux/storage/virtual-file-system/#bin-essential-user-commands","title":"/bin - Essential User Commands","text":"<p>What it is: Contains essential command-line programs that all users need and that must be available even in single-user mode (emergency recovery).</p> <p>What\u2019s inside:</p> <ul> <li>Basic commands like <code>ls</code>, <code>cp</code>, <code>mv</code>, <code>cat</code>, <code>mkdir</code></li> <li>Shell programs like <code>bash</code>, <code>sh</code></li> <li>Basic utilities you need to fix a broken system</li> </ul> <p>Example:</p> <pre><code># See what's in /bin\nls /bin\n\n# Check where a command lives\nwhich ls\n# Output: /bin/ls\n</code></pre> <p>Why it matters: If these files get deleted or corrupted, your system becomes very difficult to use or repair.</p>"},{"location":"linux/storage/virtual-file-system/#boot-boot-files","title":"/boot - Boot Files","text":"<p>What it is: Everything needed to start your Linux system.</p> <p>What\u2019s inside:</p> <ul> <li>The Linux kernel (vmlinuz)</li> <li>Initial RAM disk (initramfs or initrd)</li> <li>Bootloader configuration (GRUB)</li> </ul> <p>Example:</p> <pre><code># View boot files\nls -lh /boot\n\n# Typical contents:\n# vmlinuz-5.15.0-58-generic    (Linux kernel)\n# initrd.img-5.15.0-58-generic (Initial RAM disk)\n# grub/                        (Bootloader)\n</code></pre> <p>Why it matters: Your computer can\u2019t start Linux without these files. Never delete anything here unless you know exactly what you\u2019re doing!</p> <p>Real-world scenario: If you\u2019re dual-booting Windows and Linux and Windows update breaks GRUB, you\u2019ll need to access <code>/boot/grub/</code> to fix the bootloader.</p>"},{"location":"linux/storage/virtual-file-system/#dev-device-files","title":"/dev - Device Files","text":"<p>What it is: Special files that represent hardware devices. In Linux, \u201ceverything is a file,\u201d including hardware.</p> <p>What\u2019s inside:</p> <ul> <li>Disk drives: <code>/dev/sda</code>, <code>/dev/sdb</code>, <code>/dev/nvme0n1</code></li> <li>Partitions: <code>/dev/sda1</code>, <code>/dev/sda2</code></li> <li>Terminals: <code>/dev/tty1</code>, <code>/dev/pts/0</code></li> <li>Null device: <code>/dev/null</code> (the \u201cblack hole\u201d)</li> <li>Random data: <code>/dev/random</code>, <code>/dev/urandom</code></li> </ul> <p>Examples:</p> <pre><code># List all disk devices\nls -l /dev/sd*\n\n# List NVMe devices\nls -l /dev/nvme*\n\n# View information about a disk\nfdisk -l /dev/sda\n\n# The \"black hole\" - discard output\necho \"This disappears\" &gt; /dev/null\n\n# Generate random data\nhead -c 16 /dev/urandom | base64\n</code></pre> <p>Real-world scenario: When you plug in a USB drive, it appears as <code>/dev/sdb</code> or similar. You can then mount it to access its contents.</p>"},{"location":"linux/storage/virtual-file-system/#etc-configuration-files","title":"/etc - Configuration Files","text":"<p>What it is: System-wide configuration files. Nearly every program stores its settings here.</p> <p>What\u2019s inside:</p> <ul> <li><code>/etc/fstab</code> - Filesystem mount configuration</li> <li><code>/etc/passwd</code> - User account information</li> <li><code>/etc/group</code> - Group information</li> <li><code>/etc/hosts</code> - Static hostname to IP mappings</li> <li><code>/etc/hostname</code> - System hostname</li> <li><code>/etc/ssh/sshd_config</code> - SSH server configuration</li> <li><code>/etc/network/</code> - Network configuration</li> </ul> <p>Examples:</p> <pre><code># View mount configuration\ncat /etc/fstab\n\n# Check system hostname\ncat /etc/hostname\n\n# View user accounts\ncat /etc/passwd\n\n# List all configuration files\nls /etc\n</code></pre> <p>Why it matters: This is where you configure most system behavior. Always backup files before editing them!</p> <p>Real-world scenario: You want a USB drive to mount automatically on boot. You add an entry to <code>/etc/fstab</code>: <pre><code>UUID=1234-5678 /mnt/usb vfat defaults 0 0\n</code></pre></p>"},{"location":"linux/storage/virtual-file-system/#home-user-home-directories","title":"/home - User Home Directories","text":"<p>What it is: Personal directories for regular users. Each user gets their own space.</p> <p>What\u2019s inside:</p> <ul> <li><code>/home/john/</code> - John\u2019s personal files</li> <li><code>/home/jane/</code> - Jane\u2019s personal files</li> <li>Each user can only write to their own directory (by default)</li> </ul> <p>Examples:</p> <pre><code># Go to your home directory\ncd ~\n# or just\ncd\n\n# See whose home directories exist\nls /home\n\n# Check your current user\nwhoami\n\n# Check your home directory\necho $HOME\n</code></pre> <p>Why separate /home? Many people put <code>/home</code> on a separate partition or disk. This way, you can reinstall the operating system without losing your personal files.</p>"},{"location":"linux/storage/virtual-file-system/#root-root-users-home","title":"/root - Root User\u2019s Home","text":"<p>What it is: The home directory for the root (administrator) user.</p> <p>Why separate? The root user is special. Their home directory is in <code>/root</code> (not <code>/home/root</code>) to ensure it\u2019s available even if <code>/home</code> fails to mount.</p> <p>Example:</p> <pre><code># Switch to root\nsudo su -\n\n# Check location\npwd\n# Output: /root\n\n# Go back to regular user\nexit\n</code></pre>"},{"location":"linux/storage/virtual-file-system/#tmp-temporary-files","title":"/tmp - Temporary Files","text":"<p>What it is: A place for programs to store temporary data. Gets cleaned out regularly (usually on reboot).</p> <p>What\u2019s inside:</p> <ul> <li>Temporary files created by applications</li> <li>Lock files</li> <li>Session data</li> </ul> <p>Examples:</p> <pre><code># Create a temp file\necho \"test\" &gt; /tmp/mytest.txt\n\n# List temp files\nls /tmp\n\n# Many systems clean /tmp on reboot\n# So don't store anything important here!\n</code></pre> <p>Why it matters: Perfect for testing or temporary work. After reboot, it\u2019s clean.</p> <p>Real-world scenario: You\u2019re testing a script that creates files. Use <code>/tmp</code> so you don\u2019t clutter your home directory, and it auto-cleans: <pre><code>./test-script.sh &gt; /tmp/output.log\n# After testing, reboot cleans it up automatically\n</code></pre></p>"},{"location":"linux/storage/virtual-file-system/#var-variable-data","title":"/var - Variable Data","text":"<p>What it is: Data that changes frequently during system operation.</p> <p>What\u2019s inside:</p> <ul> <li><code>/var/log/</code> - Log files</li> <li><code>/var/spool/</code> - Print and mail queues</li> <li><code>/var/www/</code> - Web server files (common location)</li> <li><code>/var/lib/</code> - State information for applications</li> <li><code>/var/cache/</code> - Cached data</li> </ul> <p>Examples:</p> <pre><code># View system logs\nls /var/log\n\n# Check recent log entries\ntail /var/log/syslog\ntail /var/log/messages\n\n# Web server files (if Apache installed)\nls /var/www/html\n\n# Check disk usage (can grow large!)\ndu -sh /var/log\n</code></pre> <p>Why it matters: <code>/var/log</code> can fill up your disk! Regular monitoring is essential.</p> <p>Real-world scenario: Your root partition is full. Investigation shows: <pre><code>df -h /\n# Filesystem      Size  Used Avail Use% Mounted on\n# /dev/sda1        20G   19G     0 100% /\n\ndu -sh /var/log\n# 15G    /var/log\n\n# Old logs are filling the disk!\n# Solution: Clean old logs\nfind /var/log -name \"*.log\" -mtime +30 -delete\n</code></pre></p>"},{"location":"linux/storage/virtual-file-system/#usr-user-programs-and-data","title":"/usr - User Programs and Data","text":"<p>What it is: Unix System Resources. Contains most user applications and utilities.</p> <p>What\u2019s inside:</p> <ul> <li><code>/usr/bin/</code> - Most command-line programs</li> <li><code>/usr/sbin/</code> - System administration programs</li> <li><code>/usr/lib/</code> - Libraries for programs</li> <li><code>/usr/local/</code> - Locally compiled/installed software</li> <li><code>/usr/share/</code> - Shared data (documentation, icons, etc.)</li> </ul> <p>Examples:</p> <pre><code># Most commands you use are here\nls /usr/bin\n\n# Where is Python?\nwhich python3\n# Output: /usr/bin/python3\n\n# Where is Apache?\nwhich apache2\n# Output: /usr/sbin/apache2\n\n# Locally installed software\nls /usr/local/bin\n</code></pre> <p>Why /usr/local? System package managers install to <code>/usr</code>, but software you compile yourself goes to <code>/usr/local</code>. This keeps them separate and organized.</p>"},{"location":"linux/storage/virtual-file-system/#special-virtual-filesystems","title":"Special Virtual Filesystems","text":""},{"location":"linux/storage/virtual-file-system/#proc-process-and-system-information","title":"/proc - Process and System Information","text":"<p>What it is: Not a real filesystem on disk! It\u2019s a window into the kernel\u2019s view of the system. Everything here is generated in real-time.</p> <p>What\u2019s inside:</p> <ul> <li>System information (CPU, memory, etc.)</li> <li>Process information (one directory per running process)</li> <li>Kernel parameters you can read and change</li> </ul> <p>Examples:</p> <pre><code># CPU information\ncat /proc/cpuinfo\n\n# Memory information\ncat /proc/meminfo\n\n# System load\ncat /proc/loadavg\n\n# Currently mounted filesystems\ncat /proc/mounts\n\n# Network connections\ncat /proc/net/tcp\n\n# Information about process 1234\nls /proc/1234/\ncat /proc/1234/status\ncat /proc/1234/cmdline\n\n# All running processes have a directory\nls /proc/ | grep -E '^[0-9]+$'\n</code></pre> <p>Real-world scenario - Checking memory: <pre><code># How much RAM do I have?\ngrep MemTotal /proc/meminfo\n# MemTotal:       16384000 kB\n\n# How much is free?\ngrep MemAvailable /proc/meminfo\n# MemAvailable:   8192000 kB\n</code></pre></p> <p>Real-world scenario - Finding what\u2019s listening on a port: <pre><code># What's listening on port 80?\ncat /proc/net/tcp | grep :0050\n# Then look up the process by its inode\n</code></pre></p>"},{"location":"linux/storage/virtual-file-system/#sys-device-and-kernel-information","title":"/sys - Device and Kernel Information","text":"<p>What it is: Another virtual filesystem. Provides information about devices and allows you to configure hardware.</p> <p>What\u2019s inside:</p> <ul> <li>Device information</li> <li>Hardware configuration</li> <li>Kernel module parameters</li> </ul> <p>Examples:</p> <pre><code># Information about your disks\nls /sys/block/\n\n# Check if disk is rotational (HDD=1, SSD=0)\ncat /sys/block/sda/queue/rotational\n\n# Network interface information\nls /sys/class/net/\n\n# Check if network interface is up\ncat /sys/class/net/eth0/operstate\n\n# MAC address\ncat /sys/class/net/eth0/address\n</code></pre> <p>Real-world scenario - Check if drive is SSD: <pre><code># Check all drives\nfor drive in /sys/block/sd*; do\n    echo -n \"$(basename $drive): \"\n    cat $drive/queue/rotational\ndone\n\n# Output:\n# sda: 1  (HDD)\n# sdb: 0  (SSD)\n</code></pre></p>"},{"location":"linux/storage/virtual-file-system/#working-with-files-and-directories","title":"Working with Files and Directories","text":""},{"location":"linux/storage/virtual-file-system/#ls-list-files","title":"ls - List Files","text":"<p>What it does: Shows you what files and directories exist in a location.</p> <p>Why use it: It\u2019s usually your first command when exploring a directory - \u201cwhat\u2019s here?\u201d</p> <p>Examples:</p> <pre><code># Basic list\nls\n\n# Detailed list with permissions, owner, size, date\nls -l\n\n# Show hidden files (names starting with .)\nls -a\n\n# Human-readable sizes\nls -lh\n\n# Sort by modification time, newest first\nls -lt\n\n# Sort by size, largest first\nls -lS\n\n# Show everything, sorted by time, human-readable\nls -lath\n\n# List a specific directory without entering it\nls -l /var/log\n</code></pre> <p>Understanding ls -l output: <pre><code>ls -l myfile.txt\n-rw-r--r-- 1 john users 1024 Oct 28 10:30 myfile.txt\n\u2502\u2502\u2502\u2502\u2502\u2502\u2502\u2502\u2502  \u2502 \u2502    \u2502     \u2502    \u2502           \u2514\u2500 filename\n\u2502\u2502\u2502\u2502\u2502\u2502\u2502\u2502\u2502  \u2502 \u2502    \u2502     \u2502    \u2514\u2500 date modified\n\u2502\u2502\u2502\u2502\u2502\u2502\u2502\u2502\u2502  \u2502 \u2502    \u2502     \u2514\u2500 size (bytes)\n\u2502\u2502\u2502\u2502\u2502\u2502\u2502\u2502\u2502  \u2502 \u2502    \u2514\u2500 group owner\n\u2502\u2502\u2502\u2502\u2502\u2502\u2502\u2502\u2502  \u2502 \u2502\u2500 user owner\n\u2502\u2502\u2502\u2502\u2502\u2502\u2502\u2502\u2502  \u2514\u2500 number of hard links\n\u2502\u2514\u2534\u2534\u2534\u2534\u2534\u2534\u2534\u2500 permissions\n\u2514\u2500 file type (- = regular file, d = directory, l = link)\n</code></pre></p> <p>File type indicators:</p> <ul> <li><code>-</code> Regular file</li> <li><code>d</code> Directory</li> <li><code>l</code> Symbolic link</li> <li><code>b</code> Block device (like hard drives)</li> <li><code>c</code> Character device (like terminals)</li> </ul>"},{"location":"linux/storage/virtual-file-system/#cd-change-directory","title":"cd - Change Directory","text":"<p>What it does: Moves you to a different directory.</p> <p>Why use it: Navigation! You need to be in the right place to work on files.</p> <p>Examples:</p> <pre><code># Go to /var/log\ncd /var/log\n\n# Go to your home directory\ncd ~\n# or just\ncd\n\n# Go back to previous directory\ncd -\n\n# Go up one level\ncd ..\n\n# Go up two levels\ncd ../..\n\n# Go to a subdirectory of current location\ncd ./subdir\n\n# Relative to home\ncd ~/Documents\n</code></pre> <p>Real-world scenario: <pre><code># Working in /var/www/html\ncd /var/www/html\n\n# Need to edit config in /etc\ncd /etc/nginx\n\n# Want to go back\ncd -\n# Now back in /var/www/html\n</code></pre></p>"},{"location":"linux/storage/virtual-file-system/#pwd-print-working-directory","title":"pwd - Print Working Directory","text":"<p>What it does: Shows you where you currently are in the filesystem.</p> <p>Why use it: Ever get lost? <code>pwd</code> tells you exactly where you are.</p> <p>Examples:</p> <pre><code># Where am I?\npwd\n# Output: /home/john/Documents\n\n# After changing directories\ncd /var/log\npwd\n# Output: /var/log\n</code></pre>"},{"location":"linux/storage/virtual-file-system/#mkdir-make-directory","title":"mkdir - Make Directory","text":"<p>What it does: Creates new directories.</p> <p>Why use it: You need a place to organize your files!</p> <p>Examples:</p> <pre><code># Create a single directory\nmkdir projects\n\n# Create nested directories (including parents)\nmkdir -p work/2024/reports\n\n# Create multiple directories\nmkdir dir1 dir2 dir3\n\n# Create with specific permissions\nmkdir -m 755 public_html\n\n# Create project structure\nmkdir -p myproject/{src,bin,docs,tests}\n</code></pre> <p>Real-world scenario - Project setup: <pre><code># Set up a new web project\nmkdir -p ~/projects/mywebsite/{html,css,js,images}\n\n# Result:\n# mywebsite/\n# \u251c\u2500\u2500 html/\n# \u251c\u2500\u2500 css/\n# \u251c\u2500\u2500 js/\n# \u2514\u2500\u2500 images/\n</code></pre></p>"},{"location":"linux/storage/virtual-file-system/#rm-remove-files","title":"rm - Remove Files","text":"<p>What it does: Deletes files and directories.</p> <p>Why use it: Clean up unwanted files, free up space.</p> <p>\u26a0\ufe0f WARNING: There\u2019s no \u201cRecycle Bin\u201d in Linux command line. Deleted = GONE!</p> <p>Examples:</p> <pre><code># Delete a file\nrm oldfile.txt\n\n# Delete multiple files\nrm file1.txt file2.txt file3.txt\n\n# Delete a directory and its contents\nrm -r oldfolder\n\n# Force delete without asking\nrm -f stubborn.txt\n\n# Delete directory and everything in it (dangerous!)\nrm -rf directory\n\n# Interactive mode - asks before each deletion\nrm -i *.txt\n\n# Delete all .log files older than 30 days\nfind /var/log -name \"*.log\" -mtime +30 -exec rm {} \\;\n</code></pre> <p>Real-world scenario - Clean up old logs: <pre><code># Find large log files\nfind /var/log -type f -size +100M\n\n# Delete them (carefully!)\nfind /var/log -type f -size +100M -exec rm {} \\;\n</code></pre></p> <p>Safety tip: Use <code>rm -i</code> (interactive) when deleting important files: <pre><code>rm -i important*\n# rm: remove regular file 'important-data.txt'? n (you type 'n' to keep it)\n</code></pre></p>"},{"location":"linux/storage/virtual-file-system/#cp-copy-files","title":"cp - Copy Files","text":"<p>What it does: Makes a copy of files or directories.</p> <p>Why use it: Backups, duplicating files, copying to different locations.</p> <p>Examples:</p> <pre><code># Copy a file\ncp original.txt copy.txt\n\n# Copy to a different directory\ncp file.txt /backup/\n\n# Copy directory and all contents\ncp -r folder/ /backup/\n\n# Copy and preserve permissions, ownership, timestamps\ncp -a /etc/nginx /backup/nginx-backup\n\n# Copy multiple files to a directory\ncp file1.txt file2.txt file3.txt /destination/\n\n# Interactive - ask before overwriting\ncp -i file.txt existing-file.txt\n\n# Update - only copy if source is newer\ncp -u *.txt /backup/\n</code></pre> <p>Real-world scenario - Backup before editing: <pre><code># Before editing important config\ncp /etc/nginx/nginx.conf /etc/nginx/nginx.conf.backup\n\n# Make changes\nvi /etc/nginx/nginx.conf\n\n# If something breaks, restore\ncp /etc/nginx/nginx.conf.backup /etc/nginx/nginx.conf\n</code></pre></p>"},{"location":"linux/storage/virtual-file-system/#mv-move-or-rename-files","title":"mv - Move or Rename Files","text":"<p>What it does: Moves files to new locations OR renames them (it\u2019s the same operation).</p> <p>Why use it: Organize files, rename things.</p> <p>Examples:</p> <pre><code># Rename a file\nmv oldname.txt newname.txt\n\n# Move to different directory\nmv file.txt /backup/\n\n# Move multiple files\nmv file1.txt file2.txt file3.txt /destination/\n\n# Rename a directory\nmv old-folder new-folder\n\n# Move and rename at the same time\nmv /tmp/file.txt /home/john/newfile.txt\n\n# Interactive mode\nmv -i file.txt existing.txt\n</code></pre> <p>Real-world scenario: <pre><code># Organizing downloaded files\nmv ~/Downloads/*.pdf ~/Documents/PDFs/\nmv ~/Downloads/*.jpg ~/Pictures/\n</code></pre></p>"},{"location":"linux/storage/virtual-file-system/#find-search-for-files","title":"find - Search for Files","text":"<p>What it does: Searches for files based on various criteria (name, size, date, etc.).</p> <p>Why use it: \u201cI know I saved that file somewhere\u2026\u201d - <code>find</code> will locate it!</p> <p>Examples:</p> <pre><code># Find files by name\nfind /home -name \"report.pdf\"\n\n# Case-insensitive search\nfind /home -iname \"*.PDF\"\n\n# Find all directories\nfind /var -type d\n\n# Find all files\nfind /var -type f\n\n# Find files larger than 100MB\nfind /home -type f -size +100M\n\n# Find files modified in last 7 days\nfind /var/log -type f -mtime -7\n\n# Find files NOT modified in last 30 days\nfind /tmp -type f -mtime +30\n\n# Find and execute command on results\nfind /tmp -type f -mtime +30 -exec rm {} \\;\n\n# Find files by owner\nfind /home -user john\n\n# Find files with specific permissions\nfind /var/www -type f -perm 0777\n</code></pre> <p>Real-world scenario - Find large files: <pre><code># Disk is full, find what's using space\nfind / -type f -size +1G 2&gt;/dev/null\n\n# Output:\n# /var/log/huge.log (5GB)\n# /home/john/video.mp4 (2GB)\n</code></pre></p> <p>Real-world scenario - Find recently changed configs: <pre><code># What configs changed in last hour?\nfind /etc -type f -mmin -60\n\n# Useful after installing software to see what changed\n</code></pre></p>"},{"location":"linux/storage/virtual-file-system/#file-permissions","title":"File Permissions","text":""},{"location":"linux/storage/virtual-file-system/#understanding-linux-permissions","title":"Understanding Linux Permissions","text":"<p>Every file has three levels of permissions:</p> <ul> <li>User (u): The owner of the file</li> <li>Group (g): The group that owns the file  </li> <li>Others (o): Everyone else</li> </ul> <p>Each level can have three types of permissions:</p> <ul> <li>Read \u00ae: Can view the file/list directory</li> <li>Write (w): Can modify the file/add files to directory</li> <li>Execute (x): Can run the file/enter the directory</li> </ul> <p>Example: <pre><code>ls -l script.sh\n-rwxr-xr-- 1 john developers 2048 Oct 28 10:30 script.sh\n \u2502\u2502\u2502\u2502\u2502\u2502\u2502\u2502\u2502\n \u2502\u2502\u2502\u2502\u2502\u2502\u2502\u2514\u2534\u2500 others: r-- (read only)\n \u2502\u2502\u2502\u2514\u2534\u2534\u2500\u2500\u2500 group: r-x (read and execute)\n \u2514\u2534\u2534\u2500\u2500\u2500\u2500\u2500 user: rwx (read, write, execute)\n</code></pre></p>"},{"location":"linux/storage/virtual-file-system/#chmod-change-permissions","title":"chmod - Change Permissions","text":"<p>What it does: Changes who can read, write, or execute a file.</p> <p>Why use it: Control access to your files, make scripts executable.</p> <p>Examples using symbolic mode:</p> <pre><code># Make file executable for everyone\nchmod +x script.sh\n\n# Make file executable only for owner\nchmod u+x script.sh\n\n# Remove write permission for group and others\nchmod go-w file.txt\n\n# Add read permission for everyone\nchmod a+r document.pdf\n\n# Set exact permissions: owner=rwx, group=rx, others=r\nchmod u=rwx,g=rx,o=r script.sh\n</code></pre> <p>Examples using numeric mode:</p> <p>Each permission has a number: - r (read) = 4 - w (write) = 2 - x (execute) = 1</p> <p>Add them up for each group:</p> <pre><code># 755 = rwxr-xr-x (common for executables)\nchmod 755 script.sh\n\n# 644 = rw-r--r-- (common for regular files)\nchmod 644 document.txt\n\n# 700 = rwx------ (only owner can do anything)\nchmod 700 private-script.sh\n\n# 600 = rw------- (only owner can read/write)\nchmod 600 private-data.txt\n\n# 777 = rwxrwxrwx (everyone can do everything - usually bad!)\nchmod 777 shared.sh\n</code></pre> <p>Real-world scenario - Make script executable: <pre><code># Download a script\nwget https://example.com/install.sh\n\n# Try to run it\n./install.sh\n# Error: Permission denied\n\n# Make it executable\nchmod +x install.sh\n\n# Now it works\n./install.sh\n</code></pre></p> <p>Real-world scenario - Secure private key: <pre><code># SSH won't work if private key is too open\nchmod 600 ~/.ssh/id_rsa\n\n# SSH directory itself\nchmod 700 ~/.ssh\n</code></pre></p>"},{"location":"linux/storage/virtual-file-system/#chown-change-owner","title":"chown - Change Owner","text":"<p>What it does: Changes who owns a file or directory.</p> <p>Why use it: Fix ownership issues, give files to different users.</p> <p>Examples:</p> <pre><code># Change owner to john\nchown john file.txt\n\n# Change owner and group\nchown john:developers file.txt\n\n# Change only group (or use chgrp)\nchown :developers file.txt\n\n# Change ownership recursively\nchown -R www-data:www-data /var/www/html\n\n# Change using UID instead of username\nchown 1000:1000 file.txt\n</code></pre> <p>Real-world scenario - Web server files: <pre><code># Apache needs to own web files\nchown -R www-data:www-data /var/www/mysite\n\n# But you want to edit them too\n# Add yourself to www-data group\nusermod -a -G www-data john\n\n# Make files group-writable\nchmod -R g+w /var/www/mysite\n</code></pre></p>"},{"location":"linux/storage/virtual-file-system/#links","title":"Links","text":""},{"location":"linux/storage/virtual-file-system/#what-are-links","title":"What Are Links?","text":"<p>Links are like shortcuts or pointers to files. Linux has two types:</p> <p>Symbolic Links (Soft Links):</p> <ul> <li>Like shortcuts in Windows</li> <li>Points to a file by its path</li> <li>If original is deleted, link breaks</li> <li>Can link to directories</li> <li>Can cross filesystem boundaries</li> </ul> <p>Hard Links:</p> <ul> <li>Direct reference to the file data on disk</li> <li>Multiple names for the same data</li> <li>If original is deleted, data still exists</li> <li>Cannot link to directories (usually)</li> <li>Must be on same filesystem</li> </ul>"},{"location":"linux/storage/virtual-file-system/#ln-create-links","title":"ln - Create Links","text":"<p>What it does: Creates symbolic or hard links to files.</p> <p>Why use it: Create shortcuts, maintain backward compatibility, organize files.</p> <p>Examples:</p> <pre><code># Create symbolic link\nln -s /path/to/original /path/to/link\n\n# Create hard link\nln /path/to/original /path/to/hardlink\n\n# Symbolic link to directory\nln -s /usr/share/docs ~/docs\n\n# Force overwrite existing link\nln -sf /new/target /existing/link\n\n# Create link in current directory\nln -s /var/log/syslog syslog\n</code></pre> <p>Real-world scenario - Python versions: <pre><code># You have Python 3.9 but some programs expect 'python3'\nln -s /usr/bin/python3.9 /usr/bin/python3\n\n# Now both work\npython3.9 --version\npython3 --version\n</code></pre></p> <p>Real-world scenario - Configuration management: <pre><code># Keep configs in version control\nmkdir ~/configs\nln -s ~/configs/vimrc ~/.vimrc\nln -s ~/configs/bashrc ~/.bashrc\n\n# Now you can edit ~/configs and commit changes\n</code></pre></p> <p>Checking links: <pre><code># List shows links with -&gt;\nls -l /usr/bin/python3\n# lrwxrwxrwx 1 root root 9 Mar 13  2023 /usr/bin/python3 -&gt; python3.9\n</code></pre></p>"},{"location":"linux/storage/virtual-file-system/#checking-disk-usage","title":"Checking Disk Usage","text":""},{"location":"linux/storage/virtual-file-system/#df-disk-free","title":"df - Disk Free","text":"<p>What it does: Shows how much disk space is used and available on each filesystem.</p> <p>Why use it: \u201cIs my disk full?\u201d - Quick check of space on all mounted filesystems.</p> <p>Examples:</p> <pre><code># Human-readable sizes\ndf -h\n\n# Show filesystem type too\ndf -hT\n\n# Check specific filesystem\ndf -h /home\n\n# Show inode usage (number of files)\ndf -hi\n\n# All filesystems including pseudo ones\ndf -ha\n</code></pre> <p>Output example: <pre><code>df -h\nFilesystem      Size  Used Avail Use% Mounted on\n/dev/sda1        50G   30G   18G  63% /\n/dev/sdb1       200G  150G   40G  79% /data\ntmpfs           8.0G  1.0M  8.0G   1% /run\n</code></pre></p> <p>Real-world scenario - Disk full warning: <pre><code>df -h /\n# Filesystem      Size  Used Avail Use% Mounted on\n# /dev/sda1        20G   19G  100M  99% /\n\n# Critical! Find what's using space:\ndu -sh /* | sort -h\n</code></pre></p>"},{"location":"linux/storage/virtual-file-system/#du-disk-usage","title":"du - Disk Usage","text":"<p>What it does: Shows how much disk space files and directories are using.</p> <p>Why use it: \u201cWhat\u2019s taking up all my space?\u201d - Find large files and directories.</p> <p>Examples:</p> <pre><code># Size of current directory\ndu -sh .\n\n# Size of each item in current directory\ndu -sh *\n\n# Top level only, human readable\ndu -h --max-depth=1 /var\n\n# Sort by size, largest last\ndu -sh /home/* | sort -h\n\n# Sort by size, largest first\ndu -sh /home/* | sort -rh\n\n# Find directories larger than 1GB\ndu -h /var | grep '^[0-9.]*G'\n\n# Exclude certain patterns\ndu -sh --exclude='*.log' /var\n</code></pre> <p>Real-world scenario - Find space hogs: <pre><code># Check /var which is using lots of space\ndu -sh /var/*\n# Output:\n# 50M    /var/cache\n# 15G    /var/log\n# 100M   /var/lib\n# ...\n\n# /var/log is huge! What inside?\ndu -sh /var/log/* | sort -rh | head -5\n# Output:\n# 10G    /var/log/old-logs\n# 3G     /var/log/syslog\n# 2G     /var/log/auth.log\n</code></pre></p>"},{"location":"linux/storage/virtual-file-system/#quick-reference","title":"Quick Reference","text":""},{"location":"linux/storage/virtual-file-system/#navigation","title":"Navigation","text":"<pre><code>pwd                    # Where am I?\ncd /path              # Go to path\ncd                    # Go home\ncd -                  # Go back\nls -lah               # List files\n</code></pre>"},{"location":"linux/storage/virtual-file-system/#file-operations","title":"File Operations","text":"<pre><code>mkdir dirname         # Create directory\nrm file               # Delete file\nrm -r dir             # Delete directory\ncp source dest        # Copy\nmv source dest        # Move/rename\nfind / -name file     # Search for file\n</code></pre>"},{"location":"linux/storage/virtual-file-system/#permissions","title":"Permissions","text":"<pre><code>chmod 755 file        # Change permissions\nchown user:group file # Change owner\nls -l                 # View permissions\n</code></pre>"},{"location":"linux/storage/virtual-file-system/#disk-usage","title":"Disk Usage","text":"<pre><code>df -h                 # Show disk space\ndu -sh directory      # Directory size\ndu -sh * | sort -h    # Sort by size\n</code></pre>"},{"location":"linux/storage/virtual-file-system/#system-information","title":"System Information","text":"<pre><code>cat /proc/cpuinfo     # CPU info\ncat /proc/meminfo     # Memory info\ncat /etc/os-release   # OS version\n</code></pre>"}]}